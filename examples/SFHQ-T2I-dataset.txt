================================================================================
repo title: SFHQ-T2I-dataset
repo link: https://github.com/SelfishGene/SFHQ-T2I-dataset
date processed: 2024-10-21
================================================================================
================================================================================
repo file structure:
====================
    merge_dataset_folder.py
    extract_pretrained_features.py
    README.md
    create_face_dataset.py
    explore_dataset.py
    LICENSE.txt
    face_prompt_utils.py
    figures/
        textual_search_2_Age_x_Ethnicity x Sex 1_top_10_matches.jpg
        textual_search_2_Hair Style_top_9_matches.jpg
        textual_search_2_Expression_x_Sex_top_10_matches.jpg
        textual_search_2_Hair Style x Sex_top_10_matches.jpg
        FLUX1_schnell_SDXL_images.jpg
        textual_search_2_Glasses Style_top_10_matches.jpg
        flux_images.jpg
        textual_search_2_bad things_top_10_matches.jpg
        textual_search_2_Accessories_top_10_matches.jpg
        textual_search_1_eye_color_top_8_matches.jpg
        textual_search_2_Hair_Color_top_10_matches.jpg
        textual_search_1_accessories_top_8_matches.jpg
        textual_search_2_Facial Hair_top_10_matches.jpg
        textual_search_1_glasses_top_8_matches.jpg
        textual_search_2_Age_x_Ethnicity x Sex 2_top_10_matches.jpg
        prompt_lengths_distribution.jpg
        textual_search_2_Hats_top_10_matches.jpg
        good_model_images.jpg
        textual_search_1_age_male_top_8_matches.jpg
        all_model_images.jpg
        FLUX1_dev_images_with_prompts.jpg
        textual_search_1_expression_top_8_matches.jpg
        textual_search_2_Ethnicity x Age 3_top_10_matches.jpg
        SDXL_images_with_prompts.jpg
        textual_search_1_ethnicity_top_8_matches.jpg
        textual_search_2_Lighting_top_10_matches.jpg
        textual_search_2_Physical Characteristics_top_9_matches.jpg
        textual_search_2_Age_top_10_matches.jpg
        textual_search_1_sex_top_8_matches.jpg
        textual_search_2_Eye Color_top_10_matches.jpg
        textual_search_2_Ethnicity_top_10_matches.jpg
        FLUX1_schnell_images_with_prompts.jpg
        model_distribution.jpg
        textual_search_2_Face Pose_top_10_matches.jpg
        FLUX1_pro_images_with_prompts.jpg
        textual_search_2_Background Color_top_9_matches.jpg
        textual_search_2_Background_top_10_matches.jpg
        textual_search_2_Eye Gaze_top_9_matches.jpg
        textual_search_1_age_female_top_8_matches.jpg
        textual_search_1_hair_color_top_8_matches.jpg
        textual_search_1_hats_top_8_matches.jpg
        DALLE3_images_with_prompts.jpg
        textual_search_2_Makeup_top_9_matches.jpg
        textual_search_2_Ethnicity x Age 1_top_10_matches.jpg
        textual_search_2_Facial Features_top_9_matches.jpg
        textual_search_2_Jewelry_top_9_matches.jpg
        textual_search_2_Expression_top_10_matches.jpg
        textual_search_2_Ethnicity x Age 2_top_10_matches.jpg
================================================================================
================================================================================
README.md:
==========
# Synthetic Faces High Quality - Text2Image (SFHQ-T2I)

![SFHQ-T2I dataset Flux1.pro samples](https://raw.githubusercontent.com/SelfishGene/SFHQ-T2I-dataset/main/figures/FLUX1_pro_images_with_prompts.jpg)

This dataset consists of 122,726 high-quality 1024x1024 curated face images. It was created by generating random prompt strings that were sent to multiple "text to image" models (Flux1.pro, Flux1.dev, Flux1.schnell, SDXL, DALL-E 3) and curating the results using a semi-manual process.

The prompts describe various faces with different attributes and conditions to ensure extreme variance and diversity in ethnicities, poses, accessories, jewelry, hairstyles and hair colors, expressions, backgrounds, lighting, and more. Due to our ability to control each of these attributes independently via the text prompt, this dataset exhibits a previously unprecedented degree of variance and diversity among publicly available face datasets along most facial attributes. Additionally, it is free of privacy concerns and licensing issues because all images are synthetically generated.

The SFHQ-T2I dataset features high-quality images, surpassing those of the [SFHQ dataset](https://github.com/SelfishGene/SFHQ-dataset), with most being photorealistic and of high resolution. The dataset is paired with the prompts used to generate each image, allowing for a wide range of applications in text-to-image synthesis, face analysis, and other machine learning tasks.


![SFHQ-T2I dataset all model images](https://raw.githubusercontent.com/SelfishGene/SFHQ-T2I-dataset/main/figures/all_model_images.jpg)

## Download

The dataset can be downloaded from Kaggle via the link: [SFHQ-T2I dataset on kaggle](https://www.kaggle.com/datasets/selfishgene/sfhq-t2i-synthetic-faces-from-text-2-image-models)

## Repository Contents

1. `create_face_dataset.py`: Script to generate the dataset using various text-to-image APIs (fal, openai, stability)
2. `explore_dataset.py`: Script for basic exploratory data analysis of the dataset
3. `extract_pretrained_features.py`: Utility to extract features from pretrained OpenCLIP models for the dataset images
4. `face_prompt_utils.py`: Utilities for automatically generating diverse face prompts
5. `merge_dataset_folder.py`: Script to merge multiple dataset folders
6. `figures/`: Folder containing various visualizations of the dataset

## Dataset Details

- 122,726 high quality 1024x1024 face images
- Paired (text, image) dataset
- Generated using multiple text-to-image models via randomly generated prompts:
  - Flux1.pro (3,209 images)
  - Flux1.dev (7,273 images)
  - Flux1.schnell (58,034 images)
  - SDXL (53,087 images)
  - DALL-E 3 (1,123 images)
- Unprecedented variability in various face attributes such as accessories, ethnicity and age, expressions, and more
- CSV file (`SFHQ_T2I_dataset.csv`) containing details about each image:
  - Prompt used to generate the image
  - Model used
  - Random seed, number of steps and other configuration details specific to each model

![SFHQ-T2I dataset model distribution](https://raw.githubusercontent.com/SelfishGene/SFHQ-T2I-dataset/main/figures/model_distribution.jpg)

## Example illustrations of dataset diversity 
The following figures were created by performing textual searches on the dataset using CLIP features

- accessories:  
![SFHQ-T2I dataset accessories diversity](https://raw.githubusercontent.com/SelfishGene/SFHQ-T2I-dataset/main/figures/textual_search_2_Accessories_top_10_matches.jpg)

- hair color:  
![SFHQ-T2I dataset hair color diversity](https://raw.githubusercontent.com/SelfishGene/SFHQ-T2I-dataset/main/figures/textual_search_2_Hair_Color_top_10_matches.jpg)

- lighting:  
![SFHQ-T2I dataset lighting diversity](https://raw.githubusercontent.com/SelfishGene/SFHQ-T2I-dataset/main/figures/textual_search_2_Lighting_top_10_matches.jpg)

- expression:  
![SFHQ-T2I dataset expression diversity](https://raw.githubusercontent.com/SelfishGene/SFHQ-T2I-dataset/main/figures/textual_search_2_Expression_x_Sex_top_10_matches.jpg)

- age:  
![SFHQ-T2I dataset age diversity](https://raw.githubusercontent.com/SelfishGene/SFHQ-T2I-dataset/main/figures/textual_search_2_Age_top_10_matches.jpg)

Additional examples of various textual searchers can be found in the `figures/` folder.

## Notes on the text-to-image models used

The dalle images are very bad, this is possibly done on purpose from openai point of view to avoid scandals related to face images. Also, the openai api is changing the prompt you request to its own prompts without the ability to cancel this edit, so we lack even basic control. Therefore, I strongly suggest not to use it. I initially planned to generate much more images from the Dalle-3 model due to its prompt adherence, but there appears to be a specific issue with face images. It is also more expensive than the other options like the Flux models and the results are not as good.

Both Flux1.pro and Flux1.dev are very very good. SDXL and schnell are also good and way cheaper, but they are not as good as the pro and dev models. SDXL has its own unique style, especially in its textures, but its global image structure is sometimes flawed. Schnell is similar to the pro and dev models at the global structure of the image, but it is not as sharp and the textures are not always photorealistic. A small fraction of the generated images appear like 3D models and paintings. I've kept them in the dataset despite not being the main focus of the dataset, because the main goal is increasing entropy.

## Usage

An example script demonstrating how to access and explore the data can be found in the `explore_dataset.py` file. This script shows how to load the dataset, visualize the distribution of images across models, plot prompt length distributions, and perform textual searches on the dataset using CLIP features.

## Prompt Generation

The code to generate the random prompts can be found in `face_prompt_utils.py`. This script creates highly diverse prompts by combining various facial attributes, expressions, accessories, and environmental factors.

## Creating your own dataset

You can adjust the following parameters in the `__main__` part of the `create_face_dataset.py` script:

- `output_db_folder`: The directory where images and metadata will be saved
- `sdxl_samples`, `dalle3_samples`, `flux1_pro_samples`, ...: Number of images to generate for each model
- `sdxl_config`, `dalle3_config`, `flux1_pro_config`, ...: Configuration parameters for each model

#### Running the Script

1. Ensure you have the appropriate API keys set up in the script as described above and set up the number of samples to generate for each model.

2. Run the script:
   ```
   python create_face_dataset.py
   ```
   The script will create a `.env` file with your API keys on the first run. It will then generate images using all three models and save them in the specified output folder along with a metadata CSV file.

3. The script generates:  
Images in the `{output_db_folder}/images` directory  
A `SFHQ_T2I_dataset.csv` file in the `output_db_folder` containing information about each generated image


## API Key Setup

You'll need API keys for Stability AI, OpenAI, and FAL AI. Here's how to obtain them:

1. FAL AI (for FLUX1.pro):
   - Sign up at https://www.fal.ai/
   - Generate an API key from your account dashboard

2. Stability AI (for SDXL):
   - Sign up at https://platform.stability.ai/
   - Navigate to your account dashboard and generate an API key

3. OpenAI (for DALL-E 3):
   - Sign up at https://platform.openai.com/
   - Go to the API section and create a new API key

4. Install the required packages:
   ```
   pip install stability-sdk openai fal-client pillow pandas python-dotenv
   ```

Once you have your API keys, update the following lines at the beginning of the `create_face_dataset.py` script with your actual keys:

```python
STABILITY_API_KEY = 'your-stability-ai-api-key'
OPENAI_API_KEY = 'your-openai-api-key'
FAL_API_KEY = 'your-fal-api-key'
```



## Privacy

Since all images in this dataset are synthetically generated, there are no privacy issues or license issues surrounding these images.

## Citation

If you use this dataset in your research, please cite it as follows:

```
@misc{david_beniaguev_2024_SFHQ_T2I,
    title={Synthetic Faces High Quality - Text 2 Image (SFHQ-T2I) Dataset},
    author={David Beniaguev},
    year={2024},
    url={https://github.com/SelfishGene/SFHQ-T2I-dataset},
    publisher={GitHub},
    DOI={10.34740/kaggle/dsv/9548853},
}
```

## Summary

The SFHQ-T2I dataset provides a large, diverse collection of high-quality synthetic face images paired with their generating prompts. This dataset is unique in its level of variability across multiple facial and environmental attributes, made possible by the use of various state-of-the-art text-to-image models and carefully crafted prompts.

The purpose of this dataset is to provide a rich resource for training and evaluating machine learning models in tasks related to face analysis, generation, and text-to-image synthesis, without the need to worry about privacy or license issues.
The dataset may be extended from time to time with additional labels or features, but no promises.

I hope this dataset proves useful. Feel free to use it as you see fit...

================================================================================
================================================================================
figures/textual_search_1_age_male_top_8_matches.jpg:
====================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid of 96 photographs, organized into eight rows and twelve columns. Each photograph depicts a male subject at a different age, ranging from baby to elderly.  The images are neatly arranged, with each photo having a consistent size and aspect ratio. The background of each photo is mostly dark or neutral, ensuring the subject is the clear focal point.

The photos are grouped into eight age categories: baby boy, boy toddler, child boy, teenage boy, adult male, middle-aged adult male, and elderly male.  Each age category has eight photos, representing eight different individuals across the same age range.  This arrangement suggests a comparison or matching exercise; perhaps the images are part of a dataset for facial recognition or age progression studies.

Above each photo, there is text that appears to be a filename and a descriptor, indicating the image's purpose within the dataset (e.g., "Match 1 for: baby boy," followed by the filename). This labeling system is consistent throughout the grid, providing clear identification for each photo and its intended association.  The overall structure suggests a well-organized and systematic approach to image organization and labeling.

================================================================================
================================================================================
figures/textual_search_2_Facial Features_top_9_matches.jpg:
===========================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a grid of 72 face images, organized into 8 rows and 9 columns. Each image is accompanied by text indicating the file name and a label describing a facial feature.  The labels fall into five categories: "reading glasses," "sunglasses," "bald," "goatee," and "lipstick."  The file names suggest the images come from two different datasets, SDXL and FLUX1.

The arrangement of images aims to visually demonstrate the results of an image textual search using OpenCLIP features. The search likely used a prefix, "Facial Features," to filter results from a synthetic dataset.  The grid shows images that match the textual search query for each of the five facial feature labels. The goal is to showcase the accuracy and diversity of the image retrieval system based on the textual descriptions.

The images themselves are diverse in terms of age, gender, ethnicity, and lighting conditions. This diversity is important for evaluating the robustness and generalizability of the image search algorithm. The images are high-quality and clearly show the specified facial features, making it easy to assess the relevance of each result to its label.

================================================================================
================================================================================
figures/textual_search_2_Background Color_top_9_matches.jpg:
============================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid showcasing a diverse collection of portraits, organized into a matrix of 6 columns and 8 rows. Each cell contains a portrait photograph of a person against a solid background color. 


The backgrounds are consistently colored, with sections dedicated to yellow, green, blue, purple, and red backgrounds, creating distinct blocks within the grid. Each image is labeled with a filename (suggesting it comes from a dataset) and a descriptive text label indicating the background color ("yellow background", "green background", etc.). This systematic arrangement implies that the images are curated and categorized based on background color for a specific purpose, likely related to image analysis or machine learning.


The portraits themselves feature a wide variety of individuals, exhibiting different ages, ethnicities, hairstyles, and expressions. The consistent lighting and framing suggest a controlled photographic setting, possibly a studio environment. The overall impression is that the images are part of a large dataset used for training or testing image recognition algorithms, potentially focusing on background color identification or separation.

================================================================================
================================================================================
figures/textual_search_2_Lighting_top_10_matches.jpg:
=====================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a large grid showcasing numerous photographs, arranged in a 10x10 matrix. Each photograph depicts a person, predominantly portraits, with diverse ethnicities, ages, and genders represented.  The lighting in each photo is distinct and varied, serving as a key visual element.

The images are categorized by lighting style, indicated by text labels beneath each image.  These labels include descriptive terms like "side light with shadows," "spotlight," "soft lighting," "back lighting," "golden hour," "blue hour lighting," and "studio lighting."  Each label is further accompanied by a filename, suggesting the images might be sourced from a specific dataset or project.  The consistent use of labels and filenames implies a structured and organized collection.

The overall impression is of a comprehensive visual dataset designed for image analysis and training, potentially related to lighting conditions in photography or computer vision. The diversity of subjects and the systematic variation in lighting styles make it suitable for tasks such as image classification, style transfer, or the development of algorithms sensitive to different lighting scenarios.

================================================================================
================================================================================
figures/textual_search_2_Age_top_10_matches.jpg:
================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image presents a grid of 96 images, each accompanied by a textual label describing the age and identity of the person depicted. The images are organized into rows and columns, showcasing a diverse range of individuals across different age groups, from babies to elderly people.  The age categories include "10-month-old baby," "2.5-year-old toddler," "small child," "16-year-old teenager," "30-year-old adult," and "wrinkly 70-year-old senior."  Each image is clearly labeled with its corresponding age description and a filename, suggesting it's sourced from a large dataset.  The images are high-resolution and show individuals with varying ethnicities and expressions.

The arrangement of the images suggests a systematic organization based on age, progressing from infancy to old age.  The consistency in image quality and labeling implies a controlled, likely synthetic, dataset created for image recognition or similar machine learning tasks.  The title "Image textual search using OpenCLIP features from synthetic dataset" further reinforces this purpose, indicating the images are used for testing or training an image search algorithm using the OpenCLIP model. The "Condition: Age" and "Prefix: "" clarifies that the dataset is categorized and used for age classification.


The overall impression is one of a meticulously curated collection designed for research and development in the field of computer vision and artificial intelligence, specifically focused on age recognition and image retrieval.

================================================================================
================================================================================
figures/textual_search_2_Expression_top_10_matches.jpg:
=======================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image presents a grid of 108 small images, each displaying a different human face.  The faces are diverse in age, gender, and ethnicity. Each face is labeled with a descriptive caption that categorizes the facial expression shown, such as "angry or enraged," "surprised," "smiling," "sad or depressed," "grim face," or "tongue out".  The captions also include a file name, likely indicating the source of the image.

The arrangement of the images is systematic, with each row representing a specific emotion. The layout enables a visual comparison of how different individuals express the same emotion. The overall aim seems to be to showcase a dataset of facial expressions used for training or testing a facial recognition or emotion detection model.  The diversity of faces suggests an attempt to create a robust and inclusive dataset representative of a wide range of demographics.  The consistent labeling and organization facilitate easy navigation and understanding of the dataset's contents.

================================================================================
================================================================================
figures/SDXL_images_with_prompts.jpg:
=====================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a diptych, presenting two distinct portraits side-by-side. Each portrait features a close-up shot of an older adult, focusing on their facial features and expressions.  The left portrait shows a Caucasian grandmother, appearing to be in her 60s, wearing large, round blue glasses adorned with rhinestones. Her expression is somewhat pensive, her gaze directed slightly downward.  The right portrait depicts an Amerindian individual, also appearing middle-aged, with long, dreadlocked magenta hair. Their expression is more serious and melancholic, with a direct gaze at the viewer.  Their skin shows significant texture and detail.

Below each portrait is a caption providing details such as age, ethnicity, and photographic specifications. The left caption describes a candid shot, highlighting the subject's nervous twitch and the serene setting of a lavender field in Provence. The right caption details a close-up shot taken in a historic monastery, emphasizing the subject's deep blue eyes and the use of a DSLR camera. Both captions use descriptive language to evoke the mood and atmosphere of each photograph. The overall visual style of both portraits is high-quality, with a focus on sharp details and subtle use of bokeh.

================================================================================
================================================================================
figures/FLUX1_schnell_SDXL_images.jpg:
======================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a grid of eight photos, arranged in two rows of four. Each photo is a close-up portrait of a person, showcasing a diverse range of ages, ethnicities, and styles. 


The top row features:

1. A woman with weathered features and a dark headscarf.
2. A young girl with blonde hair wearing a crown and sunglasses.
3. An elderly Asian man in traditional Native American headdress.
4. An elderly Asian man with a long white beard and a knit cap.


The bottom row features:

5. A middle-aged man with glasses and a leather jacket.
6. An elderly man with white hair and sunglasses.
7. A young Black man with dreadlocks and sunglasses, wearing a pearl necklace.
8. A young child with dark hair wearing a flower crown.


Each photo has a filename overlaid at the top, indicating the source and image identifier. The filenames suggest the images are from two different sources, "FLUX1_schnell" and "SDXL". The overall feel is a collection of striking portraits, highlighting individual character and diversity.

================================================================================
================================================================================
figures/textual_search_1_hats_top_8_matches.jpg:
================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a large grid of 72 smaller images, arranged in 9 rows and 8 columns. Each small image shows a different person wearing a hat. The hats are categorized into eight types: baseball cap, fedora, beanie, top hat, cowboy hat, and sun hat.

Each small image is labeled with a caption indicating the hat type and a file name.  The captions appear to be generated automatically, possibly by a machine learning model designed to identify and categorize the hat types. The file names suggest the images originate from different sources or datasets (SDXL, FLUX1).

The arrangement of the images implies a comparison or matching task.  It's likely that the image grid was generated to showcase the accuracy of a hat-identification algorithm, demonstrating its ability to correctly classify different hat types across a diverse set of individuals and image backgrounds. The variety in age, gender, and ethnicity of the people shown in the images contributes to a comprehensive test of the image recognition capabilities.

================================================================================
================================================================================
figures/textual_search_1_sex_top_8_matches.jpg:
===============================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid of 32 headshots, organized into eight rows of four images each.  Each headshot shows a person from the shoulders up, against a variety of backgrounds. The individuals depicted exhibit diverse ethnicities, ages, genders, and hair styles. 


Above each set of four images, text labels identify the image as a "Match" number (1-8) for either "male," "female," or "non-binary person."  Below each image is a filename, indicating the source of the picture. The consistent formatting and labeling suggest the images are part of a dataset or a result from a facial recognition or matching algorithm, perhaps for a research project or application. The diversity in the subjects suggests an attempt at inclusivity and representation in the dataset.

================================================================================
================================================================================
figures/textual_search_1_hair_color_top_8_matches.jpg:
======================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a large grid, organized into eight columns and numerous rows. Each cell within the grid contains a portrait photograph of a person.  The portraits appear to be professionally taken, with consistent lighting and backgrounds. The subjects vary widely in age, gender, and ethnicity, providing a diverse representation of individuals.

Above each column of images is a label indicating the hair color or style being represented in that column ("black hair," "brown hair," "blonde hair," "red hair," "gray hair," "bald," "blue hair," "green hair," and "pink hair"). Each individual photo is further labeled with a file name and a description like "Match 1 for: black hair".  This suggests the images are part of a dataset used for image matching or training a machine learning model, possibly for facial recognition or hair classification.  The consistent labeling and organization point to a structured and methodical approach to data collection and organization.

================================================================================
================================================================================
figures/textual_search_1_glasses_top_8_matches.jpg:
===================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid of 64 smaller images, arranged in eight rows and eight columns. Each smaller image shows a different person, mostly close-up headshots, wearing various types of eyeglasses or sunglasses. The images are neatly organized, with a consistent background color and lighting for each individual photo.

Above each row of eight images, there's a caption indicating the type of eyewear depicted—reading glasses, sunglasses, round glasses, or square glasses. Next to this caption is "Match 1," "Match 2," and so on, up to "Match 8," indicating a possible dataset or matching process. Each image also includes a filename, suggesting that the images are sourced from a larger dataset.

The people in the images are diverse, varying in age, gender, ethnicity, and hairstyle. They appear to be posed for the photos, with a neutral or slightly serious expression. The glasses in each image vary in style, frame shape, and color, showing a wide range of eyewear options. The overall structure and content suggest the image is likely a sample from a dataset used for training or testing a computer vision model, potentially one focused on eyewear recognition or classification.
================================================================================
================================================================================
figures/textual_search_2_Ethnicity x Age 1_top_10_matches.jpg:
==============================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a grid of 72 individual photographs, arranged in a 6x12 matrix. Each photograph is a close-up portrait of an elderly person from diverse ethnic backgrounds.  The portraits are high-quality, well-lit, and appear to be professionally taken.  They display a wide range of facial expressions and features, reflecting the age and individuality of the subjects.

Above each photograph, text indicates the presumed ethnicity of the individual ('asian', 'native american', 'african', 'persian', 'south-american', 'irish') and a filename. The consistent formatting suggests the images are part of a structured dataset, likely used for machine learning or artificial intelligence purposes.  The top of the image contains a title indicating that the image is related to image textual search using OpenCLIP features from a synthetic dataset, focusing on ethnicity and age.  The "old age" prefix further specifies the age range of the subjects.

The overall impression is a meticulously curated collection of photographs designed to represent a broad spectrum of ethnicities within a specific age group, likely for research or development purposes in fields such as facial recognition, age estimation, or bias detection in AI algorithms.

================================================================================
================================================================================
figures/textual_search_2_Facial Hair_top_10_matches.jpg:
========================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a grid of 72 images, each depicting a man's face with various facial hair styles. The images are neatly arranged in a 9x8 matrix.  Each image is labeled with a filename, indicating its source and a descriptive tag classifying the facial hair: "full beard," "mustache," "goatee," "sideburns," "stubble," or "shaved face."  The filenames suggest the images come from different datasets, including FLUX1 (with variations like "_dev," "_schnell," and "_pro") and SDXL.

The top of the image contains a title indicating the purpose: "Image textual search using OpenCLIP features from synthetic dataset."  A subtitle clarifies the condition as "Facial Hair" and the prefix used for the search query as "man with ".  This implies that the images are results of a text-based image search, aiming to retrieve pictures of men with specific facial hair attributes. The overall arrangement suggests an organized visualization of the results, facilitating a comparison and analysis of the different facial hair types and the effectiveness of the image search.

================================================================================
================================================================================
figures/textual_search_2_Makeup_top_9_matches.jpg:
==================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid of 72 images, organized in 6 rows and 12 columns, showcasing a variety of women's faces with different makeup styles. Each image is labeled with a descriptive caption indicating the type of makeup (e.g., "heavy makeup," "without makeup," "red lipstick," "strong eyeliner," "traditional makeup") and the source of the image (e.g., "FLUX1_pro_image_0002399.jpg"). The images illustrate a wide range of skin tones, ages, and facial features, providing a diverse representation of women.

The top of the image includes a title, "Image textual search using OpenCLIP features from synthetic dataset," indicating that the images are part of a dataset used for image recognition and search. The subtitle, "Condition: Makeup," and "Prefix: 'woman'," further specify the context and the type of search being performed.  The variety in makeup styles within the grid allows for testing and training of AI models to accurately identify and classify different makeup types.

The consistent labeling of each image makes it easy to understand the dataset's structure and purpose.  The layout effectively visualizes the diversity of makeup styles and the differences in appearance based on those styles.  The overall visual presentation is clean and well-organized, facilitating easy analysis of the image data.

================================================================================
================================================================================
figures/prompt_lengths_distribution.jpg:
========================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a comparative analysis of prompt lengths for different large language models (LLMs), visualized through two overlaid histograms.  The top histogram shows the distribution of prompt lengths in terms of the number of characters, while the bottom histogram displays the same data but measured in the number of words.  Both histograms use the same color scheme to represent each LLM: DALL-E 3 (purple), FLUX1_dev (teal), FLUX1_pro (dark green), FLUX1_schnell (brown), and SDXL (dark red).

The x-axes of both histograms represent the prompt length (characters in the top, words in the bottom), and the y-axes represent the frequency, indicating how many prompts fall within each length bin.  The overlapping nature of the histograms clearly demonstrates the relative distributions of prompt lengths for each model.  For instance, it's readily apparent that SDXL and FLUX1_schnell tend to have longer prompts than DALL-E 3, across both character and word counts.  The visual stacking allows for a direct comparison of the frequency distributions, highlighting similarities and differences in prompt length preferences among the various models.

The overall structure is clean and informative, with clear titles ("Prompt Length in Characters" and "Prompt Length in Words"), labeled axes, and a legend specifying the color-coding for each LLM.  The dark background enhances the visibility of the colored histogram bars, making the data easy to interpret. The choice of histograms as the visualization method allows for a quick grasp of the central tendency and spread of prompt lengths for each model.

================================================================================
================================================================================
figures/textual_search_2_Physical Characteristics_top_9_matches.jpg:
====================================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image presents a grid of 60 face images, meticulously organized and labeled.  Each face is accompanied by a text description indicating a specific physical characteristic, such as "large or chiseled jaw," "long white beard," "fashionable beard," or "wide eyes," and "overweight or chubby."  The images are diverse, showcasing a wide range of ages, ethnicities, genders, and facial hair styles. The consistent labeling ensures that all faces in a row share the same designated characteristic.

The top of the image displays the title "Image textual search using OpenCLIP features from synthetic dataset" and the subtitle "Condition: Physical Characteristics Prefix:".  This indicates that the image is a visual representation of a text-based image search experiment using the OpenCLIP model, and the search is focused on specific physical attributes of faces. The file names associated with each image are also provided, implying that these images come from a structured dataset.

The organization of the grid is systematic, allowing for easy comparison of faces with the same characteristic.  This visual representation is likely used to demonstrate the effectiveness of the OpenCLIP model in identifying and retrieving images based on textual descriptions of physical features. The variety in the faces suggests a robust dataset was used for training and testing the model.

================================================================================
================================================================================
figures/textual_search_1_ethnicity_top_8_matches.jpg:
=====================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid of 96 small photographs, arranged in 8 rows and 12 columns. Each photograph shows a headshot of a person, seemingly aiming for diversity in ethnicity and gender.  The photos are neatly organized, with a consistent background and framing for each individual.  The lighting is generally even and professional, suggesting the images are likely from a curated dataset or a professional photoshoot.


Above each set of three images within each row, there's a label indicating the ethnicity classification the images are meant to represent (e.g., Caucasian, African, Asian, Hispanic, Middle Eastern, Scandinavian, Native American).  Each individual photo within the grid also includes a file name and a label indicating its position within the set of images for that ethnicity (e.g., "Match 1 for: Caucasian"). This organization strongly suggests the image is a sample from a larger dataset used for facial recognition, bias detection, or similar machine learning tasks.


The overall impression is one of a carefully constructed dataset designed to represent a broad range of human appearances, likely for use in algorithmic training or testing. The uniformity and labeling highlight the systematic nature of the image collection and its intended application.

================================================================================
================================================================================
figures/FLUX1_schnell_images_with_prompts.jpg:
==============================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a diptych, meaning it's composed of two distinct photographs side-by-side.  Each photograph features a different individual and has accompanying text describing the image's details.  The top of each section displays metadata including the model's name, "FLUX1_schnell," and the filename of the image.

The left photograph is a medium shot of a young, serious-looking Futunan teenage girl. She's wearing sunglasses, a purple t-shirt, and has her hair in a bun. The lighting is described as "beauty dish lighting," suggesting a professional studio setup aiming for flattering illumination. The background is dark and unfocused, drawing attention to the subject. The text below the image details her age, ethnicity, and attire, emphasizing the serious expression in her pose.

The right photograph shows a high-angle shot of a 35-year-old Libyan man.  He has bright blue hair styled in a mohawk, steampunk goggles, and a patterned scarf. His expression is serene and contemplative as he looks upward. The man's appearance and attire suggest a stylized or artistic look. The descriptive text notes his age, ethnicity, eye and hair color, clothing, and the lighting conditions, which appear to be natural light. The setting is described as being by a lake.

================================================================================
================================================================================
figures/textual_search_1_eye_color_top_8_matches.jpg:
=====================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a large grid, consisting of 8 columns and 8 rows, containing 64 smaller images. Each smaller image is a close-up portrait of a person's face, focusing primarily on their eyes. 


The images are organized into sections based on eye color: blue, green, brown, yellow, hazel, and red.  Within each eye color section, there are eight images labeled "Match 1," "Match 2," etc., up to "Match 8." This suggests the images are part of a dataset or experiment designed to test the ability of a system (likely an AI) to match faces based on eye color.  Each image caption includes a file name, further supporting this hypothesis.


The lighting and photographic style of the portraits are relatively consistent, aiming for a neutral and even tone across all images to minimize the impact of extraneous factors on eye color recognition.  The variation in age, ethnicity, and gender of the individuals depicted aids in testing the robustness of the system being evaluated.

================================================================================
================================================================================
figures/textual_search_2_bad things_top_10_matches.jpg:
=======================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image displays a large grid of 10x10 = 100 small images. Each small image shows a different person or scene, and is accompanied by text giving a file name and a label.  The labels describe the content of the image, such as "blurred," "statue," "two people," "back of head," "hand covering face," "cat," "dog," or "animal."  The filenames appear to indicate the source of the image (e.g., SDXL, FLUX1). The images are diverse, showing a wide range of ages, ethnicities, expressions, and settings. Some images are portraits, some are full body shots and some show statues.  The overall lighting and image quality vary, reflecting a potentially diverse collection of sources.

The top of the image includes a title stating that it is a demonstration of image textual search using OpenCLIP features.  It specifies that the features are from a synthetic dataset and that the condition for image selection was "bad things". This suggests the images may have been curated based on some negative attribute or undesirable characteristic.  The purpose of the image is likely to showcase the ability of the OpenCLIP model to correctly identify and label images, even those with various qualities and potentially problematic aspects.

================================================================================
================================================================================
figures/textual_search_1_age_female_top_8_matches.jpg:
======================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a large grid composed of numerous smaller images, each showing a female face at different ages and ethnicities. The grid is organized into eight columns and eight rows, resulting in a total of 64 individual images. Each smaller image is accompanied by text that indicates the age group and the image source.

The age groups represented include baby girls, toddler girls, child girls, teenage girls, adult females, middle-aged adult females, and elderly females. Within each age group, there's a good variety of ethnic backgrounds and facial expressions, showcasing the diversity of the dataset.  The images are high-quality portraits, with a focus on the individual's face, and the lighting and background vary slightly from image to image.

The textual information under each image provides a consistent format, specifying "Match X for: [age group]" followed by the file name and path.  This suggests the image grid is a visual representation of a dataset used for matching purposes, likely in a facial recognition or image analysis context.  The consistent structure and clear labeling make it easy to understand the organization and purpose of the image collection.

================================================================================
================================================================================
figures/DALLE3_images_with_prompts.jpg:
=======================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a diptych, presenting two separate generated images side-by-side, each accompanied by a descriptive text prompt used to create it using the DALLE3 model.  The left panel features a close-up of a young, light-skinned woman with striking makeup that resembles scales, wide eyes, and a surprised expression. Her hair is styled in dark brown twist braids, and she wears a high-visibility vest and beanie. The background is a dramatic, dark and textured scene. The prompt requests an ultra-high definition image with impressive color and composition, emphasizing the woman's features and the dramatic lighting.

The right panel displays a medium close-up of a 37-year-old man with a bewildered expression, brunette undercut hairstyle, and a choker. His head is positioned directly facing the camera. The background includes a historical-looking building, creating a contrast between the modern man and the historical setting. The prompt calls for a serene, bluish hue evocative of the hour after sunset, emphasizing the man's features and the historical context of the background.  The overall structure is clean and organized, clearly displaying the generated images and their corresponding prompts.

================================================================================
================================================================================
figures/FLUX1_pro_images_with_prompts.jpg:
==========================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a diptych, meaning it's composed of two distinct photographs placed side-by-side.  Each photograph features a close-up portrait of a person, one a young Black girl and the other an older Asian woman. 


The left portrait shows a teenage girl with dark skin, dark locs, and striking luminous yellow eyes.  Her expression is serious and somewhat resigned. The background is blurred, focusing attention on her face. The caption describes the image's technical aspects, noting the camera, lens, and artistic choices (e.g., golden ratio composition).


The right portrait shows an older woman with teal-colored hair, her head tilted slightly back. She has a pleased expression and sleepy, yellow eyes. She's wearing a purple blazer. The background is again slightly blurred, emphasizing the subject. The accompanying text describes her as a Minangkabau wife and details the photo's composition and mood.

The overall effect of the diptych is to present a contrast between youth and age, different ethnicities, and varied expressions, while maintaining a visual consistency in terms of photographic style and subject focus.  The captions provide technical and contextual information about each photograph.

================================================================================
================================================================================
figures/textual_search_1_accessories_top_8_matches.jpg:
=======================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a large grid composed of many smaller square images, each featuring a different person's headshot.  The headshots are diverse, showcasing a wide range of ages, ethnicities, and genders. Each person is presented against a variety of simple backgrounds, some solid colors, and others slightly blurred to focus attention on the individual.  The lighting in each headshot varies but is generally well-lit and flattering.

Above each set of eight images, a label indicates what accessory (earrings, necklace, bandana, hat, tie, scarf, headphones, or sunglasses) that set of headshots is associated with. Each individual image within a set is labeled with the filename and a "Match #" to indicate its position within the set. This suggests the image is part of a data set for a machine learning project, likely focused on object detection or accessory classification.  The consistent formatting and labeling contribute to a structured, organized presentation of the data.

The overall impression is one of a meticulously organized collection of facial images intended for a technical purpose, likely related to computer vision or image recognition. The diversity of the subjects ensures a robust data set for training an algorithm to identify the specified accessories.

================================================================================
================================================================================
extract_pretrained_features.py:
===============================
import os
import glob
import time
import shutil
import pickle
import timm
import clip
import open_clip
import sklearn
import torch
import numpy as np
from tqdm import tqdm
from PIL import Image
import open_clip
from torchvision import transforms as pth_transforms
from timm.data import resolve_data_config
from timm.data.transforms_factory import create_transform

import warnings
warnings.simplefilter("ignore", sklearn.exceptions.DataConversionWarning)

#%% helper functions

def load_timm_model(model_name='convnext_xlarge_in22k', device='cpu'):

    pretrained_model = timm.create_model(model_name, pretrained=True, num_classes=0).eval().to(device)
    model_config_dict = resolve_data_config({}, model=pretrained_model)
    model_preprocess = create_transform(**model_config_dict)

    return pretrained_model, model_preprocess

def load_dino_model(model_name='dino_vitb8', device='cpu'):

    model_preprocess = pth_transforms.Compose([
            pth_transforms.Resize(256, interpolation=3),
            pth_transforms.CenterCrop(224),
            pth_transforms.ToTensor(),
            pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
        ])

    pretrained_model = torch.hub.load('facebookresearch/dino:main', model_name).to(device)

    return pretrained_model, model_preprocess


def load_openclip_model(model_name, device="cpu"):
    if model_name == "OpenCLIP_ViT-H-14-378-quickgelu":
        model, _, preprocess = open_clip.create_model_and_transforms('ViT-H-14-378-quickgelu', pretrained='dfn5b')
    elif model_name == "OpenCLIP_ViT-bigG-14-CLIPA-336":
        model, _, preprocess = open_clip.create_model_and_transforms('ViT-bigG-14-CLIPA-336', pretrained='datacomp1b')
    elif model_name == "OpenCLIP_ViT-SO400M-14-SigLIP-384":
        model, _, preprocess = open_clip.create_model_and_transforms('ViT-SO400M-14-SigLIP', pretrained='webli')
    elif model_name == "OpenCLIP_ViT-G-14":
        model, _, preprocess = open_clip.create_model_and_transforms('ViT-bigG-14', pretrained='laion2b_s39b_b160k')
    elif model_name == "OpenCLIP_ConvNext-XXLarge":
        model, _, preprocess = open_clip.create_model_and_transforms('convnext_xxlarge', pretrained='laion2b_s34b_b82k_augreg_soup')
    elif model_name == "OpenCLIP_ViT-H-14":
        model, _, preprocess = open_clip.create_model_and_transforms('ViT-H-14', pretrained='laion2b_s32b_b79k')
    else:
        model, _, preprocess = open_clip.create_model_and_transforms(model_name)
    
    model = model.to(device)
    model.eval()
    
    return model, preprocess

def extract_pretrained_features(base_image_folder, model_to_use='CLIP_ViTL_14@336'):

    images_folder = os.path.join(base_image_folder, 'images')
    transfer_images = False

    # First, determine if we need to transfer images and which images to transfer if we do
    if not os.path.exists(images_folder):
        base_image_files = []
        for image_file_ending in ['*.jpg', '*.png']:
            base_image_files.extend(glob.glob(os.path.join(base_image_folder, image_file_ending)))
        if base_image_files:
            transfer_images = True

    # Now, handle the images based on whether we need to transfer
    if transfer_images:
        os.makedirs(images_folder, exist_ok=True)
        for src_image_filename in base_image_files:
            shutil.move(src_image_filename, images_folder)
        all_image_filenames = [os.path.join(images_folder, os.path.basename(f)) for f in base_image_files]
        print('Images were transferred to the images folder.')
    else:
        if os.path.exists(images_folder):
            all_image_filenames = glob.glob(os.path.join(images_folder, '*.*'))
            print('Images were already in the correct location.')
        else:
            print('No images found in the base folder or in an "images" subfolder.')
            return

    if len(all_image_filenames) == 0:
        print('No images found to process.')
        return

    # Create features folder if it doesn't exist
    features_folder = os.path.join(base_image_folder, 'pretrained_features')
    os.makedirs(features_folder, exist_ok=True)

    # load requested model
    device = "cuda" if torch.cuda.is_available() else "cpu"

    print(f'loading model "{model_to_use}"...')

    if   model_to_use == 'CLIP_ViTL_14@336':
        pretrained_model, model_preprocess = clip.load("ViT-L/14@336px", device=device)
    elif model_to_use == 'CLIP_ViTL_14':
        pretrained_model, model_preprocess = clip.load("ViT-L/14", device=device)
    elif model_to_use == 'CLIP_ViTB_16':
        pretrained_model, model_preprocess = clip.load("ViT-B/16", device=device)
    elif model_to_use == 'CLIP_ViTB_32':
        pretrained_model, model_preprocess = clip.load("ViT-B/32", device=device)
    elif model_to_use == 'CLIP_ResNet50x64':
        pretrained_model, model_preprocess = clip.load("RN50x64", device=device)
    elif model_to_use == 'CLIP_ResNet50x16':
        pretrained_model, model_preprocess = clip.load("RN50x16", device=device)
    elif model_to_use == 'CLIP_ResNet50x4':
        pretrained_model, model_preprocess = clip.load("RN50x4", device=device)
    elif model_to_use == 'CLIP_ResNet50x1':
        pretrained_model, model_preprocess = clip.load("RN50", device=device)
    elif model_to_use == 'CLIP_ResNet101':
        pretrained_model, model_preprocess = clip.load("RN101", device=device)

    elif model_to_use == 'DINO_ResNet50':
        pretrained_model, model_preprocess = load_dino_model("dino_resnet50", device=device)
    elif model_to_use == 'DINO_ViTS_8':
        pretrained_model, model_preprocess = load_dino_model("dino_vits8", device=device)
    elif model_to_use == 'DINO_ViTB_8':
        pretrained_model, model_preprocess = load_dino_model("dino_vitb8", device=device)

    elif model_to_use == 'ConvNext_XL_Imagenet21k':
        pretrained_model, model_preprocess = load_timm_model(model_name='convnext_xlarge_in22k', device=device)
    elif model_to_use == 'ConvNext_XL_384_Imagenet21k_ft_1k':
        pretrained_model, model_preprocess = load_timm_model(model_name='convnext_xlarge_384_in22ft1k', device=device)
    elif model_to_use == 'ConvNext_L_Imagenet21k':
        pretrained_model, model_preprocess = load_timm_model(model_name='convnext_large_in22k', device=device)
    elif model_to_use == 'ConvNext_L_384_Imagenet21k_ft_1k':
        pretrained_model, model_preprocess = load_timm_model(model_name='convnext_large_384_in22ft1k', device=device)

    elif model_to_use == 'EffNet_L2_NS_475':
        pretrained_model, model_preprocess = load_timm_model(model_name='tf_efficientnet_l2_ns_475', device=device)
    elif model_to_use == 'EffNet_B7_NS_600':
        pretrained_model, model_preprocess = load_timm_model(model_name='tf_efficientnet_b7_ns', device=device)
    elif model_to_use == 'EffNetV2_L_480_Imagenet21k_ft_1k':
        pretrained_model, model_preprocess = load_timm_model(model_name='tf_efficientnetv2_l_in21ft1k', device=device)
    elif model_to_use == 'EffNetV2_S_384_Imagenet21k_ft_1k':
        pretrained_model, model_preprocess = load_timm_model(model_name='tf_efficientnetv2_s_in21ft1k', device=device)

    elif model_to_use == 'BEiT_L_16_512':
        pretrained_model, model_preprocess = load_timm_model(model_name='beit_large_patch16_512', device=device)
    elif model_to_use == 'BEiT_L_16_384':
        pretrained_model, model_preprocess = load_timm_model(model_name='beit_large_patch16_384', device=device)
    elif model_to_use == 'BEiT_L_16_224':
        pretrained_model, model_preprocess = load_timm_model(model_name='beit_large_patch16_224', device=device)

    elif model_to_use == 'DeiT3_L_16_384_Imagenet21k_ft_1k':
        pretrained_model, model_preprocess = load_timm_model(model_name='deit3_large_patch16_384_in21ft1k', device=device)
    elif model_to_use == 'DeiT3_H_14_224_Imagenet21k_ft_1k':
        pretrained_model, model_preprocess = load_timm_model(model_name='deit3_huge_patch14_224_in21ft1k', device=device)
    elif model_to_use == 'DeiT3_L_16_224_Imagenet21k_ft_1k':
        pretrained_model, model_preprocess = load_timm_model(model_name='deit3_large_patch16_224_in21ft1k', device=device)
    
    elif model_to_use == 'OpenCLIP_ViT-bigG-14-CLIPA-336':
        pretrained_model, model_preprocess = load_openclip_model("OpenCLIP_ViT-bigG-14-CLIPA-336", device=device)
    elif model_to_use == 'OpenCLIP_ViT-H-14-378-quickgelu':
        pretrained_model, model_preprocess = load_openclip_model("OpenCLIP_ViT-H-14-378-quickgelu", device=device)
    elif model_to_use == 'OpenCLIP_ViT-SO400M-14-SigLIP-384':
        pretrained_model, model_preprocess = load_openclip_model("OpenCLIP_ViT-SO400M-14-SigLIP-384", device=device)
    elif model_to_use == 'OpenCLIP_ViT-G-14':
        pretrained_model, model_preprocess = load_openclip_model("OpenCLIP_ViT-G-14", device=device)
    elif model_to_use == 'OpenCLIP_ConvNext-XXLarge':
        pretrained_model, model_preprocess = load_openclip_model("OpenCLIP_ConvNext-XXLarge", device=device)
    elif model_to_use == 'OpenCLIP_ViT-H-14':
        pretrained_model, model_preprocess = load_openclip_model("OpenCLIP_ViT-H-14", device=device)
    else:
        print('unrecognized modelname, not calculated any features!')
        return

    print(f'"{model_to_use}" model loaded')
    print(f'Calculating {len(all_image_filenames)} features of model "{model_to_use}"...')

    start_time = time.time()
    # Go over all images and append features to features dict
    for curr_image_filename in tqdm(all_image_filenames, desc=f'Extracting "{model_to_use}" features', unit="image"):
        curr_sample_name = os.path.splitext(os.path.basename(curr_image_filename))[0]
        curr_features_dict_filename = os.path.join(features_folder, curr_sample_name + '.pickle')

        # Check if features_dict file exists, if it doesn't, create one
        if os.path.isfile(curr_features_dict_filename):
            with open(curr_features_dict_filename, "rb") as f:
                curr_features_dict = pickle.load(f)
        else:
            curr_features_dict = {}

        # If the requested features were already calculated for this sample, skip it
        if model_to_use in curr_features_dict.keys():
            continue

        # Extract the features
        curr_image_PIL = Image.open(curr_image_filename).convert("RGB")

        with torch.no_grad():
            if 'CLIP' in model_to_use:
                curr_pretrained_features = pretrained_model.encode_image(model_preprocess(curr_image_PIL).unsqueeze(0).to(device))
            elif 'OpenCLIP' in model_to_use:
                curr_pretrained_features = pretrained_model.encode_image(model_preprocess(curr_image_PIL).unsqueeze(0).to(device))
            elif 'DINO' in model_to_use:
                curr_pretrained_features = pretrained_model(model_preprocess(curr_image_PIL).unsqueeze(0).to(device))
            elif 'ConvNext' in model_to_use:
                curr_pretrained_features = pretrained_model(model_preprocess(curr_image_PIL).unsqueeze(0).to(device))
            elif 'EffNet' in model_to_use:
                curr_pretrained_features = pretrained_model(model_preprocess(curr_image_PIL).unsqueeze(0).to(device))
            elif 'BEiT' in model_to_use:
                curr_pretrained_features = pretrained_model(model_preprocess(curr_image_PIL).unsqueeze(0).to(device))
            elif 'DeiT' in model_to_use:
                curr_pretrained_features = pretrained_model(model_preprocess(curr_image_PIL).unsqueeze(0).to(device))

        curr_features_dict[model_to_use] = curr_pretrained_features.detach().cpu().numpy()

        # Save the dictionary
        with open(curr_features_dict_filename, "wb") as f:
            pickle.dump(curr_features_dict, f)

    total_duration_min = (time.time() - start_time) / 60
    print(f'Extracted "{model_to_use}" features from {len(all_image_filenames)} images. Total time: {total_duration_min:.2f} minutes')

    return

def collect_pretrained_features_from_folder(base_image_folder, model_name, normalize_features=True, ignore_DALLE=True):
    features_folder = os.path.join(base_image_folder, 'pretrained_features')
    all_feature_dict_filenames = glob.glob(os.path.join(features_folder, '*.pickle'))
    all_image_filenames = glob.glob(os.path.join(base_image_folder, 'images', '*.*'))

    if ignore_DALLE:
        all_feature_dict_filenames = [x for x in all_feature_dict_filenames if 'DALLE3' not in x]
        all_image_filenames = [x for x in all_image_filenames if 'DALLE3' not in x]

    features_list = []
    image_filename_map = {}

    for curr_image_filename in all_image_filenames:
        curr_sample_name = os.path.splitext(os.path.basename(curr_image_filename))[0]
        curr_features_dict_filename = os.path.join(features_folder, f"{curr_sample_name}.pickle")
        
        if not os.path.exists(curr_features_dict_filename):
            continue  # Skip images without corresponding feature dict
        
        with open(curr_features_dict_filename, "rb") as f:
            curr_features_dict = pickle.load(f)
        
        if model_name not in curr_features_dict:
            continue
        
        features_list.append(curr_features_dict[model_name])
        image_filename_map[len(features_list)-1] = curr_image_filename

        if len(features_list) % 5000 == 0:
            print(f"Processed {len(features_list)} images...")

    if not features_list:
        raise ValueError("No features were collected. Please check your directories and files.")

    pretrained_image_features = np.vstack(features_list)

    # Normalize features if requested
    if normalize_features:
        pretrained_image_features /= np.linalg.norm(pretrained_image_features, axis=1)[:, np.newaxis]

    return pretrained_image_features, image_filename_map

def extract_and_collect_pretrained_features(images_base_folder, models_to_use=['CLIP_ViTL_14@336','CLIP_ResNet50x64'], nromalize_features=True):
    # this function will extract the features of all models in "models_to_use", collect the  and concatenate them

    # extracting features
    for model_to_use in models_to_use:
        extract_pretrained_features(images_base_folder, model_to_use=model_to_use)

    # collecting features
    features_list = []
    image_filename_map_list = []
    for requested_features_model in models_to_use:
        image_features, image_filename_map = collect_pretrained_features_from_folder(images_base_folder, requested_features_model, nromalize_features=nromalize_features)
        features_list.append(image_features)
        image_filename_map_list.append(image_filename_map)

    # make sure the maps are identical
    try:
        for k in range(len(image_filename_map_list) - 1):
            for key in image_filename_map_list[k].keys():
                assert image_filename_map_list[k][key] == image_filename_map_list[k + 1][key]
    except:
        print('the maps are not identical. quitting')
        return

    # concatenate the features
    combined_image_features = np.concatenate(features_list, axis=1)

    return combined_image_features, image_filename_map_list[0]

def delete_near_duplicates(base_image_folder, models_to_use=['CLIP_ViTL_14@336','CLIP_ResNet50x64'], similarity_threshold=0.99, minibatch_size=10_000):
    # this function does not assume "proper" folder stucture, but will create it and calculate features if necessary

    features_folder = os.path.join(base_image_folder, 'pretrained_features')

    # collect the requested features to calculate near duplication based on
    image_features, image_filename_map = extract_and_collect_pretrained_features(base_image_folder, models_to_use=models_to_use, nromalize_features=True)
    similarity_threshold = len(models_to_use) * similarity_threshold

    total_num_samples = image_features.shape[0]
    num_batches = np.ceil(total_num_samples / minibatch_size).astype(int)

    feature_inds_to_drop = []

    end_row_ind = 0
    for batch_ind in range(num_batches):
        start_row_ind = end_row_ind
        end_row_ind = min(start_row_ind + minibatch_size, total_num_samples)
        image_feature_curr_batch = image_features[start_row_ind:end_row_ind]
        curr_minibatch_size = image_feature_curr_batch.shape[0]

        similarity_curr_batch_to_all = np.dot(image_feature_curr_batch, image_features.T).astype(np.float32)
        similarity_curr_batch_to_all[np.arange(curr_minibatch_size), np.arange(start_row_ind, end_row_ind)] = 0
        similarity_curr_batch_to_all = similarity_curr_batch_to_all > similarity_threshold

        # zero out all removals from previous batches
        if len(feature_inds_to_drop) > 0:
            similarity_curr_batch_to_all[:,np.array(feature_inds_to_drop)] = 0

        # go over the self similarity matrix rows and determine which indices should be removed
        for curr_batch_row_ind in range(curr_minibatch_size):
            if similarity_curr_batch_to_all[curr_batch_row_ind,:].sum() > 0:
                full_features_row = start_row_ind + curr_batch_row_ind
                feature_inds_to_drop.append(full_features_row)
                # zero out the column of the removed duplicate (so that it's twins won't be removed as well)
                similarity_curr_batch_to_all[:,full_features_row] = 0

    num_to_remove = len(feature_inds_to_drop)
    message_string = 'from the folder "%s" (contains %d images) \nthere will be removed %d near-duplicates (%.1f%s of images)'
    print('----------------------------------------')
    print(message_string %(base_image_folder, total_num_samples, num_to_remove, 100 * (num_to_remove / total_num_samples), '%'))
    print('----------------------------------------')

    # remove the files
    for k in feature_inds_to_drop:
        curr_image_filename = image_filename_map[k]
        curr_sample_name = curr_image_filename.split('/')[-1].split('.')[0]
        curr_features_dict_filename = os.path.join(features_folder, curr_sample_name + '.pickle')

        os.remove(curr_image_filename)
        os.remove(curr_features_dict_filename)


def find_nearest_neighbors(folder_A, folder_B, model_name, k=5):
    # Collect features from both folders
    features_A, filenames_A = collect_pretrained_features_from_folder(folder_A, model_name)
    features_B, filenames_B = collect_pretrained_features_from_folder(folder_B, model_name)

    print(f"Folder A features shape: {features_A.shape}")
    print(f"Folder B features shape: {features_B.shape}")

    nearest_neighbors = []
    
    # Iterate over each image index in folder A
    for i in tqdm(range(len(features_A)), desc="Finding nearest neighbors"):

        # Find the top k nearest neighbors in folder B
        feature_A = features_A[i:i+1]
        similarities = np.dot(feature_A, features_B.T).flatten()
        top_k_indices = np.argsort(similarities)[-k:][::-1]
        
        # Get filenames and similarities of nearest neighbors
        neighbor_filenames = [filenames_B[idx] for idx in top_k_indices]
        neighbor_similarities = [similarities[idx] for idx in top_k_indices]
        
        nearest_neighbors.append({
            'source_image': filenames_A[i],
            'neighbors': list(zip(neighbor_filenames, neighbor_similarities))
        })

    return nearest_neighbors

================================================================================
================================================================================
figures/good_model_images.jpg:
==============================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid of 30 portrait photographs, arranged in six rows and five columns. Each portrait features a different individual, exhibiting a wide range of ages, ethnicities, and expressions. The photographs are diverse in style, some appearing more candid or documentary, while others are more posed and stylized. The lighting and background also vary significantly across the images.

The images are labeled with filenames suggesting they are part of a larger dataset, categorized into three sets: "FLUX1_pro," "FLUX1_dev," "FLUX1_schnell," and "SDXL." This likely indicates different versions or stages of image processing or data collection. The filenames also include sequential numbers, implying a systematic organization within each category.

The overall impression is one of a diverse and extensive collection of human portraits, potentially used for training a machine learning model (given the filenames and the diverse nature of the subjects). The variety in age, ethnicity, expression, and photographic style suggests a deliberate effort to create a representative and robust dataset.

================================================================================
================================================================================
figures/textual_search_2_Ethnicity x Age 3_top_10_matches.jpg:
==============================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid of 72 photos of young children from various ethnic backgrounds. Each photo is square and labeled with a presumed ethnicity ("asian," "native american," "african," "persian," "south-american," "irish") and a file name. The photos are diverse, showing children in different settings and clothing, some posed and some candid. The photos are high-quality and well-lit.

The title above the grid indicates that this is a dataset used for image textual search using OpenCLIP features, focusing on ethnicity and age (3 years old).  The arrangement of the pictures suggests a systematic categorization based on ethnicity, which is confirmed by the labels under each image.  The consistent size and framing of the images create a visually uniform and organized presentation of the dataset.

The image serves as a visual representation of a machine learning dataset, demonstrating the diversity of the data used to train a model for image search based on textual descriptions. The goal is likely to test the ability of the model to accurately identify and retrieve images of children of different ethnicities based on text prompts.

================================================================================
================================================================================
figures/textual_search_2_Eye Color_top_10_matches.jpg:
======================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a grid of 100 facial portraits, each accompanied by a filename and a descriptive label indicating the eye color. The portraits are neatly arranged in a 10x10 grid, making it easy to browse the collection.  The images appear to be from a diverse range of individuals, varying in age, gender, ethnicity, and hair styles. The image quality is consistent across the images, suggesting a curated collection.

The eye color labels are consistently applied, categorizing the eyes as 'blue', 'green', 'brown', 'yellow', 'red', or 'hazel'.  This suggests the images were selected or generated based on this specific characteristic. The filenames, which begin with "SDXL," "FLUX1_dev," or "FLUX1_schnell," indicate that the images likely originate from different datasets or image generation models. This may suggest a study or comparative analysis of different image generation techniques.

The title "Image textual search using OpenCLIP features from synthetic dataset" above the grid suggests that the image grid is a result of a search query. The query itself is specified as "Condition: Eye Color, Prefix: 'person with'". This implies that the images were retrieved based on a textual query related to eye color within a larger dataset of synthetically generated images of people.  The overall structure and contents strongly suggest a research or development context related to image analysis and retrieval.

================================================================================
================================================================================
figures/textual_search_2_Ethnicity x Age 2_top_10_matches.jpg:
==============================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a grid showcasing a diverse collection of facial portraits, organized into rows and columns. Each portrait is accompanied by text indicating the presumed ethnicity ("asian," "native american," "african," "persian," "south-american," "irish") and a filename.  The filenames suggest the images originate from various datasets (FLUX1, SDXL). The overall arrangement suggests a structured dataset designed for research or analysis related to facial recognition, ethnicity classification, or similar applications.  The consistent framing and lighting of the portraits imply a controlled environment for image capture.

The top of the image includes a title stating that the image displays the results of image textual search using OpenCLIP features.  It specifies that the data is from a synthetic dataset and focuses on "typical adult" faces, further clarifying the nature and purpose of the image collection. The consistent ethnicity labels across the dataset suggest a focus on representing diverse ethnic groups, likely for training or testing a machine learning model. The high quality and consistent style of the portraits indicate a careful curation of the dataset.

================================================================================
================================================================================
figures/textual_search_2_Age_x_Ethnicity x Sex 1_top_10_matches.jpg:
====================================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a grid of 192 photographs, meticulously arranged in a 16x12 matrix. Each photograph depicts a person, predominantly Asian females, spanning a wide range of ages. The ages are clearly labeled beneath each image, progressing from "10-month-old baby" to "wrinkly 70-year-old senior."  The images showcase diverse expressions and settings, providing a rich visual representation of age progression within a specific demographic.

Each image is accompanied by a descriptive caption that includes the age, and a filename suggesting that these images are sourced from a synthetic dataset. The consistent format and clear labelling suggest a systematic approach to data organization, likely used for research or machine learning purposes.  The overall structure is highly organized and visually informative, facilitating easy comparison and analysis of facial features across different age groups.  The consistent ethnicity and gender across the images allow for focused study of age-related changes.

================================================================================
================================================================================
figures/flux_images.jpg:
========================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a grid of twelve portraits, arranged in a 4x3 layout.  Each portrait is a headshot of a different person, showcasing a diverse range of ages, ethnicities, and styles. The individuals are presented against varied backgrounds, some simple and some more detailed, with varying lighting conditions. 


The file names displayed under each portrait indicate a structured naming convention, suggesting these images may be part of a larger dataset or project. The filenames are consistent and include prefixes like "FLUX1_pro," "FLUX1_dev," and "FLUX1_schnell," potentially indicating different stages or sources of the images.  Each filename also contains a unique numerical identifier. The overall effect is a visually diverse collection of portraits, organized systematically.

================================================================================
================================================================================
figures/all_model_images.jpg:
=============================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a grid of 72 individual portrait photographs, arranged in a 6x12 matrix. Each portrait shows a different person, exhibiting a wide range of ages, ethnicities, and expressions.  The individuals are presented in various settings, some with plain backgrounds, others with subtle environmental details.  The lighting and photographic styles also vary across the portraits, creating a diverse collection of photographic approaches.  The overall effect is a visually rich representation of human diversity.

Many of the portraits seem professionally taken, with high-quality lighting and composition.  The subjects are clearly the focal point of each image.  There's a blend of posed and candid shots, suggesting different photographic approaches and capturing a range of moods and personalities.  The age range of the subjects is substantial, from young children to elderly individuals, further highlighting the diversity presented.

Above each image is a file name, seemingly indicating the source or dataset from which these images originate.  The file names suggest different projects or image datasets, possibly indicating that the portraits were collected from multiple sources or created using various methods.  The consistent file naming convention, however, suggests organization and cataloging of the images.

================================================================================
================================================================================
create_face_dataset.py:
=======================
#%% Imports

import os
import re
import io
import time
import json
import base64
import random
import requests
import numpy as np
import pandas as pd
from PIL import Image
from tqdm import tqdm
from dotenv import load_dotenv
import asyncio
from tqdm.asyncio import tqdm as async_tqdm
import fal_client
from openai import OpenAI
from stability_sdk import client as sd_client
import stability_sdk.interfaces.gooseai.generation.generation_pb2 as generation

from face_prompt_utils import generate_face_prompt

#%% API Key Configurations

STABILITY_API_KEY = 'sk-abcdefghigklmnopqrstuvwxyz1234567890abcdefghigkl'
OPENAI_API_KEY = 'sk-abcd1234efgh5678ijkl9012mnop3456qrst7890uvwx1234yzab5678cdef9012ghij3456klmn7890opqr1234stuv'
FAL_API_KEY = 'abcdefgh-ijkl-mnop-qrst-uvwxyz123456:a1234567890abcdefghijklmnopqrabc'

def save_env_file():
    """Create a .env file with API keys if it doesn't exist."""

    env_file = '.env'
    if not os.path.exists(env_file):
        print("Creating .env file...")
        with open(env_file, 'w') as f:
            f.write(f"STABILITY_API_KEY={STABILITY_API_KEY}\n")
            f.write(f"OPENAI_API_KEY={OPENAI_API_KEY}\n")
            f.write(f"FAL_KEY={FAL_API_KEY}\n")
        print(f".env file created at {os.path.abspath(env_file)}")
        print("Please edit the .env file with your actual API keys before running the script again.")
        exit()

def setup_api_keys():
    # Load the .env file
    load_dotenv()
    
    # Check if all required keys are present
    required_keys = ['STABILITY_API_KEY', 'OPENAI_API_KEY', 'FAL_KEY']
    missing_keys = [key for key in required_keys if not os.getenv(key)]
    
    if missing_keys:
        print(f"Error: The following API keys are missing in the .env file: {', '.join(missing_keys)}")
        print("Please add them to the .env file and run the script again.")
    else:
        print("API keys loaded successfully.")

# Call the setup function at the beginning of the script
save_env_file()
setup_api_keys()

# print to the screen all the API keys that were loaded
key_name_list = ['STABILITY_API_KEY', 'OPENAI_API_KEY', 'FAL_KEY']
for key_name in key_name_list:
    print(f'{key_name} = {os.getenv(key_name)}')

#%% API clients

openai_client = OpenAI(api_key = os.getenv('OPENAI_API_KEY'))

#%% Constants

SDXL_STYLES = [
    "3d-model", "analog-film", "anime", "cinematic", "comic-book", "digital-art",
    "enhance", "fantasy-art", "isometric", "line-art", "low-poly", "modeling-compound",
    "neon-punk", "origami", "photographic", "pixel-art", "tile-texture"
]

SDXL_STYLES = ["analog-film", "cinematic", "photographic", "enhance"]
DALLE3_IMAGE_SIZES = ["1024x1024", "1024x1792", "1792x1024"]
DALLE3_STYLES = ["vivid", "natural"]
DALLE3_QUALITIES = ["standard", "hd"]
FLUX_IMAGE_SIZES = ["square_hd", "square", "portrait_4_3", "portrait_16_9", "landscape_4_3", "landscape_16_9"]

FLUX_API_MODEL_NAME_DICT = {
    'FLUX1_pro': 'fal-ai/flux-pro',
    'FLUX1_dev': 'fal-ai/flux/dev',
    'FLUX1_schnell': 'fal-ai/flux/schnell'
}

#%% Helper functions

def generate_image_SDXL(prompt, engine_id, cfg_scale, steps, seed, style_preset):
    stability_api = sd_client.StabilityInference(key=os.getenv('STABILITY_API_KEY'), engine=engine_id)

    params = {
        "prompt": prompt,
        "cfg_scale": cfg_scale,
        "steps": steps,
        "seed": seed,
        "style_preset": style_preset
    }

    response = stability_api.generate(**params)

    for resp in response:
        for artifact in resp.artifacts:
            if artifact.type == generation.ARTIFACT_IMAGE:
                return Image.open(io.BytesIO(artifact.binary))

    return None

def generate_image_DALLE3(prompt, size='1024x1024', quality='standard', style='vivid', response_format='url'):
    
    response = openai_client.images.generate(
        model="dall-e-3",
        prompt=prompt,
        size=size,
        quality=quality,
        style=style,
        response_format=response_format
    )
    
    if response_format == "b64_json":
        image_data = base64.b64decode(response.data[0].b64_json)
        image_PIL = Image.open(io.BytesIO(image_data))
    elif response_format == "url":
        image_url = response.data[0].url
        image_data = io.BytesIO(requests.get(image_url).content)
        image_PIL = Image.open(image_data)

    revised_prompt = response.data[0].revised_prompt

    return image_PIL, revised_prompt

def generate_image_FLUX(prompt, api_model_name, seed, num_inference_steps, image_size='square_hd', guidance_scale=3.5):
    
    if api_model_name == 'fal-ai/flux-pro':
        arguments = {
            "prompt": prompt,
            "image_size": image_size,
            "num_inference_steps": num_inference_steps,
            "seed": seed,
            "guidance_scale": guidance_scale,
            "safety_tolerance": "5",
            "sync_mode": True
        }
    elif api_model_name == 'fal-ai/flux/dev':
        arguments = {
            "prompt": prompt,
            "image_size": image_size,
            "num_inference_steps": num_inference_steps,
            "seed": seed,
            "guidance_scale": guidance_scale,
            "enable_safety_checker": False,
            "sync_mode": True
        }
    elif api_model_name == 'fal-ai/flux/schnell':
        arguments = {
            "prompt": prompt,
            "image_size": image_size,
            "num_inference_steps": num_inference_steps,
            "seed": seed,
            "enable_safety_checker": False,
            "sync_mode": True
        }

    handler = fal_client.submit(api_model_name, arguments=arguments)
    result = handler.get()

    image_url = result['images'][0]['url']
    if image_url.startswith('data:image/jpeg;base64,'):
        image_data = io.BytesIO(base64.b64decode(image_url.split(',')[1]))
    else:
        image_data = io.BytesIO(requests.get(image_url).content)

    image_PIL = Image.open(image_data)

    return image_PIL

async def generate_image_FLUX_async(prompt, api_model_name, seed, num_inference_steps=50, image_size='square_hd', guidance_scale=3.5):
    if api_model_name == 'fal-ai/flux-pro':
        arguments = {
            "prompt": prompt,
            "image_size": image_size,
            "num_inference_steps": num_inference_steps,
            "seed": seed,
            "guidance_scale": guidance_scale,
            "safety_tolerance": "5",
            "sync_mode": False
        }
    elif api_model_name == 'fal-ai/flux/dev':
        arguments = {
            "prompt": prompt,
            "image_size": image_size,
            "num_inference_steps": num_inference_steps,
            "seed": seed,
            "guidance_scale": guidance_scale,
            "enable_safety_checker": False,
            "sync_mode": False
        }
    elif api_model_name == 'fal-ai/flux/schnell':
        arguments = {
            "prompt": prompt,
            "image_size": image_size,
            "num_inference_steps": num_inference_steps,
            "seed": seed,
            "enable_safety_checker": False,
            "sync_mode": False
        }

    handler = await fal_client.submit_async(api_model_name, arguments=arguments)
    result = await handler.get()

    image_url = result['images'][0]['url']
    if image_url.startswith('data:image/jpeg;base64,'):
        image_data = io.BytesIO(base64.b64decode(image_url.split(',')[1]))
    else:
        image_data = io.BytesIO(requests.get(image_url).content)

    image_PIL = Image.open(image_data)

    return image_PIL

def generate_image_with_retry(generate_func, max_retries=2, **kwargs):
    for attempt in range(max_retries):
        try:
            return generate_func(**kwargs)
        except Exception as e:
            print(f"Error occurred: {e}")
            if attempt < max_retries - 1:
                wait_time = random.uniform(0.5, 2)
                print(f"Retrying in {wait_time:.2f} seconds...")
                time.sleep(wait_time)
            else:
                print("Max retries reached. Skipping this generation.")
                return None

def get_existing_image_count(image_folder, model_prefix):
    # Regular expression to match the number at the end of the filename
    pattern = re.compile(rf"{re.escape(model_prefix)}_image_(\d+)\.jpg")
    
    max_number = 0
    for filename in os.listdir(image_folder):
        match = pattern.match(filename)
        if match:
            number = int(match.group(1))
            max_number = max(max_number, number)
    
    return max_number

def create_dataset_SDXL(num_samples, image_folder, engine_id, steps, jpeg_quality=90):
    metadata = []
    total_time = 0
    start_index = get_existing_image_count(image_folder, "SDXL")
    
    with tqdm(total=num_samples, desc="Generating SDXL images") as pbar:
        for i in range(num_samples):
            prompt = get_random_prompt()
            style_preset = random.choice(SDXL_STYLES)
            seed = random.randint(0, 2**32 - 1)
            cfg_scale = random.randint(5, 8)
            
            start_time = time.time()
            image = generate_image_with_retry(
                generate_image_SDXL,
                prompt=prompt,
                engine_id=engine_id,
                cfg_scale=cfg_scale,
                steps=steps,
                seed=seed,
                style_preset=style_preset
            )
            end_time = time.time()
            
            if image:
                image_filename = f"SDXL_image_{start_index + i + 1:07d}.jpg"
                image_path = os.path.join(image_folder, image_filename)
                
                image.save(image_path, "JPEG", quality=jpeg_quality)
                
                configs = {
                    "engine_id": engine_id,
                    "cfg_scale": cfg_scale,
                    "steps": steps,
                    "seed": seed,
                    "style_preset": style_preset
                }
                
                metadata.append({
                    "image_filename": image_filename,
                    "model_used": "SDXL",
                    "text_prompt": prompt,
                    "configs": json.dumps(configs),
                })
                
                total_time += (end_time - start_time)
                pbar.update(1)
    
    print(f"SDXL: Generated {num_samples} images in {total_time/60:.2f} minutes (avg: {total_time/num_samples:.2f} seconds per image)")
    return pd.DataFrame(metadata)

def create_dataset_DALLE3(num_samples, image_folder, size, quality, jpeg_quality=90):
    metadata = []
    total_time = 0
    start_index = get_existing_image_count(image_folder, "DALLE3")
    
    with tqdm(total=num_samples, desc="Generating DALL-E 3 images") as pbar:
        for i in range(num_samples):
            prompt = get_random_prompt()
            style = random.choice(DALLE3_STYLES)
            
            start_time = time.time()
            result = generate_image_with_retry(
                generate_image_DALLE3,
                prompt=prompt,
                size=size,
                quality=quality,
                style=style
            )
            end_time = time.time()
            
            if result:
                image, revised_prompt = result
                image_filename = f"DALLE3_image_{start_index + i + 1:07d}.jpg"
                image_path = os.path.join(image_folder, image_filename)
                
                image.save(image_path, "JPEG", quality=jpeg_quality)
                
                configs = {
                    "size": size,
                    "quality": quality,
                    "style": style,
                    "orig_prompt": prompt
                }
                
                metadata.append({
                    "image_filename": image_filename,
                    "model_used": "DALLE3",
                    "text_prompt": revised_prompt,
                    "configs": json.dumps(configs),
                })
                
                total_time += (end_time - start_time)
                pbar.update(1)
    
    print(f"DALL-E 3: Generated {len(metadata)} images in {total_time/60:.2f} minutes (avg: {total_time/len(metadata):.2f} seconds per image)")
    return pd.DataFrame(metadata)

def create_dataset_FLUX(num_samples, flux_model, image_folder, num_inference_steps, image_size, jpeg_quality=90):
    metadata = []
    total_time = 0
    start_index = get_existing_image_count(image_folder, flux_model)
    
    flux_api_model_name = FLUX_API_MODEL_NAME_DICT[flux_model]

    with tqdm(total=num_samples, desc=f"Generating {flux_model} images") as pbar:
        for i in range(num_samples):
            prompt = get_random_prompt()
            seed = random.randint(0, 2**32 - 1)
            guidance_scale = random.uniform(2.5, 4.0) if random.random() < 0.5 else 3.5
            
            start_time = time.time()
            image = generate_image_with_retry(
                generate_image_FLUX,
                prompt=prompt,
                api_model_name=flux_api_model_name,
                seed=seed,
                num_inference_steps=num_inference_steps,
                image_size=image_size,
                guidance_scale=guidance_scale
            )
            end_time = time.time()
            
            if image:
                image_filename = f"{flux_model}_image_{start_index + i + 1:07d}.jpg"
                image_path = os.path.join(image_folder, image_filename)
                
                image.save(image_path, "JPEG", quality=jpeg_quality)
                
                if flux_model == 'FLUX1_pro':
                    configs = {
                        "image_size": image_size,
                        "num_inference_steps": num_inference_steps,
                        "seed": seed,
                        "guidance_scale": guidance_scale,
                        "safety_tolerance": "5",
                    }
                elif flux_model == 'FLUX1_dev':
                    configs = {
                        "image_size": image_size,
                        "num_inference_steps": num_inference_steps,
                        "seed": seed,
                        "guidance_scale": guidance_scale,
                        "enable_safety_checker": False,
                    }
                elif flux_model == 'FLUX1_schnell':
                    configs = {
                        "image_size": image_size,
                        "num_inference_steps": num_inference_steps,
                        "seed": seed,
                        "enable_safety_checker": False,
                    }

                metadata.append({
                    "image_filename": image_filename,
                    "model_used": flux_model,
                    "text_prompt": prompt,
                    "configs": json.dumps(configs),
                })
                
                total_time += (end_time - start_time)
                pbar.update(1)
    
    print(f"{flux_model}: Generated {len(metadata)} images in {total_time/60:.2f} minutes (avg: {total_time/len(metadata):.2f} seconds per image)")
    return pd.DataFrame(metadata)

async def create_dataset_FLUX_parallel(num_samples, flux_model, image_folder, num_inference_steps, image_size, jpeg_quality=90, max_concurrent_calls=5):
    dataset_start_time = time.time()

    metadata = []
    start_index = get_existing_image_count(image_folder, flux_model)    
    flux_api_model_name = FLUX_API_MODEL_NAME_DICT[flux_model]
    semaphore = asyncio.Semaphore(max_concurrent_calls)

    async def process_single_image(i):
        async with semaphore:
            prompt = get_random_prompt()
            seed = random.randint(0, 2**32 - 1)
            guidance_scale = random.uniform(2.5, 4.0) if random.random() < 0.5 else 3.5
            
            sample_start_time = time.time()
            try:
                image = await generate_image_FLUX_async(
                    prompt=prompt,
                    api_model_name=flux_api_model_name,
                    seed=seed,
                    num_inference_steps=num_inference_steps,
                    image_size=image_size,
                    guidance_scale=guidance_scale
                )
            except Exception as e:
                print(f"Error generating image for {flux_model}: {e}")
                return None
            sample_end_time = time.time()
            
            if image:
                image_filename = f"{flux_model}_image_{start_index + i + 1:07d}.jpg"
                image_path = os.path.join(image_folder, image_filename)
                
                image.save(image_path, "JPEG", quality=jpeg_quality)
                
                configs = {
                    "image_size": image_size,
                    "num_inference_steps": num_inference_steps,
                    "seed": seed,
                    "guidance_scale": guidance_scale,
                }
                if flux_model == 'FLUX1_pro':
                    configs["safety_tolerance"] = "5"
                elif flux_model in ['FLUX1_dev', 'FLUX1_schnell']:
                    configs["enable_safety_checker"] = False

                sample_durations_sec = sample_end_time - sample_start_time
                return {
                    "image_filename": image_filename,
                    "model_used": flux_model,
                    "text_prompt": prompt,
                    "configs": json.dumps(configs),
                }
            return None

    tasks = [process_single_image(i) for i in range(num_samples)]    
    results = await async_tqdm.gather(*tasks, desc=f"Generating {flux_model} images")
    metadata = [result for result in results if result is not None]
    total_time = time.time() - dataset_start_time
    print(f"{flux_model}: Generated {len(metadata)} images in {total_time/60:.2f} minutes (avg: {total_time/len(metadata):.2f} seconds per image)")
    
    return pd.DataFrame(metadata)

def update_csv(new_df, csv_path):
    if os.path.exists(csv_path):
        existing_df = pd.read_csv(csv_path)
        combined_df = pd.concat([existing_df, new_df], ignore_index=True)
    else:
        combined_df = new_df
    
    combined_df.to_csv(csv_path, index=False)
    return combined_df

def get_random_prompt():
    output_prompt = generate_face_prompt()
    return output_prompt

#%%

if __name__ == "__main__":

    # Explicit configuration variables
    # output_db_folder = r"datasample_001"
    output_db_folder = r"datasample_002"
    
    os.makedirs(output_db_folder, exist_ok=True)

    call_dev_pro_async = True

    # FLUX1.dev (about 1150 images per 1 hour when async is on, costs ~$29 per 1150 images)
    flux1_dev_samples = 10
    flux1_dev_config = {
        "image_size": "square_hd",
        "num_inference_steps": 50,
        'jpeg_quality': 90
    }

    # FLUX1.pro (about 1100 images per 1 hour when async is on, costs ~$55 per 1100 images)
    flux1_pro_samples = 10
    flux1_pro_config = {
        "image_size": "square_hd",
        "num_inference_steps": 50,
        'jpeg_quality': 90
    }

    # SDXL (about 550 images per 1 hour, costs ~$2 per 550 images)
    sdxl_samples = 10
    sdxl_config = {
        "engine_id": "stable-diffusion-xl-1024-v1-0",
        "steps": 70,
        'jpeg_quality': 90
    }

    # FLUX1.schnell (about 2000 images per 1 hour, costs ~$6 per 2000 images)
    flux1_schnell_samples = 10
    flux1_schnell_config = {
        "image_size": "square_hd",
        "num_inference_steps": 12,
        'jpeg_quality': 90
    }

    # DALL-E 3 (about 233 images per 1 hour, costs ~$8.6 per 233 images)
    dalle3_samples = 10
    dalle3_config = {
        "size": "1024x1024",
        "quality": "standard",
        'jpeg_quality': 90
    }

    # Create the mixed dataset
    image_folder = os.path.join(output_db_folder, "images")
    os.makedirs(image_folder, exist_ok=True)
    csv_path = os.path.join(output_db_folder, "SFHQ_T2I_dataset.csv")
    
    print("\nStarting image generation...\n")
    
    if call_dev_pro_async:
        max_concurrent_calls = 10

        loop = asyncio.get_event_loop()

        if flux1_dev_samples > 0:
            flux1_dev_df = loop.run_until_complete(create_dataset_FLUX_parallel(
                flux1_dev_samples, 'FLUX1_dev', image_folder, max_concurrent_calls=max_concurrent_calls, **flux1_dev_config
            ))
            combined_df = update_csv(flux1_dev_df, csv_path)
            print(f"CSV updated with {len(flux1_dev_df)} FLUX1_dev images")

        if flux1_pro_samples > 0:
            flux1_pro_df = loop.run_until_complete(create_dataset_FLUX_parallel(
                flux1_pro_samples, 'FLUX1_pro', image_folder, max_concurrent_calls=max_concurrent_calls, **flux1_pro_config
            ))
            combined_df = update_csv(flux1_pro_df, csv_path)
            print(f"CSV updated with {len(flux1_pro_df)} FLUX1_pro images")

        loop.close()
    else:
        if flux1_dev_samples > 0:
            flux1_dev_df = create_dataset_FLUX(flux1_dev_samples, 'FLUX1_dev', image_folder, **flux1_dev_config)
            combined_df = update_csv(flux1_dev_df, csv_path)
            print(f"CSV updated with {len(flux1_dev_df)} FLUX1_dev images")

        if flux1_pro_samples > 0:
            flux1_pro_df = create_dataset_FLUX(flux1_pro_samples, 'FLUX1_pro', image_folder, **flux1_pro_config)
            combined_df = update_csv(flux1_pro_df, csv_path)
            print(f"CSV updated with {len(flux1_pro_df)} FLUX1_pro images")

    if flux1_schnell_samples > 0:
        flux1_schnell_df = create_dataset_FLUX(flux1_schnell_samples, 'FLUX1_schnell', image_folder, **flux1_schnell_config)
        combined_df = update_csv(flux1_schnell_df, csv_path)
        print(f"CSV updated with {len(flux1_schnell_df)} FLUX1_schnell images")

    if sdxl_samples > 0:
        sdxl_df = create_dataset_SDXL(sdxl_samples, image_folder, **sdxl_config)
        combined_df = update_csv(sdxl_df, csv_path)
        print(f"CSV updated with {len(sdxl_df)} SDXL images")
    
    if dalle3_samples > 0:
        dalle3_df = create_dataset_DALLE3(dalle3_samples, image_folder, **dalle3_config)
        combined_df = update_csv(dalle3_df, csv_path)
        print(f"CSV updated with {len(dalle3_df)} DALLE3 images")
    
    print("\nDataset creation completed!\n")
    print(f"Total images in the dataset per model:")
    for model in ["SDXL", "DALLE3", "FLUX1_pro", "FLUX1_dev", "FLUX1_schnell"]:
        count = len(combined_df[combined_df['model_used'] == model])
        print(f"- {count} {model} images")
    print(f"Combined total of images: {len(combined_df)}")
    print(f"\nMetadata saved to 'SFHQ_T2I_dataset.csv'")


#%%


================================================================================
================================================================================
figures/textual_search_2_Hats_top_10_matches.jpg:
=================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image displays a grid of 96 small images, each showing a person wearing a hat. The grid is organized into 8 rows and 12 columns.  Above the grid is a title: "Image textual search using OpenCLIP features from synthetic dataset Condition: Hats Prefix: "person wearing"".  This indicates that the images are part of a dataset used for testing an image recognition system's ability to identify people wearing different types of hats.

Each small image within the grid is labeled with the type of hat the person is wearing (e.g., "baseball cap", "fedora", "beanie", "top hat", "cowboy hat", "sun hat") and a filename.  The filenames suggest that the images originate from various sources, possibly including different datasets or image collections.  The hats are diverse in style and color, and the people depicted in the images show a wide range of ages, genders, and ethnicities.

The overall structure of the image is clear and organized, presenting a large sample of images for visual analysis and demonstrating the variety of hat types included in the dataset.  The labeling allows for easy identification of each image's contents and its source.

================================================================================
================================================================================
figures/textual_search_2_Jewelry_top_9_matches.jpg:
===================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image presents a grid of 60 small images, each showing a different person wearing various types of jewelry.  The images are neatly arranged in a 6x10 matrix. Each image is accompanied by a text label indicating the type of jewelry depicted ("gold chain," "pearl necklace," "earrings," "diamond," "crown") and a filename.  The filenames suggest the images are sourced from a large dataset, possibly a synthetic one.

The overall purpose of the image appears to be demonstrating the results of an image textual search using OpenCLIP features. The search condition is specified as "person with" followed by a type of jewelry, showcasing the ability of the system to retrieve relevant images based on textual descriptions.  The variety of people depicted—in terms of age, gender, ethnicity, and style—suggests a diverse and representative dataset was used for training or testing. The images are high-quality and well-lit, suggesting they are likely from a curated collection.

================================================================================
================================================================================
figures/FLUX1_dev_images_with_prompts.jpg:
==========================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a diptych, meaning it's split into two distinct sections side-by-side. Each section contains a portrait photograph with accompanying text describing the subject. 


The left side shows a close-up portrait of a teenage boy, appearing to be of Micronesian descent. He is wearing a newsboy cap, round glasses, and a mustard-yellow jacket. His expression is one of wide-eyed excitement or surprise. The background is blurred, showing a snowy landscape. The descriptive text emphasizes his enthusiastic expression, clothing details, and the lighting used in the photo, highlighting the textures of his skin and clothing.


The right side shows a portrait of a teenage girl, seemingly of Inca descent, wearing a yellow and green basketball uniform. She has a more serious, almost resigned expression. The background is blurred, showing an outdoor sports field at sunset. The descriptive text highlights her expression, the dramatic lighting which casts shadows on one side of her face, her physique, and her attire.

Both portraits are professionally lit and composed, showcasing different photographic styles to emphasize the unique features and mood of each subject. The text provides detailed descriptions intended for image cataloging or metadata purposes, going beyond basic descriptions to include details of ethnicity, age, clothing, and lighting techniques.

================================================================================
================================================================================
figures/textual_search_2_Background_top_10_matches.jpg:
=======================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid showcasing 72 images, arranged in 6 rows and 12 columns. Each image is a portrait-style photograph of a person, set against various backgrounds.  The backgrounds are categorized and labeled above each image column: "urban cityscape," "natural landscape," "stone wall background," "wooden wall background," "beach background," and "night background."

Each image is accompanied by a text label indicating its source, specifying whether it's from the SDXL or FLUX1 dataset and including a unique image identifier. The overall structure is highly organized and systematic, suggesting a data visualization or dataset exploration tool. The images feature a diverse range of people in terms of age, ethnicity, and gender, and the backgrounds are carefully selected to represent the specified categories.  The consistent formatting and clear labeling make it easy to understand the organization and purpose of the image.

================================================================================
================================================================================
figures/textual_search_2_Ethnicity_top_10_matches.jpg:
======================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid of 96 small images, each showing a different person's face. The images are organized into 8 rows and 12 columns.  Each image is labeled with a descriptive term (e.g., "asian," "native american," "african," "persian," "south-american," "irish") and a filename.  The labels suggest that the images are part of a dataset used for training or testing an image recognition model to classify people's ethnicity.

The dataset appears to be diverse, featuring individuals of various ethnic backgrounds, ages, and genders.  The images are generally high-quality portraits, with good lighting and focus. The individuals are presented in a variety of attire and settings, adding to the complexity of the dataset. The consistent labeling and file naming convention suggests a structured and organized approach to data collection and management.

The title "Image textual search using OpenCLIP features from synthetic dataset Condition: Ethnicity" indicates that the images were likely generated synthetically and are being used to evaluate the performance of an OpenCLIP model in a text-based image search context focused on ethnicity.  The presence of a "Prefix" indicating an empty string suggests further experimentation or analysis of the model's ability to classify ethnicity based solely on visual features.

================================================================================
================================================================================
figures/textual_search_2_Hair Style_top_9_matches.jpg:
======================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image presents a grid of 60 photographs, each showing a person with a different hairstyle.  The images are neatly arranged in six rows of ten columns.  Each image is labeled with a descriptive text indicating the hairstyle ("straight hair," "curly hair," "high top hairstyle," "bob-cut hairstyle," "afro hairstyle") and a filename.  The filenames appear to be a structured format indicating the source dataset and a unique image identifier.

The hairstyles showcased are diverse, representing a wide range of textures, lengths, and styles.  There's a good representation of different ethnicities and genders among the individuals depicted.  The photographic style is consistent across the images, with each portrait appearing well-lit and professionally taken, likely from a curated dataset designed for image recognition or AI training.

The title "Image textual search using OpenCLIP features from synthetic dataset Condition: Hair Style" indicates the purpose of the image compilation. It suggests the images are used to test or demonstrate the capabilities of a text-based image search system (OpenCLIP) in identifying and categorizing different hairstyles within a synthetic dataset.  The prefix "Prefix: " suggests the possibility of further filtering or specification in the search.

================================================================================
================================================================================
figures/textual_search_1_expression_top_8_matches.jpg:
======================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a large grid composed of 96 smaller squares, each containing a different portrait photograph of a person's face. Each portrait is labeled with metadata indicating the image source and a classification of the person's facial expression.  The expressions are categorized into eight distinct emotional categories: happy, sad, angry, surprised, neutral, disgusted, fearful, and tongue out.

Each row of the grid presents eight images, showcasing the variety of faces expressing a single emotion. For instance, the first row shows eight different individuals exhibiting happiness, ranging in age, ethnicity, and the intensity of their smiles. This structure facilitates a visual comparison of how diversely individuals express the same emotion.  The metadata helps to track the source of each image, indicating different image datasets used for the compilation.

The overall composition is highly organized, creating a systematic representation of facial expressions. This structure is likely intended for research or analysis purposes, perhaps related to facial expression recognition, emotion AI, or similar fields. The diversity of individuals depicted suggests the effort to create a robust and representative dataset for training or testing algorithms.

================================================================================
================================================================================
figures/textual_search_2_Face Pose_top_10_matches.jpg:
======================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a grid of 70 images, each showcasing a different person's face.  The images are meticulously organized and labeled, demonstrating a systematic approach to data organization. Each image has a caption indicating the image's source (e.g., "FLUX1_schnell_image_0017649.jpg") and a description of the person's pose (e.g., "looking straight ahead," "turned sideways," "tilted upwards," "tilted downwards," "three-quarter view," "profile view").

The image grid is structured to show variations in head pose.  The top rows primarily feature faces looking straight ahead, followed by rows depicting faces turned sideways, then tilted upwards, tilted downwards, three-quarter views, and finally, profile views. This arrangement facilitates a visual comparison of how facial features change with different head orientations.  The images are diverse, featuring people of various ethnicities, ages, and genders, adding to the dataset's comprehensiveness.

The overall design suggests the image is a sample from a larger dataset used for research or development, likely in the field of computer vision or artificial intelligence. The clear labeling and structured organization indicate a focus on data quality and consistency, crucial elements for training machine learning models that can accurately recognize and interpret facial poses. The title "Image textual search using OpenCLIP features from synthetic dataset" further confirms this purpose, highlighting the use of this image collection in an image retrieval system based on textual descriptions.

================================================================================
================================================================================
figures/textual_search_2_Accessories_top_10_matches.jpg:
========================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image displays a large grid of 144 small images, each featuring a person's head and shoulders.  Each image is labeled with a filename and a descriptive tag indicating the type of accessory the person is wearing (e.g., "earrings," "necklace," "bandana," "hat," "tie," "scarf," "headphones," "sunglasses"). The arrangement is very organized, with the same accessory type grouped together in blocks of 12 images, then repeated for each accessory type.

The images themselves appear to be high-quality photographs with diverse subjects in terms of ethnicity, gender, age, and style.  The backgrounds vary, but many seem to be neutral or simple to keep the focus on the individual and their accessory. The lighting and composition of the photos are consistent, suggesting they may be from a curated dataset created for image recognition or similar machine learning tasks.

Above the grid, text indicates that the images are part of a synthetic dataset used for image textual search using OpenCLIP features.  The condition specifies "Accessories," and the prefix for the search is "person wearing."  This context suggests the purpose of the image is to visually demonstrate the capabilities of a system capable of identifying accessories worn by people in images.

================================================================================
================================================================================
merge_dataset_folder.py:
========================
#%% Imports

import os
import time
import shutil
import pandas as pd
from tqdm import tqdm

#%% Helper functions

def merge_datasets(source_folders, output_folder):
    # Create output folders
    output_image_folder = os.path.join(output_folder, "images")
    os.makedirs(output_image_folder, exist_ok=True)
    
    # Initialize a list to store all metadata
    all_metadata = []
    
    # Initialize counters for each model
    model_counters = {
        "SDXL": 1,
        "DALLE3": 1,
        "FLUX1_pro": 1,
        "FLUX1_dev": 1,
        "FLUX1_schnell": 1
    }
    
    start_time = time.time()
    # Process each source folder
    for source_folder in source_folders:
        source_image_folder = os.path.join(source_folder, "images")
        source_csv_path = os.path.join(source_folder, "SFHQ_T2I_dataset.csv")
        
        # Read the CSV file
        df = pd.read_csv(source_csv_path)
        
        # Get all image files in the source folder
        image_files = [f for f in os.listdir(source_image_folder) if f.endswith('.jpg')]
        
        print(f"Processing '{source_folder}' ...")
        for image_file in tqdm(image_files):
            # Find the corresponding metadata
            metadata_row = df[df['image_filename'] == image_file]
            
            if not metadata_row.empty:
                model = metadata_row['model_used'].iloc[0]
                new_image_name = f"{model}_image_{model_counters[model]:07d}.jpg"
                
                # Copy and rename the image
                shutil.copy(
                    os.path.join(source_image_folder, image_file),
                    os.path.join(output_image_folder, new_image_name)
                )
                
                # Update metadata
                new_metadata = {
                    'image_filename': new_image_name,
                    'model_used': model,
                    'text_prompt': metadata_row['text_prompt'].iloc[0],
                    'configs': metadata_row['configs'].iloc[0]
                }
                all_metadata.append(new_metadata)
                
                # Increment the counter for this model
                model_counters[model] += 1
    
    # Create the final dataframe and save it
    final_df = pd.DataFrame(all_metadata)
    final_df = final_df.sort_values(by='image_filename').reset_index(drop=True)
    final_csv_path = os.path.join(output_folder, "SFHQ_T2I_dataset.csv")
    final_df.to_csv(final_csv_path, index=False)
    
    total_duration_minutes = (time.time() - start_time) / 60
    print(f"\nDataset merging completed! Total duration: {total_duration_minutes:.2f} minutes")
    print(f"Total images in the merged dataset per model:")
    for model, count in model_counters.items():
        print(f"- {count - 1} {model} images")
    print(f"Combined total of images: {len(final_df)}")
    print(f"\nMetadata saved to '{final_csv_path}'")


#%% Usage

if __name__ == "__main__":

    source_folders = [
        r"datasample_001",
        r"datasample_002",
        r"datasample_003",
    ]

    output_folder = r"merged_clean_dataset"
    
    merge_datasets(source_folders, output_folder)

#%%


================================================================================
================================================================================
figures/textual_search_2_Hair_Color_top_10_matches.jpg:
=======================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid of 72 small images, each showing a person with a distinct hair color. The images are organized into rows and columns, with each row representing a different hair color category (white or gray, yellow or blond, green, blue, purple or pink, red or orange).  Each image is labeled with a descriptive caption indicating the hair color and the source filename, which includes either "SDXL_image" or "FLUX1_schnell_image" or "FLUX1_dev_image" prefix suggesting they are sourced from different datasets.

The overall purpose of the image appears to be to showcase a collection of images categorized by hair color, likely used for training or testing a computer vision model. The consistent format of the filenames and captions points toward an automated process for generating the image grid, possibly from a larger database of images. The variety of hairstyles, hair textures, and skin tones within each hair color category indicates a diverse dataset. The title "Image textual search using OpenCLIP features from synthetic dataset" further suggests the images are used for research and development related to image retrieval based on textual descriptions.

================================================================================
================================================================================
figures/textual_search_2_Expression_x_Sex_top_10_matches.jpg:
=============================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a grid of 72 small square images, each showing a different man's face displaying a variety of expressions. The images are neatly arranged in 9 rows and 8 columns.  Each image is labeled with a descriptive text phrase above it (e.g., "angry or enraged," "surprised," "smiling," "sad or depressed," "grim face," "tounge out") and a file name below, indicating the source and identification of the image within a larger dataset.

The expressions depicted range from intense emotions like anger and sadness to more neutral ones like surprise or a simple smile. Some images show extreme expressions, while others have more subtle nuances. The men in the images are diverse in age, ethnicity, and hairstyle, adding to the variety.  The background of each individual image varies, with some having simple backgrounds and others showing more detail.  The overall effect is a comprehensive visual representation of a wide range of male facial expressions.

The title at the top, "Image textual search using OpenCLIP features from synthetic dataset Condition: Expression x Sex Prefix: 'man'," explains the purpose of the image collection, which is to demonstrate the effectiveness of a specific image search technique (OpenCLIP) in classifying and retrieving images based on facial expressions in a dataset specifically focused on male faces.

================================================================================
================================================================================
figures/textual_search_2_Hair Style x Sex_top_10_matches.jpg:
=============================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid of 72 small images, each showing a woman's face.  The images are organized into rows and columns, with each row representing a different hair color and length category (short blond, long blond, short red, long red, short black, long black).  Each image is labeled with a descriptive text indicating the hair type ("short blond hair," "long red hair," etc.) and a filename.

The purpose of the image is to showcase the results of an image textual search using OpenCLIP features from a synthetic dataset.  The search query is implied by the title: "Image textual search using OpenCLIP features from synthetic dataset Condition: Hair Style x Sex Prefix: "woman with "".  The image demonstrates the ability of the system to retrieve images of women based on specific hair characteristics.

The filenames suggest that the images come from several different sources or datasets (FLUX1_pro, FLUX1_schnell, SDXL).  The uniform size and presentation of the images create a clean, organized visualization of the search results, clearly demonstrating the effectiveness of the image search algorithm in retrieving relevant images based on textual descriptions.

================================================================================
================================================================================
figures/textual_search_2_Age_x_Ethnicity x Sex 2_top_10_matches.jpg:
====================================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid showcasing a diverse collection of images, each depicting an African male at different ages, ranging from infancy to old age.  The grid is organized systematically, with each row representing a distinct age group (baby, toddler, teenager, adult, senior). Within each age group, there are multiple images displaying variations in facial expressions, hairstyles, and clothing.  Each image is accompanied by a caption that specifies the age and filename.

The captions consistently use a specific naming convention: "age" + "description" + "filename". For example, "10 month old baby" SDXL_image_0009277.jpg. This systematic naming and organization suggest the images are part of a structured dataset used for machine learning or image recognition purposes.  The top of the image indicates that the images are results from a textual image search using OpenCLIP features from a synthetic dataset, with a specific condition for "African male" across different ages.  The dataset appears to be carefully curated to ensure representation of various ages and appearances within the specified demographic.

================================================================================
================================================================================
LICENSE.txt:
============
MIT License

Copyright (c) 2024 David Beniaguev

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

================================================================================
================================================================================
figures/model_distribution.jpg:
===============================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image presents a bar chart visualizing the distribution of images across different models. The chart's title clearly states, "Distribution of Images Across Models."  The horizontal axis (x-axis) labels the various models: FLUX1_schnell, SDXL, FLUX1_dev, FLUX1_pro, and DALL-E3. The vertical axis (y-axis) represents the "Number of Images," ranging from 0 to 60,000.

The chart shows a significant disparity in the number of images per model.  FLUX1_schnell and SDXL have substantially more images than the other models, with FLUX1_schnell having the highest count (approximately 58,034) and SDXL having a slightly lower count (approximately 53,087).  The remaining models, FLUX1_dev, FLUX1_pro, and DALL-E3, show progressively fewer images, with DALL-E3 having the least (approximately 1,123).  Each bar is labeled with its corresponding numerical value, indicating the precise number of images for each model.  The chart uses a light teal color for the bars, which contrasts well against the dark background, making the data easy to read and interpret.

================================================================================
================================================================================
figures/textual_search_2_Glasses Style_top_10_matches.jpg:
==========================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a grid showcasing a large number of images, arranged in a matrix of 6 columns and 8 rows. Each image depicts a person wearing a different style of glasses or sunglasses.  The images are neatly organized, and each is labeled with the type of eyewear ("round glasses," "square glasses," "cat-eye glasses," "rimless glasses," "aviator sunglasses," "sport sunglasses") and the filename of the source image (e.g., "FLUX1_schnell_image_0053027.jpg"). The filenames suggest the images come from multiple datasets, possibly labeled and organized for machine learning purposes.

The image's purpose is to illustrate the results of an image textual search using OpenCLIP features from a synthetic dataset. The text at the top explains this, indicating that the search was conditioned on a phrase related to glasses styles ("person wearing"). The careful organization and labeling suggest a demonstration of a successful image retrieval system, where input text queries are used to find relevant images from a large database.  The variety of glasses styles and the diversity of faces in the images makes the dataset seem quite comprehensive.

================================================================================
================================================================================
figures/textual_search_2_Eye Gaze_top_9_matches.jpg:
====================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image displays a grid of 60 face images, organized into six rows and ten columns. Each image shows a person's head and shoulders, exhibiting various facial expressions and gazes.  The images are clearly labeled with descriptive text indicating the direction of the person's gaze (looking directly at camera, looking to the left, looking up, looking down, eyes closed) and the source of the image (SDXL_image, FLUX1_schnell_image, FLUX1_dev_image, FLUX1_pro_image) along with a unique image identifier (e.g., SDXL_image_0011769.jpg).

The overall structure is designed to showcase the results of an image textual search using OpenCLIP features from a synthetic dataset. The search condition "Eye Gaze" and the prefix "person" are clearly stated at the top, indicating the parameters used for the image retrieval process. The diversity of faces, ethnicities, ages, and genders within the image grid suggests a robust and representative dataset was used for the search.  The consistent labeling provides clear metadata for each image, allowing for easy analysis of the search results.

================================================================================
================================================================================
face_prompt_utils.py:
=====================
#%%

import random
import numpy as np

#%% data

lighting_descriptions_dict = {
    'Natural Lighting': [
        "Illuminated by the soft glow of natural sunlight",
        "Bathed in the gentle embrace of daylight",
        "Highlighted by the subtle nuances of natural light",
    ],
    'Artificial Lighting': [
        "Enhanced by the steady gleam of artificial light sources",
        "Illuminated by the consistent radiance of man-made lighting",
        "Under the clear influence of artificial luminance",
        "Bathed in the consistent glow of artificial light sources",
        "Illuminated by artificial lights that replicate natural tones",
        "Highlighted by the controlled ambiance of artificial lighting",
    ],
    'Side Lighting': [
        "With one side of the face distinctly lit, casting a dramatic play of shadows",
        "Highlighted predominantly from one side, creating a depth of contrasts",
        "Illuminated from the side, accentuating textures and contours",
        "Illuminated from the side, creating stark contrasts of light and shadow",
        "With one side of the face brightly lit and the other in deep shadow",
        "Defined by the dramatic interplay of light and shadow from the side",
    ],
    'Loop Lighting': [
        "With a soft shadow gracing the cheek from loop lighting",
        "Characterized by the distinct loop shadow on the cheek",
        "Illuminated with the classic loop lighting technique, casting a subtle shadow",
        "Characterized by the small shadow of the nose creating a loop on the cheek",
        "Highlighted with a loop lighting technique, casting subtle shadows",
        "With the signature loop shadow adding depth and dimension",
    ],
    'Butterfly Lighting': [
        "With a soft shadow under the nose reminiscent of a butterfly",
        "Illuminated from above, casting a butterfly-like shadow",
        "With the characteristic shadow of butterfly lighting under the nose",
        "Defined by the butterfly-shaped shadow under the nose",
        "Illuminated with the classic butterfly lighting technique, creating an elegant effect",
        "Bathed in light from above, casting a distinct butterfly shadow",
    ],
    'Backlighting': [
        "With light emanating from the back, creating a halo effect",
        "Highlighted from behind, casting a gentle glow around the silhouette",
        "Illuminated predominantly from the back, emphasizing the contours",
    ],
    'Soft/Diffused Lighting': [
        "Bathed in the gentle, diffused light that softens features",
        "Under the soft glow that evenly illuminates without harsh shadows",
        "Illuminated with a diffused light, creating a dreamy ambiance",
    ],
    'Golden Hour': [
        "Basked in the warm, golden tones of the hour before sunset",
        "Golden hour light bathes the scene, casting a warm glow and elongating shadows",
        "Illuminated by the magical soft light of the golden hour",
        "Under the enchanting, warm glow of the golden hour",
    ],
    'Blue Hour': [
        "Surrounded by the cool, ethereal tones of the blue hour",
        "Illuminated by the serene, bluish hue of the hour after sunset",
        "Under the mystical, twilight ambiance of the blue hour",
    ],
    'Split Lighting': [
        "With half the face brightly lit and the other in shadow, creating a split effect",
        "Defined by the dramatic contrast of light and shadow on the face",
        "Illuminated with a split lighting technique, emphasizing duality",
    ],
    'Rembrandt Lighting': [
        "Characterized by the distinct triangle of light on the cheek",
        "Illuminated in the classic Rembrandt style, balancing light and shadow",
        "With the signature Rembrandt triangle gracing the face",
    ],
    'Top-Down Lighting': [
        "Illuminated from above, casting definitive shadows below features",
        "The overhead fluorescent lights cast a cool, even tone over the scene, adding a modern urban vibe",
        "With light source directly overhead, emphasizing depth",
        "Bathed in the light coming from the top, creating a theatrical effect",
    ],
    'Broad Lighting': [
        "With the face predominantly lit on its broadest side, emphasizing width",
        "Highlighted with broad lighting, casting minimal shadows",
        "Illuminated in a manner that enhances the face's broader perspective",
    ],
    'Short Lighting': [
        "With the face illuminated from its narrow side, emphasizing depth and contour",
        "Defined by the short lighting technique, adding drama and intensity",
        "Highlighted in a way that captures the face's contours and depth",
    ],
    'Flash Lighting': [
        "Illuminated with a sharp burst of light, capturing vivid details",
        "Defined by the sudden, bright illumination from a flash",
        "With features crisply lit by the distinct light of a flash",
    ],
    'Ambient Lighting': [
        "Surrounded by the soft, even tones of ambient light",
        "Basked in the gentle and consistent glow of ambient lighting",
        "With the nuances highlighted by the surrounding ambient light",
    ],
    'Directional Lighting': [
        "Illuminated with light coming from a specific direction, emphasizing depth",
        "Defined by the strong, unidirectional light source",
        "With shadows and highlights created by a clear directional light",
    ],
    'Fill Lighting': [
        "Balanced with fill light to soften shadows and even out contrasts",
        "With features gently lit by fill lighting, reducing harshness",
        "Softened by the subtle effects of fill light, creating harmony",
    ],
    'High Key Lighting': [
        "Surrounded by an abundance of bright light, reducing harsh shadows",
        "With features softly lit by high key lighting, evoking an airy atmosphere",
        "Illuminated in a manner that minimizes contrast and shadow, typical of high key lighting",
    ],
    'Low Key Lighting': [
        "Engulfed in deep shadows and minimal light, creating a moody ambiance",
        "Defined by the stark contrast and drama of low key lighting",
        "With features accentuated by the intense interplay of light and dark characteristic of low key lighting",
    ],
    'Motivated Lighting': [
        "Illuminated in a way that feels organic and inspired by elements within the scene",
        "With lighting that seems naturally sourced from items in the environment",
        "Highlighted by light that appears to have a clear, believable source within the context",
    ],
    'Practical Lighting': [
        "Lit by visible light sources present in the scene like lamps or candles",
        "With the warm and genuine glow from practical lights setting the tone",
        "Bathed in the authentic luminescence of actual light fixtures within the shot",
    ],
    'Bounced Lighting': [
        "Softly illuminated by light that's been reflected off surfaces, reducing harshness",
        "With a gentle and even glow resulting from bounced light",
        "Highlighted by the diffused and broadened effect of light that's been redirected",
    ],
    'Hard Lighting': [
        "Defined by the sharp shadows and bright highlights of direct light",
        "With features crisply lit by a focused light source, creating strong contrasts",
        "Illuminated in a manner that emphasizes texture and form through hard light",
    ],
    'Three-Point Lighting': [
        "Illuminated with the classic three-point setup, balancing key, fill, and back lights",
        "Defined by the harmony of three-point lighting, creating depth and texture",
        "With key, fill, and back lights working in concert to create a rich visual experience",
        "Highlighted by the versatility of three-point lighting, offering a balanced look",
    ],
    'Flat Lighting': [
        "Bathed in flat lighting that minimizes shadows",
        "With even illumination across the face, characteristic of flat lighting",
        "Illuminated in a manner that reduces shadow and contrast, typical of flat lighting",
        "Highlighted by the soft and shadowless effect of flat lighting",
    ],
    'Rim Lighting': [
        "With the edges softly outlined by rim lighting",
        "Illuminated from behind, creating a distinct rim of light",
        "Defined by the ethereal outline created by rim lighting",
        "Highlighted by the radiant border of rim lighting",
    ],
    'Clamshell Lighting': [
        "With features softly lit by the dual glow of clamshell lighting",
        "Defined by the flattering, even light of a clamshell setup",
        "Illuminated with clamshell lighting, producing soft and beauty-enhancing effects",
        "Highlighted by the glamour-inducing clamshell lighting technique",
    ],
    'Cross Lighting': [
        "Illuminated by lights from opposite sides, creating dynamic contrasts",
        "With textures and dimensions emphasized by the effects of cross lighting",
        "Defined by the interplay of dual light sources in a cross lighting setup",
        "With features enriched by the opposing forces of cross lighting",
    ],
    'Kicker Lighting': [
        "With a subtle highlight along the edge from kicker lighting",
        "Illuminated by a low-angle kicker light, adding depth",
        "Defined by the accentuating edge glow of kicker lighting",
        "Highlighted by the low and side-angled kicker light",
    ],
    'Cinematic Lighting': [
        "Illuminated in a cinematic style, evoking mood and atmosphere",
        "With the dramatic flair commonly found in cinematic lighting setups",
        "Defined by the atmospheric depth of cinematic lighting",
        "Surrounded by the emotional ambiance characteristic of cinematic lighting",
    ],
    'Stage Lighting': [
        "Lit with the broad and dynamic range of stage lighting",
        "With features emphasized by theatrical stage lights",
        "Defined by the vibrant and dramatic nature of stage lighting",
        "Bathed in the spotlight, typical of stage lighting setups",
    ],
    'Beauty Dish Lighting': [
        "With features softly lit by the focused glow of a beauty dish",
        "Illuminated by the flattering and directional light of a beauty dish",
        "Defined by the unique soft yet focused light of a beauty dish",
        "Highlighted by the beauty-enhancing qualities of beauty dish lighting",
    ],
    'Tungsten Lighting': [
        "Bathed in the warm, yellow-orange glow of tungsten lighting",
        "Illuminated by the classic, warm tones of a tungsten light source",
        "With features highlighted by the cozy atmosphere created by tungsten lighting",
        "Defined by the nostalgic and warm feel of tungsten lighting",
    ],
}

ethnicities_dict = {
    'European': [
        'Austrian', 'Portuguese', 'Russian', 'German', 'French', 'English', 'Swedish', 'Danish', 'Norwegian', 
        'Polish', 'Lithuanian', 'Hungarian', 'Italian', 'Spanish', 'Irish', 'Greek', 'Canadian', 'Romanian', 
        'Serbian', 'Croatian', 'Belgian', 'Icelandic', 'Swiss', 'Luxembourgish', 'Maltese', 'Andorran', 
        'Monacan', 'Liechtensteiner', 'San Marinese', 'Vatican', 'Maltese', 'Finnish', 'Latvian', 'Estonian', 
        'Macedonian', 'Albanian', 'Bosnian', 'Kosovar', 'Montenegrin', 'Moldovan', 'Bulgarian', 'Czech', 
        'Slovak', 'Armenian', 'Azerbaijani', 'Belarusian', 'Ukrainian', 'British', 'Dutch', 'Slovenian', 
        'Estonian', 'Cypriot'
    ],
    'Sub-Saharan African': [
        'Nigerian', 'Kenyan', 'Ghanaian', 'Ethiopian', 'South African', 'Congolese', 'Somali', 'Ugandan',
        'Tanzanian', 'Rwandan', 'Malawian', 'Zambian', 'Zimbabwean', 'Angolan', 'Botswanan', 'Madagascan', 'Gabonese',
        'Namibian', 'Senegalese', 'Cameroonian', 'Ivorian', 'Guinean', 'Liberian', 'Sierra Leonean', 'Burkinabe',
        'Malian', 'Togolese', 'Beninese', 'Nigerien', 'Chadian', 'Central African', 'South Sudanese', 'Eritrean', 'Djiboutian',
        'Comoran', 'Seychellois', 'Mauritian', 'Cape Verdean'
    ],
    'Middle Eastern': [
        'Israeli', 'Iraqi', 'Egyptian', 'Iranian', 'Afghan', 'Arab', 'Turkish', 'Persian', 'Georgian', 
        'Yemeni', 'Saudi Arabian', 'Lybian', 'Jordanian', 'Syrian', 'Lebanese', 'Omani', 'Palestinian', 
        'Qatari', 'Emirati', 'Bahraini', 'Kuwaiti', 'Azerbaijani', 'Armenian'
    ],
    'Latin American': [
        'Brazilian', 'Mexican', 'Argentinian', 'Colombian', 'Peruvian', 'Chilean', 'Ecuadorian', 'Bolivian', 
        'Venezuelan', 'Uruguayan', 'Paraguayan', 'Costa Rican', 'Panamanian', 'Nicaraguan', 'Guatemalan', 
        'Salvadoran', 'Honduran', 'Cuban', 'Dominican', 'Puerto Rican'
    ],
    'Oceanian': [
        'Australian', 'New Zealander', 'Fijian', 'Samoan', 'Tongan', 'Papuan', 'Guamanian', 'Palauan', 
        'Micronesian', 'Marshallese', 'Nauruan', 'Solomon Islander', 'Vanuatuan', 'Ni-Vanuatu', 'New Caledonian', 
        'French Polynesian', 'Tokelauan', 'Tuvaluan', 'Wallisian', 'Futunan'
    ],
    'Caribbean': [
        'Cuban', 'Jamaican', 'Haitian', 'Dominican', 'Trinidadian', 'Barbadian', 'Bahamian', 'Grenadian', 
        'Saint Lucian', 'Antiguan', 'Vincentian', 'Kittitian', 'Nevisian', 'Montserratian', 'Bermudian'
    ],
    'Central Asian': [
        'Uzbek', 'Kazakh', 'Tajik', 'Turkmen', 'Kyrgyz', 'Uzbekistani', 'Turkistani', 'Uyghur', 'Tatar', 
        'Karakalpak', 'Bashkir', 'Kumyk', 'Balkar', 'Karachay', 'Avar'
    ],
    'West Asian': [
        'Armenian', 'Azerbaijani', 'Georgian', 'Turkish', 'Kurdish', 'Assyrian', 'Alevi', 'Druze', 'Yazidi', 
        'Maronite', 'Alawite', 'Circassian', 'Laz', 'Gilaki', 'Mazandarani'
    ],
    'North African': [
        'Egyptian', 'Moroccan', 'Algerian', 'Tunisian', 'Libyan', 'Sudanese', 'Mauritanian', 'Berber', 'Amazigh', 
        'Nubian', 'Coptic', 'Tuareg', 'Riffian', 'Kabyle', 'Sahrawi'
    ],
    'Scandinavian': [
        'Swedish', 'Norwegian', 'Danish', 'Icelandic', 'Finnish', 'Sami', 'Faroese', 'Gotlander', 'Orcadian', 
        'Shetlander', 'Ålandic', 'Jämtlander'
    ],
    'North American': [
        'American', 'Canadian', 'Mexican', 'Greenlandic', 'Alaskan', 'Texan', 'Quebecois', 'Cajun', 'Hawaiian', 
        'Newfoundlander', 'Cree', 'Inuvialuit', 'Métis', 'Gwich’in', 'Haida', 'Tlingit'
    ],
    'Arctic': [
        'Inuit', 'Saami', 'Chukchi', 'Yupik', 'Aleut', 'Kalaallit', 'Nenets', 'Khanty', 'Evenki', 'Selkup', 
        'Yamalo', 'Enets', 'Nganasan', 'Veps', 'Koryaks'
    ],
    'Southeast Asian': [
        'Thai', 'Laos', 'Cambodian', 'Malaysian', 'Filipino', 'Indonesian', 'Burmese', 'Singaporean', 
        'Vietnamese', 'Bruneian', 'Timorese', 'Javanese', 'Balinese', 'Sundanese', 'Minangkabau'
    ],
    'Balkan': [
        'Bulgarian', 'Greek', 'Romanian', 'Serbian', 'Croatian', 'Bosnian', 'Slovenian', 'Montenegrin', 
        'Macedonian', 'Albanian', 'Kosovar', 'Vlach', 'Pomak', 'Torlakian', 'Gagauz', 'Aromanian'
    ],
    'Polynesian': [
        'Hawaiian', 'Maori', 'Samoan', 'Tongan', 'Tahitian', 'Niuean', 'Tokelauan', 'Tuvaluan', 'Rapanui',
        'Marquesan', 'Mangarevan', 'Pukapukan', 'Rarotongan', 'Tahitian', 'Tuamotuan', 'Rennell Islander'
    ],
    'Micronesian': [
        'Guamanian', 'Palauan', 'Marshallese', 'Nauruan', 'Micronesian', 'Kosraean', 'Yapese', 'Pohnpeian', 
        'Chuukese', 'Angauran', 'Sonsorolese', 'Tobi Islander', 'Woleaian', 'Ulithian', 'Carolinian'
    ],
    'Melanesian': [
        'Fijian', 'Papuan', 'Vanuatuan', 'Solomon Islander', 'Ni-Vanuatu', 'New Caledonian', 'Kanak', 'Bougainvillean',
        'Ambrym Islander', 'Aniwa Islander', 'Futuna Islander', 'Erromango Islander', 'Tannese', 'Motu', 'Tolai'
    ],
    'Indigenous American': [
        'Navajo', 'Mayan', 'Inca', 'Guarani', 'Mapuche', 'Quechua', 'Aymara', 'Taino', 'Kuna', 'Wayuu',
        'Cherokee', 'Lakota', 'Apache', 'Iroquois', 'Zapotec', 'Mixtec', 'Quechuan'
    ],
    'Australasian': [
        'Australian', 'New Zealander', 'Papuan', 'Melanesian', 'Polynesian', 'Micronesian',
        'Torres Strait Islander', 'Tiwi Islander', 'Anangu', 'Noongar', 'Palawa', 'Yolngu', 'Koori'
    ],
    'Caucasian': [
        'Georgian', 'Chechen', 'Dagestani', 'Armenian', 'Azerbaijani', 'Abkhaz', 'Ossetian', 'Circassian',
        'Ingush', 'Kabardian', 'Balkar', 'Karachay', 'Lezgian', 'Aghul', 'Tabasaran'
    ],
    'East Asian': [
        'Chinese', 'Vietnamese', 'Japanese', 'Korean', 'Mongolian', 'Taiwanese',
        'Hong Kongese', 'Macanese', 'Ryukyuan', 'Ainu', 'Hui', 'Uighur'
    ],
    'South Asian': [
        'Indian', 'Pakistani', 'Bangladeshi', 'Sri Lankan', 'Nepalese', 'Bhutanese', 'Maldivian',
        'Sinhalese', 'Tamil', 'Pashtun', 'Sindhi', 'Punjabi', 'Gujarati'
    ],
}

eye_colors_dict = {
    "European": [
        "blue", "green", "gray", "hazel", "brown", "amber", "ice-blue", "steel gray",
        "sea green", "pale blue", "deep blue", "emerald green", "light brown", "dark brown",
        "grey-blue", "hazel-green", "turquoise", "aqua", "violet", "olive"
    ],
    "Sub-Saharan African": [
        "dark brown", "black", "amber", "deep brown", "onyx", "chocolate brown",
        "copper", "golden", "honey colored"
    ],
    "Middle Eastern": [
        "brown", "dark brown", "black", "hazel", "amber", "green", "blue",
        "honey colored", "golden", "hazel-green"
    ],
    "Latin American": [
        "brown", "dark brown", "black", "hazel", "amber", "green", "blue", "gray",
        "honey colored", "golden", "copper"
    ],
    "Oceanian": [
        "dark brown", "black", "light brown", "amber", "hazel", "deep brown"
    ],
    "Caribbean": [
        "brown", "dark brown", "black", "hazel", "amber", "green", "honey colored"
    ],
    "Central Asian": [
        "brown", "dark brown", "black", "hazel", "amber", "green", "blue", "gray"
    ],
    "West Asian": [
        "brown", "dark brown", "black", "hazel", "amber", "green", "blue", "gray"
    ],
    "North African": [
        "brown", "dark brown", "black", "hazel", "amber", "green", "honey colored"
    ],
    "Scandinavian": [
        "blue", "ice-blue", "gray", "green", "pale blue", "light brown", "icy grey",
        "sea green", "turquoise", "aqua"
    ],
    "North American": [
        "blue", "green", "gray", "hazel", "brown", "amber", "dark brown", "black",
        "hazel-green", "light brown", "honey colored"
    ],
    "Arctic": [
        "dark brown", "black", "hazel", "deep brown"
    ],
    "Southeast Asian": [
        "dark brown", "black", "light brown", "amber", "hazel", "copper"
    ],
    "Balkan": [
        "brown", "dark brown", "black", "hazel", "green", "blue", "gray", "amber"
    ],
    "Polynesian": [
        "dark brown", "black", "light brown", "amber", "deep brown"
    ],
    "Micronesian": [
        "dark brown", "black", "light brown", "deep brown"
    ],
    "Melanesian": [
        "dark brown", "black", "light brown", "amber", "deep brown"
    ],
    "Indigenous American": [
        "dark brown", "black", "light brown", "amber", "hazel", "copper"
    ],
    "Australasian": [
        "dark brown", "black", "light brown", "amber", "hazel", "blue", "green",
        "gray", "honey colored"
    ],
    "Caucasian": [
        "brown", "dark brown", "black", "hazel", "green", "blue", "gray", "amber",
        "hazel-green", "light brown"
    ],
    "East Asian": [
        "dark brown", "black", "light brown", "amber", "copper"
    ],
    "South Asian": [
        "dark brown", "black", "light brown", "amber", "hazel", "copper", "honey colored"
    ],
    "Interesting Colors": [
        'blue', 'green', 'teal', 'hazel', 'brown', 'amber', 'ice-blue', 'steel gray',
        'cyan', 'violet', 'red', 'pink', 'orange', 'yellow', 'gold', 'silver', 'black',
        'white', 'gray', 'purple', 'turquoise', 'aqua', 'emerald', 'sapphire', 'ruby',
        'topaz', 'amethyst', 'jade', 'opal', 'pearl', 'sable', 'copper', 'bronze', 'brass',
        'platinum', 'rose gold', 'champagne', 'mahogany', 'caramel', 'cinnamon', 'coffee'
    ]
}

hair_colors_dict = {
    "European": [
        "blond", "light brown", "dark brown", "black", "red", "auburn", "strawberry blond", "platinum blond",
        "ash blond", "dirty blond", "golden blond", "chestnut", "mahogany", "copper", "ginger",
        "silver-gray", "white", "raven-black", "jet black", "honey blond", "sandy"
    ],
    "Sub-Saharan African": [
        "black", "dark brown", "brown", "reddish-brown", "auburn", "jet black",
        "ebony", "chocolate brown", "espresso"
    ],
    "Middle Eastern": [
        "black", "dark brown", "brown", "light brown", "auburn", "jet black", "chestnut brown"
    ],
    "Latin American": [
        "black", "dark brown", "brown", "light brown", "auburn", "reddish-brown",
        "jet black", "mahogany", "chestnut brown"
    ],
    "Oceanian": [
        "black", "dark brown", "brown", "jet black", "ebony"
    ],
    "Caribbean": [
        "black", "dark brown", "brown", "reddish-brown", "auburn", "jet black", "ebony"
    ],
    "Central Asian": [
        "black", "dark brown", "brown", "light brown", "jet black"
    ],
    "West Asian": [
        "black", "dark brown", "brown", "light brown", "auburn", "jet black"
    ],
    "North African": [
        "black", "dark brown", "brown", "light brown", "jet black", "ebony"
    ],
    "Scandinavian": [
        "blond", "light brown", "dark brown", "platinum blond", "ash blond", "golden blond",
        "strawberry blond", "silver-gray", "white"
    ],
    "North American": [
        "blond", "light brown", "dark brown", "black", "red", "auburn", "strawberry blond", "platinum blond",
        "ash blond", "dirty blond", "golden blond", "chestnut", "mahogany", "copper", "ginger",
        "silver-gray", "white", "raven-black", "jet black", "honey blond", "sandy"
    ],
    "Arctic": [
        "black", "dark brown", "jet black"
    ],
    "Southeast Asian": [
        "black", "dark brown", "brown", "jet black", "ebony"
    ],
    "Balkan": [
        "dark brown", "black", "light brown", "auburn", "chestnut brown", "mahogany"
    ],
    "Polynesian": [
        "black", "dark brown", "brown", "jet black", "ebony"
    ],
    "Micronesian": [
        "black", "dark brown", "jet black", "ebony"
    ],
    "Melanesian": [
        "black", "dark brown", "reddish-brown", "jet black", "ebony"
    ],
    "Indigenous American": [
        "black", "dark brown", "brown", "reddish-brown", "jet black", "ebony"
    ],
    "Australasian": [
        "black", "dark brown", "brown", "blond", "red", "auburn", "jet black", "ebony"
    ],
    "Caucasian": [
        "dark brown", "black", "light brown", "auburn", "red", "chestnut brown",
        "mahogany", "jet black"
    ],
    "East Asian": [
        "black", "dark brown", "light brown", "auburn", "reddish-brown", "jet black", "ebony"
    ],
    "South Asian": [
        "black", "dark brown", "brown", "auburn", "reddish-brown", "jet black", "ebony"
    ],
    "Interesting Colors": [
        'blue', 'green', 'teal', 'hazel', 'brown', 'amber', 'ice-blue', 'steel gray',
        'cyan', 'violet', 'red', 'pink', 'orange', 'yellow', 'gold', 'silver', 'black',
        'white', 'gray', 'purple', 'turquoise', 'aqua', 'emerald', 'sapphire', 'ruby',
        'topaz', 'amethyst', 'jade', 'opal', 'pearl', 'sable', 'copper', 'bronze', 'brass',
        'platinum', 'rose gold', 'champagne', 'mahogany', 'caramel', 'cinnamon', 'coffee'
    ]
}

expressions_list = [
    'with a fearful expression', 'with a whimsical twirl', 'with a contemptuous sneer', 'with an eager nod',
    'with a sulky expression', 'with a concerned expression', 'with a wistful expression', 'with tongue sticking out',
    'with a elated expression', 'with a nostalgic expression', 'with a guilty expression', 'with a optimistic expression',
    'with a bored expression', 'with a uninterested expression', 'with a perplexed scratch', 'with a bewildered expression',
    'with an excited expression', 'with a serene expression', 'with a stunned expression', 'with a vigilant watch',
    'with a grateful expression', 'with a sad gaze', 'with a laughing expression', 'with a cautious approach',
    'with a relieved sigh', 'with a sorrowful expression', 'with a scared expression', 'with a cheerful expression',
    'with an inquisitive tilt', 'with a scary expression', 'with a distressed expression', 'with a angry expression',
    'with a proud expression', 'with a resentful expression', 'with a timid expression', 'with a frustrated expression',
    'with a surprised look', 'with a inquisitive expression', 'with a intense expression', 'with a hollow stare',
    'with a solemn expression', 'with a gloomy stare', 'with a zany expression', 'with a contemplative expression',
    'with a jealous glance', 'with an ecstatic cheer', 'with a pleased expression', 'with an enraged expression',
    'with a hollow stare expression', 'with a irritated expression', 'with a humbled bow', 'with an appreciative nod',
    'with a joyful smile', 'with a astonished expression', 'with a satisfied expression', 'with a worried frown',
    'with a playful smile', 'with a melancholy look', 'with a mournful cry', 'with a determined stride',
    'with a lonely expression', 'with a jubilant dance', 'with a annoyed expression', 'with a disgusted expression',
    'with a amused expression', 'with a humbled expression', 'with a triumphant expression', 'with a ecstatic expression',
    'with a dreamy expression', 'with a blank stare expression', 'with a nervous expression', 'with a panic expression',
    'with a disgruntled expression', 'with a shy expression', 'with a determined expression', 'with a pensive look',
    'with a melancholy expression', 'with a longing gaze', 'with a regretful sigh', 'with a grumpy grunt',
    'with a skeptical eyebrow', 'with a focused expression', 'with a indignant expression', 'with a confused look',
    'with a calm expression', 'with a stoic expression', 'with an annoyed grimace', 'with an arrogant posture',
    'with a wary eye', 'with an uneasy shuffle', 'with a disbelief expression', 'with an anxious fidget',
    'with a longing expression', 'with an overwhelmed gasp', 'with mouth open in surprise', 'with a puzzled expression',
    'with a joyful expression', 'with a hysteric expression', 'with a anxious expression', 'with a embarrassed expression',
    'with a confused expression', 'with a satisfied smile', 'with a resigned shrug', 'with a hysterical laugh',
    'with a curious glance', 'with a apprehensive expression', 'with a hopeful expression', 'with a determined look',
    'with a crying expression', 'with a mad expression', 'with a whimsical expression', 'with a vexed expression',
    'with a sly smile', 'with an optimistic smile', 'with a zany hop', 'with a relieved expression',
    'with a smiling expression', 'with a indifferent expression', 'with a mournful expression', 'with a shocked expression',
    'with a serious expression', 'with a pessimistic mutter', 'with a vexed stomp', 'with a awkward expression',
    'with a thoughtful gaze', 'with a frowning expression', 'with a frustrated gesture', 'with a beaming expression',
    'with tears in the eyes', 'with a grinning expression', 'with a jealous expression', 'with a fearful look',
    'with a deflated posture', 'with a grieving expression', 'with a confident expression', 'with a depressed expression',
    'with an amused smirk', 'with a pleased nod', 'with an enthusiastic expression', 'with a overwhelmed expression',
    'with a frown', 'with a irate expression', 'with a gloomy expression', 'with a hopeful gaze',
    'with a content expression', 'with a thoughtful expression', 'with a cheerful demeanor', 'with an apprehensive gaze',
    'with a heartbroken expression', 'with a detached expression', 'with a disoriented expression', 'with a tormented expression',
    'with an awkward smile', 'with a timid step', 'with a bewildered look', 'with a wide smile',
    'with a uneasy expression', 'with a content smile', 'with a bored yawn', 'with a disappointed expression',
    'with a appreciative expression', 'with a furious expression', 'with an elated jump', 'with a enthusiastic expression',
    'with a pensive expression', 'with a resigned expression', 'with a panicked expression', 'with a sulky pout',
    'with a tense posture', 'with a melancholic expression', 'with a excited expression', 'with an angry look',
    'with a sad expression', 'with a resentful glare', 'with a disappointed frown', 'with a apathetic expression',
    'with a proud stance', 'with a beaming smile', 'with a grumpy expression', 'with a frowning face',
    'with an indignant huff', 'with an inquisitive look', 'with a eager expression', 'with a stressed expression',
    'with a funny expression', 'with a triumphant roar', 'with a skeptical expression', 'with an apathetic stare',
    'with an irate shout', 'with a worried expression', 'with a cautious expression', 'with a disgruntled scowl',
    'with a delighted expression', 'with a scowl', 'with a happy demeanor', 'with a curious expression',
    'with an indifferent shrug', 'with a deflated expression', 'with an embarrassed blush', 'with a distressed cry',
    'with a surprised expression', 'with a horrified look', 'with a perplexed expression', 'with a look of disbelief',
    'with a contempt expression', 'with a lonely look', 'with a blank stare', 'with a exasperated expression',
    'with a interested expression', 'with a jubilant expression', 'with a vigilant expression', 'with a flabbergasted expression',
    'with a regretful expression', 'with a neutral expression', 'with an admirable expression', 'with a nervous twitch',
    'with a grateful smile', 'with an exasperated sigh', 'with a ashamed expression', 'with a enraged expression',
    'with a disgusted look', 'with a solemn face', 'with a pessimistic expression', 'with a calm demeanor',
    'with a wary expression', 'with a admirable expression', 'with a baffled expression', 'with a wistful glance',
    'with a horrified expression', 'with a tense expression', 'with a joyous laugh', 'with an interested look',
    'with a mischievous grin', 'with an ashamed face', 'with a arrogant expression', 'with a scary look', 'with a happy expression'
]

hair_styles_list = [
    "flowing hair", "short curly hair", "bald head", "wavy hair", "short spiky hair",
    "long straight hair", "short straight hair", "long curly hair", "shoulder-length hair", "tidy hair",
    "shaven head", "buzz cut hair", "bob cut hair", "afro hair", "dreadlocks",
    "braided hair", "ponytail hair", "hair bun", "shiny hair", "mullet hair",
    "pixie cut hair", "undercut hair", "fade haircut", "taper haircut", "quiff hair",
    "faux hawk hair", "pompadour hair", "wet hair look", "crew cut hair", "side-parted hair",
    "mohawk hair", "comb over hair", "slicked-back hair", "shaggy hair", "layered hair",
    "choppy hair", "asymmetrical haircut", "feathered hair", "cropped hair", "blunt cut hair",
    "razor cut hair", "textured hair", "coiled hair", "ringlet hair", "cornrows hair",
    "finger-waved hair", "pin curled hair", "beehive hair", "pageboy haircut", "hime cut hair",
    "pixie-bob haircut", "lob haircut", "jheri curl hair", "curtain hair", "top knot hair",
    "man bun hair", "twisted hair", "locs hair", "permed hair", "hair with bangs",
    "hair with fringe", "balayage hair", "hair with highlights", "hair with lowlights", "hair with chunky highlights",
    "hair with frosted tips", "medium-length hair", "coily hair", "side-part hair", "middle-part hair",
    "twist-out hair", "bantu knots hair", "box braids hair", "goddess braids hair", "faux locs hair",
    "twist braids hair", "finger coils hair", "perm rod set hair", "sidecut hair", "wolf cut hair",
    "curtain bangs hair", "baby bangs hair", "side-swept bangs hair", "ducktail hair", "liberty spikes hair",
    "deathhawk hair", "French twist hair", "chignon hair", "bouffant hair", "victory rolls hair",
    "crown braid hair", "milkmaid braids hair", "space buns hair", "ombre hair", "dip-dyed hair",
    "streaked hair", "colorful hair", "pastel-colored hair", "neon-colored hair", "Gibson Girl hair",
    "hi-top fade hair", "flattop hair", "Rockabilly hair", "Teddy Boy hair", "Mod hair", "hair adorned with flowers",
    "Hippie hair", "sculptural hair", "geometric hair", "futuristic hair", "avant-garde hair",
    "editorial hair", "haute couture hair", "gravity-defying hair", "buzz cut with designs", "tapered fade hair",
    "high and tight hair", "textured top hair", "long on top, short on sides hair", "chin-length hair", "collarbone-length hair",
    "mid-back length hair", "waist-length hair", "half-up, half-down hair", "side-braided hair", "braided mohawk hair",
    "faux hawk with braided sides", "twisted updo hair", "messy bun with loose tendrils", "sleek ponytail with baby hairs", 
    "hair with jewelry", "hair with ornate pins", "hair with a headband", "hair with a wrap", "hair with colorful extensions"
]

face_poses_list = [
    "facing directly at the camera", "with a slight turn to the left", "with a slight turn to the right",
    "in three-quarter view to the left", "in three-quarter view to the right", "in full left profile",
    "in full right profile", "looking up at the camera", "looking down at the camera",
    "with chin slightly raised", "with chin slightly lowered", "with head tilted to the left",
    "with head tilted to the right", "with a subtle lean forward", "with a subtle lean backward",
    "with shoulders at an angle", "with head slightly rotated left", "with head slightly rotated right",
    "in side-profile view", "with head leaning to the left", "with head leaning to the right",
    "with chin jutted out", "with chin tucked in", "with head tilted back",
    "with head tilted forward", "at a low angle to the camera", "at a high angle to the camera",
    "facing away from the camera", "looking over left shoulder", "looking over right shoulder",
    "with head cocked to the left", "with head cocked to the right", "at eye level with the camera",
    "camera slightly below eye level", "camera slightly above eye level", "facing the camera",
    "in profile", "looking away", "in three-quarter view", "looking up", "looking down", "with head tilted to one side",
    "with eyes looking off-camera", "tilting their head slightly to the right", "tilting their head slightly to the left",
    "with a straight head position", "with their chin lifted slightly", "with their chin lowered a bit",
    "in a left profile view", "in a right profile view", "with their head slightly tilted back",
    "with their head leaning forward", "with a slight three-quarter view to the right", "with a slight three-quarter view to the left",
    "with their head leaning to the right", "with their head leaning to the left", "with their head slightly rotated to the right",
    "with their head slightly rotated to the left", "with their chin slightly jutted out", "with their chin slightly tucked in",
    "with a three-quarter view of their face", "with a neutral head position", "with their head slightly tilted to the left",
    "with their head tilted back a little", "with their head leaning slightly forward", "with their head rotated a bit to the right",
    "with their head rotated a bit to the left", "with their chin pointing slightly upwards", "with their face resting on their hand"
]

#%% helper functions

def get_prompt_start():
    prompt_start_list = [
        "Photo of a", "Portrait of a", "Photograph of a", "Medium Shot of a", "Close-Up of a", "An artistic portrayal of a",
        "Headshot of a", "Face of a", "Facial portrait of a", "Studio portrait of a", "Candid portrait of a", "A serene image of a",
        "Profile view of a", "Character study of a", "Expressive portrait of a", "Cinematic portrait of a", "A candid portrait of a",
        "A portrait photo of a", "A professional photograph of a", "A professional portrait photograph of a", "A pro portrait photo of a", 
        "A high-resolution image of a", "A captivating picture of a", "An enchanting photo of a", "A studio shot of a", "A casual snapshot of a",
        "A meticulously composed portrait of a", "An authentic picture of a", "A magazine-quality portrait of a", "A compelling photograph of a",
        "A striking portrait of a", "A vintage photograph of a", "A black and white portrait of a", "An evocative image of a", 
        "A low-angle shot of a", "A wide-angle shot of a", "An atmospheric portrait of a", "A moody portrayal of a", "A whimsical image of a",
        "An extreme wide shot of a", "A wide shot of a", "A full shot of a", "A medium wide shot of a", "A medium close-up of a",
        "An extreme close-up of a", "An eye-level shot of a", "A Dutch angle shot of a", "A tracking shot of a", "A pan shot of a",
        "A tilt shot of a", "A dolly shot of a", "A zoom shot of a", "An over-the-shoulder shot of a", "A POV shot of a",
        "A cutaway shot of a", "An insert shot of a", "An aerial portrait shot of a", "A high-angle shot of a",
    ]

    return random.choice(prompt_start_list)

def get_random_glasses():
    glasses_list = [
        "wearing classic rectangular glasses", "with round vintage-style glasses", "sporting cat-eye frames",
        "with aviator-style glasses", "wearing oversized square glasses", "with sleek rimless glasses",
        "sporting horn-rimmed glasses", "with retro browline glasses", "wearing geometric hexagonal frames",
        "with trendy clear frame glasses", "sporting thick-framed hipster glasses", "with oval wire-frame glasses",
        "wearing sporty wraparound glasses", "with stylish half-rim glasses", "sporting colorful acetate frames",
        "with sophisticated titanium frames", "wearing bold colored glasses", "with minimalist thin metal frames",
        "sporting funky asymmetrical glasses", "with classic wayfarers", "wearing trendy blue light blocking glasses",
        "with elegant gold-rimmed glasses", "sporting futuristic shield glasses", "with retro round sunglasses",
        "wearing clip-on sunglasses", "with gradient lens sunglasses", "sporting mirrored aviator sunglasses",
        "with polarized sports sunglasses", "wearing fashionable oversized sunglasses", "with classic clubmaster sunglasses",
        "sporting trendy transparent sunglasses", "with vintage cat-eye sunglasses", "wearing modern shield sunglasses",
        "with retro square sunglasses", "sporting stylish browline sunglasses", "with cool wrap-around sunglasses", 
        "with cyberpunk LED glasses", "wearing steampunk goggles", "sporting futuristic visor sunglasses",
        "with monocle", "wearing diamond-studded glasses", 
    ]
    
    return random.choice(glasses_list)

def get_random_gaze_direction():
    gaze_direction_list = [
        "looking directly into the camera", "gazing off to the side", 
        "looking down in thought", "looking upwards", "with eyes closed", 
        "staring to the side", "with a sidelong glance", "looking past the camera",
        "with a far-off look", "with a focused gaze", "with a wandering gaze",
        "looking into the distance", "with eyes nearly shut", 
        "with eyes fixed on an unseen object", "glancing over their shoulder",
        "with eyes darting around nervously", "staring intently at something off-camera",
        "with a thousand-yard stare", "looking through half-lidded eyes",
        "with eyes wide in surprise", "squinting against bright light",
        "with a dreamy, unfocused gaze", "looking down demurely",
        "with eyes crinkled in laughter", "peering curiously at the viewer",
        "with a piercing stare", "looking up through their lashes",
        "with eyes reflecting deep contemplation", "gazing longingly into the distance",
        "with a mischievous twinkle in their eyes", "looking sideways with suspicion",
        "with eyes brimming with tears", "staring defiantly at the camera",
        "with a vacant, expressionless gaze", "looking up in wonder",
        "with eyes narrowed in concentration", "gazing lovingly at someone off-camera",
        "with a faraway look of nostalgia", "looking down with a shy smile",
        "with eyes alight with excitement", "staring off into space pensively",
        "with a haunted look in their eyes", "glancing furtively to the side",
        "with eyes filled with determination", "looking straight ahead with resolve",
        "with a distant, melancholic gaze", "peering intently at something in their hands",
        "with eyes dancing with amusement", "staring blankly ahead",
        "with a wistful gaze towards the horizon", "looking down with a furrowed brow",
        "with eyes half-closed in contentment", "gazing upward with hope",
        "with a sharp, analytical stare", "looking sideways with skepticism",
        "with eyes wide with wonder", "staring intensely at their own reflection",
        "with a distant gaze, lost in memory", "looking directly at the viewer with vulnerability",
        "with eyes scanning the environment alertly", "gazing into middle distance, deep in thought",
        "with a penetrating stare that seems to see through the viewer", "looking down with eyes closed, in meditation",
        "with eyes darting back and forth, reading something", "staring off-camera with a look of longing",
        "with eyes widened in fear or shock", "gazing at their own hands with fascination",
        "with a soft, compassionate look in their eyes", "staring at the ground with a mix of shame and regret",
        "with eyes twinkling with inner joy", "looking past the camera with a stoic expression"
    ]

    return np.random.choice(gaze_direction_list)

def ger_facial_hair_description():
    facial_hair_list = [
        "clean-shaven", "with light stubble", "with heavy stubble",
        "with a short, neat beard", "with a full, thick beard", "with a long, flowing beard",
        "with a well-groomed goatee", "with a circle beard", "with a chin strap beard",
        "with a neat mustache", "with a handlebar mustache", "with a horseshoe mustache",
        "with mutton chops", "with friendly sideburns", "with a soul patch",
        "with a Van Dyke beard", "with a Garibaldi beard", "with a ducktail beard",
        "with a French fork beard", "with a Bandholz beard", "with a yeard",
        "with a ZZ Top-style beard", "with a 5 o'clock shadow", "with designer stubble",
        "with a pencil mustache", "with a Fu Manchu mustache", "with an imperial mustache",
        "with a Dali mustache", "with a walrus mustache", "with a chevron mustache",
        "with a Hollywoodian beard", "with a short boxed beard", "with a Verdi beard",
        "with a Spartan beard", "with a Norse beard", "with a Viking-style beard",
        "with a neatly trimmed beard", "with an unkempt beard", "with a patchy beard",
        "with a salt-and-pepper beard", "with a graying beard", "with a shabby chic beard",
        "with a faded beard", "with a tapered beard", "with a pointy beard",
        "with a braided beard", "with a forked beard", "with a sculpted beard",
        "with an Asian-style mustache", "with a handlebar-and-goatee combo",
        "with a thin-line beard", "with a disconnected mustache",
        "with a chin curtain beard", "with a Klingon-style beard",
        "with a wild, untamed beard", "with a precisely lined beard",
        "with a barely-there mustache", "with a bushy mustache",
        "with a curled mustache", "with waxed mustache tips",
        "with a scruffy beard", "with a lumberjack-style beard",
        "with a hipster beard", "with an artistically trimmed beard",
        "with a multi-colored dyed beard", "with a glitter beard",
        "with a freestyle beard", "with a neck beard",
        "with mutton chops connected to a mustache", "with a chin puff",
        "with an anchor beard", "with a Balbo beard", "with a royal beard", 
        "with a Zappa-style beard", "with a Hulihee beard",
        "with a long goatee", "with sideburns connected to a mustache",
        "with a mustache-free beard", "with a beard-free mustache",
        "with a pencil-thin chin strap", "with a double mustache",
        "with a triangle beard", "with an inverted T-shape beard",
    ]

    return random.choice(facial_hair_list)

def get_makeup_description():
    makeup_list = [
        "with natural, barely-there makeup", "wearing a classic red lip and winged eyeliner", "with a smoky eye and nude lips",
        "featuring a bold cat-eye and coral lipstick", "with a fresh, dewy look and pink blush", "wearing dramatic false eyelashes and glossy lips",
        "with a bronzed, sun-kissed glow", "featuring metallic eyeshadow and matte lips", "with a no-makeup makeup look",
        "wearing bold, colorful eyeshadow and neutral lips", "with perfectly contoured cheekbones", "featuring glossy eyelids and a subtle lip tint",
        "with a gothic-inspired dark lip and pale complexion", "wearing pastel eyeshadow and peach blush", "with a monochromatic makeup look in earthy tones",
        "featuring glitter accents around the eyes", "with a bold, avant-garde makeup design", "wearing a 1950s-inspired pin-up look",
        "with a subtle brown smoky eye and pink lips", "featuring holographic highlighter on cheekbones", "with minimal eye makeup and a bold berry lip",
        "wearing blue mascara and orange-tinted lips", "with graphic eyeliner designs", "featuring ombre lips from dark to light",
        "with strategically placed facial gems or rhinestones", "wearing an ethereal, fairy-like makeup look", "with a bold unibrow statement",
        "featuring neon eyeliner accents", "with a soft, romantic rose-gold palette", "wearing dramatic stage makeup with exaggerated features",
        "with a 1960s-inspired Twiggy lash look", "featuring bright, color-blocked eyeshadow", "with a glossy, wet-look eye makeup",
        "wearing a subtle everyday makeup with focus on skincare", "with an edgy, punk-inspired dark eye and bright lip", 
        "with artfully applied freckles", "wearing mermaid-inspired shimmery scales on cheekbones", "with a soft focus, blurred lip look",
        "featuring negative space eyeliner designs", "with an airbrushed, flawless complexion", "wearing ice princess-inspired frosty tones",
        "with a sun-striping technique using bronzer", "featuring floating crease liner", "with a soft focus hazy eye look",
        "wearing a classic French girl inspired minimal makeup", "with deconstructed bright eyeshadow placement", "featuring a cut-crease eyeshadow technique",
        "with an extreme contour and highlight", "wearing a watercolor-inspired soft wash of colors", "featuring a gradient lip from dark center to light edges",
    ]

    makeup_description = random.choice(makeup_list)
    return makeup_description

def get_location_setting_background():
    locations_settings_backgrounds_list = [
        "in a corn field", "in a wheat field", "in a rice field", "in a sunflower field", "in a strawberry field", 
        "in a lavender field", "in a tulip field", "in a pumpkin patch", "in a flower garden", "in a vegetable garden",
        "in a water garden", "in a rose garden", 'in a grass hill', 'in a grass field', 'in a grassy meadow', 'in a grassy plain',
        "at the desert", "in the forest", "in the park", "at the garden", "at the beach", "outside in wild nature",
        "at the lake", "outside near a mountain", "near a waterfall", "in a cherry blossom park",
        "on a snowy mountaintop", "in a botanical garden", "on a scenic cliff", "on a tropical island", "in a secluded cave",
        "in a vineyard", "in an orchard", "at a coral reef", "in a bamboo forest", "in an ice cave", "in a tulip field",
        "in a pumpkin patch", "next to a scenic pond", "in a tea plantation", "in a coffee plantation", "in an olive grove",
        "in a date palm grove", "in a mossy forest", "in a zen garden", "in a maze garden", "in a grass field",
        "in an alpine meadow", "on a rocky mountain ridge", "in a misty mountain valley", "near a glacial lake", 
        "in a dense tropical rainforest", "in an old-growth redwood forest", "in a misty cloud forest",
        "in a colorful autumn deciduous forest", "in a sparse boreal forest", "near a tranquil mountain stream",
        "in a red rock desert canyon", "among towering sand dunes", "in a rocky desert with cacti",
        "in a salt flat desert", "in a desert oasis with palm trees", "near a coral reef", "on a pebble beach",
        "on a rocky coastal cliff", "on a pristine white sand beach", "in a mangrove swamp",
        "in a rolling grassland prairie", "in an African savanna", "in a wildflower-filled meadow",
        "in a high-altitude steppe", "in a grassy wetland marsh", "near a melting glacier", "in a snowy taiga forest",
        "on the Arctic tundra", "near an Antarctic ice shelf", "in a field of Arctic wildflowers",
        "near an active volcano", "in a field of hardened lava", "near a bubbling mud pot",
        "next to a steaming geyser", "in a volcanic crater lake", "in a slot canyon", "among giant boulders",
        "on the banks of a meandering river", "near a thundering waterfall", "in a river canyon",
        "on a misty river at dawn", "near a series of cascading rapids", "on a snow-capped mountain peak",
        "among bizarre rock hoodoos", "in a limestone karst landscape", "near a natural stone arch",
        "on a remote tropical island", "on a volcanic island coastline", "in a lush island jungle interior",
        "on a windswept subarctic island", "near a fjord on a mountainous island"
        "in a verdant tea plantation with red-clothed pickers", "in a bamboo forest with golden sunlight filtering through",
        "in a lush rainforest with colorful tropical birds", "in a mossy ancient forest with pink cherry blossoms",
        "in a terraced rice field with workers in conical hats", "in a topiary garden with whimsical shapes",
        "in a dense fern gully with a small, clear stream", "in a tropical botanical garden with exotic flowers",
        "on a golf course with white sand bunkers", "in a vineyard with purple grapes ready for harvest",
        "in a field of tall grass with red poppies scattered throughout", "in a misty pine forest with orange mushrooms",
        "in an English garden maze with blooming roses", "in a lush valley with a rainbow arching overhead",
        "in a green tea field with Mount Fuji in the background", "in a traditional Peruvian weaving village",
        "in a vast sunflower field", "on a beach with golden sand and blue water", "in a field of yellow rapeseed flowers",
        "among fall foliage with golden leaves", "in a wheat field ready for harvest", "in a desert with golden sand dunes",
        "in a field of yellow tulips", "surrounded by autumn birch trees with yellow leaves", "in a field of yellow daffodils",
        "on a hillside covered in yellow wildflowers", "in a lemon grove with ripe yellow fruit", "in a field of goldenrod flowers",
        "among yellow aspen trees in autumn", "in a field of yellow marigolds", "surrounded by yellow ginkgo trees in fall"
        "in the village", "in the city", "at home", "on the couch", "in the livingroom", "near the fireplace",
        "in a cosy wooden cabin", "in a ski lodge", "in a mansion", "in a villa", "in a photography studio",
        "in a studio apartment", "in a penthouse apartment", "in Times Square, New York", "in the Red Square, Moscow",
        "in Los Angeles", "in Hollywood", "in a Bel Air villa", "in Paris", "in San Francisco", "in London", "in New York",
        "in Berlin", "in Tokyo", "in Chicago", "in Rome", "in Barcelona", "in Canada", "in Toronto", "in Alaska",
        "in Antarctica", "in the office", "at a luxury hotel", "in the kitchen", "at the balcony", "in a studio",
        "on the Great Wall of China", "in a historic castle", "at a famous landmark", "in an ancient ruin",
        "in a modern skyscraper", "on a bustling street market", "on a charming bridge", "at a picturesque harbor",
        "in a bustling cafe", "in a majestic palace", "in an art gallery", "in a world-famous museum", "in a theater",
        "in a fish market", "in a clock tower", "at a lighthouse", "in an old village", "in a professional photography studio",
        "at a historic monastery", "in an art deco building", "in a gothic cathedral", "at a scenic viewpoint",
        "at a picturesque quarry", "next to a windmill", "at a historic fort", "in an aquarium", "in a planetarium",
        "at a scenic dock", "on a historic ship", "in a bustling subway station", "in a busy city street", "on a yacht",
        "in a quiet village", "in a quiet library", "in a bustling airport terminal", "in a lively sports stadium",
        "in an elegant art gallery", "in a high-tech laboratory", "in an opulent palace", "in a cutting-edge skyscraper",
        "in a charming bed and breakfast", "in a bustling food market", "in a historic lighthouse",
        "in a whimsical fairy-tale inspired theme park", "at a remote Arctic research station", "on a cruise ship",
        "in a traditional Mongolian yurt camp", "at a bustling Broadway theater", "in a serene botanical garden",
        "next to a yellow stone wall", "next to a red brick wall", "next to a green tile wall", "next to a purple stone wall",
        "next to a blue stone wall", "next to a white stone wall", "next to a black stone wall", "next to a brown stone wall",
        "next to a wooden lime wall", "next to a orange brick wall", "next to a magenta stone wall", "next to a cyan stone wall",
        "next to a wooden wall", "next to a stone wall", "next to a marble wall", "next to a brick wall",
        "next to a glass window", "next to a yellow wall", "next to a red wall", "next to a green tile wall",
        "next to a purple wall", "next to a yellow wooden wall", "next to an orange stone wall",
        "next to a green stone wall", "next to a purple marble wall", "next to a blue marble wall",
        "next to a white marble wall", "next to a black marble wall", "next to a brown tile wall",
        "next to an ancient stone wall", "with a plain background", "with a blurred background",
        "against a white backdrop", "against a black backdrop", "with a colorful backdrop",
        "with a textured background", "with a gradient background", "with a bokeh effect background",
        "at the Grand Canyon", "next to Victoria Falls", "next to Niagara Falls", "at a music festival",
        "on a historic battlefield", "on a sailboat", "in a hot air balloon", "under the northern lights",
        "next to a historic statue", "in a traditional tea house", "in a serene butterfly sanctuary",
        "in an ancient underground cave system", "in a vibrant street art alley", "in a misty Scottish highland",
        "at a futuristic vertical farm", "in a traditional Japanese onsen", "at a bustling spice market in Marrakech",
        "in an otherworldly salt flat", "at a bioluminescent beach at night", "in a lush tropical treehouse resort",
        "at a historic Route 66 diner", "in a neon-lit cyberpunk cityscape", "in a tranquil lavender field in Provence",
        "in a serene Scandinavian fjord", "at a colorful hot air balloon festival",
        "in a mystical fog-covered ancient forest", "at a cutting-edge renewable energy farm",
        "outdoors", "indoors", "in an outdoor setting", "in a studio setting", "against an urban setting",
        "against a nature backdrop", "against a studio background", "against a beach scene"
        "in an old steel mill", "at a bustling shipyard", "in a modern automotive factory",
        "at an active construction site", "in a textile manufacturing plant", "at a wind turbine farm",
        "in a high-tech electronics assembly line", "at a busy seaport with cargo containers",
        "in a traditional blacksmith's workshop", "at a state-of-the-art recycling facility",
        "in a university lecture hall", "in a elementary school classroom", "at a public library reading room",
        "in a high school science lab", "at a coding bootcamp workspace", "in a music conservatory practice room",
        "at a culinary school kitchen", "in a medical school anatomy lab", "at an art school studio",
        "on a professional basketball court", "at an Olympic swimming pool", "in a state-of-the-art gymnasium",
        "on a golf course green", "at a baseball stadium dugout", "in a boxing ring corner", "on a soccer field sideline",
        "on an athletics track stadium", "at a professional football stadium", "on a track & field course",
        "on a tennis court baseline", "at a rock climbing wall", "in a yoga studio", "at a horse racing track",
        "in the cockpit of a commercial airliner", "on the deck of a luxury cruise ship",
        "at a bustling train station platform", "in the cabin of a high-speed bullet train",
        "at a busy airport terminal", "in the back of a yellow taxi cab", "on a city bus during rush hour",
        "in a sleek, modern subway car", "at a car rental facility", "in the control room of a cargo ship",
        "at a colorful Holi festival celebration", "during a traditional Japanese tea ceremony",
        "at a lively Carnival parade in Rio", "during a solemn Native American powwow",
        "at a vibrant Chinese New Year celebration", "during a formal Western wedding ceremony",
        "at a lively Oktoberfest beer hall", "during a traditional Indian Diwali festival",
        "at a Mexican Day of the Dead celebration", "during a Moroccan Ramadan evening feast",
        "in front of a large abstract mural", "surrounded by classical marble sculptures",
        "in a glass-blowing studio mid-creation", "at a pottery wheel shaping clay",
        "in front of a wall of colorful street art", "in a dance studio with mirrored walls",
        "at an outdoor installation art exhibit", "in a photography darkroom", "in a marine biology research vessel", 
        "at a bustling art gallery opening night", "in a theater prop and costume workshop",
        "in a cutting-edge robotics laboratory", "at a particle accelerator facility", "at an archaeological dig site",
        "in a clean room for semiconductor manufacturing", "at a radio telescope array", "in a renewable energy research center", 
        "in a genetic research laboratory", "at a weather monitoring station", "at a space mission control center",
    ]

    return random.choice(locations_settings_backgrounds_list)

def get_skin_description(ethnicity_group=None, stereotype_prob=0.7):
    skin_tones_by_ethnicity_dict = {
        "European":            ["fair", "light", "pale", "ivory", "porcelain", "rosy", "peach", "cream", "alabaster", "milky", "cool beige"],
        "Sub-Saharan African": ["dark brown", "deep brown", "chocolate", "ebony", "mahogany", "espresso", "rich brown"],
        "Middle Eastern":      ["olive", "tan", "medium", "golden", "warm beige", "light brown", "honey"],
        "Latin American":      ["olive", "tan", "caramel", "bronze", "golden", "medium", "coffee", "mocha"],
        "Oceanian":            ["tan", "golden brown", "deep brown", "bronze", "copper"],
        "Caribbean":           ["caramel", "golden brown", "deep brown", "mahogany", "cocoa"],
        "Central Asian":       ["light", "medium", "olive", "golden", "wheat"],
        "West Asian":          ["olive", "medium", "golden", "warm beige", "tan"],
        "North African":       ["olive", "tan", "golden", "caramel", "light brown", "medium brown"],
        "Scandinavian":        ["very fair", "pale", "porcelain", "ivory", "rosy"],
        "North American":      ["fair", "light", "medium", "olive", "tan", "brown", "dark brown"],
        "Arctic":              ["light", "fair", "golden", "ruddy"],
        "Southeast Asian":     ["light brown", "medium brown", "tan", "golden", "caramel"],
        "Balkan":              ["light", "medium", "olive", "golden", "tan"],
        "Polynesian":          ["golden brown", "tan", "bronze", "deep brown"],
        "Micronesian":         ["tan", "golden brown", "bronze", "medium brown"],
        "Melanesian":          ["deep brown", "dark brown", "ebony", "rich brown"],
        "Indigenous American": ["tan", "copper", "bronze", "reddish-brown", "golden brown"],
        "Australasian":        ["fair", "tan", "golden", "deep brown", "reddish-brown"],
        "Caucasian":           ["fair", "light", "pale", "ivory", "rosy", "peach", "beige"],
        "East Asian":          ["light", "fair", "ivory", "warm beige", "golden", "porcelain"],
        "South Asian":         ["tan", "caramel", "honey", "golden brown", "deep brown", "wheat", "bronze"]
    }

    skin_characteristics = [
        "smooth", "soft", "silky", "velvety", "radiant", "glowing", "dewy", "matte", "textured", "porous", 
        "freckled", "sun-kissed", "weathered", "leathery", "wrinkled", "lined", "age-spotted", "blemished", 
        "scarred", "pockmarked", "clear", "unblemished", "youthful", "mature", "supple", "firm", "taut", 
        "saggy", "dry", "oily", "combination", "sensitive", "rough", "calloused", "flushed", "ruddy", "pale", 
        "sallow", "ashen", "vibrant", "luminous", "dull", "mottled", "patchy", "even-toned", "uneven", 
        "translucent", "opaque", "porcelain-like", "alabaster-like", "bronzed", "sun-damaged", "tanned", 
        "untanned", "sunburnt", "detailed", "detailed texture", "detailed pores", "lustrous", "healthy", 
        "glassy", "plump", "hydrated", "moisturized", "flaky", "peeling", "bumpy", "dimpled", "velvety", 
        "buttery", "waxy", "papery", "tight", "loose", "elastic", "toned", "polished", "raw", "chapped"
    ]

    all_skin_tones = get_all_unique_dict_values(skin_tones_by_ethnicity_dict)
    if ethnicity_group is None or random.random() > stereotype_prob:
        tone = random.choice(all_skin_tones)
    else:
        tone = random.choice(skin_tones_by_ethnicity_dict.get(ethnicity_group, all_skin_tones))

    characteristic = random.choice(skin_characteristics)

    return f"{tone}, {characteristic} skin"

def get_hats_and_headwear(sex_group=None, stereotype_prob=0.8):
    hats_and_headwear_dict = {
        "Male": [
            "wearing a baseball cap", "with a fedora", "sporting a beanie", "with a flat cap", "wearing a turban",
            "wearing a cowboy hat", "with a top hat", "wearing a bowler hat", "with a newsboy cap",
            "sporting a trucker hat", "with a bucket hat", "wearing a military cap", "with a golf visor",
            "sporting a bandana", "with a beret", "wearing a sombrero", "with a turban", "with a mexican hat",
            "sporting a kippah", "with a fez", "wearing a ushanka", "with a porkpie hat", "with a scarf",
            "sporting a panama hat", "with a boater hat", "wearing a deerstalker", "with a trapper hat",
            "sporting a taqiyah", "with a tam o' shanter", "wearing a tricorn hat", "with a helmet", "with a steampunk top hat",
            "over-ear headphones", "with an earpice", "with in-ear headphones", "with a bluetooth headset", "with a VR headset",
        ],
        "Female": [
            "wearing a sun hat", "with a beret", "sporting a fascinator", "with a cloche hat", "with a traditional Chinese hair stick",
            "wearing a pillbox hat", "with a wide-brimmed hat", "with a headband", "with a headscarf",
            "wearing a beanie", "with a flower crown", "sporting a fedora", "with a bucket hat", "wearing a hijab",
            "wearing a turban", "with a baseball cap", "sporting a bandana", "with a hijab", "wearing a hair wrap",
            "wearing a veiled hat", "with a cowboy hat", "sporting a newsboy cap", "with a beret", "wearing a beaded African headwrap",
            "wearing a knit hat", "with a visor", "sporting a bonnet", "with a tam hat", "with a scarf",
            "wearing a cocktail hat", "with a lampshade hat", "sporting a toque", "with a furry trapper hat",
            "over-ear headphones", "with an earpice", "with in-ear headphones", "with a bluetooth headset", "with a VR headset",
        ],
        "Unisex": [
            "with a beanie", "wearing a snapback cap", "sporting a bucket hat", "with a bandana", "with a scrunchie", 
            "with a traditional Native American headdress", "with a military beret",
            "wearing a fedora", "with a baseball cap", "sporting a sun visor", "with a headband", "with hairpins",
            "wearing a flat cap", "with a beret", "sporting a trucker hat", "with a cowboy hat", "with a summer scarf",
            "wearing a knit cap", "with a military cap", "sporting a panama hat", "with a headscarf", "wearing a traditional Russian ushanka",
            "wearing a turban", "with a boonie hat", "sporting a cadet cap", "with a helmet", "with a winter scarf", "wearing a crown",
            "wearing a straw hat", "with a ski mask", "sporting a floppy hat", "with a trapper hat", "with decorative bobby pins",
        ]
    }
    
    all_headwear = get_all_unique_dict_values(hats_and_headwear_dict)
    if sex_group is None or random.random() > stereotype_prob:
        return random.choice(all_headwear)
    else:
        return random.choice(hats_and_headwear_dict.get(sex_group, all_headwear))

def get_random_jewelry(sex_group=None, stereotype_prob=0.8):
    jewelry_dict = {
        "Male": [
            "wearing a chain necklace", "with a simple ear stud", "sporting a bolo tie",
            "with a dog tag necklace", "wearing a small hoop earring", "with a nose stud",
            "sporting a tribal necklace", "with an eyebrow ring", "wearing a leather cord necklace",
            "with a septum piercing", "sporting a single diamond stud", "with a small gauge ear piercing",
            "wearing a thin gold chain", "with a curved barbell eyebrow piercing", "with a bowtie", 
            "with multiple ear piercings", "wearing a pendant necklace", "with a helix ear piercing",
            "with a tragus piercing", "wearing a silver chain", "sporting a shark tooth necklace",
            "with a crystal stud earring", "sporting a tongue piercing", "with a daith piercing"
        ],
        "Female": [
            "wearing hoop earrings", "with a pendant necklace", "sporting a choker", "with an eyebrow ring",
            "with pearl earrings", "wearing a statement necklace", "with chandelier earrings",
            "sporting a delicate chain necklace", "with a nose stud", "wearing drop earrings",
            "with a pearl choker", "sporting multiple ear piercings", "with a septum ring",
            "wearing a locket necklace", "with stud earrings", "sporting a bib necklace", "wearing geometric earrings",
            "with a nose ring", "wearing tassel earrings", "with a layered necklace", "wearing climbing vine ear cuffs",
            "sporting a crystal choker", "with ear cuffs", "wearing a cameo necklace",
            "with a tongue piercing", "sporting dangle earrings", "with a septum clicker", "with a bowtie",
        ],
        "Unisex": [
            "wearing a simple necklace", "with stud earrings", "sporting a nose ring", "with a bowtie",
            "with a choker", "wearing a pendant", "with multiple ear piercings", "with a dermal piercing",
            "sporting an ear cuff", "with a septum piercing", "wearing a chain necklace", 
            "with hoop earrings", "sporting a tongue stud", "with a cartilage piercing",
            "wearing a beaded necklace", "with a tragus piercing", "sporting a nose stud",
            "with an industrial bar piercing", "wearing a collar necklace", "with a conch piercing",
            "sporting a septum ring", "with dangle earrings", "wearing a torque necklace",
            "with a labret piercing", "sporting a helix piercing", "with a rook piercing", "with a lip ring", 
        ]
    }
    
    all_jewelry = get_all_unique_dict_values(jewelry_dict)
    if sex_group is None or random.random() > stereotype_prob:
        return random.choice(all_jewelry)
    else:
        return random.choice(jewelry_dict.get(sex_group, all_jewelry))

def get_weight_description():
    weight_descriptions_list = [
        "very slim", "slender", "lean", "willowy", "lithe", "waifish", "svelte",
        "thin", "skinny", "gaunt", "bony", "emaciated",
        "of average build", "with a moderate frame", "neither thin nor overweight",
        "with a balanced physique", "of normal weight", "with a typical body type",
        "curvy", "full-figured", "plump", "chubby", "rounded", "soft",
        "with a bit of extra weight", "slightly heavy-set",
        "heavyset", "portly", "stout", "corpulent", "rotund", "plush",
        "plus-sized", "full-bodied", "generously proportioned",
        "muscular", "athletic", "well-built", "toned", "fit", "strapping",
        "brawny", "burly", "robust", "solid",
        "with a unique body type", "with a distinctive physique",
        "with an unconventional build", "with an interesting silhouette"
    ]
    return random.choice(weight_descriptions_list)

def get_random_time_of_day():
    times_of_day_list = [
        'at dawn', 'at dusk', 'at twilight', 'during sunset', 'during sunrise', 'at midnight', 'at afternoon', 
        'at late afternoon', 'at golden hour', 'at midday', 'at noon', 'at night', 'in the evening', 'in the morning', 
        'in the afternoon', 'at the stroke of midnight', 'in the wee hours', 'at the crack of dawn', 'at high noon',
        'during the witching hour', 'at brunch time', 'during tea time', 'at supper time', 'during happy hour',
        'at the eleventh hour', 'during siesta time', 'at bedtime', 'at the break of day', 'during the dog days of summer',
        'during early morning', 'during late morning', 'during early evening', 'during late evening',
        'at cocktail hour', 'during lunchtime', 'during dinnertime', 'at the blue hour', 'at the magic hour',
        'during rush hour', 'at daybreak', 'at sundown', 'during civil twilight', 'during nautical twilight',
        'during astronomical twilight', 'at first light', 'at last light', 'during solar noon', 'during solar midnight'
    ]
    time_of_day = np.random.choice(times_of_day_list)
    return time_of_day

def get_random_weather_condition():
    weather_conditions_list = [
        'while its raining', 'while its snowing', 'when scorching hot', 'in perfect weather',
        'during a thunderstorm', 'during a heatwave', 'during a cold snap', 'during a drizzle', 'during a hailstorm',
        'during a sandstorm', 'during a snowstorm', 'during a windstorm', 'during a foggy day', 'during a cloudy day',
        'during a sunny day', 'during an overcast day', 'during a monsoon', 'during a hurricane', 'during a tornado',
        'during a blizzard', 'during an earthquake', 'during a solar eclipse', 'during a lunar eclipse', 'during a meteor shower',
        'during high tide', 'during low tide', 'during a rainbow', 'during a flood', 'during a drought',
        'during a wildfire', 'during a volcanic eruption', 'during an avalanche', 'during a cyclone', 'during a typhoon',
        'during an ice storm', 'during a misty morning', 'during a humid afternoon', 'during a dry evening', 'during a muggy night'
    ]
    weather_condition = np.random.choice(weather_conditions_list)
    return weather_condition

def get_eye_description(ethnicity_group=None, stereotype_prob=0.3):

    all_eye_colors = get_all_unique_dict_values(eye_colors_dict)
    if ethnicity_group is None or np.random.rand() > stereotype_prob:
        eye_colors_list = all_eye_colors
    else:
        eye_colors_list = eye_colors_dict.get(ethnicity_group, all_eye_colors)

    color = random.choice(eye_colors_list)

    eye_styles_list = [
        "Captivating {color} eyes", "{color} eyes lost in thought", "Piercing {color} eyes", 'huge {color} eyes',
        "Striking {color} eyes", "{color} eyes, large and expressive", "Small {color} eyes", 'large {color} eyes',
        "Inviting {color} eyes", "{color} eyes, wide open", "Closed, {color} eyes", "Sparkling {color} eyes",
        "Twinkling {color} eyes", "plain {color} eyes", "Regular {color} eyes", "Mysterious {color} eyes",
        "Expressive {color} eyes", "Glistening {color} eyes", "Luminous {color} eyes", "Focused {color} eyes",
        "Dreamy {color} eyes", "intense {color} eyes", "Gentle {color} eyes", "Curious {color} eyes", 'large striking {color} eyes',
        "Alluring {color} eyes", "haunting {color} eyes", "Innocent {color} eyes", "Hypnotic {color} eyes",
        "Mesmerizing {color} eyes", "animated {color} eyes", "Sleepy {color} eyes", "Observant {color} eyes",
        "deep set {color} eyes", "bulging {color} eyes", "Hooded, {color} eyes", "Almond shaped, {color} eyes",
        "round {color} eyes", "wide {color} eyes", "Narrow, {color} eyes", "Cat-like {color} eyes",
        "winking {color} eyes", "dilated pupil, {color} eyes", "{color} eyes with an enigmatic hue", 'deep {color} eyes',
        "{color} eyes radiating vibrant light", "{color} eyes deep in introspection", "{color} eyes resolute and firm",
        "{color} eyes reflecting a whimsical sparkle", "{color} eyes sorrowful and deep", 
        "{color} eyes brimming with joy", "{color} eyes in a contemplative state", 
        "{color} eyes emanating serenity", "{color} eyes ablaze with intensity"
    ]

    eye_style = random.choice(eye_styles_list)
    eye_description = eye_style.format(color=color)

    return eye_description

def get_clothing_description(sex_group=None, sterotype_prob=0.6):

    clothing_color_list = random.choice([
        "red", "blue", "green", "yellow", "purple", "pink", "orange",
        "black", "white", "gray", "brown", "navy", "teal", "maroon", "olive",
        "beige", "turquoise", "lavender", "crimson", "indigo", "magenta",
        "chartreuse", "burgundy", "periwinkle", "coral", "mustard", "plum",
        "khaki", "mauve", "salmon", "mint", "gold", "silver", "bronze",
        "copper", "platinum", "pastel pink", "pastel blue", "pastel green",
        "pastel yellow", "pastel purple", "emerald", "sapphire", "ruby",
        "amethyst", "topaz", "garnet", "ivory", "cream", "tan", "taupe",
        "charcoal", "slate"
    ])

    clothing_patterns_list = [
        "striped", "polka dot", "floral", "plaid", "checkered", "paisley",
        "herringbone", "houndstooth", "geometric pattern"
    ]

    clothing_types_dict = {
        "casual": {
            "neutral": ["t-shirt", "jeans", "sweater", "hoodie", "shorts", "tracksuit"],
            "male": ["polo shirt"],
            "female": ["leggings", "yoga pants", "tank top"]
        },
        "formal": {
            "neutral": ["suit"],
            "male": ["tuxedo", "dress shirt", "tie"],
            "female": ["cocktail dress", "evening gown", "blouse", "pencil skirt"]
        },
        "professional": {
            "neutral": ["business suit", "slacks"],
            "male": ["necktie"],
            "female": ["pantsuit", "blouse", "knee-length skirt"]
        },
        "outerwear": {
            "neutral": ["jacket", "coat", "trench coat", "parka", "windbreaker", "peacoat", "leather jacket", "denim jacket", "bomber jacket"]
        },
        "dresses_skirts": {
            "female": ["sundress", "maxi dress", "mini skirt", "midi skirt", "wrap dress", "shirt dress", "A-line dress", "pleated skirt", "tulle skirt"],
            "male": ["scottish kilt", "togas"]
        },
        "ethnic": {
            "neutral": ["kaftan", "poncho", "tunic"],
            "male": ["kurta", "sherwani", "kilt", "lederhosen"],
            "female": ["sari", "cheongsam", "dirndl", "ao dai", "hanbok", "qipao", "yukata"]
        },
        "uniform": {
            "neutral": ["military uniform", "police uniform", "firefighter uniform", "doctor's white coat", "chef's uniform", "pilot's uniform", "nurse's scrubs", "judge's robe", "academic regalia"]
        },
        "sports": {
            "neutral": ["soccer jersey", "basketball uniform", "tennis whites", "cycling gear", "martial arts gi"],
            "female": ["leotard", "gymnast outfit", "yoga attire"]
        },
        "workwear": {
            "neutral": ["overalls", "coveralls", "high-visibility vest", "lab coat", "welder's protective gear"]
        },
        "unique": {
            "neutral": ["avant-garde designer piece", "futuristic bodysuit", "steampunk-inspired outfit", "cyberpunk ensemble"]
        },
        "historical": {
            "neutral": ["Renaissance costume"],
            "male": ["Victorian-era suit", "1920s gangster style"],
            "female": ["Victorian-era dress", "1920s flapper style", "1950s rockabilly fashion"]
        },
        "religious": {
            "neutral": ["ceremonial tribal wear", "traditional wedding attire"],
            "male": ["monk's robe"],
            "female": ["nun's habit"]
        }
    }

    if sex_group is None or np.random.rand() > sterotype_prob:
        sex_group = random.choice(['male', 'female', 'neutral'])

    assert sex_group in ['male', 'female', 'neutral']

    clothing_category = random.choice(list(clothing_types_dict.keys()))
    all_clothing_category_items = get_all_unique_dict_values(clothing_types_dict[clothing_category])
    clothing_items_list = clothing_types_dict[clothing_category].get(sex_group, all_clothing_category_items)
    if sex_group in ['male', 'female']:
        clothing_items_list += clothing_types_dict[clothing_category].get('neutral', all_clothing_category_items)
    
    clothing_item = random.choice(clothing_items_list)
    clothing_color = random.choice(clothing_color_list)

    if clothing_category in ["casual", "formal", "professional", "outerwear", "dresses_skirts"]:
        if random.random() < 0.2:
            pattern = random.choice(clothing_patterns_list)
            clothing_str = f"wearing a {pattern} {clothing_item}"
        else:
            clothing_str = f"wearing a {clothing_color} {clothing_item}"
    elif clothing_category in ["ethnic", "unique", "historical"]:
        clothing_str = f"dressed in {clothing_item}"
    elif clothing_category in ["uniform", "sports", "workwear", "religious"]:
        clothing_str = f"in {clothing_item}"
    else:
        clothing_str = f"wearing {clothing_item}"

    # if we are not using a stereotype, we can mix and match clothing items from different categories
    if np.random.rand() > sterotype_prob:
        clothing_items_list = [item for sublist in clothing_types_dict[clothing_category].values() for item in sublist]
        clothing_str = f"wearing {random.choice(clothing_items_list)}"
        return clothing_str

    return clothing_str

def get_random_modifier_string():

    modifier_str = ''
    if np.random.rand() < 0.2:
        modifier_str = modifier_str + 'wearing traditional attire, '
    if np.random.rand() < 0.1:
        modifier_str = modifier_str + 'casual pose, '
    if np.random.rand() < 0.3:
        modifier_str = modifier_str + np.random.choice(['photography, ', 'professional photography, ', 'photorealism, ', 'ultrarealistic uhd faces, '])
    if np.random.rand() < 0.2:
        modifier_str = modifier_str + np.random.choice(['hyper realism, ',  'realistic, ', 'ultra realistic, ',
                                                        'highly detailed, ', 'very detailed, ', 'hyper detailed, ', 'detailed, '])
    if np.random.rand() < 0.6:
        modifier_str = modifier_str + np.random.choice(['detailed skin, ', 'detailed skin texture, ', 'detailed skin pores, '])
    if np.random.rand() < 0.5:
        modifier_str = modifier_str + 'bokeh, '
    if np.random.rand() < 0.5:
        modifier_str = modifier_str + np.random.choice(['film, ', 'still from a film, ', 'raw candid cinema, ', 'cinematic movie still, '])
    if np.random.rand() < 0.2:
        modifier_str = modifier_str + np.random.choice(['head shot, ', 'medium shot, ', 'wide shot, ', 'zoomed out, '])
    if np.random.rand() < 0.2:
        modifier_str = modifier_str + np.random.choice(['triadic color scheme, ', 'vivid color, ', 'remarkable color, ', 'color graded, '])
    if np.random.rand() < 0.5:
        modifier_str = modifier_str + np.random.choice(['studio lighting, ', 'volumetric lighting, ', 'subsurface scatter, ', 'natural light, ', 'soft light, ', 'hard light, ',
                                                        'atmospheric lighting, ', 'cinematic lighting, ', 'dramatic lighting, ', 'hard rim lighting photography, '])
    if np.random.rand() < 0.3:
        modifier_str = modifier_str + np.random.choice(['4k, ', '8k, ', 'uhd, ', 'ultra hd, ', 'high quality, ', 'HDR, '])
    if np.random.rand() < 0.3:
        modifier_str = modifier_str + np.random.choice(['nikon d850, ', 'kodachrome 25, ', 'kodak ultra max 800, ', 'kodak portra 160, ', 'DSLR camera, ',
                                                        'canon eos r3, ', 'Ilford HP5 400, ', 'samsung nx300m, ', 'sony a6000, ', 'olympus om-d, ',
                                                        'panasonic lumix dmc-gx85, ', 'fujifilm x70, ', 'canon eos, ', 'color graded porta 400 film, '])
    if np.random.rand() < 0.1:
        modifier_str = modifier_str + np.random.choice(['120mm, ', '85mm, ', '50mm, ', '35mm, '])
    if np.random.rand() < 0.1:
        modifier_str = modifier_str + np.random.choice(['f/1.4, ', 'f/2.5, ', 'f/3.2, ', 'f/1.1, '])
    if np.random.rand() < 0.1:
        modifier_str = modifier_str + np.random.choice(['35mm film roll photo, ', 'film, ', 'porta 400 film, '])
    if np.random.rand() < 0.1:
        modifier_str = modifier_str + np.random.choice(['iso 120, ', 'iso 210, '])
    if np.random.rand() < 0.1:
        modifier_str = modifier_str + np.random.choice(['wide lens, ', 'lens flare, ', 'sharp focus, ', 'hasselblad, '])
    if np.random.rand() < 0.3:
        modifier_str = modifier_str + np.random.choice(['alluring, ', 'beautiful, ', 'breath-taking, ', 'captivating, ', 'addorable, ', 'intricate, ',
                                                        'chic, ', 'classy, ', 'curvaceous, ', 'breath-cute, ', 'fashionable, ', 'elegant, ',
                                                        'gorgeous, ', 'graceful, ', 'lovely, ', 'mesmerizing, ', 'petite, ', 'pretty, ', 'tall, ',
                                                        'radiant, ', 'ravishing, ', 'slim, ', 'stunning, ', 'stylish, ', 'sultry, ', 'sweet, ',
                                                        'affectionate, ', 'ardent, ', 'articulate, ', 'at ease, ', 'attentive, ', 'awake, ',
                                                        'aware, ', 'boyish, ', 'brave, ', 'broad-shouldered, ', 'calm, ', 'voluptuous, ',
                                                        'caring, ', 'centered, ', 'charming, ', 'chiseled cheekbones, ', 'sharp features, ',
                                                        'classic good looks, ', 'clean-shaven, ', 'clever, ', 'compassionate, ', 'candid, ', 'attractive, ',
                                                        'confident, ', 'conscious, ', 'considerate, ', 'content, ', 'cosmopolitan, ',
                                                        'courageous, ', 'courteous, ', 'cultured, ', 'dark skin, ', 'dashing, ',
                                                        'debonair, ', 'defined jawline, ', 'devoted, ', 'educated, ', 'eloquent, ',
                                                        'faithful, ', 'fearless, ', 'firm skin, ', 'focused, ', 'full lips, ',
                                                        'fully engaged, ', 'gentle, ', 'glowing skin, ', 'grounded, ', 'handsome, ',
                                                        'handsome features, ', 'in the moment, ', 'insightful, ', 'intelligent, ',
                                                        'intense, ', 'kind, ', 'loyal, ', 'mannerly, ', 'mischievous, ', 'muscular, ',
                                                        'olive skin, ', 'passionate, ', 'peaceful, ', 'polite, ', 'porcelain skin, ',
                                                        'present, ', 'refined, ', 'reliable, ', 'rugged, ', 'secure, ', 'self assured, ',
                                                        'sensitive, ', 'serene, ', 'smooth skin, ', 'soft skin, ', 'sophisticated, ',
                                                        'square-jawed, ', 'stable, ', 'strong, ', 'strong chin, ', 'suave, ',
                                                        'sun kissed skin, ', 'thick, ', 'trustworthy, ', 'twinkling eyes, ', 'urbane, ',
                                                        'well mannered, ', 'well spoken, ', 'well traveled, ', 'well-built, ', 'witty, ', 'worldly, '])
    if np.random.rand() < 0.2:
        modifier_str = modifier_str + np.random.choice(['subtle shadows, ', 'shadow, '])
    if np.random.rand() < 0.05:
        modifier_str = modifier_str + np.random.choice(['mist, ', 'wet, ', 'foggy background, '])
    if np.random.rand() < 0.05:
        modifier_str = modifier_str + 'golden ratio composition, '
    if np.random.rand() < 0.05:
        modifier_str = modifier_str + 'dramatic, '
    if np.random.rand() < 0.05:
        modifier_str = modifier_str + 'award winning photograph, '
    if np.random.rand() < 0.05:
        modifier_str = modifier_str + 'epic composition, '
    if np.random.rand() < 0.05:
        modifier_str = modifier_str + 'pexels, '
    if np.random.rand() < 0.05:
        modifier_str = modifier_str + 'high contrast, '

    return modifier_str[:-2]

def get_all_unique_dict_values(property_dict):
    all_unique_values = []
    for key, values in property_dict.items():
        all_unique_values.extend(values)
    all_unique_values = list(set(all_unique_values))

    return all_unique_values

def get_random_ethnicity(ethnicity_group=None, top_level_prob=0.5):    
    if ethnicity_group is None:
        ethnicity_group = np.random.choice(list(ethnicities_dict.keys()))

    if np.random.rand() < top_level_prob:
        ethnicity = ethnicity_group
    else:
        ethnicity = np.random.choice(ethnicities_dict[ethnicity_group])
    
    return ethnicity

def get_age_sex_ethnicity(ethnicity_group=None, sex_group=None, age_group=None):
    # Age group definitions
    age_groups = {
        "baby": (1, 18),          # 1-18 months
        "toddler": (1, 4),        # 1-4 years
        "child": (4, 12),         # 4-12 years
        "teenager": (13, 19),     # 13-19 years
        "young adult": (18, 38),  # 18-38 years
        "middle-aged": (30, 55),  # 30-55 years
        "elderly": (50, 100)      # 50-100 years
    }

    # sample age group if not provided
    if age_group is None:
        age_group = np.random.choice(list(age_groups.keys()))

    # sample sex group if not provided
    if sex_group is None:
        sex_group = np.random.choice(['male', 'female'])

    # sample ethnicity based on the cluster
    ethnicity = get_random_ethnicity(ethnicity_group)

    # sample age based on age group range
    age_min, age_max = age_groups[age_group]
    age = np.random.choice(range(age_min, age_max + 1))

    if age_group == 'baby':
        sex = 'boy' if sex_group == 'male' else 'girl'
        age_sex_ethnicity_str = f'{age} month old {ethnicity} baby {sex}'
    elif age_group == 'toddler':
        sex = 'boy' if sex_group == 'male' else 'girl'
        age_sex_ethnicity_str = f'{age} year old {ethnicity} toddler {sex}'
    elif age_group == 'child':
        sex = 'boy' if sex_group == 'male' else 'girl'
        age_sex_ethnicity_str = f'{age} year old {ethnicity} {sex}'
    elif age_group == 'teenager':
        sex = 'boy' if sex_group == 'male' else 'girl'
        age_sex_ethnicity_str = f'{age} year old {ethnicity} teenage {sex}'
    elif age_group == 'young adult':
        if sex_group == 'male':
            sex = np.random.choice(['man', 'guy', 'person', 'male', 'brother'])
        else:
            sex = np.random.choice(['woman', 'dame', 'lady', 'female', 'sister'])
        sex = f'young {sex}' if np.random.rand() < 0.25 else sex
        age_sex_ethnicity_str = f'{age} year old {ethnicity} {sex}'
    elif age_group == 'middle-aged':
        if sex_group == 'male':
            sex = np.random.choice(['man', 'guy', 'husband', 'person', 'male', 'father', 'gentleman'])
        else:
            sex = np.random.choice(['woman', 'dame', 'wife', 'lady', 'female', 'mother', 'gentlewoman'])
        sex = f'middle-aged {sex}' if np.random.rand() < 0.25 else sex
        age_sex_ethnicity_str = f'{age} year old {ethnicity} {sex}'
    elif age_group == 'elderly':
        if sex_group == 'male':
            sex = np.random.choice(['man', 'grandfather', 'grandpa', 'person', 'father', 'husband', 'gentleman'])
        else:
            sex = np.random.choice(['woman', 'grandmother', 'grandma', 'lady', 'mother', 'wife', 'gentlewoman'])
        sex = f'elderly {sex}' if np.random.rand() < 0.25 else sex
        age_sex_ethnicity_str = f'{age} year old {ethnicity} {sex}'

    return age_sex_ethnicity_str

def get_random_expression():
    return random.choice(expressions_list)

def get_lighting_atmosphere(lighting_category=None):
    if lighting_category is None:
        lighting_category = random.choice(list(lighting_descriptions_dict.keys()))
    return random.choice(lighting_descriptions_dict[lighting_category])

def get_hair_description(ethnicity_group=None, sterotype_prob=0.3):

    all_hair_colors = get_all_unique_dict_values(hair_colors_dict)
    if ethnicity_group is None or np.random.rand() > sterotype_prob:
        hair_colors_list = all_hair_colors
    else:
        hair_colors_list = hair_colors_dict.get(ethnicity_group, all_hair_colors)
    
    color = random.choice(hair_colors_list)
    style = random.choice(hair_styles_list)
    return f"with {color} {style}"

def get_random_face_pose():
    return random.choice(face_poses_list)


#%% main face prompt generator function

def generate_face_prompt(ethnicity_group=None, sex_group=None, age_group=None,
                         lighting_category=None, num_elements_to_add=None):
    
    # Sample demographic information
    if ethnicity_group is None:
        ethnicity_group = random.choice(list(ethnicities_dict.keys()))
    if sex_group is None:
        sex_group = np.random.choice(['male', 'female'], p=[0.5, 0.5])
    if age_group is None:
        age_group = np.random.choice(['baby', 'toddler', 'child', 'teenager', 'young adult', 'middle-aged', 'elderly'],
                                     p=[0.05, 0.05, 0.05, 0.2, 0.2, 0.2, 0.25])

    # Generate base prompt
    prompt_start = get_prompt_start()
    age_sex_ethnicity = get_age_sex_ethnicity(ethnicity_group, sex_group, age_group)
    base_prompt = f"{prompt_start} {age_sex_ethnicity}, "

    # Generate additional elements
    elements = [
        get_random_face_pose(),
        get_random_gaze_direction(),
        get_hair_description(ethnicity_group),
        get_eye_description(ethnicity_group),
        get_skin_description(ethnicity_group),
        get_clothing_description(sex_group),
        get_hats_and_headwear(sex_group),
        get_random_jewelry(sex_group),
        get_random_glasses(),
        get_random_time_of_day(),
        get_random_weather_condition(),
        get_weight_description(),
        get_random_modifier_string(),
        get_lighting_atmosphere(lighting_category),
    ]
    elements = elements + elements[-3:]  # repeat last 3 elements to increase their probability
    
    # add facial hair possibility when needed
    if sex_group == 'male' and age_group in ['teenager', 'young adult', 'middle-aged', 'elderly']:
        elements.append(ger_facial_hair_description())

    # add makeup possibility when needed
    if sex_group == 'female' and age_group in ['teenager', 'young adult', 'middle-aged', 'elderly']:
        elements.append(get_makeup_description())
        elements = elements + [elements[-1]] # repeat makeup description to increase its probability
        elements = elements + [elements[-1]] # repeat makeup description to increase its probability

    # Randomly select subset of elements
    if num_elements_to_add is None:
        num_elements_to_add = random.randint(4, 9)
    selected_elements = random.sample(elements, min(num_elements_to_add, len(elements)))
    selected_elements = list(set(selected_elements)) # remove duplicates
    selected_elements = [get_random_expression()] + selected_elements # always add expression at the beginning
    selected_elements.append(get_location_setting_background()) # always add location setting background at the end

    # Combine base prompt with selected elements
    full_prompt = f"{base_prompt} {', '.join(selected_elements)}"

    return full_prompt

def display_conditions(conditions_dict):
    print('Conditions:')
    print('-----------')
    for key, value in conditions_dict.items():
        print(f'  {key} = {value}')

def get_formatted_prompt_for_display(prompt, max_line_length=85):
    parts = [part.strip() for part in prompt.split(',')]
    formatted_prompt = ""
    current_line = ""

    for i, part in enumerate(parts):
        if len(current_line) + len(part) > max_line_length:
            if formatted_prompt:
                formatted_prompt += ',\n'
            formatted_prompt += current_line
            current_line = part
        else:
            if current_line:
                current_line += ", " + part
            else:
                current_line = part

    if current_line:
        if formatted_prompt:
            formatted_prompt += ',\n'
        formatted_prompt += current_line

    return formatted_prompt


#%% main test

if __name__ == "__main__":
    num_samples_per_type = 10
    show_conditional_sampling = True
    show_conditional_sampling = False
    print("Generating face prompts...\n")

    all_ethinicity_groups = list(ethnicities_dict.keys())
    all_lighting_categories = list(lighting_descriptions_dict.keys())
    all_age_groups = ['baby', 'toddler', 'child', 'teenager', 'young adult', 'middle-aged', 'elderly']
    all_sex_groups = ['male', 'female']

    print('=' * 100)
    print("1. Unconditioned sampling:")
    for i in range(num_samples_per_type):
        prompt = generate_face_prompt()
        print(f"Prompt {i+1}: \n----------")
        print(get_formatted_prompt_for_display(prompt))
        print()
    print('=' * 100)
    print('\n\n')

    if show_conditional_sampling:
        print('=' * 100)
        print("2. Partially Conditioned sampling:")
        conditions_dict_list = [
            {"ethnicity_group": random.choice(all_ethinicity_groups), 'sex_group': random.choice(all_sex_groups), 'age_group': random.choice(all_age_groups)},
            {"ethnicity_group": random.choice(all_ethinicity_groups), 'sex_group': random.choice(all_sex_groups)},
            {'age_group': random.choice(all_age_groups), "lighting_category": random.choice(all_lighting_categories)},
            {'sex_group': random.choice(all_sex_groups), 'age_group': random.choice(all_age_groups)},
            {'sex_group': random.choice(all_sex_groups), 'age_group': random.choice(all_age_groups)},
            {"lighting_category": random.choice(all_lighting_categories), "num_elements_to_add": np.random.randint(2, 5)},
            {"lighting_category": random.choice(all_lighting_categories), "num_elements_to_add": np.random.randint(4, 10)},
            {"lighting_category": random.choice(all_lighting_categories), "num_elements_to_add": np.random.randint(7, 12)},
        ]

        for i, conditions_dict in enumerate(conditions_dict_list):
            prompt = generate_face_prompt(**conditions_dict)
            print('=' * 80)
            display_conditions(conditions_dict)
            print('-' * 40)
            print(f"Prompt {i+1}: \n---------")
            print(get_formatted_prompt_for_display(prompt))
            print('=' * 80)
        print('=' * 100)
        print('\n\n')

        print('=' * 100)
        print("3. Fully Conditioned sampling:")
        for i in range(num_samples_per_type):
            ethnicity_group = random.choice(all_ethinicity_groups)
            sex_group = random.choice(all_sex_groups)
            age_group = random.choice(all_age_groups)
            lighting_category = random.choice(all_lighting_categories)
            num_elements_to_add = np.random.randint(1, 5)

            prompt = generate_face_prompt(
                ethnicity_group=ethnicity_group, 
                sex_group=sex_group,
                age_group=age_group,
                lighting_category=lighting_category,
                num_elements_to_add=num_elements_to_add
            )

            conditions_dict = {
                'ethnicity_group': ethnicity_group, 
                'sex_group': sex_group,
                'age_group': age_group,
                'lighting_category': lighting_category,
                'num_elements_to_add': num_elements_to_add
            }

            print('=' * 80)
            display_conditions(conditions_dict)
            print('-' * 40)
            print(f"Prompt {i+1}: \n---------")
            print(get_formatted_prompt_for_display(prompt))
            print('=' * 80)
        print('=' * 100)
        print('\n\n')

# %%

================================================================================
================================================================================
explore_dataset.py:
===================
#%% Imports 

import os
import glob
import torch
import open_clip
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from PIL import Image
from extract_pretrained_features import extract_pretrained_features, load_openclip_model
from extract_pretrained_features import collect_pretrained_features_from_folder

#%% Helper functions

def plot_model_distribution(df):
    model_counts = df['model_used'].value_counts().sort_values(ascending=False)
    
    fig, ax = plt.subplots(figsize=(12, 6))
    ax.bar(model_counts.index, model_counts.values)
    ax.set_title('Distribution of Images Across Models')
    ax.set_xlabel('Model')
    ax.set_ylabel('Number of Images')
    plt.xticks(rotation=45, ha='right')
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)

    # Add value labels on top of each bar
    for i, v in enumerate(model_counts.values):
        ax.text(i, v, str(v), ha='center', va='bottom')
    
    return fig

def plot_prompt_length_distribution(df):
    # Calculate prompt lengths
    df['prompt_chars'] = df['text_prompt'].str.len()
    df['prompt_words'] = df['text_prompt'].str.split().str.len()

    # Set up the plot
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))

    # Colors for each model
    models = df['model_used'].unique()
    colors = plt.cm.rainbow(np.linspace(0, 1, len(models)))

    # Plot character length distribution
    for model, color in zip(models, colors):
        model_data = df[df['model_used'] == model]['prompt_chars']
        ax1.hist(model_data, bins=50, alpha=0.5, label=model, color=color)
    
    ax1.set_title('Prompt Length in Characters', fontsize=15)
    ax1.set_xlabel('Number of Characters', fontsize=13)
    ax1.set_ylabel('Frequency', fontsize=13)
    # ax1.set_yscale('log')
    ax1.legend(fontsize=14)

    # Plot word length distribution
    for model, color in zip(models, colors):
        model_data = df[df['model_used'] == model]['prompt_words']
        ax2.hist(model_data, bins=20, alpha=0.5, label=model, color=color)
    
    ax2.set_title('Prompt Length in Words', fontsize=15)
    ax2.set_xlabel('Number of Words', fontsize=13)
    ax2.set_ylabel('Frequency', fontsize=13)
    # ax2.set_yscale('log')
    ax2.legend(fontsize=14)

    plt.tight_layout()
    return fig

def format_prompt(prompt, max_width=85, min_width=55):
    words = prompt.split()
    lines = []
    current_line = ""

    for word in words:
        if len(current_line) + len(word) + 1 > max_width:
            lines.append(current_line)
            current_line = word
        else:
            if current_line:
                current_line += " " + word
            else:
                current_line = word

            if len(current_line) >= min_width and (current_line.endswith(',') or current_line.endswith('.')):
                lines.append(current_line) 
                current_line = ""

    if current_line:
        lines.append(current_line)

    return '\n'.join(lines)

def remove_borders(ax):
    ax.set_xticks([])
    ax.set_yticks([])
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['bottom'].set_visible(False)
    ax.spines['left'].set_visible(False)

def display_single_random_image(df, image_folder, model_to_use=None):
    if model_to_use is not None:
        df_to_use = df[df['model_used'] == model_to_use]
    else:
        df_to_use = df
    
    random_row = df_to_use.sample(n=1).iloc[0]
    image_path = os.path.join(image_folder, random_row['image_filename'])
    
    img = Image.open(image_path)
    
    fig, ax = plt.subplots(figsize=(14, 11))
    plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.2)
    ax.imshow(img)
    
    title = f"Model: {random_row['model_used']}, filename = '{random_row['image_filename']}'"
    ax.set_title(title, fontsize=14)
    
    formatted_prompt = format_prompt(random_row['text_prompt'])
    ax.set_xlabel(formatted_prompt, fontsize=11)
    
    return fig

def display_single_model_images(df, image_folder, model_to_use=None):
    if model_to_use is not None:
        df_to_use = df[df['model_used'] == model_to_use]
    else:
        df_to_use = df

    if len(df_to_use) < 2:
        raise ValueError("Not enough images to display.")

    random_rows = df_to_use.sample(n=2)

    image_paths = []
    titles = []
    prompts = []
    for _, row in random_rows.iterrows():
        image_path = os.path.join(image_folder, row['image_filename'])
        image_paths.append(image_path)
        title = f"Model: {row['model_used']}\nfilename = '{row['image_filename']}'"
        titles.append(title)
        formatted_prompt = format_prompt(row['text_prompt'], max_width=80, min_width=55)
        prompts.append(formatted_prompt)

    fig, axes = plt.subplots(1, 2, figsize=(20, 13))
    fig.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.2, wspace=0.05)

    for ax, img_path, title, prompt in zip(axes, image_paths, titles, prompts):
        img = Image.open(img_path)
        ax.imshow(img)
        ax.set_title(title, fontsize=18)
        remove_borders(ax)
        ax.set_xlabel(prompt, fontsize=15, ha='center', va='top')

    return fig

def display_multi_model_images(df, image_folder, num_cols=5, models_names=None, display_prompt=False):
    if models_names is None:
        models_names = df['model_used'].unique().tolist()
    num_rows = len(models_names)
    
    if display_prompt:
        fig, axes = plt.subplots(num_rows, num_cols, figsize=(4*num_cols, 6*num_rows))
        fig.subplots_adjust(hspace=0.3, wspace=0.2, top=0.9)
    else:
        fig, axes = plt.subplots(num_rows, num_cols, figsize=(4*num_cols, 4*num_rows))
        fig.subplots_adjust(hspace=0.12, wspace=0.04)
    
    for row, model in enumerate(models_names):
        model_df = df[df['model_used'] == model]
        sample_df_rows = model_df.sample(n=min(num_cols, len(model_df)))
        
        for col, (_, sample_df_row) in enumerate(sample_df_rows.iterrows()):
            ax = axes[row, col] if num_rows > 1 else axes[col]
            
            image_path = os.path.join(image_folder, sample_df_row['image_filename'])
            img = Image.open(image_path)
            
            ax.imshow(img)
            remove_borders(ax)

            ax.set_title(sample_df_row['image_filename'], fontsize=10, wrap=True)
            
            if col == 0:
                ax.set_ylabel(model, fontsize=16)
            
            if display_prompt:
                formatted_prompt = format_prompt(sample_df_row['text_prompt'], max_width=40, min_width=20)
                ax.text(0, -0.1, formatted_prompt, fontsize=5, ha='left', va='top', 
                        wrap=True, transform=ax.transAxes)
    
    return fig

#%%

if __name__ == "__main__":

    #%% Set the style of the plots
    plt.style.use('dark_background')

    # Set the paths
    dataset_path = r"SFHQ_T2I_dataset"

    csv_path = os.path.join(dataset_path, "SFHQ_T2I_dataset.csv")
    image_folder = os.path.join(dataset_path, "images")

    # Load the dataset csv file
    df = pd.read_csv(csv_path)

    # print several randomly selected prompts to screen
    for i in range(10):
        random_row = df.sample(n=1).iloc[0]
        curr_row_prompt = random_row['text_prompt']
        num_chars = len(curr_row_prompt)
        num_words = len(curr_row_prompt.split())
        print('=' * 90)
        print(f'num chars: {num_chars}, num words: {num_words}')
        print('-' * 30)
        print(format_prompt(curr_row_prompt))
    print('=' * 90)

    # Plot the distribution of images across models
    fig_distribution = plot_model_distribution(df)

    # Plot the distribution of prompt lengths
    fig_prompt_lengths = plot_prompt_length_distribution(df)

    # Display two random images for each model
    model_to_use = 'FLUX1_pro'
    fig_single_pro = display_single_model_images(df, image_folder, model_to_use=model_to_use)

    model_to_use = 'FLUX1_dev'
    fig_single_dev = display_single_model_images(df, image_folder, model_to_use=model_to_use)

    model_to_use = 'FLUX1_schnell'
    fig_single_schnell = display_single_model_images(df, image_folder, model_to_use=model_to_use)

    model_to_use = 'SDXL'
    fig_single_sdxl = display_single_model_images(df, image_folder, model_to_use=model_to_use)

    model_to_use = 'DALLE3'
    fig_single_dalle3 = display_single_model_images(df, image_folder, model_to_use=model_to_use)

    # Display multiple random images for each model
    models_names = ['FLUX1_pro', 'FLUX1_dev', 'FLUX1_schnell', 'SDXL', 'DALLE3']
    num_cols = 8
    fig_all = display_multi_model_images(df, image_folder, num_cols, models_names=models_names)

    models_names = ['FLUX1_pro', 'FLUX1_dev', 'FLUX1_schnell', 'SDXL']
    num_cols = 6
    fig_good_ones = display_multi_model_images(df, image_folder, num_cols, models_names=models_names)

    models_names = ['FLUX1_pro', 'FLUX1_dev', 'FLUX1_schnell']
    num_cols = 4
    fig_flux = display_multi_model_images(df, image_folder, num_cols, models_names=models_names)

    models_names = ['FLUX1_schnell', 'SDXL']
    num_cols = 4
    fig_bulk = display_multi_model_images(df, image_folder, num_cols, models_names=models_names)

    #%% Optionally, save the figures

    # save_figures = False
    save_figures = True

    if save_figures:
        output_folder_path = "figures"
        os.makedirs(output_folder_path, exist_ok=True)

        fig_distribution.savefig(os.path.join(output_folder_path, 'model_distribution.jpg'), bbox_inches='tight')
        fig_prompt_lengths.savefig(os.path.join(output_folder_path, 'prompt_lengths_distribution.jpg'), bbox_inches='tight')

        fig_single_pro.savefig(os.path.join(output_folder_path, 'FLUX1_pro_images_with_prompts.jpg'), bbox_inches='tight')
        fig_single_dev.savefig(os.path.join(output_folder_path, 'FLUX1_dev_images_with_prompts.jpg'), bbox_inches='tight')
        fig_single_schnell.savefig(os.path.join(output_folder_path, 'FLUX1_schnell_images_with_prompts.jpg'), bbox_inches='tight')
        fig_single_sdxl.savefig(os.path.join(output_folder_path, 'SDXL_images_with_prompts.jpg'), bbox_inches='tight')
        fig_single_dalle3.savefig(os.path.join(output_folder_path, 'DALLE3_images_with_prompts.jpg'), bbox_inches='tight')

        fig_all.savefig(os.path.join(output_folder_path, 'all_model_images.jpg'), bbox_inches='tight')
        fig_good_ones.savefig(os.path.join(output_folder_path, 'good_model_images.jpg'), bbox_inches='tight')
        fig_flux.savefig(os.path.join(output_folder_path, 'flux_images.jpg'), bbox_inches='tight')
        fig_bulk.savefig(os.path.join(output_folder_path, 'FLUX1_schnell_SDXL_images.jpg'), bbox_inches='tight')

    #%% select open clip model to use for textual search

    model_type = 'OpenCLIP'
    model_type = 'SigLIP'

    if model_type == 'OpenCLIP':
        model_name = "OpenCLIP_ViT-H-14-378-quickgelu"
        base_model_name = "ViT-H-14-378-quickgelu"
    elif model_type == 'SigLIP':
        model_name = "OpenCLIP_ViT-SO400M-14-SigLIP-384"
        base_model_name = "ViT-SO400M-14-SigLIP"

    # Extract features
    extract_features = False
    # extract_features = True
    if extract_features:
        extract_pretrained_features(dataset_path, model_to_use=model_name)

    # Collect previously extracted features
    image_features, image_filename_map = collect_pretrained_features_from_folder(dataset_path, model_name, normalize_features=True)
    print(f"Image features shape: {image_features.shape}")

    #%% Load the OpenCLIP model for text encoding

    device = "cuda" if torch.cuda.is_available() else "cpu"
    model, preprocess = load_openclip_model(model_name, device)

    # define some text prompts for exploration
    text_prompts = {
        "age_male": ["baby boy", "boy todller", "child boy", "teenage boy", "adult male", "middle-aged adult male", "elderly male"],
        "age_female": ["baby girl", "girl todller", "child girl", "teenage girl", "adult female", "middle-aged adult female", "elderly female"],
        "expression": ["happy", "sad", "angry", "surprised", "neutral", "disgusted", "fearful", "tounge out"],
        "sex": ["male", "female", "non-binary person"],
        "ethnicity": ["Caucasian", "African", "Asian", "Hispanic", "Middle Eastern", "Scandinavian", "Native American"],
        "hair_color": ["black hair", "brown hair", "blonde hair", "red hair", "gray hair", "bald", "blue hair", "green hair", "pink hair"],
        "hats": ["baseball cap", "fedora", "beanie", "top hat", "cowboy hat", "sun hat"],
        "glasses": ["reading glasses", "sunglasses", "round glasses", "square glasses"],
        "accessories": ["earrings", "necklace", "bandana", "hat", "tie", "scarf", "headphones", "sunglasses"],
        "eye_color": ["blue eyes", "green eyes", "brown eyes", "yellow eyes", "hazel eyes", "red eyes"],
    }

    # define prefixes for each category
    prefixes = {
        "age_male": "A photo of a {prompt}",
        "age_female": "A photo of a {prompt}",
        "expression": "A photo of a person who is {prompt}",
        "sex": "A photo of a {prompt}",
        "ethnicity": "A photo of a {prompt} person",
        "hair_color": "A photo of a person with {prompt}",
        "hats": "A photo of a person wearing a {prompt}",
        "glasses": "A photo of a person wearing {prompt}",
        "accessories": "A photo of a person with {prompt}",
        "eye_color": "A photo of a person with {prompt}",
    }

    # encode the text prompts using the OpenCLIP text encoder
    tokenizer = open_clip.get_tokenizer(base_model_name)
    encoded_text_prompts = {}
    for category, prompts in text_prompts.items():
        prefix = prefixes.get(category, "A photo of a person with {prompt}")
        formatted_prompts = [prefix.format(prompt=prompt) for prompt in prompts]
        tokens = tokenizer(formatted_prompts).to(device)
        with torch.no_grad():
            encoded_text_prompts[category] = model.encode_text(tokens).float().cpu().numpy()
        encoded_text_prompts[category] /= np.linalg.norm(encoded_text_prompts[category], axis=1)[:, np.newaxis]

    # Calculate similarities between the texts and images and visualize results
    def visualize_category(category, image_features, encoded_prompts, prompts, num_cols=5, plot_boxplot=False):
        similarities = np.dot(image_features, encoded_prompts.T)
        
        if plot_boxplot:
            plt.figure(figsize=(12, 6))
            plt.boxplot(similarities)
            plt.title(f"Distribution of {category}")
            plt.xticks(range(1, len(prompts) + 1), prompts, rotation=45)
            plt.ylabel("Similarity Score")
            plt.tight_layout()
            plt.show()

        top_matches = np.argsort(similarities, axis=0)[::-1][:num_cols]
        
        fig = plt.figure(figsize=(3 * num_cols, 3 * len(prompts)))
        fig.patch.set_facecolor('black')
        
        for i, prompt in enumerate(prompts):
            for j in range(num_cols):
                ax = plt.subplot(len(prompts), num_cols, i * num_cols + j + 1)
                ax.set_facecolor('black')
                
                curr_image_filename = image_filename_map[top_matches[j, i]]
                base_filename = os.path.basename(curr_image_filename)
                img = Image.open(curr_image_filename)
                plt.imshow(img)
                plt.title(f"Match {j+1} for: {prompt}\n{base_filename}", fontsize=8, color='white')
                plt.axis('off')
        
        plt.tight_layout()

        return fig

    num_cols = 8
    for category, encoded_prompts in encoded_text_prompts.items():
        fig = visualize_category(category, image_features, encoded_prompts, text_prompts[category], num_cols=num_cols)

        if save_figures:
            figure_name_str = f'textual_search_1_{category}_top_{num_cols}_matches.jpg'
            fig.savefig(os.path.join(output_folder_path, figure_name_str), bbox_inches='tight')
        
    #%% make some textual searches on the dataset

    conditions_dict = {
        "Hair Color": {
            "text_prefix": "",
            "text_strings": ['white or gray hair', 'yellow or blond hair', 'green hair', 'blue hair', 'purple or pink hair', 'red or orange hair']
        },
        "Hair Style": {
            "text_prefix": "",
            "text_strings": ['straight hair', 'curly hair', 'high top hairstyle', 'bob-cut hairstyle', 'afro hairstyle']
        },
        "Hair Style x Sex": {
            "text_prefix": "woman with ",
            "text_strings": ['short blond hair', 'long blond hair', 'short red hair', 'long red hair', 'short black hair', 'long black hair']
        },
        "Makeup": {
            "text_prefix": "woman ",
            "text_strings": ['heavy makeup', 'without makeup', 'red lipstick', 'strong eyeliner', 'traditional makeup']
        },
        "Background Color": {
            "text_prefix": "",
            "text_strings": ['yellow background', 'green background', 'blue background', 'purple background', 'red background']
        },
        "Facial Features": {
            "text_prefix": "",
            "text_strings": ['reading glasses', 'sunglasses', 'bald', 'goatee', 'lipstick']
        },
        "Physical Characteristics": {
            "text_prefix": "",
            "text_strings": ['large or chiseled jaw', 'long white beard', 'fashionable beard', 'wide eyes', 'overweight or chubby']
        },
        "Expression": {
            "text_prefix": "",
            "text_strings": ['angry or enraged', 'surprised', 'smiling', 'sad or depressed', 'grim face', 'tounge out']
        },
        "Expression x Sex": {
            "text_prefix": "man ",
            "text_strings": ['angry or enraged', 'surprised', 'smiling', 'sad or depressed', 'grim face', 'tounge out']
        },
        "Ethnicity": {
            "text_prefix": "",
            "text_strings": ['asian', 'native american', 'african', 'persian', 'south-american', 'irish']
        },
        "Ethnicity x Age 1": {
            "text_prefix": "old age ",
            "text_strings": ['asian', 'native american', 'african', 'persian', 'south-american', 'irish']
        },
        "Ethnicity x Age 2": {
            "text_prefix": "typical adult ",
            "text_strings": ['asian', 'native american', 'african', 'persian', 'south-american', 'irish']
        },
        "Ethnicity x Age 3": {
            "text_prefix": "young child ",
            "text_strings": ['asian', 'native american', 'african', 'persian', 'south-american', 'irish']
        },
        "Age": {
            "text_prefix": "",
            "text_strings": ['10 month old baby', '2.5 year old toddler', 'small child', '16 year old teenager', '30 year old adult', 'wrinkly 70 year old senior']
        },
        "Age x Ethnicity x Sex 1": {
            "text_prefix": "asian female ",
            "text_strings": ['10 month old baby', '2.5 year old toddler', 'small child', '16 year old teenager', '30 year old adult', 'wrinkly 70 year old senior']
        },
        "Age x Ethnicity x Sex 2": {
            "text_prefix": "african male ",
            "text_strings": ['10 month old baby', '2.5 year old toddler', 'small child', '16 year old teenager', '30 year old adult', 'wrinkly 70 year old senior']
        },
        "Accessories": {
            "text_prefix": "person wearing ",
            "text_strings": ['earrings', 'necklace', 'bandana', 'hat', 'tie', 'scarf', 'headphones', 'sunglasses']
        },
        "Hats": {
            "text_prefix": "person wearing ",
            "text_strings": ['baseball cap', 'fedora', 'beanie', 'top hat', 'cowboy hat', 'sun hat']
        },
        "Jewelry": {
            "text_prefix": "person with ",
            "text_strings": ['gold chain', 'pearl necklace', 'earrings', 'diamond', 'crown']
        },
        "Face Pose": {
            "text_prefix": "person ",
            "text_strings": ['looking straight ahead', 'turned sideways', 'tilted upwards', 'tilted downwards', 'three-quarter view', 'profile view']
        },
        "Eye Gaze": {
            "text_prefix": "person ",
            "text_strings": ['looking directly at camera', 'looking to the left', 'looking up', 'looking down', 'eyes closed']
        },
        "Glasses Style": {
            "text_prefix": "person wearing ",
            "text_strings": ['round glasses', 'square glasses', 'cat-eye glasses', 'rimless glasses', 'aviator sunglasses', 'sport sunglasses']
        },
        "Facial Hair": {
            "text_prefix": "man with ",
            "text_strings": ['full beard', 'mustache', 'goatee', 'sideburns', 'stubble', 'shaved face']
        },
        "Lighting": {
            "text_prefix": "",
            "text_strings": ['side light with shadows', 'spotlight', 'soft lighting', 'back lighting', 'golden hour', 'blue hour lighting', 'studio lighting']
        },
        "Background": {
            "text_prefix": "",
            "text_strings": ['urban cityscape', 'natural landscape', 'stone wall background', 'wodden wall background', 'beach background', 'night background']
        },
        "Eye Color": {
            "text_prefix": "person with ",
            "text_strings": ['blue eyes', 'green eyes', 'brown eyes', 'yellow eyes', 'hazel eyes', 'red eyes']
        },
        "bad things": {
            "text_prefix": "",
            "text_strings": ['blurred', 'statue', 'two people', 'back of head', 'hand covering face', 'cat', 'dog', 'animal']
        },
    }

    for selected_condition in conditions_dict.keys():
        print(selected_condition)

        text_prefix = conditions_dict[selected_condition]['text_prefix']
        text_strings = conditions_dict[selected_condition]['text_strings']

        # will randomly display "num_top_images_to_show" among the top "num_top_image_candidates" best matching queries
        num_top_images_to_show = len(text_strings) + 4
        num_top_images_to_show = min(max(4, num_top_images_to_show), 10)
        num_top_image_candidates = int(1.0 * num_top_images_to_show)

        title_fontsize = 20

        # attach prefix and extract text features
        text_strings_full = [(text_prefix + x) for x in text_strings]
        tokenized_text_samples = tokenizer(text_strings_full).to(device)
        with torch.no_grad():
            openclip_text_features = model.encode_text(tokenized_text_samples).float().cpu().numpy()
        openclip_text_features /= np.linalg.norm(openclip_text_features, axis=1)[:, np.newaxis]  # normalize to unit norm

        # perform inner product to get image-text similarity score
        image_text_similarity = np.dot(image_features, openclip_text_features.T)

        num_rows = len(text_strings)
        num_cols = num_top_images_to_show

        fig, ax = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(5 * num_cols, 6 * num_rows))
        fig.patch.set_facecolor('0.0')
        fig.subplots_adjust(left=0.003, right=0.997, bottom=0.01, top=0.91, hspace=0.16, wspace=0.04)
        fig.suptitle(f'Image textual search using OpenCLIP features from synthetic dataset\nCondition: {selected_condition}\nPrefix: "{text_prefix}"', fontsize=30, color='white')

        all_basenames = []
        for row_ind, q_str in enumerate(text_strings):
            # get top "num_top_image_candidates" matching queries sorted from best matching downward
            query_best_inds = list(np.argsort(image_text_similarity[:,row_ind])[-num_top_image_candidates:])
            query_best_inds.reverse()
            # randomly select "num_top_images_to_show" from that list
            query_best_inds = np.random.choice(query_best_inds, size=num_top_images_to_show, replace=False)

            for col_ind in range(num_cols):
                curr_row_filename = image_filename_map[query_best_inds[col_ind]]
                curr_basename = os.path.basename(curr_row_filename)
                curr_image = Image.open(curr_row_filename).convert("RGB")
                ax[row_ind,col_ind].imshow(curr_image)
                ax[row_ind,col_ind].set_axis_off()
                ax[row_ind,col_ind].set_title(f"'{q_str}'\n{curr_basename}", fontsize=title_fontsize, color='white')

                all_basenames.append(curr_basename)

        if save_figures:
            figure_name_str = f'textual_search_2_{selected_condition}_top_{num_top_images_to_show}_matches.jpg'
            fig.savefig(os.path.join(output_folder_path, figure_name_str), bbox_inches='tight')


#%%





================================================================================
