================================================================================
repo title: repo_to_text
repo link: https://github.com/SelfishGene/repo_to_text
date processed: 2024-10-21
================================================================================
================================================================================
repo file structure:
====================
    README.md
    repo_to_text_core.py
    convert_repos_to_text_docs.py
    api_key_manager.py
    requirements.txt
    setup.py
    examples/
        SFHQ-dataset.txt
        SFHQ-T2I-dataset.txt
        ImSME-dataset.txt
================================================================================
================================================================================
README.md:
==========
# Repo to Text

Repo to Text is a Python tool that converts GitHub repositories and local folders into text format to be used for either training LLMs or using copy pasting into LLM context. this repo provids AI-powered analysis of images, CSV files, and Jupyter notebooks.

## Features

- Process GitHub repositories and local folders and create a single text file for each repository
- Convert images to text by describing them using Google's Gemini Flash model (fast and free for up to 15 requests per minute)
- Covert Jupyter notebooks to text where the input (code/markdown) & output of each cell is clearly delimitered (if output contains images, they are also described using Flash)
- Describe large CSV files also using Flash instead of just dumping their entire content
- Handle API rate limits with built-in retry logic

## Installation

For local development, clone the repository and install it in editable mode:

```bash
git clone https://github.com/SelfishGene/repo_to_text.git
cd repo_to_text
pip install -e .
```

## Usage

To use Repo to Text, you need to set up your API keys first.  
The following code will create a `.env` file in your project root and load it into your environment variables:

```python
from repo_to_text.api_key_manager import initialize_api_keys

API_KEYS = {
    'GEMINI_API_KEY': 'abcdefghijklmnopqrstuvwxyz1234567890abc',
}

initialize_api_keys(api_keys_dict=API_KEYS)
```

Then, you can use the tool as follows from your Python scripts:

```python
from repo_to_text import convert_repos_to_text

# For GitHub repositories
github_repos = [
    "https://github.com/username/repo1",
    "https://github.com/username/repo2"
]
convert_repos_to_text(github_repos, "output_dir", is_github=True)

# For local folders
local_folders = [
    "/path/to/folder1",
    "/path/to/folder2"
]
convert_repos_to_text(local_folders, "output_dir", is_github=False)
```

## License

This project is licensed under the MIT License.

================================================================================
================================================================================
examples/SFHQ-dataset.txt:
==========================
================================================================================
repo title: SFHQ-dataset
repo link: https://github.com/SelfishGene/SFHQ-dataset
date processed: 2024-10-21
================================================================================
================================================================================
repo file structure:
====================
    README.md
    explore_dataset.py
    LICENSE
    images/
        SFHQ_variability_age_X_ethnicity_1.jpg
        bring_to_life_process_paintings.jpg
        SFHQ_sample_landmarks_segmentation.jpg
        SFHQ_variability_hair_color.jpg
        SFHQ_variability_age.jpg
        SFHQ_sample_2x4.jpg
        SFHQ_sample_4x8.jpg
        bring_to_life_process_stable_diffusion.jpg
        SFHQ_sample_3x6.jpg
        SFHQ_variability_age_X_ethnicity_2.jpg
        SFHQ_variability_hair_style.jpg
        SFHQ_variability_ethnicity.jpg
        bring_to_life_process_3D_models.jpg
        SFHQ_variability_expression.jpg
        SFHQ_sample_1x2.jpg
================================================================================
================================================================================
README.md:
==========
# Synthetic Faces High Quality (SFHQ) dataset  

![SFHQ dataset sample images](https://raw.githubusercontent.com/SelfishGene/SFHQ-dataset/main/images/SFHQ_sample_4x8.jpg)

**NOTE**: New higher quality dataset of synthetic faces: [SFHQ-T2I-dataset](https://github.com/SelfishGene/SFHQ-T2I-dataset)
----

The original SFHQ dataset consists of 4 parts, totaling ~425,000 curated high quality 1024x1024 synthetic face images.  
It was created by "bringing to life" and turning to photorealistic face images from multiple "inspiration" sources (paintings, drawings, 3D models, text to image generators, etc) using a process similar to what is described [in this short twitter thread](https://twitter.com/DavidBeniaguev/status/1376020024511627273?s=20&t=kH9J5mV9hL8e3y8PruuB5Q). The process involves encoding the images into StyleGAN2 latent space and performing a small manipulation that turns each image into a photo-realistic image. These resulting candidate images are then further curated using a semi-manual semi-automatic process with the help of the lightweight [visual taste aprroximator](https://github.com/SelfishGene/visual_taste_approximator) tool

The dataset also contains facial landmarks (an extended set of 110 landmark points) and face parsing semantic segmentation maps. An example script (`explore_dataset.py`) is provided ([live kaggle notebook here](https://www.kaggle.com/code/selfishgene/explore-synthetic-faces-hq-sfhq-dataset)) and demonstrates how to access landmarks, segmentation maps, and textually search withing the dataset (with CLIP image/text feature vectors), and also performs some exploratory analysis of the dataset.

Example illustation of landmarks and segmentation maps below:  
![SFHQ dataset landmarks and segmentation](https://raw.githubusercontent.com/SelfishGene/SFHQ-dataset/main/images/SFHQ_sample_landmarks_segmentation.jpg)

## Download
The dataset can be downloaded via kaggle:
- [Part 1](https://www.kaggle.com/datasets/selfishgene/synthetic-faces-high-quality-sfhq-part-1) consists of 89,785 HQ 1024x1024 curated face images. It uses "inspiration" images from [Artstation-Artistic-face-HQ dataset (AAHQ)](https://github.com/onion-liu/aahq-dataset), [Close-Up Humans dataset](https://opensynthetics.com/dataset/close-up-humans-dataset-by-synthesis-ai/) and [UIBVFED dataset](http://ugivia.uib.es/uibvfed/).  
- [Part 2](https://www.kaggle.com/datasets/selfishgene/synthetic-faces-high-quality-sfhq-part-2) consists of 91,361 HQ 1024x1024 curated face images. It uses "inspiration" images from [Face Synthetics dataset](https://github.com/microsoft/FaceSynthetics) and by sampling from the [Stable Diffusion v1.4](https://github.com/CompVis/stable-diffusion) text to image generator using varied face portrait prompts. 
- [Part 3](https://www.kaggle.com/datasets/selfishgene/synthetic-faces-high-quality-sfhq-part-3) consists of 118,358 HQ 1024x1024 curated face images. It uses "inspiration" images by sampling from [StyleGAN2](https://github.com/NVlabs/stylegan2-ada-pytorch) mapping network with very high truncation psi coefficients to increase diversity of the generation. Here, the [e4e](https://github.com/omertov/encoder4editing) encoder is basically used a new kind of truncation trick.
- [Part 4](https://www.kaggle.com/datasets/selfishgene/synthetic-faces-high-quality-sfhq-part-4) consists of 125,754 HQ 1024x1024 curated face images. It uses "inspiration" images by sampling from the [Stable Diffusion v2.1](https://github.com/Stability-AI/stablediffusion) text to image generator using varied face portrait prompts. 


## More Details about dataset generation and collection
1. The original inspiration images are taken from:
    - [Artstation-Artistic-face-HQ Dataset (AAHQ)](https://github.com/onion-liu/aahq-dataset) which contains mainly painting, drawing and 3D models of faces (part 1)
    - [Close-Up Humans Dataset](https://opensynthetics.com/dataset/close-up-humans-dataset-by-synthesis-ai/) that contains 3D models of faces (part 1)
    - [UIBVFED Dataset](http://ugivia.uib.es/uibvfed/) that also contain 3D models of faces (part 1)
    - [Face Synthetics Dataset](https://github.com/microsoft/FaceSynthetics) which contains 3D models of faces (part 2)
    - generated images using [stable diffusion v1.4 model](https://github.com/CompVis/stable-diffusion) using various face portrait prompts that span a wide range of ethnicities, ages, expressions, hairstyles, etc. (part 2)
    - StyleGAN2 mapping network sampled with larger that 1 truncation psi values, and then using a new truncation trick in which instead of moving towards the average w_avg vector, we move towards the encoded w_e4e vector to correct the example. illustation is provided in the section below (part 3)
    - generated images using [stable diffusion v2.1 model](https://github.com/Stability-AI/stablediffusion) using various face portrait prompts that span a wide range of ethnicities, ages, expressions, hairstyles, etc. (part 4)
1. Each inspiration image was encoded by [encoder4editing (e4e)](https://github.com/omertov/encoder4editing) into [StyleGAN2](https://github.com/NVlabs/stylegan2-ada-pytorch) latent space (StyleGAN2 is a generative face model tained on [FFHQ dataset](https://github.com/NVlabs/ffhq-dataset)) and multiple candidate images were generated from each inspiration image
1. These candidate images were then further curated and verified as being photo-realistic and high quality by a single human (me) and a machine learning assistant model that was trained to approximate my own human judgments and helped me scale myself to asses the quality of all images in the dataset. The code for the tool used for this purpuse [can be found here](https://github.com/SelfishGene/visual_taste_approximator)
1. Near duplicates and images that were too similar were removed using CLIP features (no two images in the dataset have CLIP similarity score of greater than ~0.92)
1. From each image various pre-trained features were extracted and provided here for convenience, in particular CLIP features for fast textual query of the dataset, the feaures are under `pretrained_features/` folder
1. From each image, semantic segmentation maps were extracted using [Face Parsing BiSeNet](https://github.com/zllrunning/face-parsing.PyTorch) and are provided in the dataset under under `segmentations/` folder
1. From each image, an extended landmark set was extracted that also contain inner and outer hairlines (these are unique landmarks that are usually not extracted by other algorithms). These landmarks were extracted using [Dlib](https://github.com/davisking/dlib), [Face Alignment](https://github.com/1adrianb/face-alignment) and some post processing of [Face Parsing BiSeNet](https://github.com/zllrunning/face-parsing.PyTorch) and are provided in the dataset under `landmarks/` folder
1. NOTE: semantic segmentation and landmarks were first calculated on scaled down version of 256x256 images, and then upscaled to 1024x1024

Example of dataset generation process on artistic illustations and paintings taken from [AAHQ](https://github.com/onion-liu/aahq-dataset) (part 1):  
![SFHQ dataset paintings](https://raw.githubusercontent.com/SelfishGene/SFHQ-dataset/main/images/bring_to_life_process_paintings.jpg)

Example of dataset generation process on 3D models taken from [Face Synthetics](https://github.com/microsoft/FaceSynthetics), [Close-Up Humans](https://opensynthetics.com/dataset/close-up-humans-dataset-by-synthesis-ai/), and [UIBVFED](http://ugivia.uib.es/uibvfed/) (parts 1 & 2):  
![SFHQ dataset 3D models](https://raw.githubusercontent.com/SelfishGene/SFHQ-dataset/main/images/bring_to_life_process_3D_models.jpg)

Example of dataset generation process of correcting faults in face images generated by [Stable Diffusion](https://github.com/CompVis/stable-diffusion) (parts 2 & 4):  
![SFHQ dataset stable diffusion](https://raw.githubusercontent.com/SelfishGene/SFHQ-dataset/main/images/bring_to_life_process_stable_diffusion.jpg)

Example of dataset generation process of using the StyleGAN2 mapping network samples with high truncation psi and correcting with e4e encoder (part 3):  
![SFHQ dataset encoder based truncation](https://i.ibb.co/NT7BJy5/Figure-1-brief-stages-psi-08-to-20-1.jpg)


## Demonstation of variability in the dataset 
we deomonstate the variability of the images in the dataset by textual query of the dataset with [CLIP](https://github.com/openai/CLIP) ViT-L/14@336 model embeddings (NOTE: these demonstation images are only of parts 1&2, the dataset with parts 3&4 is much more varied, please try out for yourself or check the [script on kaggle](https://www.kaggle.com/code/selfishgene/explore-synthetic-faces-hq-sfhq-dataset-2)):  
- Hair color:  
![SFHQ dataset variability haircolor](https://raw.githubusercontent.com/SelfishGene/SFHQ-dataset/main/images/SFHQ_variability_hair_color.jpg)

- Age:  
![SFHQ dataset variability age](https://raw.githubusercontent.com/SelfishGene/SFHQ-dataset/main/images/SFHQ_variability_age.jpg)

- Ethnicity:  
![SFHQ dataset variability ethnicity](https://raw.githubusercontent.com/SelfishGene/SFHQ-dataset/main/images/SFHQ_variability_ethnicity.jpg)

- Facial expression:  
![SFHQ dataset variability expression](https://raw.githubusercontent.com/SelfishGene/SFHQ-dataset/main/images/SFHQ_variability_expression.jpg)  

Additional variability demonstations can be found under `images/`

## Privacy
Since all images in this dataset are synthetically generated there are no privacy issues or license issues surrounding these images.  

## Summary
Overall the 4 parts of this dataset contain ~425,000 high quality and curated synthetic face images that have no privacy issues or license issues surrounding them.  

This dataset contains a high degree of variability on the axes of identity, ethnicity, age, pose, expression, lighting conditions, hair-style, hair-color, facial hair. It lacks variability in accessories axes such as hats or earphones as well as various jewelry. It also doesn't contain any occlusions except the self-occlusion of hair occluding the forehead, the ears and rarely the eyes. This dataset naturally inherits all the biases of it's original datasets (FFHQ, AAHQ, Close-Up Humans, Face Synthetics, LAION-5B) and the StyleGAN2 and Stable Diffusion models.  

The purpose of this dataset is to be of sufficiently high quality that new machine learning models can be trained using this data alone or provide meaningful augmentation to other data sources, including the training of generative face models such as StyleGAN. The dataset may be extended from time to time with additional supervision labels (e.g. text descriptions), but no promises.

Hope this is helpful to some of you, feel free to use as you see fit...

## Citation

```
@misc{david_beniaguev_2022_SFHQ,
	title={Synthetic Faces High Quality (SFHQ) dataset},
	url={https://github.com/SelfishGene/SFHQ-dataset},
	DOI={10.34740/kaggle/dsv/4737549},
	publisher={GitHub},
	author={David Beniaguev},
	year={2022}
}
```



================================================================================
================================================================================
images/SFHQ_variability_age_X_ethnicity_2.jpg:
==============================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image showcases a grid of 144 photographs, each depicting a different synthetically generated image of an African male at various ages and life stages. The images are arranged in a 12x12 grid.  Each row presents a specific age group, starting from infancy ("10 month old baby") and progressing through toddlerhood, childhood, teenage years, adulthood, and finally, old age ("wrinkly 70 year old senior").  Within each row, there are twelve variations of the same age group, showing slight differences in facial features, hair, and expression.  This suggests the images are generated from a model that produces diverse instances within each age category.

Above the grid, a title indicates that this is a demonstration of image textual search using CLIP (Contrastive Language–Image Pre-training) features. The text "prefix_text = "african male"" specifies that the search is specifically targeting images of African males.  This implies the images were created and organized for testing the efficacy of CLIP's image-text matching capabilities within a particular demographic.

The overall structure and content of the image suggest a study or experiment related to AI-generated images, age progression, and the accuracy of CLIP's image search functionality for diverse populations. The consistency of the "African male" label across all images emphasizes the focus on this specific demographic in the image generation and retrieval process.

================================================================================
================================================================================
images/SFHQ_sample_1x2.jpg:
===========================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a diptych, meaning it's divided into two distinct halves, presented side-by-side. 


The left half features a close-up portrait of an older Asian man with graying dark hair, parted to the side. His expression is one of a gentle smile. The skin on his face shows the texture and lines typical of aging. He's wearing a simple, dark gray or brown collared shirt. The background is a muted beige tone, which is out of focus, drawing attention to the subject.


The right half shows a close-up portrait of a younger man of Middle Eastern or Hispanic descent with short, dark hair. He has a broad, bright smile revealing his teeth. His skin is smooth, and his expression is friendly and open. He's wearing a dark blue collared shirt. The background is an out-of-focus, purplish hue.


The overall contrast between the two images is striking, highlighting the age difference and different ethnicities of the two individuals. Both portraits are well-lit and sharply focused on their faces, suggesting a professional or studio setting for the photographs.  At the very top of each half is small text, likely file information.

================================================================================
================================================================================
explore_dataset.py:
===================
import os
import numpy as np
import matplotlib.pyplot as plt
import PIL
import glob
import clip
import torch
import pickle
import matplotlib

#%% display random subset of images from sample images


datasets_root_folder = '/home/Datasets/' # change to where dataset was downloaded

dataset_path_part_1 = os.path.join(datasets_root_folder, 'SFHQ_part1')
dataset_path_part_2 = os.path.join(datasets_root_folder, 'SFHQ_part2')

display_background_color = '0.05'
text_color = '1.0'
title_fontsize = 16

matplotlib.rcParams['text.color'] = text_color
matplotlib.rcParams['font.size'] = title_fontsize

dataset_part_to_choose = np.random.choice([1, 2])

if dataset_part_to_choose == 1:
    dataset_path = dataset_path_part_1
    sample_images_tiny_folder  = os.path.join(dataset_path, 'tiny sample (30 images)')
    sample_images_small_folder = os.path.join(dataset_path, 'small sample (550 images)')
elif dataset_part_to_choose == 2:
    dataset_path = dataset_path_part_2
    sample_images_tiny_folder  = os.path.join(dataset_path, 'a tiny sample (140 images)')
    sample_images_small_folder = os.path.join(dataset_path, 'a small sample (650 images)')

all_images_folder          = os.path.join(dataset_path, 'images')
pretrained_features_folder = os.path.join(dataset_path, 'pretrained_features')
landmarks_folder           = os.path.join(dataset_path, 'landmarks')
segmentations_folder       = os.path.join(dataset_path, 'segmentations')

num_rows = 4
num_cols = 8
num_images = num_rows * num_cols
title_fontsize = 16

if num_images <= 30:
    sample_image_filenames = glob.glob(os.path.join(sample_images_tiny_folder, '*.jpg'))
else:
    sample_image_filenames = glob.glob(os.path.join(sample_images_small_folder, '*.jpg'))

selected_images = np.random.choice(sample_image_filenames, size=num_images, replace=False)

plt.close('all')
fig = plt.figure(figsize=(40,30))
fig.patch.set_facecolor(display_background_color)
fig.subplots_adjust(left=0.003, right=0.997, bottom=0.003, top=0.99, hspace=0.02, wspace=0.02)
for k, curr_image_filename in enumerate(selected_images):
    curr_image = PIL.Image.open(curr_image_filename).convert("RGB")

    plt.subplot(num_rows, num_cols, k + 1); plt.imshow(curr_image); plt.axis('off')
    plt.title('image "%s"' %(curr_image_filename.split('/')[-1].split('.')[0]), fontsize=title_fontsize)


#%% display random subset of images from the data along with their landmarks

num_images_to_show = 6

num_rows = 3
num_cols = num_images_to_show

sample_image_filenames = glob.glob(os.path.join(all_images_folder, '*.jpg'))
selected_images = np.random.choice(sample_image_filenames, size=num_images_to_show, replace=False)

title_fontsize = 16

plt.close('all')
fig = plt.figure(figsize=(40,30))
fig.patch.set_facecolor(display_background_color)
plt.subplots_adjust(left=0.01, right=0.99, bottom=0.01, top=0.98, hspace=0.03, wspace=0.04)
for k, curr_image_filename in enumerate(selected_images):

    curr_sample_name = curr_image_filename.split('/')[-1].split('.')[0]

    curr_image_filename = os.path.join(all_images_folder, curr_sample_name + '.jpg')
    curr_landmarks_filename = os.path.join(landmarks_folder, curr_sample_name + '.npz')
    curr_segmentation_filename = os.path.join(segmentations_folder, curr_sample_name + '.png')

    curr_image = PIL.Image.open(curr_image_filename).convert("RGB")
    curr_landmarks = np.load(curr_landmarks_filename)['landmarks']
    # curr_segmentation = imageio.imread(curr_segmentation_filename)
    curr_segmentation = PIL.Image.open(curr_segmentation_filename).convert("L")

    plt.subplot(num_rows, num_cols, k + 1 + 0 * num_cols)
    plt.imshow(curr_image); plt.axis('off')
    plt.title('image "%s"' %(curr_sample_name), fontsize=title_fontsize)

    plt.subplot(num_rows, num_cols, k + 1 + 1 * num_cols)
    plt.imshow(curr_image); plt.axis('off')
    plt.scatter(curr_landmarks[:,0],curr_landmarks[:,1], c='r')
    plt.title('image with landmarks overlayed', fontsize=title_fontsize)

    plt.subplot(num_rows, num_cols, k + 1 + 2 * num_cols)
    plt.imshow(curr_segmentation); plt.axis('off')
    plt.scatter(curr_landmarks[:,0],curr_landmarks[:,1], c='r')
    plt.title('segmentation mask with landmarks overlayed', fontsize=title_fontsize)


#%% demonstate segmentation overlay

num_images_to_show = 4
num_cols = 8
num_rows = num_images_to_show

sample_image_filenames = glob.glob(os.path.join(all_images_folder, '*.jpg'))
sample_image_filenames = glob.glob(os.path.join(sample_images_small_folder, '*.jpg'))
selected_images = np.random.choice(sample_image_filenames, size=num_images_to_show, replace=False)

darkening_mult_factor = 0.35
title_fontsize = 18

plt.close('all')
fig = plt.figure(figsize=(40,30))
fig.patch.set_facecolor(display_background_color)
plt.subplots_adjust(left=0.01, right=0.99, bottom=0.02, top=0.98, hspace=0.04, wspace=0.03)
for k, curr_image_filename in enumerate(selected_images):
    curr_sample_name = curr_image_filename.split('/')[-1].split('.')[0]

    curr_image_filename = os.path.join(all_images_folder, curr_sample_name + '.jpg')
    curr_segmentation_filename = os.path.join(segmentations_folder, curr_sample_name + '.png')

    curr_image = np.array(PIL.Image.open(curr_image_filename).convert("RGB"))
    curr_segmentation = np.array(PIL.Image.open(curr_segmentation_filename).convert("L"))

    only_face = (curr_segmentation >= 1) & (curr_segmentation <= 13)
    edited_face_1 = curr_image.copy()
    edited_face_1[~only_face] = darkening_mult_factor * edited_face_1[~only_face]

    only_face_skin = (curr_segmentation == 1)
    edited_face_2 = curr_image.copy()
    edited_face_2[~only_face_skin] = darkening_mult_factor * edited_face_2[~only_face_skin]

    only_face_parts = (curr_segmentation > 1) & (curr_segmentation <= 13)
    edited_face_3 = curr_image.copy()
    edited_face_3[~only_face_parts] = darkening_mult_factor * edited_face_3[~only_face_parts]

    only_background_neck_and_shirt = (curr_segmentation == 0) | ((curr_segmentation >= 14) & (curr_segmentation <= 16))
    edited_face_4 = curr_image.copy()
    edited_face_4[only_background_neck_and_shirt] = darkening_mult_factor * edited_face_4[only_background_neck_and_shirt]

    only_hair_and_hats = (curr_segmentation >= 17)
    edited_face_5 = curr_image.copy()
    edited_face_5[~only_hair_and_hats] = darkening_mult_factor * edited_face_5[~only_hair_and_hats]

    only_background_neck_and_shirt = (curr_segmentation == 0) | ((curr_segmentation >= 14) & (curr_segmentation <= 16))
    edited_face_6 = curr_image.copy()
    edited_face_6[~only_background_neck_and_shirt] = darkening_mult_factor * edited_face_6[~only_background_neck_and_shirt]

    plt.subplot(num_rows, num_cols, k * num_cols + 1); plt.imshow(curr_image); plt.axis('off'); plt.title('original "%s"' %(curr_sample_name), fontsize=title_fontsize)
    plt.subplot(num_rows, num_cols, k * num_cols + 2); plt.imshow(curr_segmentation); plt.axis('off'); plt.title('face parsing', fontsize=title_fontsize)
    plt.subplot(num_rows, num_cols, k * num_cols + 3); plt.imshow(edited_face_1); plt.axis('off'); plt.title('face only', fontsize=title_fontsize)
    plt.subplot(num_rows, num_cols, k * num_cols + 4); plt.imshow(edited_face_2); plt.axis('off'); plt.title('skin only', fontsize=title_fontsize)
    plt.subplot(num_rows, num_cols, k * num_cols + 5); plt.imshow(edited_face_3); plt.axis('off'); plt.title('face parts', fontsize=title_fontsize)
    plt.subplot(num_rows, num_cols, k * num_cols + 6); plt.imshow(edited_face_4); plt.axis('off'); plt.title('face and hair', fontsize=title_fontsize)
    plt.subplot(num_rows, num_cols, k * num_cols + 7); plt.imshow(edited_face_5); plt.axis('off'); plt.title('hair and hats only', fontsize=title_fontsize)
    plt.subplot(num_rows, num_cols, k * num_cols + 8); plt.imshow(edited_face_6); plt.axis('off'); plt.title('background, neck and shirt', fontsize=title_fontsize)


#%% gather all CLIP ViT/14 @ 336 embeddings into a single matrix


def collect_pretrained_features_from_folder(base_image_folder, requested_features_model='CLIP_ViTL_14@336', nromalize_features=True):
    # this function assumes that the folder stucture is proper and features dict contains the requested features

    images_folder = os.path.join(base_image_folder, 'images')
    features_folder = os.path.join(base_image_folder, 'pretrained_features')
    all_feature_dict_filenames = glob.glob(os.path.join(features_folder, '*.pickle'))
    all_image_filenames = glob.glob(os.path.join(images_folder, '*.*'))

    try:
        curr_features_dict = pickle.load(open(all_feature_dict_filenames[0], "rb"))
        num_features = curr_features_dict[requested_features_model].shape[1]
    except:
        print('the requested features were not calculated.')
        return [],[]

    num_images = len(all_feature_dict_filenames)

    # create matrix to fill
    pretrained_image_features_matrix = np.zeros((num_images, num_features))

    # go over all samples and collect the features
    image_filename_map = {}
    for k, curr_image_filename in enumerate(all_image_filenames):
        curr_sample_name = curr_image_filename.split('/')[-1].split('.')[0]
        curr_features_dict_filename = os.path.join(features_folder, curr_sample_name + '.pickle')
        curr_features_dict = pickle.load(open(curr_features_dict_filename, "rb"))
        pretrained_image_features_matrix[k,:] = curr_features_dict[requested_features_model]
        image_filename_map[k] = curr_image_filename

    # normalize features to unit norm
    if nromalize_features:
        pretrained_image_features_matrix /= np.linalg.norm(pretrained_image_features_matrix, axis=1, keepdims=True)

    return pretrained_image_features_matrix, image_filename_map


# extract features for all filenames from two parts of the dataset
CLIP_image_features_pt1, image_filename_map_pt1 = collect_pretrained_features_from_folder(dataset_path_part_1, requested_features_model='CLIP_ViTL_14@336', nromalize_features=True)
CLIP_image_features_pt2, image_filename_map_pt2 = collect_pretrained_features_from_folder(dataset_path_part_2, requested_features_model='CLIP_ViTL_14@336', nromalize_features=True)

# merge the image features and filenames from both datasets for simple querying
CLIP_image_features = np.concatenate((CLIP_image_features_pt1, CLIP_image_features_pt2), axis=0)
image_filename_map = image_filename_map_pt1.copy()
for k in range(CLIP_image_features_pt2.shape[0]):
    image_filename_map[CLIP_image_features_pt1.shape[0] + k] = image_filename_map_pt2[k]

# load the clip model
device = "cuda" if torch.cuda.is_available() else "cpu"
CLIP_model, CLIP_preprocess = clip.load("ViT-L/14@336px", device=device)


#%% make some textual searches


# please uncomment desired queries (or just make up some of your own)


# hair related (color x style)
text_prefix = ''
text_strings = ['white or gray hair', 'yellow or blond hair', 'green hair', 'blue hair', 'purple or pink hair', 'red or orange hair']

# text_prefix = 'woman with '
# text_strings = ['short blond hair', 'long blond hair', 'short red hair', 'long red hair', 'short black hair', 'long black hair']

# text_prefix = ''
# text_strings = ['straight hair', 'curly hair', 'high top hairstyle', 'bob-cut hairstyle', 'afro hairstyle']


# various random properties
# text_prefix = 'woman '
# text_strings = ['heavy makeup', 'without makeup', 'red lipstick', 'strong eyeliner']

# text_prefix = ''
# text_strings = ['yellow background', 'green background', 'blue background', 'purple background', 'red background']

# text_prefix = ''
# text_strings = ['reading glasses', 'sunglasses', 'bald', 'goatee', 'lipstick']

# text_prefix = ''
# text_strings = ['large or chiseled jaw', 'long white beard', 'fashionable beard', 'long forehead', 'overweight or chubby']


# expression
# text_prefix = ''
# text_strings = ['angry or enraged', 'surprised', 'smiling', 'sad or depressed', 'grim face']

# text_prefix = 'man '
# text_strings = ['angry or enraged', 'surprised', 'smiling', 'sad or depressed', 'grim face']


# ethnicity (with age cross)
# text_prefix = ''
# text_strings = ['asian', 'indian', 'african', 'persian', 'south-american', 'irish']

# text_prefix = 'old age '
# text_strings = ['asian', 'indian', 'african', 'persian', 'south-american', 'irish']

# text_prefix = 'typical adult '
# text_strings = ['asian', 'indian', 'african', 'persian', 'south-american', 'irish']

# text_prefix = 'young child '
# text_strings = ['asian', 'indian', 'african', 'persian', 'south-american', 'irish']


# age (with ethnicity cross)
# text_prefix = ''
# text_strings = ['10 month old baby', '2.5 year old toddler', 'small child', '16 year old teenager', '30 year old adult', 'wrinkly 70 year old senior']

# text_prefix = 'asian female '
# text_strings = ['10 month old baby', '2.5 year old toddler', 'small child', '16 year old teenager', '30 year old adult', 'wrinkly 70 year old senior']

# text_prefix = 'african male '
# text_strings = ['10 month old baby', '2.5 year old toddler', 'small child', '16 year old teenager', '30 year old adult', 'wrinkly 70 year old senior']


# will randomly display "num_top_images_to_show" among the top "num_top_image_candidates" best matching queries
num_top_images_to_show = 2 * len(text_strings) + 1
num_top_image_candidates = int(3 * num_top_images_to_show)

title_fontsize = 14

# attach prefix and extract text features
text_strings_full = [(text_prefix + x) for x in text_strings]
tokenized_text_samples = torch.cat([clip.tokenize(text_strings_full)]).cuda()
CLIP_text_features = CLIP_model.encode_text(tokenized_text_samples).detach().cpu().numpy()
CLIP_text_features /= np.linalg.norm(CLIP_text_features, axis=1, keepdims=True) # normalize to unit norm

# perform inner product to get image-text similarity score
image_text_similarity  = np.dot(CLIP_image_features , CLIP_text_features.T)

num_rows = len(text_strings)
num_cols = num_top_images_to_show

plt.close('all')
fig, ax = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(40, 32))
fig.patch.set_facecolor(display_background_color)
fig.subplots_adjust(left=0.003,right=0.997,bottom=0.01,top=0.925,hspace=0.13,wspace=0.03)
fig.suptitle('image textual search using CLIP features from synthetic dataset \nprefix_text = "%s"' %(text_prefix), fontsize=25)
for row_ind, q_str in enumerate(text_strings):

    # get top "num_top_image_candidates" matching queries sorted from best matching downward
    query_best_inds = list(np.argsort(image_text_similarity[:,row_ind])[-num_top_image_candidates:])
    query_best_inds.reverse()
    # randomly select "num_top_images_to_show" from that list
    query_best_inds = np.random.choice(query_best_inds, size=num_top_images_to_show, replace=False)

    for col_ind in range(num_cols):
        curr_image = PIL.Image.open(image_filename_map[query_best_inds[col_ind]]).convert("RGB")
        ax[row_ind,col_ind].imshow(curr_image); ax[row_ind,col_ind].set_axis_off()
        ax[row_ind,col_ind].set_title("'%s'" %(q_str), fontsize=title_fontsize)


#%%



================================================================================
================================================================================
images/SFHQ_sample_4x8.jpg:
===========================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid of 40 small, square images, arranged in 4 rows and 10 columns. Each square contains a close-up headshot of a different person.  The individuals depicted are diverse in age, gender, ethnicity, and hairstyle. There is a wide range of skin tones and facial features represented. The background behind each person is simple and uncluttered, focusing attention on the individual's face.

Above each individual image is a short text string that appears to be a file name or identifier, suggesting that these are images from a dataset. The consistent format of these identifiers ("SFHQ_pt[1 or 2]_0000[number]") implies a systematic organization of the images.  The overall style of the images suggests a potential use in machine learning, facial recognition, or other applications requiring a large and diverse set of human faces.  The images appear digitally enhanced or processed, giving them a slightly artificial or smoothed look.

================================================================================
================================================================================
images/bring_to_life_process_paintings.jpg:
===========================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a grid of 18 portrait images arranged in three rows of six.  The top row displays six original "inspiration" images; these are diverse portraits of individuals with varying hair colors, skin tones, and styles.  They range from realistic painted portraits to stylized digital paintings and illustrations.

The two rows below the top row show photorealistic renderings generated from the inspiration images.  Each column represents a different original image from the top row, and the second and third rows contain two versions of each image generated using different parameters, labeled "photoreal candidate 1, e4e_strict_prior" and "photoreal candidate 2, ie4e_low_prior" respectively. These labels suggest that the image generation process used different models or settings to produce the two sets of results.  The generated images attempt to recreate the style and features of the original "inspiration" pictures, but with variations in detail, lighting, and overall appearance. The differences illustrate how parameters in AI image generation can lead to varying results.

================================================================================
================================================================================
images/SFHQ_variability_expression.jpg:
=======================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image displays a grid of 120 face images, organized into ten rows of twelve columns. Each face image is accompanied by text indicating the query used to retrieve it. The queries are emotion-related, including "angry or enraged", "surprised", "smiling", "sad or depressed", and "grim face".  The faces in each column represent the results for a particular query.  The images themselves appear to be computer-generated, showcasing a diverse range of ages, genders, and ethnicities.  The background is a consistent dark gray, making the faces stand out clearly.

The title of the image, "image textual search using CLIP features from synthetic dataset prefix_text = ''", indicates that the images are from a synthetic dataset and were retrieved using a CLIP (Contrastive Language–Image Pre-training) model. This explains how the model linked textual descriptions to visual representations of facial expressions.  The consistent use of the same queries across the rows suggests an attempt to test the model's robustness and consistency in retrieving images matching the specified emotional states or facial expressions.

The overall structure is highly organized and systematic, presenting a clear visualization of a text-to-image retrieval experiment.  The consistent labeling and layout make it easy to compare the results for each query and assess the performance of the CLIP model.

================================================================================
================================================================================
images/bring_to_life_process_stable_diffusion.jpg:
==================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a grid of 18 photos arranged in three rows of six columns.  Each column represents a different individual, and each row represents a different image generation stage or technique. 

The top row shows the "original inspiration" images – six diverse faces of varying ages, ethnicities, and genders. These appear to be real photographs used as source material for image generation.

The second and third rows display the results of two different photorealistic image generation methods (or "candidates"). Candidate 1 ("e4e_strict_prior") and Candidate 2 ("ie4e_low_prior") each generate a version of the six original faces.  The generated images attempt to replicate the original photos but show subtle differences in style, detail, and realism.  The differences likely reflect variations in the algorithms or parameters used during the generation process.  The differences between the two candidates are not immediately striking but are present in fine details of facial features and skin texture.

================================================================================
================================================================================
images/SFHQ_sample_2x4.jpg:
===========================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid of eight photographs, arranged in two rows of four. Each photograph is a close-up portrait of a person's face, filling the entire frame. 


The subjects are diverse in age, gender, and ethnicity. There are older adults, younger adults, and a child. The ethnicities represented appear to include East Asian, Caucasian, African, and mixed-race individuals. The subjects’ expressions vary, ranging from neutral to smiling. Each photograph has a caption at the top that appears to be a file name or identifier.


The overall impression is one of diversity and a focus on individual facial features and expressions. The consistent framing and close-up nature of the photos create a uniform visual style.

================================================================================
================================================================================
images/SFHQ_variability_hair_color.jpg:
=======================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image showcases the results of an image textual search using CLIP (Contrastive Language–Image Pre-training) features from a synthetic dataset.  The top of the image displays the title "image textual search using CLIP features from synthetic dataset" and indicates that the `prefix_text` variable is set to an empty string.

The main body of the image is a grid of 132 small images, organized into 12 rows of 11 columns. Each row represents a different hair color query: "white or gray hair," "yellow or blond hair," "green hair," "blue hair," "purple or pink hair," and "red or orange hair." Within each row, the 11 images are the results returned by the CLIP search for that particular hair color query.  The images depict diverse individuals, varying in age, gender, and ethnicity, all seemingly generated synthetically.  Each image is labeled above it with the relevant query, making it clear which search term produced each image result.

The structure is highly organized and methodical, presenting a clear visual representation of how the CLIP model performs image retrieval based on textual descriptions of hair color. The synthetic nature of the dataset allows for a controlled experiment to assess the effectiveness of the CLIP search.

================================================================================
================================================================================
images/SFHQ_sample_3x6.jpg:
===========================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a grid of 20 individual portraits, arranged in four rows of five images each.  Each portrait shows a different person, displaying a wide range of ages, ethnicities, and genders.  The individuals are presented in a fairly consistent manner, with most images showing a head-and-shoulders shot against a plain or slightly textured background.  There's a diverse representation of skin tones, hair colors, and facial features. The expressions are generally neutral or slightly pleasant, with minimal variation in posing.

Above each portrait, a small text label is visible, seemingly identifying the image with a file name or similar identifier.  The identifiers suggest some sort of database or collection of images, possibly for facial recognition or similar purposes. The consistency of the labels and the uniformity of the image presentation reinforce the impression of a systematic organization of the images.  The overall effect is that of a diverse and well-organized dataset of facial images.

================================================================================
================================================================================
images/SFHQ_variability_hair_style.jpg:
=======================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image showcases the results of an image textual search using the CLIP (Contrastive Language–Image Pre-training) model.  The search was performed on a synthetic dataset, and the title indicates that a prefix text was used, although the specific prefix is not shown. The image is organized into a grid of small square images, each displaying a face.

Each row of images represents a different search query related to hairstyles.  The queries are clearly labeled above each row: "straight hair," "curly hair," "high top hairstyle," "bob-cut hairstyle," and "afro hairstyle."  Within each row, multiple images are presented, showing a variety of faces with the specified hairstyle, demonstrating the model's ability to retrieve relevant results based on the textual description.  The diversity of faces within each row also suggests that the dataset contains a range of ethnicities and genders.  The consistent sizing and arrangement of the images make the results easy to compare and analyze.

================================================================================
================================================================================
images/SFHQ_variability_age.jpg:
================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a grid showcasing the results of an image textual search using CLIP (Contrastive Language–Image Pre-training) features.  The search was performed on a synthetic dataset of images. The top of the image displays the title "image textual search using CLIP features from synthetic dataset" and indicates that the search was conducted using the prefix text "".

The grid itself is organized into rows and columns. Each cell contains a face image of a person, and above each image is a text label describing the person's age and life stage (e.g., "10 month old baby," "2.5 year old toddler," "16 year old teenager," "30 year old adult," "wrinkly 70 year old senior"). The images within each row seem to represent variations in the same age group or life stage, showing diverse ethnicities and facial features.  The age groups progress from infancy to seniorhood across the rows.

The overall structure is designed to demonstrate the effectiveness of the CLIP model's ability to retrieve relevant images based on textual descriptions.  The use of a synthetic dataset likely allows for a controlled experiment, eliminating variability from real-world image quality and annotation inconsistencies.

================================================================================
================================================================================
images/SFHQ_sample_landmarks_segmentation.jpg:
==============================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a grid showcasing six sets of three images each, arranged in three rows.  Each set focuses on a different individual's face. The first row presents the original facial images.  The second row displays the same faces but with red dots overlaid, indicating detected facial landmarks.  These landmarks are likely used for facial recognition or analysis purposes, showing key points like the eyes, nose, and mouth.  The third row presents a segmented version of the faces.  The segmentation process separates the image into regions representing different parts of the face (hair, skin, clothing), resulting in a color-coded representation, and retains the landmarks from the previous row.  The color scheme of the segmentation varies slightly between images, but generally uses a purple, yellow, and teal palette.

The top row of images shows a variety of individuals with diverse ethnic backgrounds, ages, and hairstyles, suggesting a dataset designed to be representative and robust in its scope.  Each original image in the top row has a filename associated with it, implying that this is part of a larger collection of images.  The consistent application of landmark detection and segmentation across all six sets highlights the process's effectiveness.


Overall, the image provides a clear visual demonstration of facial landmark detection and segmentation.  It is likely a sample output from a computer vision algorithm or a part of a research paper illustrating its capabilities in handling diverse facial features and creating accurate facial masks.

================================================================================
================================================================================
images/bring_to_life_process_3D_models.jpg:
===========================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a grid of 18 photos arranged in three rows of six columns. Each column represents a different person, and each row shows different versions of the same person's face. The top row displays the "original inspiration" images—presumably the source images used to generate the faces in the rows below.  These images vary in style, showing both photographs and what appears to be 3D rendered faces.

The second and third rows show the results of a photorealistic face generation process.  Each column shows two variations of a generated face for the same individual inspiration image. The labels below each row indicate that the second row ("photoreal candidate 1") uses a "strict prior" in the generation process, while the third row ("photoreal candidate 2") employs a "low prior".  This suggests that the generation algorithm used different levels of constraint or guidance when creating these faces.  The differences between the "strict prior" and "low prior" results can be seen in subtle variations in facial features and overall appearance for each person.  The "strict prior" versions appear closer to the original inspiration images, while the "low prior" versions exhibit more variation.

================================================================================
================================================================================
images/SFHQ_variability_age_X_ethnicity_1.jpg:
==============================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image displays a grid of 144 individual photographs, meticulously arranged in 12 rows and 12 columns. Each photograph features a headshot of a woman, seemingly of Asian descent, at different ages and stages of life.  The age range is extensive, progressing from infancy (10-month-old babies) through toddlerhood (2.5 years old), childhood ("small child"), adolescence (16-year-old teenagers), adulthood (30-year-old adults), and finally, old age ("wrinkly 70-year-old seniors").  Each image is labeled with the corresponding age description.

The consistent framing and lighting across all photographs create a uniform and organized visual presentation. The background of each individual image is relatively simple and uncluttered, focusing attention on the subject's face.  The arrangement suggests a systematic study or dataset of facial features across the lifespan, possibly used for research in areas like age estimation, facial recognition, or image generation.  The title indicates the images were generated synthetically and the textual search was performed using CLIP features.

================================================================================
================================================================================
images/SFHQ_variability_ethnicity.jpg:
======================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image presents a grid of 144 face images, neatly organized into 12 rows and 12 columns. Each face image is square and shows a diverse range of individuals, seemingly generated synthetically.  Above each column, a label indicates the ethnicity associated with the faces in that column. The labels are: 'asian', 'indian', 'african', 'persian', 'south-american', and 'irish'.  Each ethnicity is represented by 24 faces.

The title of the image indicates that the images are the result of an image textual search using CLIP (Contrastive Language–Image Pre-training) features from a synthetic dataset. The search query was "typical adult", aiming to retrieve images representing a typical adult from each of the specified ethnic groups. The faces in the image are diverse in age, hair style, and facial features, representing a broad range within each ethnic category.  The consistent lighting and background suggest a controlled, likely artificial, environment for image generation.

================================================================================

================================================================================
================================================================================
repo_to_text_core.py:
=====================
#%% Imports

import os
import io
import csv
import time
import json
import shutil
import base64
import random
import zipfile
import fnmatch
import requests
import mimetypes
import pandas as pd
from PIL import Image
from pathlib import Path
from datetime import datetime
import google.generativeai as genai
from google.api_core import exceptions

#%% Gemini Defenitions and Functions

GEMINI_MODEL_NAME = "gemini-1.5-flash-002"

def is_image_file(file_path):
    return Path(file_path).suffix.lower() in {'.jpg', '.jpeg', '.png'}

def is_csv_file(file_path):
    return Path(file_path).suffix.lower() == '.csv'

def is_ipython_notebook(file_path):
    return Path(file_path).suffix.lower() == '.ipynb'

def setup_gemini_model():
    genai.configure(api_key=os.environ['GEMINI_API_KEY'])
    gemini_model = genai.GenerativeModel(model_name=f"models/{GEMINI_MODEL_NAME}")
    return gemini_model

def analyze_image(image_path, model, temperature=0.7):

    image = Image.open(image_path)
    prompt = "Describe the structure and contents of this image in a few paragraphs"
    
    input_content = [image, prompt]
    response = model.generate_content(
        input_content,
        generation_config={"temperature": temperature}
    )
    
    # output_token_count = response.usage_metadata.candidates_token_count

    current_date = datetime.now().strftime("%Y-%m-%d")
    description = f"Description of the following image by {GEMINI_MODEL_NAME}, performed on {current_date}:\n{response.text}"
    
    return description

def process_csv_file(file_path, model):
    """Process a CSV file and return its description."""
    df = pd.read_csv(file_path)
    row_count = len(df)
    col_count = len(df.columns)
    col_names = df.columns[:10].tolist()

    programmatic_description = (
        f"CSV file with {row_count} rows and {col_count} columns. "
        f"First several column names are {col_names}"
    )

    if row_count > 50:
        # For larger CSV files, use Gemini to generate a description
        csv_content = df.head(50).to_string(index=False)
        prompt = f"Describe the following CSV file in a few paragraphs in abstract terms. Focus on the structure, potential purpose, and any patterns you notice:\n\n{csv_content}"
        
        response = model.generate_content(prompt)
        gemini_description = response.text
    else:
        # For smaller CSV files, use the entire content
        gemini_description = df.to_string(index=False)

    return f"{programmatic_description}\n\n{gemini_description}"

def analyze_image_with_retries(image_path, model, temperature=0.7, max_retries=10, initial_sleep=6, show_prints=False):
    start_time = time.time()
    retries = 0
    sleep_time = initial_sleep

    while retries < max_retries:
        try:
            image = Image.open(image_path)
            prompt = "Describe the structure and contents of this image in a few paragraphs"
            
            input_content = [image, prompt]
            response = model.generate_content(
                input_content,
                generation_config={"temperature": temperature}
            )
            time.sleep(random.uniform(0.2, 0.5))

            current_date = datetime.now().strftime("%Y-%m-%d")
            try:
                description = f"Description of the following image by {model.model_name}, performed on {current_date}:\n{response.text}"
            except:
                description = f"failed to describe image by {model.model_name}"
            
            end_time = time.time()
            if show_prints:
                print(f"Image analysis completed in {end_time - start_time:.2f} seconds after {retries} retries.")
            
            return description

        except exceptions.GoogleAPICallError as e:
            retries += 1
            if show_prints:
                print(f"Attempt {retries} failed. Retrying in ~{sleep_time} seconds...")
            time.sleep(random.uniform(sleep_time - 1, sleep_time + 1))
            sleep_time = min(sleep_time * 1.3, 60)

    raise Exception(f"Failed to analyze image after {max_retries} attempts.")

def process_csv_file_with_retries(file_path, model, max_retries=10, initial_sleep=6, show_prints=False):
    start_time = time.time()
    retries = 0
    sleep_time = initial_sleep

    while retries < max_retries:
        try:
            df = pd.read_csv(file_path)
            row_count = len(df)
            col_count = len(df.columns)
            col_names = df.columns[:10].tolist()

            programmatic_description = (
                f"CSV file with {row_count} rows and {col_count} columns. "
                f"First several column names are {col_names}"
            )

            if row_count > 50:
                # For larger CSV files, use Gemini to generate a description
                csv_content = df.head(50).to_string(index=False)
                prompt = f"Describe the following CSV file in a few paragraphs in abstract terms. Focus on the structure, potential purpose, and any patterns you notice:\n\n{csv_content}"
                
                response = model.generate_content(prompt)
                try:
                    gemini_description = response.text
                except:
                    gemini_description = f"failed to describe CSV file by {model.model_name}"
                time.sleep(random.uniform(0.2, 0.5))
            else:
                # For smaller CSV files, use the entire content
                gemini_description = df.to_string(index=False)

            end_time = time.time()
            if show_prints:
                print(f"CSV processing completed in {end_time - start_time:.2f} seconds after {retries} retries.")

            return f"{programmatic_description}\n\n{gemini_description}"

        except exceptions.GoogleAPICallError as e:
            retries += 1
            if show_prints:
                print(f"Attempt {retries} failed. Retrying in ~{sleep_time} seconds...")
            time.sleep(random.uniform(sleep_time - 1, sleep_time + 1))
            sleep_time = min(sleep_time * 1.3, 60)

    raise Exception(f"Failed to process CSV file after {max_retries} attempts.")

def get_image_from_data(image_data):
    image_bytes = base64.b64decode(image_data)
    image = Image.open(io.BytesIO(image_bytes))
    return image

def convert_ipython_to_cell_list(notebook_filename):
    with open(notebook_filename, 'r', encoding='utf-8') as f:
        nb = json.load(f)
    cells = nb.get('cells', [])
    cell_list = []
    for idx, cell in enumerate(cells):
        ipython_cell = {}
        cell_type = cell.get('cell_type', '')
        source = ''.join(cell.get('source', ''))
        ipython_cell['input text'] = source
        if cell_type == 'code':
            if source.lstrip().startswith('!'):
                ipython_cell['input cell type'] = 'shell'
            else:
                ipython_cell['input cell type'] = 'python'
            output_text = ''
            output_figures = []
            outputs = cell.get('outputs', [])
            for output in outputs:
                output_type = output.get('output_type', '')
                if output_type == 'stream':
                    text = ''.join(output.get('text', ''))
                    output_text += text
                elif output_type in ('execute_result', 'display_data'):
                    data = output.get('data', {})
                    if 'text/plain' in data:
                        text = ''.join(data['text/plain'])
                        output_text += text + '\n'
                    if 'image/png' in data:
                        image_data = data['image/png']
                        image = get_image_from_data(image_data)
                        output_figures.append(image)
                elif output_type == 'error':
                    traceback = output.get('traceback', [])
                    text = ''.join(traceback)
                    output_text += text + '\n'
            ipython_cell['output_text'] = output_text
            ipython_cell['output_figures'] = output_figures
        elif cell_type == 'markdown':
            ipython_cell['input cell type'] = 'markdown'
            ipython_cell['output_text'] = ''
            ipython_cell['output_figures'] = []
        else:
            ipython_cell['input cell type'] = cell_type
            ipython_cell['output_text'] = ''
            ipython_cell['output_figures'] = []
        ipython_cell['cell metadata'] = cell.get('metadata', {})
        cell_list.append(ipython_cell)
    return cell_list

def process_ipython_notebook_file(notebook_filename, gemini_model, temp_dir='temp_repo_folder'):
    cell_list = convert_ipython_to_cell_list(notebook_filename)
    
    cell_delimiter_length = 60
    output_text = "-" * cell_delimiter_length + "\n"
    images_processed = 0

    for idx, cell in enumerate(cell_list):
        output_text += f"Cell index: {idx + 1}\n"
        output_text += f"Input Cell Type: {cell['input cell type']}\n"
        output_text += "Input Text:\n"
        output_text += "-----------\n"
        output_text += cell['input text']
        output_text += "\n\n"
        output_text += "Output Text:\n"
        output_text += "------------\n"
        output_text += cell['output_text'] + "\n"
        
        # Process images if present
        images = cell['output_figures']
        if len(images) > 0:
            output_text += "\n"
            output_text += "Output Images:\n"
            output_text += "--------------\n"
        for img_idx, img in enumerate(images):
            # Generate a unique filename for the temporary image
            temp_image_path = os.path.join(temp_dir, f"temp_image_{idx}_{img_idx}.png")
            
            # Save the PIL Image object as a temporary file
            img.save(temp_image_path, format='PNG')
            
            # Analyze the image using the existing analyze_image function
            image_description = analyze_image_with_retries(temp_image_path, gemini_model)
            output_text += f"\nImage {img_idx + 1} Description:\n{image_description}\n"
            images_processed += 1
        
        output_text += "\n"
        output_text += "-" * cell_delimiter_length + "\n"
    
    print(f"Total images processed in this notebook: {images_processed}")
    return output_text, images_processed

#%% Functions

def should_ignore(path):
    """Check if a file or folder should be ignored, including parent directories."""
    path = Path(path)
    
    exact_ignores = {'.git', '.github', '.venv', 'node_modules', '.next', '__pycache__', '.DS_Store'}
    pattern_ignores = ['*.pyc', '*.log', '.env*', '.venv*', 'package-lock.json', 'yarn.lock']
    ignored_extensions = {'.exe', '.dll', '.so', '.dylib'}
    max_file_size_MB = 10

    # Check all parent directories
    for parent in path.parents:
        if parent.name in exact_ignores or any(fnmatch.fnmatch(parent.name, pattern) for pattern in pattern_ignores):
            return True

    name = path.name
    
    if name in exact_ignores:
        return True
    
    for pattern in pattern_ignores:
        if fnmatch.fnmatch(name, pattern):
            return True
    
    if name.startswith('.') and name != '.':
        return True
    
    if path.is_file():
        if path.stat().st_size > max_file_size_MB * 1024 * 1024:
            return True
        
        if path.suffix.lower() in ignored_extensions:
            return True
    
    return False

def is_text_file(file_path):
    """Check if a file is a text file."""
    text_file_extensions = {
        '.json', '.txt', '.py', '.js', '.html', '.css', '.md', 
        '.hoc', '.mod', '.asc', '.c', '.h', '.cpp',
    }

    mime_type, _ = mimetypes.guess_type(file_path)
    if mime_type and mime_type.startswith('text'):
        return True
    
    return Path(file_path).suffix.lower() in text_file_extensions

def generate_file_structure(root_dir):
    """Generate a file structure representation, including folder names, while respecting ignore rules."""
    file_structure = []
    for root, dirs, files in os.walk(root_dir):
        # Apply should_ignore to directories
        dirs[:] = [d for d in dirs if not should_ignore(os.path.join(root, d))]
        
        level = root.replace(root_dir, '').count(os.sep)
        indent = '    ' * level
        folder_name = os.path.basename(root)
        
        if folder_name and root != root_dir:  # Skip the root directory itself
            if not should_ignore(root):
                file_structure.append(f"{indent}{folder_name}/")
            else:
                continue  # Skip this directory and its contents
        
        subindent = '    ' * (level + 1)
        for f in files:
            file_path = os.path.join(root, f)
            if not should_ignore(file_path):
                file_structure.append(f"{subindent}{f}")
    
    return file_structure

def read_file_contents(file_path):
    """Read and return the contents of a file."""
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
            return file.read()
    except Exception as e:
        return f"Error reading file {file_path}: {str(e)}"

def write_to_output(file_path, content, mode='a'):
    """Write content to the output file."""
    with open(file_path, mode, encoding='utf-8') as f:
        f.write(content + '\n')

def delimiter_block(content):
    """Create a delimiter block for content."""
    delimiter = '=' * 80
    return f"{delimiter}\n{content}\n{delimiter}"

def file_content_block(file_path, content):
    """Create a content block for a file."""
    delimiter = '=' * 80
    path_delimiter = '=' * (len(str(file_path)) + 1)
    return f"{delimiter}\n{file_path}:\n{path_delimiter}\n{content}\n{delimiter}"

def download_repo_as_zip(repo_url, download_dir):
    """Download a GitHub repository as a zip file and extract it."""
    repo_name = repo_url.split('/')[-1]
    branches = ['main', 'master']
    for branch in branches:
        zip_url = f"{repo_url}/archive/refs/heads/{branch}.zip"
        print(f"Trying to download from {zip_url}")
        response = requests.get(zip_url)
        print(f"Response status code: {response.status_code}")
        if response.status_code == 200:
            zip_path = os.path.join(download_dir, f"{repo_name}.zip")
            with open(zip_path, 'wb') as f:
                f.write(response.content)
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(download_dir)
            os.remove(zip_path)
            extracted_dir = os.path.join(download_dir, f"{repo_name}-{branch}")
            return repo_name, extracted_dir
    raise Exception(f"Failed to download repository from both main and master branches: {repo_url}")

#%% Core Functions

def process_github_repo(repo_url, output_dir, download_dir='temp_repo_folder', describe_images=True, describe_csv_files=True, process_notebooks=True):
    """Process a GitHub repository and create a dataset."""
    if not os.path.exists(download_dir):
        os.makedirs(download_dir)

    repo_name, extracted_dir = download_repo_as_zip(repo_url, download_dir)
    output_file = os.path.join(output_dir, f"{repo_name}.txt")

    # Write repo title and link
    date_string = datetime.now().strftime("%Y-%m-%d")
    starting_string = f"repo title: {repo_name}\nrepo link: {repo_url}\ndate processed: {date_string}"
    write_to_output(output_file, delimiter_block(starting_string), mode='w')

    # Write file structure
    file_structure = generate_file_structure(extracted_dir)
    file_structure_content = '\n'.join(file_structure)
    write_to_output(output_file, file_content_block("repo file structure", file_structure_content))

    # Set up Gemini model if needed
    gemini_model = setup_gemini_model() if (describe_images or describe_csv_files or process_notebooks) else None

    # Process all files
    all_files = []
    images_described = 0
    csvs_processed = 0
    notebooks_processed = 0
    for root, _, files in os.walk(extracted_dir):
        for file in files:
            file_path = os.path.join(root, file)
            if not should_ignore(file_path) and (is_text_file(file_path) or 
                                                 (describe_images and is_image_file(file_path)) or 
                                                 (describe_csv_files and is_csv_file(file_path)) or
                                                 (process_notebooks and is_ipython_notebook(file_path))):
                all_files.append(file_path)

    # Find and process README file
    readme_files = [f for f in all_files if os.path.basename(f).lower().startswith('readme')]
    if readme_files:
        readme_file = readme_files[0]
        readme_content = read_file_contents(readme_file)
        write_to_output(output_file, file_content_block(os.path.relpath(readme_file, extracted_dir), readme_content))
        all_files.remove(readme_file)  # Remove README from all_files to avoid duplication

    # Process remaining files
    random.shuffle(all_files)
    for file_path in all_files:
        if is_text_file(file_path):
            content = read_file_contents(file_path)
        elif describe_images and is_image_file(file_path):
            content = analyze_image_with_retries(file_path, gemini_model)
            images_described += 1
        elif describe_csv_files and is_csv_file(file_path):
            content = process_csv_file_with_retries(file_path, gemini_model)
            csvs_processed += 1
        elif process_notebooks and is_ipython_notebook(file_path):
            content, notebook_images = process_ipython_notebook_file(file_path, gemini_model, temp_dir=download_dir)
            images_described += notebook_images
            notebooks_processed += 1
        else:
            continue  # Skip other file types
        write_to_output(output_file, file_content_block(os.path.relpath(file_path, extracted_dir), content))

    # Cleanup downloaded repository folder
    shutil.rmtree(download_dir)

    print(f"Processed GitHub repository: {repo_url}")
    print(f"Output file created: {output_file}")
    print(f"Total files processed: {len(all_files) + (1 if readme_files else 0)}")
    if describe_images:
        print(f"Total images described: {images_described}")
    if describe_csv_files:
        print(f"Total CSV files processed: {csvs_processed}")
    if process_notebooks:
        print(f"Total notebooks processed: {notebooks_processed}")

    return images_described, csvs_processed, notebooks_processed


def process_local_folder(folder_path, output_dir, temp_dir='temp_repo_folder', describe_images=True, describe_csv_files=True, process_notebooks=True):
    """Process a local folder and create a dataset."""
    os.makedirs(temp_dir, exist_ok=True)
    folder_name = os.path.basename(folder_path)
    output_file = os.path.join(output_dir, f"{folder_name}.txt")

    # Write repo title and link
    date_string = datetime.now().strftime("%Y-%m-%d")
    starting_string = f"Folder title: {folder_name}\nFolder path: {folder_path}\nDate processed: {date_string}"
    write_to_output(output_file, delimiter_block(starting_string), mode='w')

    # Write file structure
    file_structure = generate_file_structure(folder_path)
    file_structure_content = '\n'.join(file_structure)
    write_to_output(output_file, file_content_block("folder file structure", file_structure_content))

    # Set up Gemini model if needed
    gemini_model = setup_gemini_model() if (describe_images or describe_csv_files or process_notebooks) else None

    # Process all files
    all_files = []
    images_described = 0
    csvs_processed = 0
    notebooks_processed = 0
    for root, _, files in os.walk(folder_path):
        for file in files:
            file_path = os.path.join(root, file)
            if not should_ignore(file_path) and (is_text_file(file_path) or 
                                                 (describe_images and is_image_file(file_path)) or 
                                                 (describe_csv_files and is_csv_file(file_path)) or
                                                 (process_notebooks and is_ipython_notebook(file_path))):
                all_files.append(file_path)

    # Find and process README file
    readme_files = [f for f in all_files if os.path.basename(f).lower().startswith('readme')]
    if readme_files:
        readme_file = readme_files[0]
        readme_content = read_file_contents(readme_file)
        write_to_output(output_file, file_content_block(os.path.relpath(readme_file, folder_path), readme_content))
        all_files.remove(readme_file)  # Remove README from all_files to avoid duplication

    # Process remaining files
    random.shuffle(all_files)
    for file_path in all_files:
        if is_text_file(file_path):
            content = read_file_contents(file_path)
        elif describe_images and is_image_file(file_path):
            content = analyze_image_with_retries(file_path, gemini_model)
            images_described += 1
        elif describe_csv_files and is_csv_file(file_path):
            content = process_csv_file_with_retries(file_path, gemini_model)
            csvs_processed += 1
        elif process_notebooks and is_ipython_notebook(file_path):
            content, notebook_images = process_ipython_notebook_file(file_path, gemini_model, temp_dir=temp_dir)
            images_described += notebook_images
            notebooks_processed += 1
        else:
            continue  # Skip other file types
        write_to_output(output_file, file_content_block(os.path.relpath(file_path, folder_path), content))

    # Cleanup downloaded repository folder
    shutil.rmtree(temp_dir)

    print(f"Processed local folder: {folder_path}")
    print(f"Output file created: {output_file}")
    print(f"Total files processed: {len(all_files) + (1 if readme_files else 0)}")
    if describe_images:
        print(f"Total images described: {images_described}")
    if describe_csv_files:
        print(f"Total CSV files processed: {csvs_processed}")
    if process_notebooks:
        print(f"Total notebooks processed: {notebooks_processed}")

    return images_described, csvs_processed, notebooks_processed


def convert_repos_to_text(sources, output_dir, is_github=True, describe_images=True, describe_csv_files=True, process_notebooks=True):
    """Create a dataset from multiple sources (GitHub repos or local folders)."""
    os.makedirs(output_dir, exist_ok=True)
    total_images_described = 0
    total_csvs_processed = 0
    total_notebooks_processed = 0

    for source in sources:
        try:
            if is_github:
                images_described, csvs_processed, notebooks_processed = process_github_repo(
                    source, output_dir, 
                    describe_images=describe_images, describe_csv_files=describe_csv_files, process_notebooks=process_notebooks)
            else:
                images_described, csvs_processed, notebooks_processed = process_local_folder(
                    source, output_dir, 
                    describe_images=describe_images, describe_csv_files=describe_csv_files, process_notebooks=process_notebooks)
            total_images_described += images_described
            total_csvs_processed += csvs_processed
            total_notebooks_processed += notebooks_processed
        except Exception as e:
            print(f"Error processing {source}: {str(e)}")

        print("=" * 50)

    if describe_images:
        print(f"Total images described across all sources: {total_images_described}")
    if describe_csv_files:
        print(f"Total CSV files processed across all sources: {total_csvs_processed}")
    if process_notebooks:
        print(f"Total notebooks processed across all sources: {total_notebooks_processed}")



#%% 
================================================================================
================================================================================
setup.py:
=========
from setuptools import setup, find_packages

with open("README.md", "r", encoding="utf-8") as fh:
    long_description = fh.read()

setup(
    name="repo_to_text",
    version="0.1.0",
    author="David Beniaguev",
    description="A tool to convert repositories and local folders to text format for LLM copy pasting and training",
    long_description=long_description,
    long_description_content_type="text/markdown",
    url="https://github.com/SelfishGene/repo_to_text",
    python_requires=">=3.7",
    install_requires=[
        "requests",
        "Pillow",
        "pandas",
        "google-generativeai",
        "python-dotenv",
    ],
)

================================================================================
================================================================================
requirements.txt:
=================
requests
Pillow
pandas
google-generativeai
python-dotenv

================================================================================
================================================================================
examples/SFHQ-T2I-dataset.txt:
==============================
================================================================================
repo title: SFHQ-T2I-dataset
repo link: https://github.com/SelfishGene/SFHQ-T2I-dataset
date processed: 2024-10-21
================================================================================
================================================================================
repo file structure:
====================
    merge_dataset_folder.py
    extract_pretrained_features.py
    README.md
    create_face_dataset.py
    explore_dataset.py
    LICENSE.txt
    face_prompt_utils.py
    figures/
        textual_search_2_Age_x_Ethnicity x Sex 1_top_10_matches.jpg
        textual_search_2_Hair Style_top_9_matches.jpg
        textual_search_2_Expression_x_Sex_top_10_matches.jpg
        textual_search_2_Hair Style x Sex_top_10_matches.jpg
        FLUX1_schnell_SDXL_images.jpg
        textual_search_2_Glasses Style_top_10_matches.jpg
        flux_images.jpg
        textual_search_2_bad things_top_10_matches.jpg
        textual_search_2_Accessories_top_10_matches.jpg
        textual_search_1_eye_color_top_8_matches.jpg
        textual_search_2_Hair_Color_top_10_matches.jpg
        textual_search_1_accessories_top_8_matches.jpg
        textual_search_2_Facial Hair_top_10_matches.jpg
        textual_search_1_glasses_top_8_matches.jpg
        textual_search_2_Age_x_Ethnicity x Sex 2_top_10_matches.jpg
        prompt_lengths_distribution.jpg
        textual_search_2_Hats_top_10_matches.jpg
        good_model_images.jpg
        textual_search_1_age_male_top_8_matches.jpg
        all_model_images.jpg
        FLUX1_dev_images_with_prompts.jpg
        textual_search_1_expression_top_8_matches.jpg
        textual_search_2_Ethnicity x Age 3_top_10_matches.jpg
        SDXL_images_with_prompts.jpg
        textual_search_1_ethnicity_top_8_matches.jpg
        textual_search_2_Lighting_top_10_matches.jpg
        textual_search_2_Physical Characteristics_top_9_matches.jpg
        textual_search_2_Age_top_10_matches.jpg
        textual_search_1_sex_top_8_matches.jpg
        textual_search_2_Eye Color_top_10_matches.jpg
        textual_search_2_Ethnicity_top_10_matches.jpg
        FLUX1_schnell_images_with_prompts.jpg
        model_distribution.jpg
        textual_search_2_Face Pose_top_10_matches.jpg
        FLUX1_pro_images_with_prompts.jpg
        textual_search_2_Background Color_top_9_matches.jpg
        textual_search_2_Background_top_10_matches.jpg
        textual_search_2_Eye Gaze_top_9_matches.jpg
        textual_search_1_age_female_top_8_matches.jpg
        textual_search_1_hair_color_top_8_matches.jpg
        textual_search_1_hats_top_8_matches.jpg
        DALLE3_images_with_prompts.jpg
        textual_search_2_Makeup_top_9_matches.jpg
        textual_search_2_Ethnicity x Age 1_top_10_matches.jpg
        textual_search_2_Facial Features_top_9_matches.jpg
        textual_search_2_Jewelry_top_9_matches.jpg
        textual_search_2_Expression_top_10_matches.jpg
        textual_search_2_Ethnicity x Age 2_top_10_matches.jpg
================================================================================
================================================================================
README.md:
==========
# Synthetic Faces High Quality - Text2Image (SFHQ-T2I)

![SFHQ-T2I dataset Flux1.pro samples](https://raw.githubusercontent.com/SelfishGene/SFHQ-T2I-dataset/main/figures/FLUX1_pro_images_with_prompts.jpg)

This dataset consists of 122,726 high-quality 1024x1024 curated face images. It was created by generating random prompt strings that were sent to multiple "text to image" models (Flux1.pro, Flux1.dev, Flux1.schnell, SDXL, DALL-E 3) and curating the results using a semi-manual process.

The prompts describe various faces with different attributes and conditions to ensure extreme variance and diversity in ethnicities, poses, accessories, jewelry, hairstyles and hair colors, expressions, backgrounds, lighting, and more. Due to our ability to control each of these attributes independently via the text prompt, this dataset exhibits a previously unprecedented degree of variance and diversity among publicly available face datasets along most facial attributes. Additionally, it is free of privacy concerns and licensing issues because all images are synthetically generated.

The SFHQ-T2I dataset features high-quality images, surpassing those of the [SFHQ dataset](https://github.com/SelfishGene/SFHQ-dataset), with most being photorealistic and of high resolution. The dataset is paired with the prompts used to generate each image, allowing for a wide range of applications in text-to-image synthesis, face analysis, and other machine learning tasks.


![SFHQ-T2I dataset all model images](https://raw.githubusercontent.com/SelfishGene/SFHQ-T2I-dataset/main/figures/all_model_images.jpg)

## Download

The dataset can be downloaded from Kaggle via the link: [SFHQ-T2I dataset on kaggle](https://www.kaggle.com/datasets/selfishgene/sfhq-t2i-synthetic-faces-from-text-2-image-models)

## Repository Contents

1. `create_face_dataset.py`: Script to generate the dataset using various text-to-image APIs (fal, openai, stability)
2. `explore_dataset.py`: Script for basic exploratory data analysis of the dataset
3. `extract_pretrained_features.py`: Utility to extract features from pretrained OpenCLIP models for the dataset images
4. `face_prompt_utils.py`: Utilities for automatically generating diverse face prompts
5. `merge_dataset_folder.py`: Script to merge multiple dataset folders
6. `figures/`: Folder containing various visualizations of the dataset

## Dataset Details

- 122,726 high quality 1024x1024 face images
- Paired (text, image) dataset
- Generated using multiple text-to-image models via randomly generated prompts:
  - Flux1.pro (3,209 images)
  - Flux1.dev (7,273 images)
  - Flux1.schnell (58,034 images)
  - SDXL (53,087 images)
  - DALL-E 3 (1,123 images)
- Unprecedented variability in various face attributes such as accessories, ethnicity and age, expressions, and more
- CSV file (`SFHQ_T2I_dataset.csv`) containing details about each image:
  - Prompt used to generate the image
  - Model used
  - Random seed, number of steps and other configuration details specific to each model

![SFHQ-T2I dataset model distribution](https://raw.githubusercontent.com/SelfishGene/SFHQ-T2I-dataset/main/figures/model_distribution.jpg)

## Example illustrations of dataset diversity 
The following figures were created by performing textual searches on the dataset using CLIP features

- accessories:  
![SFHQ-T2I dataset accessories diversity](https://raw.githubusercontent.com/SelfishGene/SFHQ-T2I-dataset/main/figures/textual_search_2_Accessories_top_10_matches.jpg)

- hair color:  
![SFHQ-T2I dataset hair color diversity](https://raw.githubusercontent.com/SelfishGene/SFHQ-T2I-dataset/main/figures/textual_search_2_Hair_Color_top_10_matches.jpg)

- lighting:  
![SFHQ-T2I dataset lighting diversity](https://raw.githubusercontent.com/SelfishGene/SFHQ-T2I-dataset/main/figures/textual_search_2_Lighting_top_10_matches.jpg)

- expression:  
![SFHQ-T2I dataset expression diversity](https://raw.githubusercontent.com/SelfishGene/SFHQ-T2I-dataset/main/figures/textual_search_2_Expression_x_Sex_top_10_matches.jpg)

- age:  
![SFHQ-T2I dataset age diversity](https://raw.githubusercontent.com/SelfishGene/SFHQ-T2I-dataset/main/figures/textual_search_2_Age_top_10_matches.jpg)

Additional examples of various textual searchers can be found in the `figures/` folder.

## Notes on the text-to-image models used

The dalle images are very bad, this is possibly done on purpose from openai point of view to avoid scandals related to face images. Also, the openai api is changing the prompt you request to its own prompts without the ability to cancel this edit, so we lack even basic control. Therefore, I strongly suggest not to use it. I initially planned to generate much more images from the Dalle-3 model due to its prompt adherence, but there appears to be a specific issue with face images. It is also more expensive than the other options like the Flux models and the results are not as good.

Both Flux1.pro and Flux1.dev are very very good. SDXL and schnell are also good and way cheaper, but they are not as good as the pro and dev models. SDXL has its own unique style, especially in its textures, but its global image structure is sometimes flawed. Schnell is similar to the pro and dev models at the global structure of the image, but it is not as sharp and the textures are not always photorealistic. A small fraction of the generated images appear like 3D models and paintings. I've kept them in the dataset despite not being the main focus of the dataset, because the main goal is increasing entropy.

## Usage

An example script demonstrating how to access and explore the data can be found in the `explore_dataset.py` file. This script shows how to load the dataset, visualize the distribution of images across models, plot prompt length distributions, and perform textual searches on the dataset using CLIP features.

## Prompt Generation

The code to generate the random prompts can be found in `face_prompt_utils.py`. This script creates highly diverse prompts by combining various facial attributes, expressions, accessories, and environmental factors.

## Creating your own dataset

You can adjust the following parameters in the `__main__` part of the `create_face_dataset.py` script:

- `output_db_folder`: The directory where images and metadata will be saved
- `sdxl_samples`, `dalle3_samples`, `flux1_pro_samples`, ...: Number of images to generate for each model
- `sdxl_config`, `dalle3_config`, `flux1_pro_config`, ...: Configuration parameters for each model

#### Running the Script

1. Ensure you have the appropriate API keys set up in the script as described above and set up the number of samples to generate for each model.

2. Run the script:
   ```
   python create_face_dataset.py
   ```
   The script will create a `.env` file with your API keys on the first run. It will then generate images using all three models and save them in the specified output folder along with a metadata CSV file.

3. The script generates:  
Images in the `{output_db_folder}/images` directory  
A `SFHQ_T2I_dataset.csv` file in the `output_db_folder` containing information about each generated image


## API Key Setup

You'll need API keys for Stability AI, OpenAI, and FAL AI. Here's how to obtain them:

1. FAL AI (for FLUX1.pro):
   - Sign up at https://www.fal.ai/
   - Generate an API key from your account dashboard

2. Stability AI (for SDXL):
   - Sign up at https://platform.stability.ai/
   - Navigate to your account dashboard and generate an API key

3. OpenAI (for DALL-E 3):
   - Sign up at https://platform.openai.com/
   - Go to the API section and create a new API key

4. Install the required packages:
   ```
   pip install stability-sdk openai fal-client pillow pandas python-dotenv
   ```

Once you have your API keys, update the following lines at the beginning of the `create_face_dataset.py` script with your actual keys:

```python
STABILITY_API_KEY = 'your-stability-ai-api-key'
OPENAI_API_KEY = 'your-openai-api-key'
FAL_API_KEY = 'your-fal-api-key'
```



## Privacy

Since all images in this dataset are synthetically generated, there are no privacy issues or license issues surrounding these images.

## Citation

If you use this dataset in your research, please cite it as follows:

```
@misc{david_beniaguev_2024_SFHQ_T2I,
    title={Synthetic Faces High Quality - Text 2 Image (SFHQ-T2I) Dataset},
    author={David Beniaguev},
    year={2024},
    url={https://github.com/SelfishGene/SFHQ-T2I-dataset},
    publisher={GitHub},
    DOI={10.34740/kaggle/dsv/9548853},
}
```

## Summary

The SFHQ-T2I dataset provides a large, diverse collection of high-quality synthetic face images paired with their generating prompts. This dataset is unique in its level of variability across multiple facial and environmental attributes, made possible by the use of various state-of-the-art text-to-image models and carefully crafted prompts.

The purpose of this dataset is to provide a rich resource for training and evaluating machine learning models in tasks related to face analysis, generation, and text-to-image synthesis, without the need to worry about privacy or license issues.
The dataset may be extended from time to time with additional labels or features, but no promises.

I hope this dataset proves useful. Feel free to use it as you see fit...

================================================================================
================================================================================
figures/textual_search_1_age_male_top_8_matches.jpg:
====================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid of 96 photographs, organized into eight rows and twelve columns. Each photograph depicts a male subject at a different age, ranging from baby to elderly.  The images are neatly arranged, with each photo having a consistent size and aspect ratio. The background of each photo is mostly dark or neutral, ensuring the subject is the clear focal point.

The photos are grouped into eight age categories: baby boy, boy toddler, child boy, teenage boy, adult male, middle-aged adult male, and elderly male.  Each age category has eight photos, representing eight different individuals across the same age range.  This arrangement suggests a comparison or matching exercise; perhaps the images are part of a dataset for facial recognition or age progression studies.

Above each photo, there is text that appears to be a filename and a descriptor, indicating the image's purpose within the dataset (e.g., "Match 1 for: baby boy," followed by the filename). This labeling system is consistent throughout the grid, providing clear identification for each photo and its intended association.  The overall structure suggests a well-organized and systematic approach to image organization and labeling.

================================================================================
================================================================================
figures/textual_search_2_Facial Features_top_9_matches.jpg:
===========================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a grid of 72 face images, organized into 8 rows and 9 columns. Each image is accompanied by text indicating the file name and a label describing a facial feature.  The labels fall into five categories: "reading glasses," "sunglasses," "bald," "goatee," and "lipstick."  The file names suggest the images come from two different datasets, SDXL and FLUX1.

The arrangement of images aims to visually demonstrate the results of an image textual search using OpenCLIP features. The search likely used a prefix, "Facial Features," to filter results from a synthetic dataset.  The grid shows images that match the textual search query for each of the five facial feature labels. The goal is to showcase the accuracy and diversity of the image retrieval system based on the textual descriptions.

The images themselves are diverse in terms of age, gender, ethnicity, and lighting conditions. This diversity is important for evaluating the robustness and generalizability of the image search algorithm. The images are high-quality and clearly show the specified facial features, making it easy to assess the relevance of each result to its label.

================================================================================
================================================================================
figures/textual_search_2_Background Color_top_9_matches.jpg:
============================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid showcasing a diverse collection of portraits, organized into a matrix of 6 columns and 8 rows. Each cell contains a portrait photograph of a person against a solid background color. 


The backgrounds are consistently colored, with sections dedicated to yellow, green, blue, purple, and red backgrounds, creating distinct blocks within the grid. Each image is labeled with a filename (suggesting it comes from a dataset) and a descriptive text label indicating the background color ("yellow background", "green background", etc.). This systematic arrangement implies that the images are curated and categorized based on background color for a specific purpose, likely related to image analysis or machine learning.


The portraits themselves feature a wide variety of individuals, exhibiting different ages, ethnicities, hairstyles, and expressions. The consistent lighting and framing suggest a controlled photographic setting, possibly a studio environment. The overall impression is that the images are part of a large dataset used for training or testing image recognition algorithms, potentially focusing on background color identification or separation.

================================================================================
================================================================================
figures/textual_search_2_Lighting_top_10_matches.jpg:
=====================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a large grid showcasing numerous photographs, arranged in a 10x10 matrix. Each photograph depicts a person, predominantly portraits, with diverse ethnicities, ages, and genders represented.  The lighting in each photo is distinct and varied, serving as a key visual element.

The images are categorized by lighting style, indicated by text labels beneath each image.  These labels include descriptive terms like "side light with shadows," "spotlight," "soft lighting," "back lighting," "golden hour," "blue hour lighting," and "studio lighting."  Each label is further accompanied by a filename, suggesting the images might be sourced from a specific dataset or project.  The consistent use of labels and filenames implies a structured and organized collection.

The overall impression is of a comprehensive visual dataset designed for image analysis and training, potentially related to lighting conditions in photography or computer vision. The diversity of subjects and the systematic variation in lighting styles make it suitable for tasks such as image classification, style transfer, or the development of algorithms sensitive to different lighting scenarios.

================================================================================
================================================================================
figures/textual_search_2_Age_top_10_matches.jpg:
================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image presents a grid of 96 images, each accompanied by a textual label describing the age and identity of the person depicted. The images are organized into rows and columns, showcasing a diverse range of individuals across different age groups, from babies to elderly people.  The age categories include "10-month-old baby," "2.5-year-old toddler," "small child," "16-year-old teenager," "30-year-old adult," and "wrinkly 70-year-old senior."  Each image is clearly labeled with its corresponding age description and a filename, suggesting it's sourced from a large dataset.  The images are high-resolution and show individuals with varying ethnicities and expressions.

The arrangement of the images suggests a systematic organization based on age, progressing from infancy to old age.  The consistency in image quality and labeling implies a controlled, likely synthetic, dataset created for image recognition or similar machine learning tasks.  The title "Image textual search using OpenCLIP features from synthetic dataset" further reinforces this purpose, indicating the images are used for testing or training an image search algorithm using the OpenCLIP model. The "Condition: Age" and "Prefix: "" clarifies that the dataset is categorized and used for age classification.


The overall impression is one of a meticulously curated collection designed for research and development in the field of computer vision and artificial intelligence, specifically focused on age recognition and image retrieval.

================================================================================
================================================================================
figures/textual_search_2_Expression_top_10_matches.jpg:
=======================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image presents a grid of 108 small images, each displaying a different human face.  The faces are diverse in age, gender, and ethnicity. Each face is labeled with a descriptive caption that categorizes the facial expression shown, such as "angry or enraged," "surprised," "smiling," "sad or depressed," "grim face," or "tongue out".  The captions also include a file name, likely indicating the source of the image.

The arrangement of the images is systematic, with each row representing a specific emotion. The layout enables a visual comparison of how different individuals express the same emotion. The overall aim seems to be to showcase a dataset of facial expressions used for training or testing a facial recognition or emotion detection model.  The diversity of faces suggests an attempt to create a robust and inclusive dataset representative of a wide range of demographics.  The consistent labeling and organization facilitate easy navigation and understanding of the dataset's contents.

================================================================================
================================================================================
figures/SDXL_images_with_prompts.jpg:
=====================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a diptych, presenting two distinct portraits side-by-side. Each portrait features a close-up shot of an older adult, focusing on their facial features and expressions.  The left portrait shows a Caucasian grandmother, appearing to be in her 60s, wearing large, round blue glasses adorned with rhinestones. Her expression is somewhat pensive, her gaze directed slightly downward.  The right portrait depicts an Amerindian individual, also appearing middle-aged, with long, dreadlocked magenta hair. Their expression is more serious and melancholic, with a direct gaze at the viewer.  Their skin shows significant texture and detail.

Below each portrait is a caption providing details such as age, ethnicity, and photographic specifications. The left caption describes a candid shot, highlighting the subject's nervous twitch and the serene setting of a lavender field in Provence. The right caption details a close-up shot taken in a historic monastery, emphasizing the subject's deep blue eyes and the use of a DSLR camera. Both captions use descriptive language to evoke the mood and atmosphere of each photograph. The overall visual style of both portraits is high-quality, with a focus on sharp details and subtle use of bokeh.

================================================================================
================================================================================
figures/FLUX1_schnell_SDXL_images.jpg:
======================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a grid of eight photos, arranged in two rows of four. Each photo is a close-up portrait of a person, showcasing a diverse range of ages, ethnicities, and styles. 


The top row features:

1. A woman with weathered features and a dark headscarf.
2. A young girl with blonde hair wearing a crown and sunglasses.
3. An elderly Asian man in traditional Native American headdress.
4. An elderly Asian man with a long white beard and a knit cap.


The bottom row features:

5. A middle-aged man with glasses and a leather jacket.
6. An elderly man with white hair and sunglasses.
7. A young Black man with dreadlocks and sunglasses, wearing a pearl necklace.
8. A young child with dark hair wearing a flower crown.


Each photo has a filename overlaid at the top, indicating the source and image identifier. The filenames suggest the images are from two different sources, "FLUX1_schnell" and "SDXL". The overall feel is a collection of striking portraits, highlighting individual character and diversity.

================================================================================
================================================================================
figures/textual_search_1_hats_top_8_matches.jpg:
================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a large grid of 72 smaller images, arranged in 9 rows and 8 columns. Each small image shows a different person wearing a hat. The hats are categorized into eight types: baseball cap, fedora, beanie, top hat, cowboy hat, and sun hat.

Each small image is labeled with a caption indicating the hat type and a file name.  The captions appear to be generated automatically, possibly by a machine learning model designed to identify and categorize the hat types. The file names suggest the images originate from different sources or datasets (SDXL, FLUX1).

The arrangement of the images implies a comparison or matching task.  It's likely that the image grid was generated to showcase the accuracy of a hat-identification algorithm, demonstrating its ability to correctly classify different hat types across a diverse set of individuals and image backgrounds. The variety in age, gender, and ethnicity of the people shown in the images contributes to a comprehensive test of the image recognition capabilities.

================================================================================
================================================================================
figures/textual_search_1_sex_top_8_matches.jpg:
===============================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid of 32 headshots, organized into eight rows of four images each.  Each headshot shows a person from the shoulders up, against a variety of backgrounds. The individuals depicted exhibit diverse ethnicities, ages, genders, and hair styles. 


Above each set of four images, text labels identify the image as a "Match" number (1-8) for either "male," "female," or "non-binary person."  Below each image is a filename, indicating the source of the picture. The consistent formatting and labeling suggest the images are part of a dataset or a result from a facial recognition or matching algorithm, perhaps for a research project or application. The diversity in the subjects suggests an attempt at inclusivity and representation in the dataset.

================================================================================
================================================================================
figures/textual_search_1_hair_color_top_8_matches.jpg:
======================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a large grid, organized into eight columns and numerous rows. Each cell within the grid contains a portrait photograph of a person.  The portraits appear to be professionally taken, with consistent lighting and backgrounds. The subjects vary widely in age, gender, and ethnicity, providing a diverse representation of individuals.

Above each column of images is a label indicating the hair color or style being represented in that column ("black hair," "brown hair," "blonde hair," "red hair," "gray hair," "bald," "blue hair," "green hair," and "pink hair"). Each individual photo is further labeled with a file name and a description like "Match 1 for: black hair".  This suggests the images are part of a dataset used for image matching or training a machine learning model, possibly for facial recognition or hair classification.  The consistent labeling and organization point to a structured and methodical approach to data collection and organization.

================================================================================
================================================================================
figures/textual_search_1_glasses_top_8_matches.jpg:
===================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid of 64 smaller images, arranged in eight rows and eight columns. Each smaller image shows a different person, mostly close-up headshots, wearing various types of eyeglasses or sunglasses. The images are neatly organized, with a consistent background color and lighting for each individual photo.

Above each row of eight images, there's a caption indicating the type of eyewear depicted—reading glasses, sunglasses, round glasses, or square glasses. Next to this caption is "Match 1," "Match 2," and so on, up to "Match 8," indicating a possible dataset or matching process. Each image also includes a filename, suggesting that the images are sourced from a larger dataset.

The people in the images are diverse, varying in age, gender, ethnicity, and hairstyle. They appear to be posed for the photos, with a neutral or slightly serious expression. The glasses in each image vary in style, frame shape, and color, showing a wide range of eyewear options. The overall structure and content suggest the image is likely a sample from a dataset used for training or testing a computer vision model, potentially one focused on eyewear recognition or classification.
================================================================================
================================================================================
figures/textual_search_2_Ethnicity x Age 1_top_10_matches.jpg:
==============================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a grid of 72 individual photographs, arranged in a 6x12 matrix. Each photograph is a close-up portrait of an elderly person from diverse ethnic backgrounds.  The portraits are high-quality, well-lit, and appear to be professionally taken.  They display a wide range of facial expressions and features, reflecting the age and individuality of the subjects.

Above each photograph, text indicates the presumed ethnicity of the individual ('asian', 'native american', 'african', 'persian', 'south-american', 'irish') and a filename. The consistent formatting suggests the images are part of a structured dataset, likely used for machine learning or artificial intelligence purposes.  The top of the image contains a title indicating that the image is related to image textual search using OpenCLIP features from a synthetic dataset, focusing on ethnicity and age.  The "old age" prefix further specifies the age range of the subjects.

The overall impression is a meticulously curated collection of photographs designed to represent a broad spectrum of ethnicities within a specific age group, likely for research or development purposes in fields such as facial recognition, age estimation, or bias detection in AI algorithms.

================================================================================
================================================================================
figures/textual_search_2_Facial Hair_top_10_matches.jpg:
========================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a grid of 72 images, each depicting a man's face with various facial hair styles. The images are neatly arranged in a 9x8 matrix.  Each image is labeled with a filename, indicating its source and a descriptive tag classifying the facial hair: "full beard," "mustache," "goatee," "sideburns," "stubble," or "shaved face."  The filenames suggest the images come from different datasets, including FLUX1 (with variations like "_dev," "_schnell," and "_pro") and SDXL.

The top of the image contains a title indicating the purpose: "Image textual search using OpenCLIP features from synthetic dataset."  A subtitle clarifies the condition as "Facial Hair" and the prefix used for the search query as "man with ".  This implies that the images are results of a text-based image search, aiming to retrieve pictures of men with specific facial hair attributes. The overall arrangement suggests an organized visualization of the results, facilitating a comparison and analysis of the different facial hair types and the effectiveness of the image search.

================================================================================
================================================================================
figures/textual_search_2_Makeup_top_9_matches.jpg:
==================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid of 72 images, organized in 6 rows and 12 columns, showcasing a variety of women's faces with different makeup styles. Each image is labeled with a descriptive caption indicating the type of makeup (e.g., "heavy makeup," "without makeup," "red lipstick," "strong eyeliner," "traditional makeup") and the source of the image (e.g., "FLUX1_pro_image_0002399.jpg"). The images illustrate a wide range of skin tones, ages, and facial features, providing a diverse representation of women.

The top of the image includes a title, "Image textual search using OpenCLIP features from synthetic dataset," indicating that the images are part of a dataset used for image recognition and search. The subtitle, "Condition: Makeup," and "Prefix: 'woman'," further specify the context and the type of search being performed.  The variety in makeup styles within the grid allows for testing and training of AI models to accurately identify and classify different makeup types.

The consistent labeling of each image makes it easy to understand the dataset's structure and purpose.  The layout effectively visualizes the diversity of makeup styles and the differences in appearance based on those styles.  The overall visual presentation is clean and well-organized, facilitating easy analysis of the image data.

================================================================================
================================================================================
figures/prompt_lengths_distribution.jpg:
========================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a comparative analysis of prompt lengths for different large language models (LLMs), visualized through two overlaid histograms.  The top histogram shows the distribution of prompt lengths in terms of the number of characters, while the bottom histogram displays the same data but measured in the number of words.  Both histograms use the same color scheme to represent each LLM: DALL-E 3 (purple), FLUX1_dev (teal), FLUX1_pro (dark green), FLUX1_schnell (brown), and SDXL (dark red).

The x-axes of both histograms represent the prompt length (characters in the top, words in the bottom), and the y-axes represent the frequency, indicating how many prompts fall within each length bin.  The overlapping nature of the histograms clearly demonstrates the relative distributions of prompt lengths for each model.  For instance, it's readily apparent that SDXL and FLUX1_schnell tend to have longer prompts than DALL-E 3, across both character and word counts.  The visual stacking allows for a direct comparison of the frequency distributions, highlighting similarities and differences in prompt length preferences among the various models.

The overall structure is clean and informative, with clear titles ("Prompt Length in Characters" and "Prompt Length in Words"), labeled axes, and a legend specifying the color-coding for each LLM.  The dark background enhances the visibility of the colored histogram bars, making the data easy to interpret. The choice of histograms as the visualization method allows for a quick grasp of the central tendency and spread of prompt lengths for each model.

================================================================================
================================================================================
figures/textual_search_2_Physical Characteristics_top_9_matches.jpg:
====================================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image presents a grid of 60 face images, meticulously organized and labeled.  Each face is accompanied by a text description indicating a specific physical characteristic, such as "large or chiseled jaw," "long white beard," "fashionable beard," or "wide eyes," and "overweight or chubby."  The images are diverse, showcasing a wide range of ages, ethnicities, genders, and facial hair styles. The consistent labeling ensures that all faces in a row share the same designated characteristic.

The top of the image displays the title "Image textual search using OpenCLIP features from synthetic dataset" and the subtitle "Condition: Physical Characteristics Prefix:".  This indicates that the image is a visual representation of a text-based image search experiment using the OpenCLIP model, and the search is focused on specific physical attributes of faces. The file names associated with each image are also provided, implying that these images come from a structured dataset.

The organization of the grid is systematic, allowing for easy comparison of faces with the same characteristic.  This visual representation is likely used to demonstrate the effectiveness of the OpenCLIP model in identifying and retrieving images based on textual descriptions of physical features. The variety in the faces suggests a robust dataset was used for training and testing the model.

================================================================================
================================================================================
figures/textual_search_1_ethnicity_top_8_matches.jpg:
=====================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid of 96 small photographs, arranged in 8 rows and 12 columns. Each photograph shows a headshot of a person, seemingly aiming for diversity in ethnicity and gender.  The photos are neatly organized, with a consistent background and framing for each individual.  The lighting is generally even and professional, suggesting the images are likely from a curated dataset or a professional photoshoot.


Above each set of three images within each row, there's a label indicating the ethnicity classification the images are meant to represent (e.g., Caucasian, African, Asian, Hispanic, Middle Eastern, Scandinavian, Native American).  Each individual photo within the grid also includes a file name and a label indicating its position within the set of images for that ethnicity (e.g., "Match 1 for: Caucasian"). This organization strongly suggests the image is a sample from a larger dataset used for facial recognition, bias detection, or similar machine learning tasks.


The overall impression is one of a carefully constructed dataset designed to represent a broad range of human appearances, likely for use in algorithmic training or testing. The uniformity and labeling highlight the systematic nature of the image collection and its intended application.

================================================================================
================================================================================
figures/FLUX1_schnell_images_with_prompts.jpg:
==============================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a diptych, meaning it's composed of two distinct photographs side-by-side.  Each photograph features a different individual and has accompanying text describing the image's details.  The top of each section displays metadata including the model's name, "FLUX1_schnell," and the filename of the image.

The left photograph is a medium shot of a young, serious-looking Futunan teenage girl. She's wearing sunglasses, a purple t-shirt, and has her hair in a bun. The lighting is described as "beauty dish lighting," suggesting a professional studio setup aiming for flattering illumination. The background is dark and unfocused, drawing attention to the subject. The text below the image details her age, ethnicity, and attire, emphasizing the serious expression in her pose.

The right photograph shows a high-angle shot of a 35-year-old Libyan man.  He has bright blue hair styled in a mohawk, steampunk goggles, and a patterned scarf. His expression is serene and contemplative as he looks upward. The man's appearance and attire suggest a stylized or artistic look. The descriptive text notes his age, ethnicity, eye and hair color, clothing, and the lighting conditions, which appear to be natural light. The setting is described as being by a lake.

================================================================================
================================================================================
figures/textual_search_1_eye_color_top_8_matches.jpg:
=====================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a large grid, consisting of 8 columns and 8 rows, containing 64 smaller images. Each smaller image is a close-up portrait of a person's face, focusing primarily on their eyes. 


The images are organized into sections based on eye color: blue, green, brown, yellow, hazel, and red.  Within each eye color section, there are eight images labeled "Match 1," "Match 2," etc., up to "Match 8." This suggests the images are part of a dataset or experiment designed to test the ability of a system (likely an AI) to match faces based on eye color.  Each image caption includes a file name, further supporting this hypothesis.


The lighting and photographic style of the portraits are relatively consistent, aiming for a neutral and even tone across all images to minimize the impact of extraneous factors on eye color recognition.  The variation in age, ethnicity, and gender of the individuals depicted aids in testing the robustness of the system being evaluated.

================================================================================
================================================================================
figures/textual_search_2_bad things_top_10_matches.jpg:
=======================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image displays a large grid of 10x10 = 100 small images. Each small image shows a different person or scene, and is accompanied by text giving a file name and a label.  The labels describe the content of the image, such as "blurred," "statue," "two people," "back of head," "hand covering face," "cat," "dog," or "animal."  The filenames appear to indicate the source of the image (e.g., SDXL, FLUX1). The images are diverse, showing a wide range of ages, ethnicities, expressions, and settings. Some images are portraits, some are full body shots and some show statues.  The overall lighting and image quality vary, reflecting a potentially diverse collection of sources.

The top of the image includes a title stating that it is a demonstration of image textual search using OpenCLIP features.  It specifies that the features are from a synthetic dataset and that the condition for image selection was "bad things". This suggests the images may have been curated based on some negative attribute or undesirable characteristic.  The purpose of the image is likely to showcase the ability of the OpenCLIP model to correctly identify and label images, even those with various qualities and potentially problematic aspects.

================================================================================
================================================================================
figures/textual_search_1_age_female_top_8_matches.jpg:
======================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a large grid composed of numerous smaller images, each showing a female face at different ages and ethnicities. The grid is organized into eight columns and eight rows, resulting in a total of 64 individual images. Each smaller image is accompanied by text that indicates the age group and the image source.

The age groups represented include baby girls, toddler girls, child girls, teenage girls, adult females, middle-aged adult females, and elderly females. Within each age group, there's a good variety of ethnic backgrounds and facial expressions, showcasing the diversity of the dataset.  The images are high-quality portraits, with a focus on the individual's face, and the lighting and background vary slightly from image to image.

The textual information under each image provides a consistent format, specifying "Match X for: [age group]" followed by the file name and path.  This suggests the image grid is a visual representation of a dataset used for matching purposes, likely in a facial recognition or image analysis context.  The consistent structure and clear labeling make it easy to understand the organization and purpose of the image collection.

================================================================================
================================================================================
figures/DALLE3_images_with_prompts.jpg:
=======================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a diptych, presenting two separate generated images side-by-side, each accompanied by a descriptive text prompt used to create it using the DALLE3 model.  The left panel features a close-up of a young, light-skinned woman with striking makeup that resembles scales, wide eyes, and a surprised expression. Her hair is styled in dark brown twist braids, and she wears a high-visibility vest and beanie. The background is a dramatic, dark and textured scene. The prompt requests an ultra-high definition image with impressive color and composition, emphasizing the woman's features and the dramatic lighting.

The right panel displays a medium close-up of a 37-year-old man with a bewildered expression, brunette undercut hairstyle, and a choker. His head is positioned directly facing the camera. The background includes a historical-looking building, creating a contrast between the modern man and the historical setting. The prompt calls for a serene, bluish hue evocative of the hour after sunset, emphasizing the man's features and the historical context of the background.  The overall structure is clean and organized, clearly displaying the generated images and their corresponding prompts.

================================================================================
================================================================================
figures/FLUX1_pro_images_with_prompts.jpg:
==========================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a diptych, meaning it's composed of two distinct photographs placed side-by-side.  Each photograph features a close-up portrait of a person, one a young Black girl and the other an older Asian woman. 


The left portrait shows a teenage girl with dark skin, dark locs, and striking luminous yellow eyes.  Her expression is serious and somewhat resigned. The background is blurred, focusing attention on her face. The caption describes the image's technical aspects, noting the camera, lens, and artistic choices (e.g., golden ratio composition).


The right portrait shows an older woman with teal-colored hair, her head tilted slightly back. She has a pleased expression and sleepy, yellow eyes. She's wearing a purple blazer. The background is again slightly blurred, emphasizing the subject. The accompanying text describes her as a Minangkabau wife and details the photo's composition and mood.

The overall effect of the diptych is to present a contrast between youth and age, different ethnicities, and varied expressions, while maintaining a visual consistency in terms of photographic style and subject focus.  The captions provide technical and contextual information about each photograph.

================================================================================
================================================================================
figures/textual_search_1_accessories_top_8_matches.jpg:
=======================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a large grid composed of many smaller square images, each featuring a different person's headshot.  The headshots are diverse, showcasing a wide range of ages, ethnicities, and genders. Each person is presented against a variety of simple backgrounds, some solid colors, and others slightly blurred to focus attention on the individual.  The lighting in each headshot varies but is generally well-lit and flattering.

Above each set of eight images, a label indicates what accessory (earrings, necklace, bandana, hat, tie, scarf, headphones, or sunglasses) that set of headshots is associated with. Each individual image within a set is labeled with the filename and a "Match #" to indicate its position within the set. This suggests the image is part of a data set for a machine learning project, likely focused on object detection or accessory classification.  The consistent formatting and labeling contribute to a structured, organized presentation of the data.

The overall impression is one of a meticulously organized collection of facial images intended for a technical purpose, likely related to computer vision or image recognition. The diversity of the subjects ensures a robust data set for training an algorithm to identify the specified accessories.

================================================================================
================================================================================
extract_pretrained_features.py:
===============================
import os
import glob
import time
import shutil
import pickle
import timm
import clip
import open_clip
import sklearn
import torch
import numpy as np
from tqdm import tqdm
from PIL import Image
import open_clip
from torchvision import transforms as pth_transforms
from timm.data import resolve_data_config
from timm.data.transforms_factory import create_transform

import warnings
warnings.simplefilter("ignore", sklearn.exceptions.DataConversionWarning)

#%% helper functions

def load_timm_model(model_name='convnext_xlarge_in22k', device='cpu'):

    pretrained_model = timm.create_model(model_name, pretrained=True, num_classes=0).eval().to(device)
    model_config_dict = resolve_data_config({}, model=pretrained_model)
    model_preprocess = create_transform(**model_config_dict)

    return pretrained_model, model_preprocess

def load_dino_model(model_name='dino_vitb8', device='cpu'):

    model_preprocess = pth_transforms.Compose([
            pth_transforms.Resize(256, interpolation=3),
            pth_transforms.CenterCrop(224),
            pth_transforms.ToTensor(),
            pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
        ])

    pretrained_model = torch.hub.load('facebookresearch/dino:main', model_name).to(device)

    return pretrained_model, model_preprocess


def load_openclip_model(model_name, device="cpu"):
    if model_name == "OpenCLIP_ViT-H-14-378-quickgelu":
        model, _, preprocess = open_clip.create_model_and_transforms('ViT-H-14-378-quickgelu', pretrained='dfn5b')
    elif model_name == "OpenCLIP_ViT-bigG-14-CLIPA-336":
        model, _, preprocess = open_clip.create_model_and_transforms('ViT-bigG-14-CLIPA-336', pretrained='datacomp1b')
    elif model_name == "OpenCLIP_ViT-SO400M-14-SigLIP-384":
        model, _, preprocess = open_clip.create_model_and_transforms('ViT-SO400M-14-SigLIP', pretrained='webli')
    elif model_name == "OpenCLIP_ViT-G-14":
        model, _, preprocess = open_clip.create_model_and_transforms('ViT-bigG-14', pretrained='laion2b_s39b_b160k')
    elif model_name == "OpenCLIP_ConvNext-XXLarge":
        model, _, preprocess = open_clip.create_model_and_transforms('convnext_xxlarge', pretrained='laion2b_s34b_b82k_augreg_soup')
    elif model_name == "OpenCLIP_ViT-H-14":
        model, _, preprocess = open_clip.create_model_and_transforms('ViT-H-14', pretrained='laion2b_s32b_b79k')
    else:
        model, _, preprocess = open_clip.create_model_and_transforms(model_name)
    
    model = model.to(device)
    model.eval()
    
    return model, preprocess

def extract_pretrained_features(base_image_folder, model_to_use='CLIP_ViTL_14@336'):

    images_folder = os.path.join(base_image_folder, 'images')
    transfer_images = False

    # First, determine if we need to transfer images and which images to transfer if we do
    if not os.path.exists(images_folder):
        base_image_files = []
        for image_file_ending in ['*.jpg', '*.png']:
            base_image_files.extend(glob.glob(os.path.join(base_image_folder, image_file_ending)))
        if base_image_files:
            transfer_images = True

    # Now, handle the images based on whether we need to transfer
    if transfer_images:
        os.makedirs(images_folder, exist_ok=True)
        for src_image_filename in base_image_files:
            shutil.move(src_image_filename, images_folder)
        all_image_filenames = [os.path.join(images_folder, os.path.basename(f)) for f in base_image_files]
        print('Images were transferred to the images folder.')
    else:
        if os.path.exists(images_folder):
            all_image_filenames = glob.glob(os.path.join(images_folder, '*.*'))
            print('Images were already in the correct location.')
        else:
            print('No images found in the base folder or in an "images" subfolder.')
            return

    if len(all_image_filenames) == 0:
        print('No images found to process.')
        return

    # Create features folder if it doesn't exist
    features_folder = os.path.join(base_image_folder, 'pretrained_features')
    os.makedirs(features_folder, exist_ok=True)

    # load requested model
    device = "cuda" if torch.cuda.is_available() else "cpu"

    print(f'loading model "{model_to_use}"...')

    if   model_to_use == 'CLIP_ViTL_14@336':
        pretrained_model, model_preprocess = clip.load("ViT-L/14@336px", device=device)
    elif model_to_use == 'CLIP_ViTL_14':
        pretrained_model, model_preprocess = clip.load("ViT-L/14", device=device)
    elif model_to_use == 'CLIP_ViTB_16':
        pretrained_model, model_preprocess = clip.load("ViT-B/16", device=device)
    elif model_to_use == 'CLIP_ViTB_32':
        pretrained_model, model_preprocess = clip.load("ViT-B/32", device=device)
    elif model_to_use == 'CLIP_ResNet50x64':
        pretrained_model, model_preprocess = clip.load("RN50x64", device=device)
    elif model_to_use == 'CLIP_ResNet50x16':
        pretrained_model, model_preprocess = clip.load("RN50x16", device=device)
    elif model_to_use == 'CLIP_ResNet50x4':
        pretrained_model, model_preprocess = clip.load("RN50x4", device=device)
    elif model_to_use == 'CLIP_ResNet50x1':
        pretrained_model, model_preprocess = clip.load("RN50", device=device)
    elif model_to_use == 'CLIP_ResNet101':
        pretrained_model, model_preprocess = clip.load("RN101", device=device)

    elif model_to_use == 'DINO_ResNet50':
        pretrained_model, model_preprocess = load_dino_model("dino_resnet50", device=device)
    elif model_to_use == 'DINO_ViTS_8':
        pretrained_model, model_preprocess = load_dino_model("dino_vits8", device=device)
    elif model_to_use == 'DINO_ViTB_8':
        pretrained_model, model_preprocess = load_dino_model("dino_vitb8", device=device)

    elif model_to_use == 'ConvNext_XL_Imagenet21k':
        pretrained_model, model_preprocess = load_timm_model(model_name='convnext_xlarge_in22k', device=device)
    elif model_to_use == 'ConvNext_XL_384_Imagenet21k_ft_1k':
        pretrained_model, model_preprocess = load_timm_model(model_name='convnext_xlarge_384_in22ft1k', device=device)
    elif model_to_use == 'ConvNext_L_Imagenet21k':
        pretrained_model, model_preprocess = load_timm_model(model_name='convnext_large_in22k', device=device)
    elif model_to_use == 'ConvNext_L_384_Imagenet21k_ft_1k':
        pretrained_model, model_preprocess = load_timm_model(model_name='convnext_large_384_in22ft1k', device=device)

    elif model_to_use == 'EffNet_L2_NS_475':
        pretrained_model, model_preprocess = load_timm_model(model_name='tf_efficientnet_l2_ns_475', device=device)
    elif model_to_use == 'EffNet_B7_NS_600':
        pretrained_model, model_preprocess = load_timm_model(model_name='tf_efficientnet_b7_ns', device=device)
    elif model_to_use == 'EffNetV2_L_480_Imagenet21k_ft_1k':
        pretrained_model, model_preprocess = load_timm_model(model_name='tf_efficientnetv2_l_in21ft1k', device=device)
    elif model_to_use == 'EffNetV2_S_384_Imagenet21k_ft_1k':
        pretrained_model, model_preprocess = load_timm_model(model_name='tf_efficientnetv2_s_in21ft1k', device=device)

    elif model_to_use == 'BEiT_L_16_512':
        pretrained_model, model_preprocess = load_timm_model(model_name='beit_large_patch16_512', device=device)
    elif model_to_use == 'BEiT_L_16_384':
        pretrained_model, model_preprocess = load_timm_model(model_name='beit_large_patch16_384', device=device)
    elif model_to_use == 'BEiT_L_16_224':
        pretrained_model, model_preprocess = load_timm_model(model_name='beit_large_patch16_224', device=device)

    elif model_to_use == 'DeiT3_L_16_384_Imagenet21k_ft_1k':
        pretrained_model, model_preprocess = load_timm_model(model_name='deit3_large_patch16_384_in21ft1k', device=device)
    elif model_to_use == 'DeiT3_H_14_224_Imagenet21k_ft_1k':
        pretrained_model, model_preprocess = load_timm_model(model_name='deit3_huge_patch14_224_in21ft1k', device=device)
    elif model_to_use == 'DeiT3_L_16_224_Imagenet21k_ft_1k':
        pretrained_model, model_preprocess = load_timm_model(model_name='deit3_large_patch16_224_in21ft1k', device=device)
    
    elif model_to_use == 'OpenCLIP_ViT-bigG-14-CLIPA-336':
        pretrained_model, model_preprocess = load_openclip_model("OpenCLIP_ViT-bigG-14-CLIPA-336", device=device)
    elif model_to_use == 'OpenCLIP_ViT-H-14-378-quickgelu':
        pretrained_model, model_preprocess = load_openclip_model("OpenCLIP_ViT-H-14-378-quickgelu", device=device)
    elif model_to_use == 'OpenCLIP_ViT-SO400M-14-SigLIP-384':
        pretrained_model, model_preprocess = load_openclip_model("OpenCLIP_ViT-SO400M-14-SigLIP-384", device=device)
    elif model_to_use == 'OpenCLIP_ViT-G-14':
        pretrained_model, model_preprocess = load_openclip_model("OpenCLIP_ViT-G-14", device=device)
    elif model_to_use == 'OpenCLIP_ConvNext-XXLarge':
        pretrained_model, model_preprocess = load_openclip_model("OpenCLIP_ConvNext-XXLarge", device=device)
    elif model_to_use == 'OpenCLIP_ViT-H-14':
        pretrained_model, model_preprocess = load_openclip_model("OpenCLIP_ViT-H-14", device=device)
    else:
        print('unrecognized modelname, not calculated any features!')
        return

    print(f'"{model_to_use}" model loaded')
    print(f'Calculating {len(all_image_filenames)} features of model "{model_to_use}"...')

    start_time = time.time()
    # Go over all images and append features to features dict
    for curr_image_filename in tqdm(all_image_filenames, desc=f'Extracting "{model_to_use}" features', unit="image"):
        curr_sample_name = os.path.splitext(os.path.basename(curr_image_filename))[0]
        curr_features_dict_filename = os.path.join(features_folder, curr_sample_name + '.pickle')

        # Check if features_dict file exists, if it doesn't, create one
        if os.path.isfile(curr_features_dict_filename):
            with open(curr_features_dict_filename, "rb") as f:
                curr_features_dict = pickle.load(f)
        else:
            curr_features_dict = {}

        # If the requested features were already calculated for this sample, skip it
        if model_to_use in curr_features_dict.keys():
            continue

        # Extract the features
        curr_image_PIL = Image.open(curr_image_filename).convert("RGB")

        with torch.no_grad():
            if 'CLIP' in model_to_use:
                curr_pretrained_features = pretrained_model.encode_image(model_preprocess(curr_image_PIL).unsqueeze(0).to(device))
            elif 'OpenCLIP' in model_to_use:
                curr_pretrained_features = pretrained_model.encode_image(model_preprocess(curr_image_PIL).unsqueeze(0).to(device))
            elif 'DINO' in model_to_use:
                curr_pretrained_features = pretrained_model(model_preprocess(curr_image_PIL).unsqueeze(0).to(device))
            elif 'ConvNext' in model_to_use:
                curr_pretrained_features = pretrained_model(model_preprocess(curr_image_PIL).unsqueeze(0).to(device))
            elif 'EffNet' in model_to_use:
                curr_pretrained_features = pretrained_model(model_preprocess(curr_image_PIL).unsqueeze(0).to(device))
            elif 'BEiT' in model_to_use:
                curr_pretrained_features = pretrained_model(model_preprocess(curr_image_PIL).unsqueeze(0).to(device))
            elif 'DeiT' in model_to_use:
                curr_pretrained_features = pretrained_model(model_preprocess(curr_image_PIL).unsqueeze(0).to(device))

        curr_features_dict[model_to_use] = curr_pretrained_features.detach().cpu().numpy()

        # Save the dictionary
        with open(curr_features_dict_filename, "wb") as f:
            pickle.dump(curr_features_dict, f)

    total_duration_min = (time.time() - start_time) / 60
    print(f'Extracted "{model_to_use}" features from {len(all_image_filenames)} images. Total time: {total_duration_min:.2f} minutes')

    return

def collect_pretrained_features_from_folder(base_image_folder, model_name, normalize_features=True, ignore_DALLE=True):
    features_folder = os.path.join(base_image_folder, 'pretrained_features')
    all_feature_dict_filenames = glob.glob(os.path.join(features_folder, '*.pickle'))
    all_image_filenames = glob.glob(os.path.join(base_image_folder, 'images', '*.*'))

    if ignore_DALLE:
        all_feature_dict_filenames = [x for x in all_feature_dict_filenames if 'DALLE3' not in x]
        all_image_filenames = [x for x in all_image_filenames if 'DALLE3' not in x]

    features_list = []
    image_filename_map = {}

    for curr_image_filename in all_image_filenames:
        curr_sample_name = os.path.splitext(os.path.basename(curr_image_filename))[0]
        curr_features_dict_filename = os.path.join(features_folder, f"{curr_sample_name}.pickle")
        
        if not os.path.exists(curr_features_dict_filename):
            continue  # Skip images without corresponding feature dict
        
        with open(curr_features_dict_filename, "rb") as f:
            curr_features_dict = pickle.load(f)
        
        if model_name not in curr_features_dict:
            continue
        
        features_list.append(curr_features_dict[model_name])
        image_filename_map[len(features_list)-1] = curr_image_filename

        if len(features_list) % 5000 == 0:
            print(f"Processed {len(features_list)} images...")

    if not features_list:
        raise ValueError("No features were collected. Please check your directories and files.")

    pretrained_image_features = np.vstack(features_list)

    # Normalize features if requested
    if normalize_features:
        pretrained_image_features /= np.linalg.norm(pretrained_image_features, axis=1)[:, np.newaxis]

    return pretrained_image_features, image_filename_map

def extract_and_collect_pretrained_features(images_base_folder, models_to_use=['CLIP_ViTL_14@336','CLIP_ResNet50x64'], nromalize_features=True):
    # this function will extract the features of all models in "models_to_use", collect the  and concatenate them

    # extracting features
    for model_to_use in models_to_use:
        extract_pretrained_features(images_base_folder, model_to_use=model_to_use)

    # collecting features
    features_list = []
    image_filename_map_list = []
    for requested_features_model in models_to_use:
        image_features, image_filename_map = collect_pretrained_features_from_folder(images_base_folder, requested_features_model, nromalize_features=nromalize_features)
        features_list.append(image_features)
        image_filename_map_list.append(image_filename_map)

    # make sure the maps are identical
    try:
        for k in range(len(image_filename_map_list) - 1):
            for key in image_filename_map_list[k].keys():
                assert image_filename_map_list[k][key] == image_filename_map_list[k + 1][key]
    except:
        print('the maps are not identical. quitting')
        return

    # concatenate the features
    combined_image_features = np.concatenate(features_list, axis=1)

    return combined_image_features, image_filename_map_list[0]

def delete_near_duplicates(base_image_folder, models_to_use=['CLIP_ViTL_14@336','CLIP_ResNet50x64'], similarity_threshold=0.99, minibatch_size=10_000):
    # this function does not assume "proper" folder stucture, but will create it and calculate features if necessary

    features_folder = os.path.join(base_image_folder, 'pretrained_features')

    # collect the requested features to calculate near duplication based on
    image_features, image_filename_map = extract_and_collect_pretrained_features(base_image_folder, models_to_use=models_to_use, nromalize_features=True)
    similarity_threshold = len(models_to_use) * similarity_threshold

    total_num_samples = image_features.shape[0]
    num_batches = np.ceil(total_num_samples / minibatch_size).astype(int)

    feature_inds_to_drop = []

    end_row_ind = 0
    for batch_ind in range(num_batches):
        start_row_ind = end_row_ind
        end_row_ind = min(start_row_ind + minibatch_size, total_num_samples)
        image_feature_curr_batch = image_features[start_row_ind:end_row_ind]
        curr_minibatch_size = image_feature_curr_batch.shape[0]

        similarity_curr_batch_to_all = np.dot(image_feature_curr_batch, image_features.T).astype(np.float32)
        similarity_curr_batch_to_all[np.arange(curr_minibatch_size), np.arange(start_row_ind, end_row_ind)] = 0
        similarity_curr_batch_to_all = similarity_curr_batch_to_all > similarity_threshold

        # zero out all removals from previous batches
        if len(feature_inds_to_drop) > 0:
            similarity_curr_batch_to_all[:,np.array(feature_inds_to_drop)] = 0

        # go over the self similarity matrix rows and determine which indices should be removed
        for curr_batch_row_ind in range(curr_minibatch_size):
            if similarity_curr_batch_to_all[curr_batch_row_ind,:].sum() > 0:
                full_features_row = start_row_ind + curr_batch_row_ind
                feature_inds_to_drop.append(full_features_row)
                # zero out the column of the removed duplicate (so that it's twins won't be removed as well)
                similarity_curr_batch_to_all[:,full_features_row] = 0

    num_to_remove = len(feature_inds_to_drop)
    message_string = 'from the folder "%s" (contains %d images) \nthere will be removed %d near-duplicates (%.1f%s of images)'
    print('----------------------------------------')
    print(message_string %(base_image_folder, total_num_samples, num_to_remove, 100 * (num_to_remove / total_num_samples), '%'))
    print('----------------------------------------')

    # remove the files
    for k in feature_inds_to_drop:
        curr_image_filename = image_filename_map[k]
        curr_sample_name = curr_image_filename.split('/')[-1].split('.')[0]
        curr_features_dict_filename = os.path.join(features_folder, curr_sample_name + '.pickle')

        os.remove(curr_image_filename)
        os.remove(curr_features_dict_filename)


def find_nearest_neighbors(folder_A, folder_B, model_name, k=5):
    # Collect features from both folders
    features_A, filenames_A = collect_pretrained_features_from_folder(folder_A, model_name)
    features_B, filenames_B = collect_pretrained_features_from_folder(folder_B, model_name)

    print(f"Folder A features shape: {features_A.shape}")
    print(f"Folder B features shape: {features_B.shape}")

    nearest_neighbors = []
    
    # Iterate over each image index in folder A
    for i in tqdm(range(len(features_A)), desc="Finding nearest neighbors"):

        # Find the top k nearest neighbors in folder B
        feature_A = features_A[i:i+1]
        similarities = np.dot(feature_A, features_B.T).flatten()
        top_k_indices = np.argsort(similarities)[-k:][::-1]
        
        # Get filenames and similarities of nearest neighbors
        neighbor_filenames = [filenames_B[idx] for idx in top_k_indices]
        neighbor_similarities = [similarities[idx] for idx in top_k_indices]
        
        nearest_neighbors.append({
            'source_image': filenames_A[i],
            'neighbors': list(zip(neighbor_filenames, neighbor_similarities))
        })

    return nearest_neighbors

================================================================================
================================================================================
figures/good_model_images.jpg:
==============================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid of 30 portrait photographs, arranged in six rows and five columns. Each portrait features a different individual, exhibiting a wide range of ages, ethnicities, and expressions. The photographs are diverse in style, some appearing more candid or documentary, while others are more posed and stylized. The lighting and background also vary significantly across the images.

The images are labeled with filenames suggesting they are part of a larger dataset, categorized into three sets: "FLUX1_pro," "FLUX1_dev," "FLUX1_schnell," and "SDXL." This likely indicates different versions or stages of image processing or data collection. The filenames also include sequential numbers, implying a systematic organization within each category.

The overall impression is one of a diverse and extensive collection of human portraits, potentially used for training a machine learning model (given the filenames and the diverse nature of the subjects). The variety in age, ethnicity, expression, and photographic style suggests a deliberate effort to create a representative and robust dataset.

================================================================================
================================================================================
figures/textual_search_2_Ethnicity x Age 3_top_10_matches.jpg:
==============================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid of 72 photos of young children from various ethnic backgrounds. Each photo is square and labeled with a presumed ethnicity ("asian," "native american," "african," "persian," "south-american," "irish") and a file name. The photos are diverse, showing children in different settings and clothing, some posed and some candid. The photos are high-quality and well-lit.

The title above the grid indicates that this is a dataset used for image textual search using OpenCLIP features, focusing on ethnicity and age (3 years old).  The arrangement of the pictures suggests a systematic categorization based on ethnicity, which is confirmed by the labels under each image.  The consistent size and framing of the images create a visually uniform and organized presentation of the dataset.

The image serves as a visual representation of a machine learning dataset, demonstrating the diversity of the data used to train a model for image search based on textual descriptions. The goal is likely to test the ability of the model to accurately identify and retrieve images of children of different ethnicities based on text prompts.

================================================================================
================================================================================
figures/textual_search_2_Eye Color_top_10_matches.jpg:
======================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a grid of 100 facial portraits, each accompanied by a filename and a descriptive label indicating the eye color. The portraits are neatly arranged in a 10x10 grid, making it easy to browse the collection.  The images appear to be from a diverse range of individuals, varying in age, gender, ethnicity, and hair styles. The image quality is consistent across the images, suggesting a curated collection.

The eye color labels are consistently applied, categorizing the eyes as 'blue', 'green', 'brown', 'yellow', 'red', or 'hazel'.  This suggests the images were selected or generated based on this specific characteristic. The filenames, which begin with "SDXL," "FLUX1_dev," or "FLUX1_schnell," indicate that the images likely originate from different datasets or image generation models. This may suggest a study or comparative analysis of different image generation techniques.

The title "Image textual search using OpenCLIP features from synthetic dataset" above the grid suggests that the image grid is a result of a search query. The query itself is specified as "Condition: Eye Color, Prefix: 'person with'". This implies that the images were retrieved based on a textual query related to eye color within a larger dataset of synthetically generated images of people.  The overall structure and contents strongly suggest a research or development context related to image analysis and retrieval.

================================================================================
================================================================================
figures/textual_search_2_Ethnicity x Age 2_top_10_matches.jpg:
==============================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a grid showcasing a diverse collection of facial portraits, organized into rows and columns. Each portrait is accompanied by text indicating the presumed ethnicity ("asian," "native american," "african," "persian," "south-american," "irish") and a filename.  The filenames suggest the images originate from various datasets (FLUX1, SDXL). The overall arrangement suggests a structured dataset designed for research or analysis related to facial recognition, ethnicity classification, or similar applications.  The consistent framing and lighting of the portraits imply a controlled environment for image capture.

The top of the image includes a title stating that the image displays the results of image textual search using OpenCLIP features.  It specifies that the data is from a synthetic dataset and focuses on "typical adult" faces, further clarifying the nature and purpose of the image collection. The consistent ethnicity labels across the dataset suggest a focus on representing diverse ethnic groups, likely for training or testing a machine learning model. The high quality and consistent style of the portraits indicate a careful curation of the dataset.

================================================================================
================================================================================
figures/textual_search_2_Age_x_Ethnicity x Sex 1_top_10_matches.jpg:
====================================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a grid of 192 photographs, meticulously arranged in a 16x12 matrix. Each photograph depicts a person, predominantly Asian females, spanning a wide range of ages. The ages are clearly labeled beneath each image, progressing from "10-month-old baby" to "wrinkly 70-year-old senior."  The images showcase diverse expressions and settings, providing a rich visual representation of age progression within a specific demographic.

Each image is accompanied by a descriptive caption that includes the age, and a filename suggesting that these images are sourced from a synthetic dataset. The consistent format and clear labelling suggest a systematic approach to data organization, likely used for research or machine learning purposes.  The overall structure is highly organized and visually informative, facilitating easy comparison and analysis of facial features across different age groups.  The consistent ethnicity and gender across the images allow for focused study of age-related changes.

================================================================================
================================================================================
figures/flux_images.jpg:
========================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a grid of twelve portraits, arranged in a 4x3 layout.  Each portrait is a headshot of a different person, showcasing a diverse range of ages, ethnicities, and styles. The individuals are presented against varied backgrounds, some simple and some more detailed, with varying lighting conditions. 


The file names displayed under each portrait indicate a structured naming convention, suggesting these images may be part of a larger dataset or project. The filenames are consistent and include prefixes like "FLUX1_pro," "FLUX1_dev," and "FLUX1_schnell," potentially indicating different stages or sources of the images.  Each filename also contains a unique numerical identifier. The overall effect is a visually diverse collection of portraits, organized systematically.

================================================================================
================================================================================
figures/all_model_images.jpg:
=============================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a grid of 72 individual portrait photographs, arranged in a 6x12 matrix. Each portrait shows a different person, exhibiting a wide range of ages, ethnicities, and expressions.  The individuals are presented in various settings, some with plain backgrounds, others with subtle environmental details.  The lighting and photographic styles also vary across the portraits, creating a diverse collection of photographic approaches.  The overall effect is a visually rich representation of human diversity.

Many of the portraits seem professionally taken, with high-quality lighting and composition.  The subjects are clearly the focal point of each image.  There's a blend of posed and candid shots, suggesting different photographic approaches and capturing a range of moods and personalities.  The age range of the subjects is substantial, from young children to elderly individuals, further highlighting the diversity presented.

Above each image is a file name, seemingly indicating the source or dataset from which these images originate.  The file names suggest different projects or image datasets, possibly indicating that the portraits were collected from multiple sources or created using various methods.  The consistent file naming convention, however, suggests organization and cataloging of the images.

================================================================================
================================================================================
create_face_dataset.py:
=======================
#%% Imports

import os
import re
import io
import time
import json
import base64
import random
import requests
import numpy as np
import pandas as pd
from PIL import Image
from tqdm import tqdm
from dotenv import load_dotenv
import asyncio
from tqdm.asyncio import tqdm as async_tqdm
import fal_client
from openai import OpenAI
from stability_sdk import client as sd_client
import stability_sdk.interfaces.gooseai.generation.generation_pb2 as generation

from face_prompt_utils import generate_face_prompt

#%% API Key Configurations

STABILITY_API_KEY = 'sk-abcdefghigklmnopqrstuvwxyz1234567890abcdefghigkl'
OPENAI_API_KEY = 'sk-abcd1234efgh5678ijkl9012mnop3456qrst7890uvwx1234yzab5678cdef9012ghij3456klmn7890opqr1234stuv'
FAL_API_KEY = 'abcdefgh-ijkl-mnop-qrst-uvwxyz123456:a1234567890abcdefghijklmnopqrabc'

def save_env_file():
    """Create a .env file with API keys if it doesn't exist."""

    env_file = '.env'
    if not os.path.exists(env_file):
        print("Creating .env file...")
        with open(env_file, 'w') as f:
            f.write(f"STABILITY_API_KEY={STABILITY_API_KEY}\n")
            f.write(f"OPENAI_API_KEY={OPENAI_API_KEY}\n")
            f.write(f"FAL_KEY={FAL_API_KEY}\n")
        print(f".env file created at {os.path.abspath(env_file)}")
        print("Please edit the .env file with your actual API keys before running the script again.")
        exit()

def setup_api_keys():
    # Load the .env file
    load_dotenv()
    
    # Check if all required keys are present
    required_keys = ['STABILITY_API_KEY', 'OPENAI_API_KEY', 'FAL_KEY']
    missing_keys = [key for key in required_keys if not os.getenv(key)]
    
    if missing_keys:
        print(f"Error: The following API keys are missing in the .env file: {', '.join(missing_keys)}")
        print("Please add them to the .env file and run the script again.")
    else:
        print("API keys loaded successfully.")

# Call the setup function at the beginning of the script
save_env_file()
setup_api_keys()

# print to the screen all the API keys that were loaded
key_name_list = ['STABILITY_API_KEY', 'OPENAI_API_KEY', 'FAL_KEY']
for key_name in key_name_list:
    print(f'{key_name} = {os.getenv(key_name)}')

#%% API clients

openai_client = OpenAI(api_key = os.getenv('OPENAI_API_KEY'))

#%% Constants

SDXL_STYLES = [
    "3d-model", "analog-film", "anime", "cinematic", "comic-book", "digital-art",
    "enhance", "fantasy-art", "isometric", "line-art", "low-poly", "modeling-compound",
    "neon-punk", "origami", "photographic", "pixel-art", "tile-texture"
]

SDXL_STYLES = ["analog-film", "cinematic", "photographic", "enhance"]
DALLE3_IMAGE_SIZES = ["1024x1024", "1024x1792", "1792x1024"]
DALLE3_STYLES = ["vivid", "natural"]
DALLE3_QUALITIES = ["standard", "hd"]
FLUX_IMAGE_SIZES = ["square_hd", "square", "portrait_4_3", "portrait_16_9", "landscape_4_3", "landscape_16_9"]

FLUX_API_MODEL_NAME_DICT = {
    'FLUX1_pro': 'fal-ai/flux-pro',
    'FLUX1_dev': 'fal-ai/flux/dev',
    'FLUX1_schnell': 'fal-ai/flux/schnell'
}

#%% Helper functions

def generate_image_SDXL(prompt, engine_id, cfg_scale, steps, seed, style_preset):
    stability_api = sd_client.StabilityInference(key=os.getenv('STABILITY_API_KEY'), engine=engine_id)

    params = {
        "prompt": prompt,
        "cfg_scale": cfg_scale,
        "steps": steps,
        "seed": seed,
        "style_preset": style_preset
    }

    response = stability_api.generate(**params)

    for resp in response:
        for artifact in resp.artifacts:
            if artifact.type == generation.ARTIFACT_IMAGE:
                return Image.open(io.BytesIO(artifact.binary))

    return None

def generate_image_DALLE3(prompt, size='1024x1024', quality='standard', style='vivid', response_format='url'):
    
    response = openai_client.images.generate(
        model="dall-e-3",
        prompt=prompt,
        size=size,
        quality=quality,
        style=style,
        response_format=response_format
    )
    
    if response_format == "b64_json":
        image_data = base64.b64decode(response.data[0].b64_json)
        image_PIL = Image.open(io.BytesIO(image_data))
    elif response_format == "url":
        image_url = response.data[0].url
        image_data = io.BytesIO(requests.get(image_url).content)
        image_PIL = Image.open(image_data)

    revised_prompt = response.data[0].revised_prompt

    return image_PIL, revised_prompt

def generate_image_FLUX(prompt, api_model_name, seed, num_inference_steps, image_size='square_hd', guidance_scale=3.5):
    
    if api_model_name == 'fal-ai/flux-pro':
        arguments = {
            "prompt": prompt,
            "image_size": image_size,
            "num_inference_steps": num_inference_steps,
            "seed": seed,
            "guidance_scale": guidance_scale,
            "safety_tolerance": "5",
            "sync_mode": True
        }
    elif api_model_name == 'fal-ai/flux/dev':
        arguments = {
            "prompt": prompt,
            "image_size": image_size,
            "num_inference_steps": num_inference_steps,
            "seed": seed,
            "guidance_scale": guidance_scale,
            "enable_safety_checker": False,
            "sync_mode": True
        }
    elif api_model_name == 'fal-ai/flux/schnell':
        arguments = {
            "prompt": prompt,
            "image_size": image_size,
            "num_inference_steps": num_inference_steps,
            "seed": seed,
            "enable_safety_checker": False,
            "sync_mode": True
        }

    handler = fal_client.submit(api_model_name, arguments=arguments)
    result = handler.get()

    image_url = result['images'][0]['url']
    if image_url.startswith('data:image/jpeg;base64,'):
        image_data = io.BytesIO(base64.b64decode(image_url.split(',')[1]))
    else:
        image_data = io.BytesIO(requests.get(image_url).content)

    image_PIL = Image.open(image_data)

    return image_PIL

async def generate_image_FLUX_async(prompt, api_model_name, seed, num_inference_steps=50, image_size='square_hd', guidance_scale=3.5):
    if api_model_name == 'fal-ai/flux-pro':
        arguments = {
            "prompt": prompt,
            "image_size": image_size,
            "num_inference_steps": num_inference_steps,
            "seed": seed,
            "guidance_scale": guidance_scale,
            "safety_tolerance": "5",
            "sync_mode": False
        }
    elif api_model_name == 'fal-ai/flux/dev':
        arguments = {
            "prompt": prompt,
            "image_size": image_size,
            "num_inference_steps": num_inference_steps,
            "seed": seed,
            "guidance_scale": guidance_scale,
            "enable_safety_checker": False,
            "sync_mode": False
        }
    elif api_model_name == 'fal-ai/flux/schnell':
        arguments = {
            "prompt": prompt,
            "image_size": image_size,
            "num_inference_steps": num_inference_steps,
            "seed": seed,
            "enable_safety_checker": False,
            "sync_mode": False
        }

    handler = await fal_client.submit_async(api_model_name, arguments=arguments)
    result = await handler.get()

    image_url = result['images'][0]['url']
    if image_url.startswith('data:image/jpeg;base64,'):
        image_data = io.BytesIO(base64.b64decode(image_url.split(',')[1]))
    else:
        image_data = io.BytesIO(requests.get(image_url).content)

    image_PIL = Image.open(image_data)

    return image_PIL

def generate_image_with_retry(generate_func, max_retries=2, **kwargs):
    for attempt in range(max_retries):
        try:
            return generate_func(**kwargs)
        except Exception as e:
            print(f"Error occurred: {e}")
            if attempt < max_retries - 1:
                wait_time = random.uniform(0.5, 2)
                print(f"Retrying in {wait_time:.2f} seconds...")
                time.sleep(wait_time)
            else:
                print("Max retries reached. Skipping this generation.")
                return None

def get_existing_image_count(image_folder, model_prefix):
    # Regular expression to match the number at the end of the filename
    pattern = re.compile(rf"{re.escape(model_prefix)}_image_(\d+)\.jpg")
    
    max_number = 0
    for filename in os.listdir(image_folder):
        match = pattern.match(filename)
        if match:
            number = int(match.group(1))
            max_number = max(max_number, number)
    
    return max_number

def create_dataset_SDXL(num_samples, image_folder, engine_id, steps, jpeg_quality=90):
    metadata = []
    total_time = 0
    start_index = get_existing_image_count(image_folder, "SDXL")
    
    with tqdm(total=num_samples, desc="Generating SDXL images") as pbar:
        for i in range(num_samples):
            prompt = get_random_prompt()
            style_preset = random.choice(SDXL_STYLES)
            seed = random.randint(0, 2**32 - 1)
            cfg_scale = random.randint(5, 8)
            
            start_time = time.time()
            image = generate_image_with_retry(
                generate_image_SDXL,
                prompt=prompt,
                engine_id=engine_id,
                cfg_scale=cfg_scale,
                steps=steps,
                seed=seed,
                style_preset=style_preset
            )
            end_time = time.time()
            
            if image:
                image_filename = f"SDXL_image_{start_index + i + 1:07d}.jpg"
                image_path = os.path.join(image_folder, image_filename)
                
                image.save(image_path, "JPEG", quality=jpeg_quality)
                
                configs = {
                    "engine_id": engine_id,
                    "cfg_scale": cfg_scale,
                    "steps": steps,
                    "seed": seed,
                    "style_preset": style_preset
                }
                
                metadata.append({
                    "image_filename": image_filename,
                    "model_used": "SDXL",
                    "text_prompt": prompt,
                    "configs": json.dumps(configs),
                })
                
                total_time += (end_time - start_time)
                pbar.update(1)
    
    print(f"SDXL: Generated {num_samples} images in {total_time/60:.2f} minutes (avg: {total_time/num_samples:.2f} seconds per image)")
    return pd.DataFrame(metadata)

def create_dataset_DALLE3(num_samples, image_folder, size, quality, jpeg_quality=90):
    metadata = []
    total_time = 0
    start_index = get_existing_image_count(image_folder, "DALLE3")
    
    with tqdm(total=num_samples, desc="Generating DALL-E 3 images") as pbar:
        for i in range(num_samples):
            prompt = get_random_prompt()
            style = random.choice(DALLE3_STYLES)
            
            start_time = time.time()
            result = generate_image_with_retry(
                generate_image_DALLE3,
                prompt=prompt,
                size=size,
                quality=quality,
                style=style
            )
            end_time = time.time()
            
            if result:
                image, revised_prompt = result
                image_filename = f"DALLE3_image_{start_index + i + 1:07d}.jpg"
                image_path = os.path.join(image_folder, image_filename)
                
                image.save(image_path, "JPEG", quality=jpeg_quality)
                
                configs = {
                    "size": size,
                    "quality": quality,
                    "style": style,
                    "orig_prompt": prompt
                }
                
                metadata.append({
                    "image_filename": image_filename,
                    "model_used": "DALLE3",
                    "text_prompt": revised_prompt,
                    "configs": json.dumps(configs),
                })
                
                total_time += (end_time - start_time)
                pbar.update(1)
    
    print(f"DALL-E 3: Generated {len(metadata)} images in {total_time/60:.2f} minutes (avg: {total_time/len(metadata):.2f} seconds per image)")
    return pd.DataFrame(metadata)

def create_dataset_FLUX(num_samples, flux_model, image_folder, num_inference_steps, image_size, jpeg_quality=90):
    metadata = []
    total_time = 0
    start_index = get_existing_image_count(image_folder, flux_model)
    
    flux_api_model_name = FLUX_API_MODEL_NAME_DICT[flux_model]

    with tqdm(total=num_samples, desc=f"Generating {flux_model} images") as pbar:
        for i in range(num_samples):
            prompt = get_random_prompt()
            seed = random.randint(0, 2**32 - 1)
            guidance_scale = random.uniform(2.5, 4.0) if random.random() < 0.5 else 3.5
            
            start_time = time.time()
            image = generate_image_with_retry(
                generate_image_FLUX,
                prompt=prompt,
                api_model_name=flux_api_model_name,
                seed=seed,
                num_inference_steps=num_inference_steps,
                image_size=image_size,
                guidance_scale=guidance_scale
            )
            end_time = time.time()
            
            if image:
                image_filename = f"{flux_model}_image_{start_index + i + 1:07d}.jpg"
                image_path = os.path.join(image_folder, image_filename)
                
                image.save(image_path, "JPEG", quality=jpeg_quality)
                
                if flux_model == 'FLUX1_pro':
                    configs = {
                        "image_size": image_size,
                        "num_inference_steps": num_inference_steps,
                        "seed": seed,
                        "guidance_scale": guidance_scale,
                        "safety_tolerance": "5",
                    }
                elif flux_model == 'FLUX1_dev':
                    configs = {
                        "image_size": image_size,
                        "num_inference_steps": num_inference_steps,
                        "seed": seed,
                        "guidance_scale": guidance_scale,
                        "enable_safety_checker": False,
                    }
                elif flux_model == 'FLUX1_schnell':
                    configs = {
                        "image_size": image_size,
                        "num_inference_steps": num_inference_steps,
                        "seed": seed,
                        "enable_safety_checker": False,
                    }

                metadata.append({
                    "image_filename": image_filename,
                    "model_used": flux_model,
                    "text_prompt": prompt,
                    "configs": json.dumps(configs),
                })
                
                total_time += (end_time - start_time)
                pbar.update(1)
    
    print(f"{flux_model}: Generated {len(metadata)} images in {total_time/60:.2f} minutes (avg: {total_time/len(metadata):.2f} seconds per image)")
    return pd.DataFrame(metadata)

async def create_dataset_FLUX_parallel(num_samples, flux_model, image_folder, num_inference_steps, image_size, jpeg_quality=90, max_concurrent_calls=5):
    dataset_start_time = time.time()

    metadata = []
    start_index = get_existing_image_count(image_folder, flux_model)    
    flux_api_model_name = FLUX_API_MODEL_NAME_DICT[flux_model]
    semaphore = asyncio.Semaphore(max_concurrent_calls)

    async def process_single_image(i):
        async with semaphore:
            prompt = get_random_prompt()
            seed = random.randint(0, 2**32 - 1)
            guidance_scale = random.uniform(2.5, 4.0) if random.random() < 0.5 else 3.5
            
            sample_start_time = time.time()
            try:
                image = await generate_image_FLUX_async(
                    prompt=prompt,
                    api_model_name=flux_api_model_name,
                    seed=seed,
                    num_inference_steps=num_inference_steps,
                    image_size=image_size,
                    guidance_scale=guidance_scale
                )
            except Exception as e:
                print(f"Error generating image for {flux_model}: {e}")
                return None
            sample_end_time = time.time()
            
            if image:
                image_filename = f"{flux_model}_image_{start_index + i + 1:07d}.jpg"
                image_path = os.path.join(image_folder, image_filename)
                
                image.save(image_path, "JPEG", quality=jpeg_quality)
                
                configs = {
                    "image_size": image_size,
                    "num_inference_steps": num_inference_steps,
                    "seed": seed,
                    "guidance_scale": guidance_scale,
                }
                if flux_model == 'FLUX1_pro':
                    configs["safety_tolerance"] = "5"
                elif flux_model in ['FLUX1_dev', 'FLUX1_schnell']:
                    configs["enable_safety_checker"] = False

                sample_durations_sec = sample_end_time - sample_start_time
                return {
                    "image_filename": image_filename,
                    "model_used": flux_model,
                    "text_prompt": prompt,
                    "configs": json.dumps(configs),
                }
            return None

    tasks = [process_single_image(i) for i in range(num_samples)]    
    results = await async_tqdm.gather(*tasks, desc=f"Generating {flux_model} images")
    metadata = [result for result in results if result is not None]
    total_time = time.time() - dataset_start_time
    print(f"{flux_model}: Generated {len(metadata)} images in {total_time/60:.2f} minutes (avg: {total_time/len(metadata):.2f} seconds per image)")
    
    return pd.DataFrame(metadata)

def update_csv(new_df, csv_path):
    if os.path.exists(csv_path):
        existing_df = pd.read_csv(csv_path)
        combined_df = pd.concat([existing_df, new_df], ignore_index=True)
    else:
        combined_df = new_df
    
    combined_df.to_csv(csv_path, index=False)
    return combined_df

def get_random_prompt():
    output_prompt = generate_face_prompt()
    return output_prompt

#%%

if __name__ == "__main__":

    # Explicit configuration variables
    # output_db_folder = r"datasample_001"
    output_db_folder = r"datasample_002"
    
    os.makedirs(output_db_folder, exist_ok=True)

    call_dev_pro_async = True

    # FLUX1.dev (about 1150 images per 1 hour when async is on, costs ~$29 per 1150 images)
    flux1_dev_samples = 10
    flux1_dev_config = {
        "image_size": "square_hd",
        "num_inference_steps": 50,
        'jpeg_quality': 90
    }

    # FLUX1.pro (about 1100 images per 1 hour when async is on, costs ~$55 per 1100 images)
    flux1_pro_samples = 10
    flux1_pro_config = {
        "image_size": "square_hd",
        "num_inference_steps": 50,
        'jpeg_quality': 90
    }

    # SDXL (about 550 images per 1 hour, costs ~$2 per 550 images)
    sdxl_samples = 10
    sdxl_config = {
        "engine_id": "stable-diffusion-xl-1024-v1-0",
        "steps": 70,
        'jpeg_quality': 90
    }

    # FLUX1.schnell (about 2000 images per 1 hour, costs ~$6 per 2000 images)
    flux1_schnell_samples = 10
    flux1_schnell_config = {
        "image_size": "square_hd",
        "num_inference_steps": 12,
        'jpeg_quality': 90
    }

    # DALL-E 3 (about 233 images per 1 hour, costs ~$8.6 per 233 images)
    dalle3_samples = 10
    dalle3_config = {
        "size": "1024x1024",
        "quality": "standard",
        'jpeg_quality': 90
    }

    # Create the mixed dataset
    image_folder = os.path.join(output_db_folder, "images")
    os.makedirs(image_folder, exist_ok=True)
    csv_path = os.path.join(output_db_folder, "SFHQ_T2I_dataset.csv")
    
    print("\nStarting image generation...\n")
    
    if call_dev_pro_async:
        max_concurrent_calls = 10

        loop = asyncio.get_event_loop()

        if flux1_dev_samples > 0:
            flux1_dev_df = loop.run_until_complete(create_dataset_FLUX_parallel(
                flux1_dev_samples, 'FLUX1_dev', image_folder, max_concurrent_calls=max_concurrent_calls, **flux1_dev_config
            ))
            combined_df = update_csv(flux1_dev_df, csv_path)
            print(f"CSV updated with {len(flux1_dev_df)} FLUX1_dev images")

        if flux1_pro_samples > 0:
            flux1_pro_df = loop.run_until_complete(create_dataset_FLUX_parallel(
                flux1_pro_samples, 'FLUX1_pro', image_folder, max_concurrent_calls=max_concurrent_calls, **flux1_pro_config
            ))
            combined_df = update_csv(flux1_pro_df, csv_path)
            print(f"CSV updated with {len(flux1_pro_df)} FLUX1_pro images")

        loop.close()
    else:
        if flux1_dev_samples > 0:
            flux1_dev_df = create_dataset_FLUX(flux1_dev_samples, 'FLUX1_dev', image_folder, **flux1_dev_config)
            combined_df = update_csv(flux1_dev_df, csv_path)
            print(f"CSV updated with {len(flux1_dev_df)} FLUX1_dev images")

        if flux1_pro_samples > 0:
            flux1_pro_df = create_dataset_FLUX(flux1_pro_samples, 'FLUX1_pro', image_folder, **flux1_pro_config)
            combined_df = update_csv(flux1_pro_df, csv_path)
            print(f"CSV updated with {len(flux1_pro_df)} FLUX1_pro images")

    if flux1_schnell_samples > 0:
        flux1_schnell_df = create_dataset_FLUX(flux1_schnell_samples, 'FLUX1_schnell', image_folder, **flux1_schnell_config)
        combined_df = update_csv(flux1_schnell_df, csv_path)
        print(f"CSV updated with {len(flux1_schnell_df)} FLUX1_schnell images")

    if sdxl_samples > 0:
        sdxl_df = create_dataset_SDXL(sdxl_samples, image_folder, **sdxl_config)
        combined_df = update_csv(sdxl_df, csv_path)
        print(f"CSV updated with {len(sdxl_df)} SDXL images")
    
    if dalle3_samples > 0:
        dalle3_df = create_dataset_DALLE3(dalle3_samples, image_folder, **dalle3_config)
        combined_df = update_csv(dalle3_df, csv_path)
        print(f"CSV updated with {len(dalle3_df)} DALLE3 images")
    
    print("\nDataset creation completed!\n")
    print(f"Total images in the dataset per model:")
    for model in ["SDXL", "DALLE3", "FLUX1_pro", "FLUX1_dev", "FLUX1_schnell"]:
        count = len(combined_df[combined_df['model_used'] == model])
        print(f"- {count} {model} images")
    print(f"Combined total of images: {len(combined_df)}")
    print(f"\nMetadata saved to 'SFHQ_T2I_dataset.csv'")


#%%


================================================================================
================================================================================
figures/textual_search_2_Hats_top_10_matches.jpg:
=================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image displays a grid of 96 small images, each showing a person wearing a hat. The grid is organized into 8 rows and 12 columns.  Above the grid is a title: "Image textual search using OpenCLIP features from synthetic dataset Condition: Hats Prefix: "person wearing"".  This indicates that the images are part of a dataset used for testing an image recognition system's ability to identify people wearing different types of hats.

Each small image within the grid is labeled with the type of hat the person is wearing (e.g., "baseball cap", "fedora", "beanie", "top hat", "cowboy hat", "sun hat") and a filename.  The filenames suggest that the images originate from various sources, possibly including different datasets or image collections.  The hats are diverse in style and color, and the people depicted in the images show a wide range of ages, genders, and ethnicities.

The overall structure of the image is clear and organized, presenting a large sample of images for visual analysis and demonstrating the variety of hat types included in the dataset.  The labeling allows for easy identification of each image's contents and its source.

================================================================================
================================================================================
figures/textual_search_2_Jewelry_top_9_matches.jpg:
===================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image presents a grid of 60 small images, each showing a different person wearing various types of jewelry.  The images are neatly arranged in a 6x10 matrix. Each image is accompanied by a text label indicating the type of jewelry depicted ("gold chain," "pearl necklace," "earrings," "diamond," "crown") and a filename.  The filenames suggest the images are sourced from a large dataset, possibly a synthetic one.

The overall purpose of the image appears to be demonstrating the results of an image textual search using OpenCLIP features. The search condition is specified as "person with" followed by a type of jewelry, showcasing the ability of the system to retrieve relevant images based on textual descriptions.  The variety of people depicted—in terms of age, gender, ethnicity, and style—suggests a diverse and representative dataset was used for training or testing. The images are high-quality and well-lit, suggesting they are likely from a curated collection.

================================================================================
================================================================================
figures/FLUX1_dev_images_with_prompts.jpg:
==========================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a diptych, meaning it's split into two distinct sections side-by-side. Each section contains a portrait photograph with accompanying text describing the subject. 


The left side shows a close-up portrait of a teenage boy, appearing to be of Micronesian descent. He is wearing a newsboy cap, round glasses, and a mustard-yellow jacket. His expression is one of wide-eyed excitement or surprise. The background is blurred, showing a snowy landscape. The descriptive text emphasizes his enthusiastic expression, clothing details, and the lighting used in the photo, highlighting the textures of his skin and clothing.


The right side shows a portrait of a teenage girl, seemingly of Inca descent, wearing a yellow and green basketball uniform. She has a more serious, almost resigned expression. The background is blurred, showing an outdoor sports field at sunset. The descriptive text highlights her expression, the dramatic lighting which casts shadows on one side of her face, her physique, and her attire.

Both portraits are professionally lit and composed, showcasing different photographic styles to emphasize the unique features and mood of each subject. The text provides detailed descriptions intended for image cataloging or metadata purposes, going beyond basic descriptions to include details of ethnicity, age, clothing, and lighting techniques.

================================================================================
================================================================================
figures/textual_search_2_Background_top_10_matches.jpg:
=======================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid showcasing 72 images, arranged in 6 rows and 12 columns. Each image is a portrait-style photograph of a person, set against various backgrounds.  The backgrounds are categorized and labeled above each image column: "urban cityscape," "natural landscape," "stone wall background," "wooden wall background," "beach background," and "night background."

Each image is accompanied by a text label indicating its source, specifying whether it's from the SDXL or FLUX1 dataset and including a unique image identifier. The overall structure is highly organized and systematic, suggesting a data visualization or dataset exploration tool. The images feature a diverse range of people in terms of age, ethnicity, and gender, and the backgrounds are carefully selected to represent the specified categories.  The consistent formatting and clear labeling make it easy to understand the organization and purpose of the image.

================================================================================
================================================================================
figures/textual_search_2_Ethnicity_top_10_matches.jpg:
======================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid of 96 small images, each showing a different person's face. The images are organized into 8 rows and 12 columns.  Each image is labeled with a descriptive term (e.g., "asian," "native american," "african," "persian," "south-american," "irish") and a filename.  The labels suggest that the images are part of a dataset used for training or testing an image recognition model to classify people's ethnicity.

The dataset appears to be diverse, featuring individuals of various ethnic backgrounds, ages, and genders.  The images are generally high-quality portraits, with good lighting and focus. The individuals are presented in a variety of attire and settings, adding to the complexity of the dataset. The consistent labeling and file naming convention suggests a structured and organized approach to data collection and management.

The title "Image textual search using OpenCLIP features from synthetic dataset Condition: Ethnicity" indicates that the images were likely generated synthetically and are being used to evaluate the performance of an OpenCLIP model in a text-based image search context focused on ethnicity.  The presence of a "Prefix" indicating an empty string suggests further experimentation or analysis of the model's ability to classify ethnicity based solely on visual features.

================================================================================
================================================================================
figures/textual_search_2_Hair Style_top_9_matches.jpg:
======================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image presents a grid of 60 photographs, each showing a person with a different hairstyle.  The images are neatly arranged in six rows of ten columns.  Each image is labeled with a descriptive text indicating the hairstyle ("straight hair," "curly hair," "high top hairstyle," "bob-cut hairstyle," "afro hairstyle") and a filename.  The filenames appear to be a structured format indicating the source dataset and a unique image identifier.

The hairstyles showcased are diverse, representing a wide range of textures, lengths, and styles.  There's a good representation of different ethnicities and genders among the individuals depicted.  The photographic style is consistent across the images, with each portrait appearing well-lit and professionally taken, likely from a curated dataset designed for image recognition or AI training.

The title "Image textual search using OpenCLIP features from synthetic dataset Condition: Hair Style" indicates the purpose of the image compilation. It suggests the images are used to test or demonstrate the capabilities of a text-based image search system (OpenCLIP) in identifying and categorizing different hairstyles within a synthetic dataset.  The prefix "Prefix: " suggests the possibility of further filtering or specification in the search.

================================================================================
================================================================================
figures/textual_search_1_expression_top_8_matches.jpg:
======================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a large grid composed of 96 smaller squares, each containing a different portrait photograph of a person's face. Each portrait is labeled with metadata indicating the image source and a classification of the person's facial expression.  The expressions are categorized into eight distinct emotional categories: happy, sad, angry, surprised, neutral, disgusted, fearful, and tongue out.

Each row of the grid presents eight images, showcasing the variety of faces expressing a single emotion. For instance, the first row shows eight different individuals exhibiting happiness, ranging in age, ethnicity, and the intensity of their smiles. This structure facilitates a visual comparison of how diversely individuals express the same emotion.  The metadata helps to track the source of each image, indicating different image datasets used for the compilation.

The overall composition is highly organized, creating a systematic representation of facial expressions. This structure is likely intended for research or analysis purposes, perhaps related to facial expression recognition, emotion AI, or similar fields. The diversity of individuals depicted suggests the effort to create a robust and representative dataset for training or testing algorithms.

================================================================================
================================================================================
figures/textual_search_2_Face Pose_top_10_matches.jpg:
======================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a grid of 70 images, each showcasing a different person's face.  The images are meticulously organized and labeled, demonstrating a systematic approach to data organization. Each image has a caption indicating the image's source (e.g., "FLUX1_schnell_image_0017649.jpg") and a description of the person's pose (e.g., "looking straight ahead," "turned sideways," "tilted upwards," "tilted downwards," "three-quarter view," "profile view").

The image grid is structured to show variations in head pose.  The top rows primarily feature faces looking straight ahead, followed by rows depicting faces turned sideways, then tilted upwards, tilted downwards, three-quarter views, and finally, profile views. This arrangement facilitates a visual comparison of how facial features change with different head orientations.  The images are diverse, featuring people of various ethnicities, ages, and genders, adding to the dataset's comprehensiveness.

The overall design suggests the image is a sample from a larger dataset used for research or development, likely in the field of computer vision or artificial intelligence. The clear labeling and structured organization indicate a focus on data quality and consistency, crucial elements for training machine learning models that can accurately recognize and interpret facial poses. The title "Image textual search using OpenCLIP features from synthetic dataset" further confirms this purpose, highlighting the use of this image collection in an image retrieval system based on textual descriptions.

================================================================================
================================================================================
figures/textual_search_2_Accessories_top_10_matches.jpg:
========================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image displays a large grid of 144 small images, each featuring a person's head and shoulders.  Each image is labeled with a filename and a descriptive tag indicating the type of accessory the person is wearing (e.g., "earrings," "necklace," "bandana," "hat," "tie," "scarf," "headphones," "sunglasses"). The arrangement is very organized, with the same accessory type grouped together in blocks of 12 images, then repeated for each accessory type.

The images themselves appear to be high-quality photographs with diverse subjects in terms of ethnicity, gender, age, and style.  The backgrounds vary, but many seem to be neutral or simple to keep the focus on the individual and their accessory. The lighting and composition of the photos are consistent, suggesting they may be from a curated dataset created for image recognition or similar machine learning tasks.

Above the grid, text indicates that the images are part of a synthetic dataset used for image textual search using OpenCLIP features.  The condition specifies "Accessories," and the prefix for the search is "person wearing."  This context suggests the purpose of the image is to visually demonstrate the capabilities of a system capable of identifying accessories worn by people in images.

================================================================================
================================================================================
merge_dataset_folder.py:
========================
#%% Imports

import os
import time
import shutil
import pandas as pd
from tqdm import tqdm

#%% Helper functions

def merge_datasets(source_folders, output_folder):
    # Create output folders
    output_image_folder = os.path.join(output_folder, "images")
    os.makedirs(output_image_folder, exist_ok=True)
    
    # Initialize a list to store all metadata
    all_metadata = []
    
    # Initialize counters for each model
    model_counters = {
        "SDXL": 1,
        "DALLE3": 1,
        "FLUX1_pro": 1,
        "FLUX1_dev": 1,
        "FLUX1_schnell": 1
    }
    
    start_time = time.time()
    # Process each source folder
    for source_folder in source_folders:
        source_image_folder = os.path.join(source_folder, "images")
        source_csv_path = os.path.join(source_folder, "SFHQ_T2I_dataset.csv")
        
        # Read the CSV file
        df = pd.read_csv(source_csv_path)
        
        # Get all image files in the source folder
        image_files = [f for f in os.listdir(source_image_folder) if f.endswith('.jpg')]
        
        print(f"Processing '{source_folder}' ...")
        for image_file in tqdm(image_files):
            # Find the corresponding metadata
            metadata_row = df[df['image_filename'] == image_file]
            
            if not metadata_row.empty:
                model = metadata_row['model_used'].iloc[0]
                new_image_name = f"{model}_image_{model_counters[model]:07d}.jpg"
                
                # Copy and rename the image
                shutil.copy(
                    os.path.join(source_image_folder, image_file),
                    os.path.join(output_image_folder, new_image_name)
                )
                
                # Update metadata
                new_metadata = {
                    'image_filename': new_image_name,
                    'model_used': model,
                    'text_prompt': metadata_row['text_prompt'].iloc[0],
                    'configs': metadata_row['configs'].iloc[0]
                }
                all_metadata.append(new_metadata)
                
                # Increment the counter for this model
                model_counters[model] += 1
    
    # Create the final dataframe and save it
    final_df = pd.DataFrame(all_metadata)
    final_df = final_df.sort_values(by='image_filename').reset_index(drop=True)
    final_csv_path = os.path.join(output_folder, "SFHQ_T2I_dataset.csv")
    final_df.to_csv(final_csv_path, index=False)
    
    total_duration_minutes = (time.time() - start_time) / 60
    print(f"\nDataset merging completed! Total duration: {total_duration_minutes:.2f} minutes")
    print(f"Total images in the merged dataset per model:")
    for model, count in model_counters.items():
        print(f"- {count - 1} {model} images")
    print(f"Combined total of images: {len(final_df)}")
    print(f"\nMetadata saved to '{final_csv_path}'")


#%% Usage

if __name__ == "__main__":

    source_folders = [
        r"datasample_001",
        r"datasample_002",
        r"datasample_003",
    ]

    output_folder = r"merged_clean_dataset"
    
    merge_datasets(source_folders, output_folder)

#%%


================================================================================
================================================================================
figures/textual_search_2_Hair_Color_top_10_matches.jpg:
=======================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid of 72 small images, each showing a person with a distinct hair color. The images are organized into rows and columns, with each row representing a different hair color category (white or gray, yellow or blond, green, blue, purple or pink, red or orange).  Each image is labeled with a descriptive caption indicating the hair color and the source filename, which includes either "SDXL_image" or "FLUX1_schnell_image" or "FLUX1_dev_image" prefix suggesting they are sourced from different datasets.

The overall purpose of the image appears to be to showcase a collection of images categorized by hair color, likely used for training or testing a computer vision model. The consistent format of the filenames and captions points toward an automated process for generating the image grid, possibly from a larger database of images. The variety of hairstyles, hair textures, and skin tones within each hair color category indicates a diverse dataset. The title "Image textual search using OpenCLIP features from synthetic dataset" further suggests the images are used for research and development related to image retrieval based on textual descriptions.

================================================================================
================================================================================
figures/textual_search_2_Expression_x_Sex_top_10_matches.jpg:
=============================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a grid of 72 small square images, each showing a different man's face displaying a variety of expressions. The images are neatly arranged in 9 rows and 8 columns.  Each image is labeled with a descriptive text phrase above it (e.g., "angry or enraged," "surprised," "smiling," "sad or depressed," "grim face," "tounge out") and a file name below, indicating the source and identification of the image within a larger dataset.

The expressions depicted range from intense emotions like anger and sadness to more neutral ones like surprise or a simple smile. Some images show extreme expressions, while others have more subtle nuances. The men in the images are diverse in age, ethnicity, and hairstyle, adding to the variety.  The background of each individual image varies, with some having simple backgrounds and others showing more detail.  The overall effect is a comprehensive visual representation of a wide range of male facial expressions.

The title at the top, "Image textual search using OpenCLIP features from synthetic dataset Condition: Expression x Sex Prefix: 'man'," explains the purpose of the image collection, which is to demonstrate the effectiveness of a specific image search technique (OpenCLIP) in classifying and retrieving images based on facial expressions in a dataset specifically focused on male faces.

================================================================================
================================================================================
figures/textual_search_2_Hair Style x Sex_top_10_matches.jpg:
=============================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid of 72 small images, each showing a woman's face.  The images are organized into rows and columns, with each row representing a different hair color and length category (short blond, long blond, short red, long red, short black, long black).  Each image is labeled with a descriptive text indicating the hair type ("short blond hair," "long red hair," etc.) and a filename.

The purpose of the image is to showcase the results of an image textual search using OpenCLIP features from a synthetic dataset.  The search query is implied by the title: "Image textual search using OpenCLIP features from synthetic dataset Condition: Hair Style x Sex Prefix: "woman with "".  The image demonstrates the ability of the system to retrieve images of women based on specific hair characteristics.

The filenames suggest that the images come from several different sources or datasets (FLUX1_pro, FLUX1_schnell, SDXL).  The uniform size and presentation of the images create a clean, organized visualization of the search results, clearly demonstrating the effectiveness of the image search algorithm in retrieving relevant images based on textual descriptions.

================================================================================
================================================================================
figures/textual_search_2_Age_x_Ethnicity x Sex 2_top_10_matches.jpg:
====================================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid showcasing a diverse collection of images, each depicting an African male at different ages, ranging from infancy to old age.  The grid is organized systematically, with each row representing a distinct age group (baby, toddler, teenager, adult, senior). Within each age group, there are multiple images displaying variations in facial expressions, hairstyles, and clothing.  Each image is accompanied by a caption that specifies the age and filename.

The captions consistently use a specific naming convention: "age" + "description" + "filename". For example, "10 month old baby" SDXL_image_0009277.jpg. This systematic naming and organization suggest the images are part of a structured dataset used for machine learning or image recognition purposes.  The top of the image indicates that the images are results from a textual image search using OpenCLIP features from a synthetic dataset, with a specific condition for "African male" across different ages.  The dataset appears to be carefully curated to ensure representation of various ages and appearances within the specified demographic.

================================================================================
================================================================================
LICENSE.txt:
============
MIT License

Copyright (c) 2024 David Beniaguev

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

================================================================================
================================================================================
figures/model_distribution.jpg:
===============================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image presents a bar chart visualizing the distribution of images across different models. The chart's title clearly states, "Distribution of Images Across Models."  The horizontal axis (x-axis) labels the various models: FLUX1_schnell, SDXL, FLUX1_dev, FLUX1_pro, and DALL-E3. The vertical axis (y-axis) represents the "Number of Images," ranging from 0 to 60,000.

The chart shows a significant disparity in the number of images per model.  FLUX1_schnell and SDXL have substantially more images than the other models, with FLUX1_schnell having the highest count (approximately 58,034) and SDXL having a slightly lower count (approximately 53,087).  The remaining models, FLUX1_dev, FLUX1_pro, and DALL-E3, show progressively fewer images, with DALL-E3 having the least (approximately 1,123).  Each bar is labeled with its corresponding numerical value, indicating the precise number of images for each model.  The chart uses a light teal color for the bars, which contrasts well against the dark background, making the data easy to read and interpret.

================================================================================
================================================================================
figures/textual_search_2_Glasses Style_top_10_matches.jpg:
==========================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a grid showcasing a large number of images, arranged in a matrix of 6 columns and 8 rows. Each image depicts a person wearing a different style of glasses or sunglasses.  The images are neatly organized, and each is labeled with the type of eyewear ("round glasses," "square glasses," "cat-eye glasses," "rimless glasses," "aviator sunglasses," "sport sunglasses") and the filename of the source image (e.g., "FLUX1_schnell_image_0053027.jpg"). The filenames suggest the images come from multiple datasets, possibly labeled and organized for machine learning purposes.

The image's purpose is to illustrate the results of an image textual search using OpenCLIP features from a synthetic dataset. The text at the top explains this, indicating that the search was conditioned on a phrase related to glasses styles ("person wearing"). The careful organization and labeling suggest a demonstration of a successful image retrieval system, where input text queries are used to find relevant images from a large database.  The variety of glasses styles and the diversity of faces in the images makes the dataset seem quite comprehensive.

================================================================================
================================================================================
figures/textual_search_2_Eye Gaze_top_9_matches.jpg:
====================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image displays a grid of 60 face images, organized into six rows and ten columns. Each image shows a person's head and shoulders, exhibiting various facial expressions and gazes.  The images are clearly labeled with descriptive text indicating the direction of the person's gaze (looking directly at camera, looking to the left, looking up, looking down, eyes closed) and the source of the image (SDXL_image, FLUX1_schnell_image, FLUX1_dev_image, FLUX1_pro_image) along with a unique image identifier (e.g., SDXL_image_0011769.jpg).

The overall structure is designed to showcase the results of an image textual search using OpenCLIP features from a synthetic dataset. The search condition "Eye Gaze" and the prefix "person" are clearly stated at the top, indicating the parameters used for the image retrieval process. The diversity of faces, ethnicities, ages, and genders within the image grid suggests a robust and representative dataset was used for the search.  The consistent labeling provides clear metadata for each image, allowing for easy analysis of the search results.

================================================================================
================================================================================
face_prompt_utils.py:
=====================
#%%

import random
import numpy as np

#%% data

lighting_descriptions_dict = {
    'Natural Lighting': [
        "Illuminated by the soft glow of natural sunlight",
        "Bathed in the gentle embrace of daylight",
        "Highlighted by the subtle nuances of natural light",
    ],
    'Artificial Lighting': [
        "Enhanced by the steady gleam of artificial light sources",
        "Illuminated by the consistent radiance of man-made lighting",
        "Under the clear influence of artificial luminance",
        "Bathed in the consistent glow of artificial light sources",
        "Illuminated by artificial lights that replicate natural tones",
        "Highlighted by the controlled ambiance of artificial lighting",
    ],
    'Side Lighting': [
        "With one side of the face distinctly lit, casting a dramatic play of shadows",
        "Highlighted predominantly from one side, creating a depth of contrasts",
        "Illuminated from the side, accentuating textures and contours",
        "Illuminated from the side, creating stark contrasts of light and shadow",
        "With one side of the face brightly lit and the other in deep shadow",
        "Defined by the dramatic interplay of light and shadow from the side",
    ],
    'Loop Lighting': [
        "With a soft shadow gracing the cheek from loop lighting",
        "Characterized by the distinct loop shadow on the cheek",
        "Illuminated with the classic loop lighting technique, casting a subtle shadow",
        "Characterized by the small shadow of the nose creating a loop on the cheek",
        "Highlighted with a loop lighting technique, casting subtle shadows",
        "With the signature loop shadow adding depth and dimension",
    ],
    'Butterfly Lighting': [
        "With a soft shadow under the nose reminiscent of a butterfly",
        "Illuminated from above, casting a butterfly-like shadow",
        "With the characteristic shadow of butterfly lighting under the nose",
        "Defined by the butterfly-shaped shadow under the nose",
        "Illuminated with the classic butterfly lighting technique, creating an elegant effect",
        "Bathed in light from above, casting a distinct butterfly shadow",
    ],
    'Backlighting': [
        "With light emanating from the back, creating a halo effect",
        "Highlighted from behind, casting a gentle glow around the silhouette",
        "Illuminated predominantly from the back, emphasizing the contours",
    ],
    'Soft/Diffused Lighting': [
        "Bathed in the gentle, diffused light that softens features",
        "Under the soft glow that evenly illuminates without harsh shadows",
        "Illuminated with a diffused light, creating a dreamy ambiance",
    ],
    'Golden Hour': [
        "Basked in the warm, golden tones of the hour before sunset",
        "Golden hour light bathes the scene, casting a warm glow and elongating shadows",
        "Illuminated by the magical soft light of the golden hour",
        "Under the enchanting, warm glow of the golden hour",
    ],
    'Blue Hour': [
        "Surrounded by the cool, ethereal tones of the blue hour",
        "Illuminated by the serene, bluish hue of the hour after sunset",
        "Under the mystical, twilight ambiance of the blue hour",
    ],
    'Split Lighting': [
        "With half the face brightly lit and the other in shadow, creating a split effect",
        "Defined by the dramatic contrast of light and shadow on the face",
        "Illuminated with a split lighting technique, emphasizing duality",
    ],
    'Rembrandt Lighting': [
        "Characterized by the distinct triangle of light on the cheek",
        "Illuminated in the classic Rembrandt style, balancing light and shadow",
        "With the signature Rembrandt triangle gracing the face",
    ],
    'Top-Down Lighting': [
        "Illuminated from above, casting definitive shadows below features",
        "The overhead fluorescent lights cast a cool, even tone over the scene, adding a modern urban vibe",
        "With light source directly overhead, emphasizing depth",
        "Bathed in the light coming from the top, creating a theatrical effect",
    ],
    'Broad Lighting': [
        "With the face predominantly lit on its broadest side, emphasizing width",
        "Highlighted with broad lighting, casting minimal shadows",
        "Illuminated in a manner that enhances the face's broader perspective",
    ],
    'Short Lighting': [
        "With the face illuminated from its narrow side, emphasizing depth and contour",
        "Defined by the short lighting technique, adding drama and intensity",
        "Highlighted in a way that captures the face's contours and depth",
    ],
    'Flash Lighting': [
        "Illuminated with a sharp burst of light, capturing vivid details",
        "Defined by the sudden, bright illumination from a flash",
        "With features crisply lit by the distinct light of a flash",
    ],
    'Ambient Lighting': [
        "Surrounded by the soft, even tones of ambient light",
        "Basked in the gentle and consistent glow of ambient lighting",
        "With the nuances highlighted by the surrounding ambient light",
    ],
    'Directional Lighting': [
        "Illuminated with light coming from a specific direction, emphasizing depth",
        "Defined by the strong, unidirectional light source",
        "With shadows and highlights created by a clear directional light",
    ],
    'Fill Lighting': [
        "Balanced with fill light to soften shadows and even out contrasts",
        "With features gently lit by fill lighting, reducing harshness",
        "Softened by the subtle effects of fill light, creating harmony",
    ],
    'High Key Lighting': [
        "Surrounded by an abundance of bright light, reducing harsh shadows",
        "With features softly lit by high key lighting, evoking an airy atmosphere",
        "Illuminated in a manner that minimizes contrast and shadow, typical of high key lighting",
    ],
    'Low Key Lighting': [
        "Engulfed in deep shadows and minimal light, creating a moody ambiance",
        "Defined by the stark contrast and drama of low key lighting",
        "With features accentuated by the intense interplay of light and dark characteristic of low key lighting",
    ],
    'Motivated Lighting': [
        "Illuminated in a way that feels organic and inspired by elements within the scene",
        "With lighting that seems naturally sourced from items in the environment",
        "Highlighted by light that appears to have a clear, believable source within the context",
    ],
    'Practical Lighting': [
        "Lit by visible light sources present in the scene like lamps or candles",
        "With the warm and genuine glow from practical lights setting the tone",
        "Bathed in the authentic luminescence of actual light fixtures within the shot",
    ],
    'Bounced Lighting': [
        "Softly illuminated by light that's been reflected off surfaces, reducing harshness",
        "With a gentle and even glow resulting from bounced light",
        "Highlighted by the diffused and broadened effect of light that's been redirected",
    ],
    'Hard Lighting': [
        "Defined by the sharp shadows and bright highlights of direct light",
        "With features crisply lit by a focused light source, creating strong contrasts",
        "Illuminated in a manner that emphasizes texture and form through hard light",
    ],
    'Three-Point Lighting': [
        "Illuminated with the classic three-point setup, balancing key, fill, and back lights",
        "Defined by the harmony of three-point lighting, creating depth and texture",
        "With key, fill, and back lights working in concert to create a rich visual experience",
        "Highlighted by the versatility of three-point lighting, offering a balanced look",
    ],
    'Flat Lighting': [
        "Bathed in flat lighting that minimizes shadows",
        "With even illumination across the face, characteristic of flat lighting",
        "Illuminated in a manner that reduces shadow and contrast, typical of flat lighting",
        "Highlighted by the soft and shadowless effect of flat lighting",
    ],
    'Rim Lighting': [
        "With the edges softly outlined by rim lighting",
        "Illuminated from behind, creating a distinct rim of light",
        "Defined by the ethereal outline created by rim lighting",
        "Highlighted by the radiant border of rim lighting",
    ],
    'Clamshell Lighting': [
        "With features softly lit by the dual glow of clamshell lighting",
        "Defined by the flattering, even light of a clamshell setup",
        "Illuminated with clamshell lighting, producing soft and beauty-enhancing effects",
        "Highlighted by the glamour-inducing clamshell lighting technique",
    ],
    'Cross Lighting': [
        "Illuminated by lights from opposite sides, creating dynamic contrasts",
        "With textures and dimensions emphasized by the effects of cross lighting",
        "Defined by the interplay of dual light sources in a cross lighting setup",
        "With features enriched by the opposing forces of cross lighting",
    ],
    'Kicker Lighting': [
        "With a subtle highlight along the edge from kicker lighting",
        "Illuminated by a low-angle kicker light, adding depth",
        "Defined by the accentuating edge glow of kicker lighting",
        "Highlighted by the low and side-angled kicker light",
    ],
    'Cinematic Lighting': [
        "Illuminated in a cinematic style, evoking mood and atmosphere",
        "With the dramatic flair commonly found in cinematic lighting setups",
        "Defined by the atmospheric depth of cinematic lighting",
        "Surrounded by the emotional ambiance characteristic of cinematic lighting",
    ],
    'Stage Lighting': [
        "Lit with the broad and dynamic range of stage lighting",
        "With features emphasized by theatrical stage lights",
        "Defined by the vibrant and dramatic nature of stage lighting",
        "Bathed in the spotlight, typical of stage lighting setups",
    ],
    'Beauty Dish Lighting': [
        "With features softly lit by the focused glow of a beauty dish",
        "Illuminated by the flattering and directional light of a beauty dish",
        "Defined by the unique soft yet focused light of a beauty dish",
        "Highlighted by the beauty-enhancing qualities of beauty dish lighting",
    ],
    'Tungsten Lighting': [
        "Bathed in the warm, yellow-orange glow of tungsten lighting",
        "Illuminated by the classic, warm tones of a tungsten light source",
        "With features highlighted by the cozy atmosphere created by tungsten lighting",
        "Defined by the nostalgic and warm feel of tungsten lighting",
    ],
}

ethnicities_dict = {
    'European': [
        'Austrian', 'Portuguese', 'Russian', 'German', 'French', 'English', 'Swedish', 'Danish', 'Norwegian', 
        'Polish', 'Lithuanian', 'Hungarian', 'Italian', 'Spanish', 'Irish', 'Greek', 'Canadian', 'Romanian', 
        'Serbian', 'Croatian', 'Belgian', 'Icelandic', 'Swiss', 'Luxembourgish', 'Maltese', 'Andorran', 
        'Monacan', 'Liechtensteiner', 'San Marinese', 'Vatican', 'Maltese', 'Finnish', 'Latvian', 'Estonian', 
        'Macedonian', 'Albanian', 'Bosnian', 'Kosovar', 'Montenegrin', 'Moldovan', 'Bulgarian', 'Czech', 
        'Slovak', 'Armenian', 'Azerbaijani', 'Belarusian', 'Ukrainian', 'British', 'Dutch', 'Slovenian', 
        'Estonian', 'Cypriot'
    ],
    'Sub-Saharan African': [
        'Nigerian', 'Kenyan', 'Ghanaian', 'Ethiopian', 'South African', 'Congolese', 'Somali', 'Ugandan',
        'Tanzanian', 'Rwandan', 'Malawian', 'Zambian', 'Zimbabwean', 'Angolan', 'Botswanan', 'Madagascan', 'Gabonese',
        'Namibian', 'Senegalese', 'Cameroonian', 'Ivorian', 'Guinean', 'Liberian', 'Sierra Leonean', 'Burkinabe',
        'Malian', 'Togolese', 'Beninese', 'Nigerien', 'Chadian', 'Central African', 'South Sudanese', 'Eritrean', 'Djiboutian',
        'Comoran', 'Seychellois', 'Mauritian', 'Cape Verdean'
    ],
    'Middle Eastern': [
        'Israeli', 'Iraqi', 'Egyptian', 'Iranian', 'Afghan', 'Arab', 'Turkish', 'Persian', 'Georgian', 
        'Yemeni', 'Saudi Arabian', 'Lybian', 'Jordanian', 'Syrian', 'Lebanese', 'Omani', 'Palestinian', 
        'Qatari', 'Emirati', 'Bahraini', 'Kuwaiti', 'Azerbaijani', 'Armenian'
    ],
    'Latin American': [
        'Brazilian', 'Mexican', 'Argentinian', 'Colombian', 'Peruvian', 'Chilean', 'Ecuadorian', 'Bolivian', 
        'Venezuelan', 'Uruguayan', 'Paraguayan', 'Costa Rican', 'Panamanian', 'Nicaraguan', 'Guatemalan', 
        'Salvadoran', 'Honduran', 'Cuban', 'Dominican', 'Puerto Rican'
    ],
    'Oceanian': [
        'Australian', 'New Zealander', 'Fijian', 'Samoan', 'Tongan', 'Papuan', 'Guamanian', 'Palauan', 
        'Micronesian', 'Marshallese', 'Nauruan', 'Solomon Islander', 'Vanuatuan', 'Ni-Vanuatu', 'New Caledonian', 
        'French Polynesian', 'Tokelauan', 'Tuvaluan', 'Wallisian', 'Futunan'
    ],
    'Caribbean': [
        'Cuban', 'Jamaican', 'Haitian', 'Dominican', 'Trinidadian', 'Barbadian', 'Bahamian', 'Grenadian', 
        'Saint Lucian', 'Antiguan', 'Vincentian', 'Kittitian', 'Nevisian', 'Montserratian', 'Bermudian'
    ],
    'Central Asian': [
        'Uzbek', 'Kazakh', 'Tajik', 'Turkmen', 'Kyrgyz', 'Uzbekistani', 'Turkistani', 'Uyghur', 'Tatar', 
        'Karakalpak', 'Bashkir', 'Kumyk', 'Balkar', 'Karachay', 'Avar'
    ],
    'West Asian': [
        'Armenian', 'Azerbaijani', 'Georgian', 'Turkish', 'Kurdish', 'Assyrian', 'Alevi', 'Druze', 'Yazidi', 
        'Maronite', 'Alawite', 'Circassian', 'Laz', 'Gilaki', 'Mazandarani'
    ],
    'North African': [
        'Egyptian', 'Moroccan', 'Algerian', 'Tunisian', 'Libyan', 'Sudanese', 'Mauritanian', 'Berber', 'Amazigh', 
        'Nubian', 'Coptic', 'Tuareg', 'Riffian', 'Kabyle', 'Sahrawi'
    ],
    'Scandinavian': [
        'Swedish', 'Norwegian', 'Danish', 'Icelandic', 'Finnish', 'Sami', 'Faroese', 'Gotlander', 'Orcadian', 
        'Shetlander', 'Ålandic', 'Jämtlander'
    ],
    'North American': [
        'American', 'Canadian', 'Mexican', 'Greenlandic', 'Alaskan', 'Texan', 'Quebecois', 'Cajun', 'Hawaiian', 
        'Newfoundlander', 'Cree', 'Inuvialuit', 'Métis', 'Gwich’in', 'Haida', 'Tlingit'
    ],
    'Arctic': [
        'Inuit', 'Saami', 'Chukchi', 'Yupik', 'Aleut', 'Kalaallit', 'Nenets', 'Khanty', 'Evenki', 'Selkup', 
        'Yamalo', 'Enets', 'Nganasan', 'Veps', 'Koryaks'
    ],
    'Southeast Asian': [
        'Thai', 'Laos', 'Cambodian', 'Malaysian', 'Filipino', 'Indonesian', 'Burmese', 'Singaporean', 
        'Vietnamese', 'Bruneian', 'Timorese', 'Javanese', 'Balinese', 'Sundanese', 'Minangkabau'
    ],
    'Balkan': [
        'Bulgarian', 'Greek', 'Romanian', 'Serbian', 'Croatian', 'Bosnian', 'Slovenian', 'Montenegrin', 
        'Macedonian', 'Albanian', 'Kosovar', 'Vlach', 'Pomak', 'Torlakian', 'Gagauz', 'Aromanian'
    ],
    'Polynesian': [
        'Hawaiian', 'Maori', 'Samoan', 'Tongan', 'Tahitian', 'Niuean', 'Tokelauan', 'Tuvaluan', 'Rapanui',
        'Marquesan', 'Mangarevan', 'Pukapukan', 'Rarotongan', 'Tahitian', 'Tuamotuan', 'Rennell Islander'
    ],
    'Micronesian': [
        'Guamanian', 'Palauan', 'Marshallese', 'Nauruan', 'Micronesian', 'Kosraean', 'Yapese', 'Pohnpeian', 
        'Chuukese', 'Angauran', 'Sonsorolese', 'Tobi Islander', 'Woleaian', 'Ulithian', 'Carolinian'
    ],
    'Melanesian': [
        'Fijian', 'Papuan', 'Vanuatuan', 'Solomon Islander', 'Ni-Vanuatu', 'New Caledonian', 'Kanak', 'Bougainvillean',
        'Ambrym Islander', 'Aniwa Islander', 'Futuna Islander', 'Erromango Islander', 'Tannese', 'Motu', 'Tolai'
    ],
    'Indigenous American': [
        'Navajo', 'Mayan', 'Inca', 'Guarani', 'Mapuche', 'Quechua', 'Aymara', 'Taino', 'Kuna', 'Wayuu',
        'Cherokee', 'Lakota', 'Apache', 'Iroquois', 'Zapotec', 'Mixtec', 'Quechuan'
    ],
    'Australasian': [
        'Australian', 'New Zealander', 'Papuan', 'Melanesian', 'Polynesian', 'Micronesian',
        'Torres Strait Islander', 'Tiwi Islander', 'Anangu', 'Noongar', 'Palawa', 'Yolngu', 'Koori'
    ],
    'Caucasian': [
        'Georgian', 'Chechen', 'Dagestani', 'Armenian', 'Azerbaijani', 'Abkhaz', 'Ossetian', 'Circassian',
        'Ingush', 'Kabardian', 'Balkar', 'Karachay', 'Lezgian', 'Aghul', 'Tabasaran'
    ],
    'East Asian': [
        'Chinese', 'Vietnamese', 'Japanese', 'Korean', 'Mongolian', 'Taiwanese',
        'Hong Kongese', 'Macanese', 'Ryukyuan', 'Ainu', 'Hui', 'Uighur'
    ],
    'South Asian': [
        'Indian', 'Pakistani', 'Bangladeshi', 'Sri Lankan', 'Nepalese', 'Bhutanese', 'Maldivian',
        'Sinhalese', 'Tamil', 'Pashtun', 'Sindhi', 'Punjabi', 'Gujarati'
    ],
}

eye_colors_dict = {
    "European": [
        "blue", "green", "gray", "hazel", "brown", "amber", "ice-blue", "steel gray",
        "sea green", "pale blue", "deep blue", "emerald green", "light brown", "dark brown",
        "grey-blue", "hazel-green", "turquoise", "aqua", "violet", "olive"
    ],
    "Sub-Saharan African": [
        "dark brown", "black", "amber", "deep brown", "onyx", "chocolate brown",
        "copper", "golden", "honey colored"
    ],
    "Middle Eastern": [
        "brown", "dark brown", "black", "hazel", "amber", "green", "blue",
        "honey colored", "golden", "hazel-green"
    ],
    "Latin American": [
        "brown", "dark brown", "black", "hazel", "amber", "green", "blue", "gray",
        "honey colored", "golden", "copper"
    ],
    "Oceanian": [
        "dark brown", "black", "light brown", "amber", "hazel", "deep brown"
    ],
    "Caribbean": [
        "brown", "dark brown", "black", "hazel", "amber", "green", "honey colored"
    ],
    "Central Asian": [
        "brown", "dark brown", "black", "hazel", "amber", "green", "blue", "gray"
    ],
    "West Asian": [
        "brown", "dark brown", "black", "hazel", "amber", "green", "blue", "gray"
    ],
    "North African": [
        "brown", "dark brown", "black", "hazel", "amber", "green", "honey colored"
    ],
    "Scandinavian": [
        "blue", "ice-blue", "gray", "green", "pale blue", "light brown", "icy grey",
        "sea green", "turquoise", "aqua"
    ],
    "North American": [
        "blue", "green", "gray", "hazel", "brown", "amber", "dark brown", "black",
        "hazel-green", "light brown", "honey colored"
    ],
    "Arctic": [
        "dark brown", "black", "hazel", "deep brown"
    ],
    "Southeast Asian": [
        "dark brown", "black", "light brown", "amber", "hazel", "copper"
    ],
    "Balkan": [
        "brown", "dark brown", "black", "hazel", "green", "blue", "gray", "amber"
    ],
    "Polynesian": [
        "dark brown", "black", "light brown", "amber", "deep brown"
    ],
    "Micronesian": [
        "dark brown", "black", "light brown", "deep brown"
    ],
    "Melanesian": [
        "dark brown", "black", "light brown", "amber", "deep brown"
    ],
    "Indigenous American": [
        "dark brown", "black", "light brown", "amber", "hazel", "copper"
    ],
    "Australasian": [
        "dark brown", "black", "light brown", "amber", "hazel", "blue", "green",
        "gray", "honey colored"
    ],
    "Caucasian": [
        "brown", "dark brown", "black", "hazel", "green", "blue", "gray", "amber",
        "hazel-green", "light brown"
    ],
    "East Asian": [
        "dark brown", "black", "light brown", "amber", "copper"
    ],
    "South Asian": [
        "dark brown", "black", "light brown", "amber", "hazel", "copper", "honey colored"
    ],
    "Interesting Colors": [
        'blue', 'green', 'teal', 'hazel', 'brown', 'amber', 'ice-blue', 'steel gray',
        'cyan', 'violet', 'red', 'pink', 'orange', 'yellow', 'gold', 'silver', 'black',
        'white', 'gray', 'purple', 'turquoise', 'aqua', 'emerald', 'sapphire', 'ruby',
        'topaz', 'amethyst', 'jade', 'opal', 'pearl', 'sable', 'copper', 'bronze', 'brass',
        'platinum', 'rose gold', 'champagne', 'mahogany', 'caramel', 'cinnamon', 'coffee'
    ]
}

hair_colors_dict = {
    "European": [
        "blond", "light brown", "dark brown", "black", "red", "auburn", "strawberry blond", "platinum blond",
        "ash blond", "dirty blond", "golden blond", "chestnut", "mahogany", "copper", "ginger",
        "silver-gray", "white", "raven-black", "jet black", "honey blond", "sandy"
    ],
    "Sub-Saharan African": [
        "black", "dark brown", "brown", "reddish-brown", "auburn", "jet black",
        "ebony", "chocolate brown", "espresso"
    ],
    "Middle Eastern": [
        "black", "dark brown", "brown", "light brown", "auburn", "jet black", "chestnut brown"
    ],
    "Latin American": [
        "black", "dark brown", "brown", "light brown", "auburn", "reddish-brown",
        "jet black", "mahogany", "chestnut brown"
    ],
    "Oceanian": [
        "black", "dark brown", "brown", "jet black", "ebony"
    ],
    "Caribbean": [
        "black", "dark brown", "brown", "reddish-brown", "auburn", "jet black", "ebony"
    ],
    "Central Asian": [
        "black", "dark brown", "brown", "light brown", "jet black"
    ],
    "West Asian": [
        "black", "dark brown", "brown", "light brown", "auburn", "jet black"
    ],
    "North African": [
        "black", "dark brown", "brown", "light brown", "jet black", "ebony"
    ],
    "Scandinavian": [
        "blond", "light brown", "dark brown", "platinum blond", "ash blond", "golden blond",
        "strawberry blond", "silver-gray", "white"
    ],
    "North American": [
        "blond", "light brown", "dark brown", "black", "red", "auburn", "strawberry blond", "platinum blond",
        "ash blond", "dirty blond", "golden blond", "chestnut", "mahogany", "copper", "ginger",
        "silver-gray", "white", "raven-black", "jet black", "honey blond", "sandy"
    ],
    "Arctic": [
        "black", "dark brown", "jet black"
    ],
    "Southeast Asian": [
        "black", "dark brown", "brown", "jet black", "ebony"
    ],
    "Balkan": [
        "dark brown", "black", "light brown", "auburn", "chestnut brown", "mahogany"
    ],
    "Polynesian": [
        "black", "dark brown", "brown", "jet black", "ebony"
    ],
    "Micronesian": [
        "black", "dark brown", "jet black", "ebony"
    ],
    "Melanesian": [
        "black", "dark brown", "reddish-brown", "jet black", "ebony"
    ],
    "Indigenous American": [
        "black", "dark brown", "brown", "reddish-brown", "jet black", "ebony"
    ],
    "Australasian": [
        "black", "dark brown", "brown", "blond", "red", "auburn", "jet black", "ebony"
    ],
    "Caucasian": [
        "dark brown", "black", "light brown", "auburn", "red", "chestnut brown",
        "mahogany", "jet black"
    ],
    "East Asian": [
        "black", "dark brown", "light brown", "auburn", "reddish-brown", "jet black", "ebony"
    ],
    "South Asian": [
        "black", "dark brown", "brown", "auburn", "reddish-brown", "jet black", "ebony"
    ],
    "Interesting Colors": [
        'blue', 'green', 'teal', 'hazel', 'brown', 'amber', 'ice-blue', 'steel gray',
        'cyan', 'violet', 'red', 'pink', 'orange', 'yellow', 'gold', 'silver', 'black',
        'white', 'gray', 'purple', 'turquoise', 'aqua', 'emerald', 'sapphire', 'ruby',
        'topaz', 'amethyst', 'jade', 'opal', 'pearl', 'sable', 'copper', 'bronze', 'brass',
        'platinum', 'rose gold', 'champagne', 'mahogany', 'caramel', 'cinnamon', 'coffee'
    ]
}

expressions_list = [
    'with a fearful expression', 'with a whimsical twirl', 'with a contemptuous sneer', 'with an eager nod',
    'with a sulky expression', 'with a concerned expression', 'with a wistful expression', 'with tongue sticking out',
    'with a elated expression', 'with a nostalgic expression', 'with a guilty expression', 'with a optimistic expression',
    'with a bored expression', 'with a uninterested expression', 'with a perplexed scratch', 'with a bewildered expression',
    'with an excited expression', 'with a serene expression', 'with a stunned expression', 'with a vigilant watch',
    'with a grateful expression', 'with a sad gaze', 'with a laughing expression', 'with a cautious approach',
    'with a relieved sigh', 'with a sorrowful expression', 'with a scared expression', 'with a cheerful expression',
    'with an inquisitive tilt', 'with a scary expression', 'with a distressed expression', 'with a angry expression',
    'with a proud expression', 'with a resentful expression', 'with a timid expression', 'with a frustrated expression',
    'with a surprised look', 'with a inquisitive expression', 'with a intense expression', 'with a hollow stare',
    'with a solemn expression', 'with a gloomy stare', 'with a zany expression', 'with a contemplative expression',
    'with a jealous glance', 'with an ecstatic cheer', 'with a pleased expression', 'with an enraged expression',
    'with a hollow stare expression', 'with a irritated expression', 'with a humbled bow', 'with an appreciative nod',
    'with a joyful smile', 'with a astonished expression', 'with a satisfied expression', 'with a worried frown',
    'with a playful smile', 'with a melancholy look', 'with a mournful cry', 'with a determined stride',
    'with a lonely expression', 'with a jubilant dance', 'with a annoyed expression', 'with a disgusted expression',
    'with a amused expression', 'with a humbled expression', 'with a triumphant expression', 'with a ecstatic expression',
    'with a dreamy expression', 'with a blank stare expression', 'with a nervous expression', 'with a panic expression',
    'with a disgruntled expression', 'with a shy expression', 'with a determined expression', 'with a pensive look',
    'with a melancholy expression', 'with a longing gaze', 'with a regretful sigh', 'with a grumpy grunt',
    'with a skeptical eyebrow', 'with a focused expression', 'with a indignant expression', 'with a confused look',
    'with a calm expression', 'with a stoic expression', 'with an annoyed grimace', 'with an arrogant posture',
    'with a wary eye', 'with an uneasy shuffle', 'with a disbelief expression', 'with an anxious fidget',
    'with a longing expression', 'with an overwhelmed gasp', 'with mouth open in surprise', 'with a puzzled expression',
    'with a joyful expression', 'with a hysteric expression', 'with a anxious expression', 'with a embarrassed expression',
    'with a confused expression', 'with a satisfied smile', 'with a resigned shrug', 'with a hysterical laugh',
    'with a curious glance', 'with a apprehensive expression', 'with a hopeful expression', 'with a determined look',
    'with a crying expression', 'with a mad expression', 'with a whimsical expression', 'with a vexed expression',
    'with a sly smile', 'with an optimistic smile', 'with a zany hop', 'with a relieved expression',
    'with a smiling expression', 'with a indifferent expression', 'with a mournful expression', 'with a shocked expression',
    'with a serious expression', 'with a pessimistic mutter', 'with a vexed stomp', 'with a awkward expression',
    'with a thoughtful gaze', 'with a frowning expression', 'with a frustrated gesture', 'with a beaming expression',
    'with tears in the eyes', 'with a grinning expression', 'with a jealous expression', 'with a fearful look',
    'with a deflated posture', 'with a grieving expression', 'with a confident expression', 'with a depressed expression',
    'with an amused smirk', 'with a pleased nod', 'with an enthusiastic expression', 'with a overwhelmed expression',
    'with a frown', 'with a irate expression', 'with a gloomy expression', 'with a hopeful gaze',
    'with a content expression', 'with a thoughtful expression', 'with a cheerful demeanor', 'with an apprehensive gaze',
    'with a heartbroken expression', 'with a detached expression', 'with a disoriented expression', 'with a tormented expression',
    'with an awkward smile', 'with a timid step', 'with a bewildered look', 'with a wide smile',
    'with a uneasy expression', 'with a content smile', 'with a bored yawn', 'with a disappointed expression',
    'with a appreciative expression', 'with a furious expression', 'with an elated jump', 'with a enthusiastic expression',
    'with a pensive expression', 'with a resigned expression', 'with a panicked expression', 'with a sulky pout',
    'with a tense posture', 'with a melancholic expression', 'with a excited expression', 'with an angry look',
    'with a sad expression', 'with a resentful glare', 'with a disappointed frown', 'with a apathetic expression',
    'with a proud stance', 'with a beaming smile', 'with a grumpy expression', 'with a frowning face',
    'with an indignant huff', 'with an inquisitive look', 'with a eager expression', 'with a stressed expression',
    'with a funny expression', 'with a triumphant roar', 'with a skeptical expression', 'with an apathetic stare',
    'with an irate shout', 'with a worried expression', 'with a cautious expression', 'with a disgruntled scowl',
    'with a delighted expression', 'with a scowl', 'with a happy demeanor', 'with a curious expression',
    'with an indifferent shrug', 'with a deflated expression', 'with an embarrassed blush', 'with a distressed cry',
    'with a surprised expression', 'with a horrified look', 'with a perplexed expression', 'with a look of disbelief',
    'with a contempt expression', 'with a lonely look', 'with a blank stare', 'with a exasperated expression',
    'with a interested expression', 'with a jubilant expression', 'with a vigilant expression', 'with a flabbergasted expression',
    'with a regretful expression', 'with a neutral expression', 'with an admirable expression', 'with a nervous twitch',
    'with a grateful smile', 'with an exasperated sigh', 'with a ashamed expression', 'with a enraged expression',
    'with a disgusted look', 'with a solemn face', 'with a pessimistic expression', 'with a calm demeanor',
    'with a wary expression', 'with a admirable expression', 'with a baffled expression', 'with a wistful glance',
    'with a horrified expression', 'with a tense expression', 'with a joyous laugh', 'with an interested look',
    'with a mischievous grin', 'with an ashamed face', 'with a arrogant expression', 'with a scary look', 'with a happy expression'
]

hair_styles_list = [
    "flowing hair", "short curly hair", "bald head", "wavy hair", "short spiky hair",
    "long straight hair", "short straight hair", "long curly hair", "shoulder-length hair", "tidy hair",
    "shaven head", "buzz cut hair", "bob cut hair", "afro hair", "dreadlocks",
    "braided hair", "ponytail hair", "hair bun", "shiny hair", "mullet hair",
    "pixie cut hair", "undercut hair", "fade haircut", "taper haircut", "quiff hair",
    "faux hawk hair", "pompadour hair", "wet hair look", "crew cut hair", "side-parted hair",
    "mohawk hair", "comb over hair", "slicked-back hair", "shaggy hair", "layered hair",
    "choppy hair", "asymmetrical haircut", "feathered hair", "cropped hair", "blunt cut hair",
    "razor cut hair", "textured hair", "coiled hair", "ringlet hair", "cornrows hair",
    "finger-waved hair", "pin curled hair", "beehive hair", "pageboy haircut", "hime cut hair",
    "pixie-bob haircut", "lob haircut", "jheri curl hair", "curtain hair", "top knot hair",
    "man bun hair", "twisted hair", "locs hair", "permed hair", "hair with bangs",
    "hair with fringe", "balayage hair", "hair with highlights", "hair with lowlights", "hair with chunky highlights",
    "hair with frosted tips", "medium-length hair", "coily hair", "side-part hair", "middle-part hair",
    "twist-out hair", "bantu knots hair", "box braids hair", "goddess braids hair", "faux locs hair",
    "twist braids hair", "finger coils hair", "perm rod set hair", "sidecut hair", "wolf cut hair",
    "curtain bangs hair", "baby bangs hair", "side-swept bangs hair", "ducktail hair", "liberty spikes hair",
    "deathhawk hair", "French twist hair", "chignon hair", "bouffant hair", "victory rolls hair",
    "crown braid hair", "milkmaid braids hair", "space buns hair", "ombre hair", "dip-dyed hair",
    "streaked hair", "colorful hair", "pastel-colored hair", "neon-colored hair", "Gibson Girl hair",
    "hi-top fade hair", "flattop hair", "Rockabilly hair", "Teddy Boy hair", "Mod hair", "hair adorned with flowers",
    "Hippie hair", "sculptural hair", "geometric hair", "futuristic hair", "avant-garde hair",
    "editorial hair", "haute couture hair", "gravity-defying hair", "buzz cut with designs", "tapered fade hair",
    "high and tight hair", "textured top hair", "long on top, short on sides hair", "chin-length hair", "collarbone-length hair",
    "mid-back length hair", "waist-length hair", "half-up, half-down hair", "side-braided hair", "braided mohawk hair",
    "faux hawk with braided sides", "twisted updo hair", "messy bun with loose tendrils", "sleek ponytail with baby hairs", 
    "hair with jewelry", "hair with ornate pins", "hair with a headband", "hair with a wrap", "hair with colorful extensions"
]

face_poses_list = [
    "facing directly at the camera", "with a slight turn to the left", "with a slight turn to the right",
    "in three-quarter view to the left", "in three-quarter view to the right", "in full left profile",
    "in full right profile", "looking up at the camera", "looking down at the camera",
    "with chin slightly raised", "with chin slightly lowered", "with head tilted to the left",
    "with head tilted to the right", "with a subtle lean forward", "with a subtle lean backward",
    "with shoulders at an angle", "with head slightly rotated left", "with head slightly rotated right",
    "in side-profile view", "with head leaning to the left", "with head leaning to the right",
    "with chin jutted out", "with chin tucked in", "with head tilted back",
    "with head tilted forward", "at a low angle to the camera", "at a high angle to the camera",
    "facing away from the camera", "looking over left shoulder", "looking over right shoulder",
    "with head cocked to the left", "with head cocked to the right", "at eye level with the camera",
    "camera slightly below eye level", "camera slightly above eye level", "facing the camera",
    "in profile", "looking away", "in three-quarter view", "looking up", "looking down", "with head tilted to one side",
    "with eyes looking off-camera", "tilting their head slightly to the right", "tilting their head slightly to the left",
    "with a straight head position", "with their chin lifted slightly", "with their chin lowered a bit",
    "in a left profile view", "in a right profile view", "with their head slightly tilted back",
    "with their head leaning forward", "with a slight three-quarter view to the right", "with a slight three-quarter view to the left",
    "with their head leaning to the right", "with their head leaning to the left", "with their head slightly rotated to the right",
    "with their head slightly rotated to the left", "with their chin slightly jutted out", "with their chin slightly tucked in",
    "with a three-quarter view of their face", "with a neutral head position", "with their head slightly tilted to the left",
    "with their head tilted back a little", "with their head leaning slightly forward", "with their head rotated a bit to the right",
    "with their head rotated a bit to the left", "with their chin pointing slightly upwards", "with their face resting on their hand"
]

#%% helper functions

def get_prompt_start():
    prompt_start_list = [
        "Photo of a", "Portrait of a", "Photograph of a", "Medium Shot of a", "Close-Up of a", "An artistic portrayal of a",
        "Headshot of a", "Face of a", "Facial portrait of a", "Studio portrait of a", "Candid portrait of a", "A serene image of a",
        "Profile view of a", "Character study of a", "Expressive portrait of a", "Cinematic portrait of a", "A candid portrait of a",
        "A portrait photo of a", "A professional photograph of a", "A professional portrait photograph of a", "A pro portrait photo of a", 
        "A high-resolution image of a", "A captivating picture of a", "An enchanting photo of a", "A studio shot of a", "A casual snapshot of a",
        "A meticulously composed portrait of a", "An authentic picture of a", "A magazine-quality portrait of a", "A compelling photograph of a",
        "A striking portrait of a", "A vintage photograph of a", "A black and white portrait of a", "An evocative image of a", 
        "A low-angle shot of a", "A wide-angle shot of a", "An atmospheric portrait of a", "A moody portrayal of a", "A whimsical image of a",
        "An extreme wide shot of a", "A wide shot of a", "A full shot of a", "A medium wide shot of a", "A medium close-up of a",
        "An extreme close-up of a", "An eye-level shot of a", "A Dutch angle shot of a", "A tracking shot of a", "A pan shot of a",
        "A tilt shot of a", "A dolly shot of a", "A zoom shot of a", "An over-the-shoulder shot of a", "A POV shot of a",
        "A cutaway shot of a", "An insert shot of a", "An aerial portrait shot of a", "A high-angle shot of a",
    ]

    return random.choice(prompt_start_list)

def get_random_glasses():
    glasses_list = [
        "wearing classic rectangular glasses", "with round vintage-style glasses", "sporting cat-eye frames",
        "with aviator-style glasses", "wearing oversized square glasses", "with sleek rimless glasses",
        "sporting horn-rimmed glasses", "with retro browline glasses", "wearing geometric hexagonal frames",
        "with trendy clear frame glasses", "sporting thick-framed hipster glasses", "with oval wire-frame glasses",
        "wearing sporty wraparound glasses", "with stylish half-rim glasses", "sporting colorful acetate frames",
        "with sophisticated titanium frames", "wearing bold colored glasses", "with minimalist thin metal frames",
        "sporting funky asymmetrical glasses", "with classic wayfarers", "wearing trendy blue light blocking glasses",
        "with elegant gold-rimmed glasses", "sporting futuristic shield glasses", "with retro round sunglasses",
        "wearing clip-on sunglasses", "with gradient lens sunglasses", "sporting mirrored aviator sunglasses",
        "with polarized sports sunglasses", "wearing fashionable oversized sunglasses", "with classic clubmaster sunglasses",
        "sporting trendy transparent sunglasses", "with vintage cat-eye sunglasses", "wearing modern shield sunglasses",
        "with retro square sunglasses", "sporting stylish browline sunglasses", "with cool wrap-around sunglasses", 
        "with cyberpunk LED glasses", "wearing steampunk goggles", "sporting futuristic visor sunglasses",
        "with monocle", "wearing diamond-studded glasses", 
    ]
    
    return random.choice(glasses_list)

def get_random_gaze_direction():
    gaze_direction_list = [
        "looking directly into the camera", "gazing off to the side", 
        "looking down in thought", "looking upwards", "with eyes closed", 
        "staring to the side", "with a sidelong glance", "looking past the camera",
        "with a far-off look", "with a focused gaze", "with a wandering gaze",
        "looking into the distance", "with eyes nearly shut", 
        "with eyes fixed on an unseen object", "glancing over their shoulder",
        "with eyes darting around nervously", "staring intently at something off-camera",
        "with a thousand-yard stare", "looking through half-lidded eyes",
        "with eyes wide in surprise", "squinting against bright light",
        "with a dreamy, unfocused gaze", "looking down demurely",
        "with eyes crinkled in laughter", "peering curiously at the viewer",
        "with a piercing stare", "looking up through their lashes",
        "with eyes reflecting deep contemplation", "gazing longingly into the distance",
        "with a mischievous twinkle in their eyes", "looking sideways with suspicion",
        "with eyes brimming with tears", "staring defiantly at the camera",
        "with a vacant, expressionless gaze", "looking up in wonder",
        "with eyes narrowed in concentration", "gazing lovingly at someone off-camera",
        "with a faraway look of nostalgia", "looking down with a shy smile",
        "with eyes alight with excitement", "staring off into space pensively",
        "with a haunted look in their eyes", "glancing furtively to the side",
        "with eyes filled with determination", "looking straight ahead with resolve",
        "with a distant, melancholic gaze", "peering intently at something in their hands",
        "with eyes dancing with amusement", "staring blankly ahead",
        "with a wistful gaze towards the horizon", "looking down with a furrowed brow",
        "with eyes half-closed in contentment", "gazing upward with hope",
        "with a sharp, analytical stare", "looking sideways with skepticism",
        "with eyes wide with wonder", "staring intensely at their own reflection",
        "with a distant gaze, lost in memory", "looking directly at the viewer with vulnerability",
        "with eyes scanning the environment alertly", "gazing into middle distance, deep in thought",
        "with a penetrating stare that seems to see through the viewer", "looking down with eyes closed, in meditation",
        "with eyes darting back and forth, reading something", "staring off-camera with a look of longing",
        "with eyes widened in fear or shock", "gazing at their own hands with fascination",
        "with a soft, compassionate look in their eyes", "staring at the ground with a mix of shame and regret",
        "with eyes twinkling with inner joy", "looking past the camera with a stoic expression"
    ]

    return np.random.choice(gaze_direction_list)

def ger_facial_hair_description():
    facial_hair_list = [
        "clean-shaven", "with light stubble", "with heavy stubble",
        "with a short, neat beard", "with a full, thick beard", "with a long, flowing beard",
        "with a well-groomed goatee", "with a circle beard", "with a chin strap beard",
        "with a neat mustache", "with a handlebar mustache", "with a horseshoe mustache",
        "with mutton chops", "with friendly sideburns", "with a soul patch",
        "with a Van Dyke beard", "with a Garibaldi beard", "with a ducktail beard",
        "with a French fork beard", "with a Bandholz beard", "with a yeard",
        "with a ZZ Top-style beard", "with a 5 o'clock shadow", "with designer stubble",
        "with a pencil mustache", "with a Fu Manchu mustache", "with an imperial mustache",
        "with a Dali mustache", "with a walrus mustache", "with a chevron mustache",
        "with a Hollywoodian beard", "with a short boxed beard", "with a Verdi beard",
        "with a Spartan beard", "with a Norse beard", "with a Viking-style beard",
        "with a neatly trimmed beard", "with an unkempt beard", "with a patchy beard",
        "with a salt-and-pepper beard", "with a graying beard", "with a shabby chic beard",
        "with a faded beard", "with a tapered beard", "with a pointy beard",
        "with a braided beard", "with a forked beard", "with a sculpted beard",
        "with an Asian-style mustache", "with a handlebar-and-goatee combo",
        "with a thin-line beard", "with a disconnected mustache",
        "with a chin curtain beard", "with a Klingon-style beard",
        "with a wild, untamed beard", "with a precisely lined beard",
        "with a barely-there mustache", "with a bushy mustache",
        "with a curled mustache", "with waxed mustache tips",
        "with a scruffy beard", "with a lumberjack-style beard",
        "with a hipster beard", "with an artistically trimmed beard",
        "with a multi-colored dyed beard", "with a glitter beard",
        "with a freestyle beard", "with a neck beard",
        "with mutton chops connected to a mustache", "with a chin puff",
        "with an anchor beard", "with a Balbo beard", "with a royal beard", 
        "with a Zappa-style beard", "with a Hulihee beard",
        "with a long goatee", "with sideburns connected to a mustache",
        "with a mustache-free beard", "with a beard-free mustache",
        "with a pencil-thin chin strap", "with a double mustache",
        "with a triangle beard", "with an inverted T-shape beard",
    ]

    return random.choice(facial_hair_list)

def get_makeup_description():
    makeup_list = [
        "with natural, barely-there makeup", "wearing a classic red lip and winged eyeliner", "with a smoky eye and nude lips",
        "featuring a bold cat-eye and coral lipstick", "with a fresh, dewy look and pink blush", "wearing dramatic false eyelashes and glossy lips",
        "with a bronzed, sun-kissed glow", "featuring metallic eyeshadow and matte lips", "with a no-makeup makeup look",
        "wearing bold, colorful eyeshadow and neutral lips", "with perfectly contoured cheekbones", "featuring glossy eyelids and a subtle lip tint",
        "with a gothic-inspired dark lip and pale complexion", "wearing pastel eyeshadow and peach blush", "with a monochromatic makeup look in earthy tones",
        "featuring glitter accents around the eyes", "with a bold, avant-garde makeup design", "wearing a 1950s-inspired pin-up look",
        "with a subtle brown smoky eye and pink lips", "featuring holographic highlighter on cheekbones", "with minimal eye makeup and a bold berry lip",
        "wearing blue mascara and orange-tinted lips", "with graphic eyeliner designs", "featuring ombre lips from dark to light",
        "with strategically placed facial gems or rhinestones", "wearing an ethereal, fairy-like makeup look", "with a bold unibrow statement",
        "featuring neon eyeliner accents", "with a soft, romantic rose-gold palette", "wearing dramatic stage makeup with exaggerated features",
        "with a 1960s-inspired Twiggy lash look", "featuring bright, color-blocked eyeshadow", "with a glossy, wet-look eye makeup",
        "wearing a subtle everyday makeup with focus on skincare", "with an edgy, punk-inspired dark eye and bright lip", 
        "with artfully applied freckles", "wearing mermaid-inspired shimmery scales on cheekbones", "with a soft focus, blurred lip look",
        "featuring negative space eyeliner designs", "with an airbrushed, flawless complexion", "wearing ice princess-inspired frosty tones",
        "with a sun-striping technique using bronzer", "featuring floating crease liner", "with a soft focus hazy eye look",
        "wearing a classic French girl inspired minimal makeup", "with deconstructed bright eyeshadow placement", "featuring a cut-crease eyeshadow technique",
        "with an extreme contour and highlight", "wearing a watercolor-inspired soft wash of colors", "featuring a gradient lip from dark center to light edges",
    ]

    makeup_description = random.choice(makeup_list)
    return makeup_description

def get_location_setting_background():
    locations_settings_backgrounds_list = [
        "in a corn field", "in a wheat field", "in a rice field", "in a sunflower field", "in a strawberry field", 
        "in a lavender field", "in a tulip field", "in a pumpkin patch", "in a flower garden", "in a vegetable garden",
        "in a water garden", "in a rose garden", 'in a grass hill', 'in a grass field', 'in a grassy meadow', 'in a grassy plain',
        "at the desert", "in the forest", "in the park", "at the garden", "at the beach", "outside in wild nature",
        "at the lake", "outside near a mountain", "near a waterfall", "in a cherry blossom park",
        "on a snowy mountaintop", "in a botanical garden", "on a scenic cliff", "on a tropical island", "in a secluded cave",
        "in a vineyard", "in an orchard", "at a coral reef", "in a bamboo forest", "in an ice cave", "in a tulip field",
        "in a pumpkin patch", "next to a scenic pond", "in a tea plantation", "in a coffee plantation", "in an olive grove",
        "in a date palm grove", "in a mossy forest", "in a zen garden", "in a maze garden", "in a grass field",
        "in an alpine meadow", "on a rocky mountain ridge", "in a misty mountain valley", "near a glacial lake", 
        "in a dense tropical rainforest", "in an old-growth redwood forest", "in a misty cloud forest",
        "in a colorful autumn deciduous forest", "in a sparse boreal forest", "near a tranquil mountain stream",
        "in a red rock desert canyon", "among towering sand dunes", "in a rocky desert with cacti",
        "in a salt flat desert", "in a desert oasis with palm trees", "near a coral reef", "on a pebble beach",
        "on a rocky coastal cliff", "on a pristine white sand beach", "in a mangrove swamp",
        "in a rolling grassland prairie", "in an African savanna", "in a wildflower-filled meadow",
        "in a high-altitude steppe", "in a grassy wetland marsh", "near a melting glacier", "in a snowy taiga forest",
        "on the Arctic tundra", "near an Antarctic ice shelf", "in a field of Arctic wildflowers",
        "near an active volcano", "in a field of hardened lava", "near a bubbling mud pot",
        "next to a steaming geyser", "in a volcanic crater lake", "in a slot canyon", "among giant boulders",
        "on the banks of a meandering river", "near a thundering waterfall", "in a river canyon",
        "on a misty river at dawn", "near a series of cascading rapids", "on a snow-capped mountain peak",
        "among bizarre rock hoodoos", "in a limestone karst landscape", "near a natural stone arch",
        "on a remote tropical island", "on a volcanic island coastline", "in a lush island jungle interior",
        "on a windswept subarctic island", "near a fjord on a mountainous island"
        "in a verdant tea plantation with red-clothed pickers", "in a bamboo forest with golden sunlight filtering through",
        "in a lush rainforest with colorful tropical birds", "in a mossy ancient forest with pink cherry blossoms",
        "in a terraced rice field with workers in conical hats", "in a topiary garden with whimsical shapes",
        "in a dense fern gully with a small, clear stream", "in a tropical botanical garden with exotic flowers",
        "on a golf course with white sand bunkers", "in a vineyard with purple grapes ready for harvest",
        "in a field of tall grass with red poppies scattered throughout", "in a misty pine forest with orange mushrooms",
        "in an English garden maze with blooming roses", "in a lush valley with a rainbow arching overhead",
        "in a green tea field with Mount Fuji in the background", "in a traditional Peruvian weaving village",
        "in a vast sunflower field", "on a beach with golden sand and blue water", "in a field of yellow rapeseed flowers",
        "among fall foliage with golden leaves", "in a wheat field ready for harvest", "in a desert with golden sand dunes",
        "in a field of yellow tulips", "surrounded by autumn birch trees with yellow leaves", "in a field of yellow daffodils",
        "on a hillside covered in yellow wildflowers", "in a lemon grove with ripe yellow fruit", "in a field of goldenrod flowers",
        "among yellow aspen trees in autumn", "in a field of yellow marigolds", "surrounded by yellow ginkgo trees in fall"
        "in the village", "in the city", "at home", "on the couch", "in the livingroom", "near the fireplace",
        "in a cosy wooden cabin", "in a ski lodge", "in a mansion", "in a villa", "in a photography studio",
        "in a studio apartment", "in a penthouse apartment", "in Times Square, New York", "in the Red Square, Moscow",
        "in Los Angeles", "in Hollywood", "in a Bel Air villa", "in Paris", "in San Francisco", "in London", "in New York",
        "in Berlin", "in Tokyo", "in Chicago", "in Rome", "in Barcelona", "in Canada", "in Toronto", "in Alaska",
        "in Antarctica", "in the office", "at a luxury hotel", "in the kitchen", "at the balcony", "in a studio",
        "on the Great Wall of China", "in a historic castle", "at a famous landmark", "in an ancient ruin",
        "in a modern skyscraper", "on a bustling street market", "on a charming bridge", "at a picturesque harbor",
        "in a bustling cafe", "in a majestic palace", "in an art gallery", "in a world-famous museum", "in a theater",
        "in a fish market", "in a clock tower", "at a lighthouse", "in an old village", "in a professional photography studio",
        "at a historic monastery", "in an art deco building", "in a gothic cathedral", "at a scenic viewpoint",
        "at a picturesque quarry", "next to a windmill", "at a historic fort", "in an aquarium", "in a planetarium",
        "at a scenic dock", "on a historic ship", "in a bustling subway station", "in a busy city street", "on a yacht",
        "in a quiet village", "in a quiet library", "in a bustling airport terminal", "in a lively sports stadium",
        "in an elegant art gallery", "in a high-tech laboratory", "in an opulent palace", "in a cutting-edge skyscraper",
        "in a charming bed and breakfast", "in a bustling food market", "in a historic lighthouse",
        "in a whimsical fairy-tale inspired theme park", "at a remote Arctic research station", "on a cruise ship",
        "in a traditional Mongolian yurt camp", "at a bustling Broadway theater", "in a serene botanical garden",
        "next to a yellow stone wall", "next to a red brick wall", "next to a green tile wall", "next to a purple stone wall",
        "next to a blue stone wall", "next to a white stone wall", "next to a black stone wall", "next to a brown stone wall",
        "next to a wooden lime wall", "next to a orange brick wall", "next to a magenta stone wall", "next to a cyan stone wall",
        "next to a wooden wall", "next to a stone wall", "next to a marble wall", "next to a brick wall",
        "next to a glass window", "next to a yellow wall", "next to a red wall", "next to a green tile wall",
        "next to a purple wall", "next to a yellow wooden wall", "next to an orange stone wall",
        "next to a green stone wall", "next to a purple marble wall", "next to a blue marble wall",
        "next to a white marble wall", "next to a black marble wall", "next to a brown tile wall",
        "next to an ancient stone wall", "with a plain background", "with a blurred background",
        "against a white backdrop", "against a black backdrop", "with a colorful backdrop",
        "with a textured background", "with a gradient background", "with a bokeh effect background",
        "at the Grand Canyon", "next to Victoria Falls", "next to Niagara Falls", "at a music festival",
        "on a historic battlefield", "on a sailboat", "in a hot air balloon", "under the northern lights",
        "next to a historic statue", "in a traditional tea house", "in a serene butterfly sanctuary",
        "in an ancient underground cave system", "in a vibrant street art alley", "in a misty Scottish highland",
        "at a futuristic vertical farm", "in a traditional Japanese onsen", "at a bustling spice market in Marrakech",
        "in an otherworldly salt flat", "at a bioluminescent beach at night", "in a lush tropical treehouse resort",
        "at a historic Route 66 diner", "in a neon-lit cyberpunk cityscape", "in a tranquil lavender field in Provence",
        "in a serene Scandinavian fjord", "at a colorful hot air balloon festival",
        "in a mystical fog-covered ancient forest", "at a cutting-edge renewable energy farm",
        "outdoors", "indoors", "in an outdoor setting", "in a studio setting", "against an urban setting",
        "against a nature backdrop", "against a studio background", "against a beach scene"
        "in an old steel mill", "at a bustling shipyard", "in a modern automotive factory",
        "at an active construction site", "in a textile manufacturing plant", "at a wind turbine farm",
        "in a high-tech electronics assembly line", "at a busy seaport with cargo containers",
        "in a traditional blacksmith's workshop", "at a state-of-the-art recycling facility",
        "in a university lecture hall", "in a elementary school classroom", "at a public library reading room",
        "in a high school science lab", "at a coding bootcamp workspace", "in a music conservatory practice room",
        "at a culinary school kitchen", "in a medical school anatomy lab", "at an art school studio",
        "on a professional basketball court", "at an Olympic swimming pool", "in a state-of-the-art gymnasium",
        "on a golf course green", "at a baseball stadium dugout", "in a boxing ring corner", "on a soccer field sideline",
        "on an athletics track stadium", "at a professional football stadium", "on a track & field course",
        "on a tennis court baseline", "at a rock climbing wall", "in a yoga studio", "at a horse racing track",
        "in the cockpit of a commercial airliner", "on the deck of a luxury cruise ship",
        "at a bustling train station platform", "in the cabin of a high-speed bullet train",
        "at a busy airport terminal", "in the back of a yellow taxi cab", "on a city bus during rush hour",
        "in a sleek, modern subway car", "at a car rental facility", "in the control room of a cargo ship",
        "at a colorful Holi festival celebration", "during a traditional Japanese tea ceremony",
        "at a lively Carnival parade in Rio", "during a solemn Native American powwow",
        "at a vibrant Chinese New Year celebration", "during a formal Western wedding ceremony",
        "at a lively Oktoberfest beer hall", "during a traditional Indian Diwali festival",
        "at a Mexican Day of the Dead celebration", "during a Moroccan Ramadan evening feast",
        "in front of a large abstract mural", "surrounded by classical marble sculptures",
        "in a glass-blowing studio mid-creation", "at a pottery wheel shaping clay",
        "in front of a wall of colorful street art", "in a dance studio with mirrored walls",
        "at an outdoor installation art exhibit", "in a photography darkroom", "in a marine biology research vessel", 
        "at a bustling art gallery opening night", "in a theater prop and costume workshop",
        "in a cutting-edge robotics laboratory", "at a particle accelerator facility", "at an archaeological dig site",
        "in a clean room for semiconductor manufacturing", "at a radio telescope array", "in a renewable energy research center", 
        "in a genetic research laboratory", "at a weather monitoring station", "at a space mission control center",
    ]

    return random.choice(locations_settings_backgrounds_list)

def get_skin_description(ethnicity_group=None, stereotype_prob=0.7):
    skin_tones_by_ethnicity_dict = {
        "European":            ["fair", "light", "pale", "ivory", "porcelain", "rosy", "peach", "cream", "alabaster", "milky", "cool beige"],
        "Sub-Saharan African": ["dark brown", "deep brown", "chocolate", "ebony", "mahogany", "espresso", "rich brown"],
        "Middle Eastern":      ["olive", "tan", "medium", "golden", "warm beige", "light brown", "honey"],
        "Latin American":      ["olive", "tan", "caramel", "bronze", "golden", "medium", "coffee", "mocha"],
        "Oceanian":            ["tan", "golden brown", "deep brown", "bronze", "copper"],
        "Caribbean":           ["caramel", "golden brown", "deep brown", "mahogany", "cocoa"],
        "Central Asian":       ["light", "medium", "olive", "golden", "wheat"],
        "West Asian":          ["olive", "medium", "golden", "warm beige", "tan"],
        "North African":       ["olive", "tan", "golden", "caramel", "light brown", "medium brown"],
        "Scandinavian":        ["very fair", "pale", "porcelain", "ivory", "rosy"],
        "North American":      ["fair", "light", "medium", "olive", "tan", "brown", "dark brown"],
        "Arctic":              ["light", "fair", "golden", "ruddy"],
        "Southeast Asian":     ["light brown", "medium brown", "tan", "golden", "caramel"],
        "Balkan":              ["light", "medium", "olive", "golden", "tan"],
        "Polynesian":          ["golden brown", "tan", "bronze", "deep brown"],
        "Micronesian":         ["tan", "golden brown", "bronze", "medium brown"],
        "Melanesian":          ["deep brown", "dark brown", "ebony", "rich brown"],
        "Indigenous American": ["tan", "copper", "bronze", "reddish-brown", "golden brown"],
        "Australasian":        ["fair", "tan", "golden", "deep brown", "reddish-brown"],
        "Caucasian":           ["fair", "light", "pale", "ivory", "rosy", "peach", "beige"],
        "East Asian":          ["light", "fair", "ivory", "warm beige", "golden", "porcelain"],
        "South Asian":         ["tan", "caramel", "honey", "golden brown", "deep brown", "wheat", "bronze"]
    }

    skin_characteristics = [
        "smooth", "soft", "silky", "velvety", "radiant", "glowing", "dewy", "matte", "textured", "porous", 
        "freckled", "sun-kissed", "weathered", "leathery", "wrinkled", "lined", "age-spotted", "blemished", 
        "scarred", "pockmarked", "clear", "unblemished", "youthful", "mature", "supple", "firm", "taut", 
        "saggy", "dry", "oily", "combination", "sensitive", "rough", "calloused", "flushed", "ruddy", "pale", 
        "sallow", "ashen", "vibrant", "luminous", "dull", "mottled", "patchy", "even-toned", "uneven", 
        "translucent", "opaque", "porcelain-like", "alabaster-like", "bronzed", "sun-damaged", "tanned", 
        "untanned", "sunburnt", "detailed", "detailed texture", "detailed pores", "lustrous", "healthy", 
        "glassy", "plump", "hydrated", "moisturized", "flaky", "peeling", "bumpy", "dimpled", "velvety", 
        "buttery", "waxy", "papery", "tight", "loose", "elastic", "toned", "polished", "raw", "chapped"
    ]

    all_skin_tones = get_all_unique_dict_values(skin_tones_by_ethnicity_dict)
    if ethnicity_group is None or random.random() > stereotype_prob:
        tone = random.choice(all_skin_tones)
    else:
        tone = random.choice(skin_tones_by_ethnicity_dict.get(ethnicity_group, all_skin_tones))

    characteristic = random.choice(skin_characteristics)

    return f"{tone}, {characteristic} skin"

def get_hats_and_headwear(sex_group=None, stereotype_prob=0.8):
    hats_and_headwear_dict = {
        "Male": [
            "wearing a baseball cap", "with a fedora", "sporting a beanie", "with a flat cap", "wearing a turban",
            "wearing a cowboy hat", "with a top hat", "wearing a bowler hat", "with a newsboy cap",
            "sporting a trucker hat", "with a bucket hat", "wearing a military cap", "with a golf visor",
            "sporting a bandana", "with a beret", "wearing a sombrero", "with a turban", "with a mexican hat",
            "sporting a kippah", "with a fez", "wearing a ushanka", "with a porkpie hat", "with a scarf",
            "sporting a panama hat", "with a boater hat", "wearing a deerstalker", "with a trapper hat",
            "sporting a taqiyah", "with a tam o' shanter", "wearing a tricorn hat", "with a helmet", "with a steampunk top hat",
            "over-ear headphones", "with an earpice", "with in-ear headphones", "with a bluetooth headset", "with a VR headset",
        ],
        "Female": [
            "wearing a sun hat", "with a beret", "sporting a fascinator", "with a cloche hat", "with a traditional Chinese hair stick",
            "wearing a pillbox hat", "with a wide-brimmed hat", "with a headband", "with a headscarf",
            "wearing a beanie", "with a flower crown", "sporting a fedora", "with a bucket hat", "wearing a hijab",
            "wearing a turban", "with a baseball cap", "sporting a bandana", "with a hijab", "wearing a hair wrap",
            "wearing a veiled hat", "with a cowboy hat", "sporting a newsboy cap", "with a beret", "wearing a beaded African headwrap",
            "wearing a knit hat", "with a visor", "sporting a bonnet", "with a tam hat", "with a scarf",
            "wearing a cocktail hat", "with a lampshade hat", "sporting a toque", "with a furry trapper hat",
            "over-ear headphones", "with an earpice", "with in-ear headphones", "with a bluetooth headset", "with a VR headset",
        ],
        "Unisex": [
            "with a beanie", "wearing a snapback cap", "sporting a bucket hat", "with a bandana", "with a scrunchie", 
            "with a traditional Native American headdress", "with a military beret",
            "wearing a fedora", "with a baseball cap", "sporting a sun visor", "with a headband", "with hairpins",
            "wearing a flat cap", "with a beret", "sporting a trucker hat", "with a cowboy hat", "with a summer scarf",
            "wearing a knit cap", "with a military cap", "sporting a panama hat", "with a headscarf", "wearing a traditional Russian ushanka",
            "wearing a turban", "with a boonie hat", "sporting a cadet cap", "with a helmet", "with a winter scarf", "wearing a crown",
            "wearing a straw hat", "with a ski mask", "sporting a floppy hat", "with a trapper hat", "with decorative bobby pins",
        ]
    }
    
    all_headwear = get_all_unique_dict_values(hats_and_headwear_dict)
    if sex_group is None or random.random() > stereotype_prob:
        return random.choice(all_headwear)
    else:
        return random.choice(hats_and_headwear_dict.get(sex_group, all_headwear))

def get_random_jewelry(sex_group=None, stereotype_prob=0.8):
    jewelry_dict = {
        "Male": [
            "wearing a chain necklace", "with a simple ear stud", "sporting a bolo tie",
            "with a dog tag necklace", "wearing a small hoop earring", "with a nose stud",
            "sporting a tribal necklace", "with an eyebrow ring", "wearing a leather cord necklace",
            "with a septum piercing", "sporting a single diamond stud", "with a small gauge ear piercing",
            "wearing a thin gold chain", "with a curved barbell eyebrow piercing", "with a bowtie", 
            "with multiple ear piercings", "wearing a pendant necklace", "with a helix ear piercing",
            "with a tragus piercing", "wearing a silver chain", "sporting a shark tooth necklace",
            "with a crystal stud earring", "sporting a tongue piercing", "with a daith piercing"
        ],
        "Female": [
            "wearing hoop earrings", "with a pendant necklace", "sporting a choker", "with an eyebrow ring",
            "with pearl earrings", "wearing a statement necklace", "with chandelier earrings",
            "sporting a delicate chain necklace", "with a nose stud", "wearing drop earrings",
            "with a pearl choker", "sporting multiple ear piercings", "with a septum ring",
            "wearing a locket necklace", "with stud earrings", "sporting a bib necklace", "wearing geometric earrings",
            "with a nose ring", "wearing tassel earrings", "with a layered necklace", "wearing climbing vine ear cuffs",
            "sporting a crystal choker", "with ear cuffs", "wearing a cameo necklace",
            "with a tongue piercing", "sporting dangle earrings", "with a septum clicker", "with a bowtie",
        ],
        "Unisex": [
            "wearing a simple necklace", "with stud earrings", "sporting a nose ring", "with a bowtie",
            "with a choker", "wearing a pendant", "with multiple ear piercings", "with a dermal piercing",
            "sporting an ear cuff", "with a septum piercing", "wearing a chain necklace", 
            "with hoop earrings", "sporting a tongue stud", "with a cartilage piercing",
            "wearing a beaded necklace", "with a tragus piercing", "sporting a nose stud",
            "with an industrial bar piercing", "wearing a collar necklace", "with a conch piercing",
            "sporting a septum ring", "with dangle earrings", "wearing a torque necklace",
            "with a labret piercing", "sporting a helix piercing", "with a rook piercing", "with a lip ring", 
        ]
    }
    
    all_jewelry = get_all_unique_dict_values(jewelry_dict)
    if sex_group is None or random.random() > stereotype_prob:
        return random.choice(all_jewelry)
    else:
        return random.choice(jewelry_dict.get(sex_group, all_jewelry))

def get_weight_description():
    weight_descriptions_list = [
        "very slim", "slender", "lean", "willowy", "lithe", "waifish", "svelte",
        "thin", "skinny", "gaunt", "bony", "emaciated",
        "of average build", "with a moderate frame", "neither thin nor overweight",
        "with a balanced physique", "of normal weight", "with a typical body type",
        "curvy", "full-figured", "plump", "chubby", "rounded", "soft",
        "with a bit of extra weight", "slightly heavy-set",
        "heavyset", "portly", "stout", "corpulent", "rotund", "plush",
        "plus-sized", "full-bodied", "generously proportioned",
        "muscular", "athletic", "well-built", "toned", "fit", "strapping",
        "brawny", "burly", "robust", "solid",
        "with a unique body type", "with a distinctive physique",
        "with an unconventional build", "with an interesting silhouette"
    ]
    return random.choice(weight_descriptions_list)

def get_random_time_of_day():
    times_of_day_list = [
        'at dawn', 'at dusk', 'at twilight', 'during sunset', 'during sunrise', 'at midnight', 'at afternoon', 
        'at late afternoon', 'at golden hour', 'at midday', 'at noon', 'at night', 'in the evening', 'in the morning', 
        'in the afternoon', 'at the stroke of midnight', 'in the wee hours', 'at the crack of dawn', 'at high noon',
        'during the witching hour', 'at brunch time', 'during tea time', 'at supper time', 'during happy hour',
        'at the eleventh hour', 'during siesta time', 'at bedtime', 'at the break of day', 'during the dog days of summer',
        'during early morning', 'during late morning', 'during early evening', 'during late evening',
        'at cocktail hour', 'during lunchtime', 'during dinnertime', 'at the blue hour', 'at the magic hour',
        'during rush hour', 'at daybreak', 'at sundown', 'during civil twilight', 'during nautical twilight',
        'during astronomical twilight', 'at first light', 'at last light', 'during solar noon', 'during solar midnight'
    ]
    time_of_day = np.random.choice(times_of_day_list)
    return time_of_day

def get_random_weather_condition():
    weather_conditions_list = [
        'while its raining', 'while its snowing', 'when scorching hot', 'in perfect weather',
        'during a thunderstorm', 'during a heatwave', 'during a cold snap', 'during a drizzle', 'during a hailstorm',
        'during a sandstorm', 'during a snowstorm', 'during a windstorm', 'during a foggy day', 'during a cloudy day',
        'during a sunny day', 'during an overcast day', 'during a monsoon', 'during a hurricane', 'during a tornado',
        'during a blizzard', 'during an earthquake', 'during a solar eclipse', 'during a lunar eclipse', 'during a meteor shower',
        'during high tide', 'during low tide', 'during a rainbow', 'during a flood', 'during a drought',
        'during a wildfire', 'during a volcanic eruption', 'during an avalanche', 'during a cyclone', 'during a typhoon',
        'during an ice storm', 'during a misty morning', 'during a humid afternoon', 'during a dry evening', 'during a muggy night'
    ]
    weather_condition = np.random.choice(weather_conditions_list)
    return weather_condition

def get_eye_description(ethnicity_group=None, stereotype_prob=0.3):

    all_eye_colors = get_all_unique_dict_values(eye_colors_dict)
    if ethnicity_group is None or np.random.rand() > stereotype_prob:
        eye_colors_list = all_eye_colors
    else:
        eye_colors_list = eye_colors_dict.get(ethnicity_group, all_eye_colors)

    color = random.choice(eye_colors_list)

    eye_styles_list = [
        "Captivating {color} eyes", "{color} eyes lost in thought", "Piercing {color} eyes", 'huge {color} eyes',
        "Striking {color} eyes", "{color} eyes, large and expressive", "Small {color} eyes", 'large {color} eyes',
        "Inviting {color} eyes", "{color} eyes, wide open", "Closed, {color} eyes", "Sparkling {color} eyes",
        "Twinkling {color} eyes", "plain {color} eyes", "Regular {color} eyes", "Mysterious {color} eyes",
        "Expressive {color} eyes", "Glistening {color} eyes", "Luminous {color} eyes", "Focused {color} eyes",
        "Dreamy {color} eyes", "intense {color} eyes", "Gentle {color} eyes", "Curious {color} eyes", 'large striking {color} eyes',
        "Alluring {color} eyes", "haunting {color} eyes", "Innocent {color} eyes", "Hypnotic {color} eyes",
        "Mesmerizing {color} eyes", "animated {color} eyes", "Sleepy {color} eyes", "Observant {color} eyes",
        "deep set {color} eyes", "bulging {color} eyes", "Hooded, {color} eyes", "Almond shaped, {color} eyes",
        "round {color} eyes", "wide {color} eyes", "Narrow, {color} eyes", "Cat-like {color} eyes",
        "winking {color} eyes", "dilated pupil, {color} eyes", "{color} eyes with an enigmatic hue", 'deep {color} eyes',
        "{color} eyes radiating vibrant light", "{color} eyes deep in introspection", "{color} eyes resolute and firm",
        "{color} eyes reflecting a whimsical sparkle", "{color} eyes sorrowful and deep", 
        "{color} eyes brimming with joy", "{color} eyes in a contemplative state", 
        "{color} eyes emanating serenity", "{color} eyes ablaze with intensity"
    ]

    eye_style = random.choice(eye_styles_list)
    eye_description = eye_style.format(color=color)

    return eye_description

def get_clothing_description(sex_group=None, sterotype_prob=0.6):

    clothing_color_list = random.choice([
        "red", "blue", "green", "yellow", "purple", "pink", "orange",
        "black", "white", "gray", "brown", "navy", "teal", "maroon", "olive",
        "beige", "turquoise", "lavender", "crimson", "indigo", "magenta",
        "chartreuse", "burgundy", "periwinkle", "coral", "mustard", "plum",
        "khaki", "mauve", "salmon", "mint", "gold", "silver", "bronze",
        "copper", "platinum", "pastel pink", "pastel blue", "pastel green",
        "pastel yellow", "pastel purple", "emerald", "sapphire", "ruby",
        "amethyst", "topaz", "garnet", "ivory", "cream", "tan", "taupe",
        "charcoal", "slate"
    ])

    clothing_patterns_list = [
        "striped", "polka dot", "floral", "plaid", "checkered", "paisley",
        "herringbone", "houndstooth", "geometric pattern"
    ]

    clothing_types_dict = {
        "casual": {
            "neutral": ["t-shirt", "jeans", "sweater", "hoodie", "shorts", "tracksuit"],
            "male": ["polo shirt"],
            "female": ["leggings", "yoga pants", "tank top"]
        },
        "formal": {
            "neutral": ["suit"],
            "male": ["tuxedo", "dress shirt", "tie"],
            "female": ["cocktail dress", "evening gown", "blouse", "pencil skirt"]
        },
        "professional": {
            "neutral": ["business suit", "slacks"],
            "male": ["necktie"],
            "female": ["pantsuit", "blouse", "knee-length skirt"]
        },
        "outerwear": {
            "neutral": ["jacket", "coat", "trench coat", "parka", "windbreaker", "peacoat", "leather jacket", "denim jacket", "bomber jacket"]
        },
        "dresses_skirts": {
            "female": ["sundress", "maxi dress", "mini skirt", "midi skirt", "wrap dress", "shirt dress", "A-line dress", "pleated skirt", "tulle skirt"],
            "male": ["scottish kilt", "togas"]
        },
        "ethnic": {
            "neutral": ["kaftan", "poncho", "tunic"],
            "male": ["kurta", "sherwani", "kilt", "lederhosen"],
            "female": ["sari", "cheongsam", "dirndl", "ao dai", "hanbok", "qipao", "yukata"]
        },
        "uniform": {
            "neutral": ["military uniform", "police uniform", "firefighter uniform", "doctor's white coat", "chef's uniform", "pilot's uniform", "nurse's scrubs", "judge's robe", "academic regalia"]
        },
        "sports": {
            "neutral": ["soccer jersey", "basketball uniform", "tennis whites", "cycling gear", "martial arts gi"],
            "female": ["leotard", "gymnast outfit", "yoga attire"]
        },
        "workwear": {
            "neutral": ["overalls", "coveralls", "high-visibility vest", "lab coat", "welder's protective gear"]
        },
        "unique": {
            "neutral": ["avant-garde designer piece", "futuristic bodysuit", "steampunk-inspired outfit", "cyberpunk ensemble"]
        },
        "historical": {
            "neutral": ["Renaissance costume"],
            "male": ["Victorian-era suit", "1920s gangster style"],
            "female": ["Victorian-era dress", "1920s flapper style", "1950s rockabilly fashion"]
        },
        "religious": {
            "neutral": ["ceremonial tribal wear", "traditional wedding attire"],
            "male": ["monk's robe"],
            "female": ["nun's habit"]
        }
    }

    if sex_group is None or np.random.rand() > sterotype_prob:
        sex_group = random.choice(['male', 'female', 'neutral'])

    assert sex_group in ['male', 'female', 'neutral']

    clothing_category = random.choice(list(clothing_types_dict.keys()))
    all_clothing_category_items = get_all_unique_dict_values(clothing_types_dict[clothing_category])
    clothing_items_list = clothing_types_dict[clothing_category].get(sex_group, all_clothing_category_items)
    if sex_group in ['male', 'female']:
        clothing_items_list += clothing_types_dict[clothing_category].get('neutral', all_clothing_category_items)
    
    clothing_item = random.choice(clothing_items_list)
    clothing_color = random.choice(clothing_color_list)

    if clothing_category in ["casual", "formal", "professional", "outerwear", "dresses_skirts"]:
        if random.random() < 0.2:
            pattern = random.choice(clothing_patterns_list)
            clothing_str = f"wearing a {pattern} {clothing_item}"
        else:
            clothing_str = f"wearing a {clothing_color} {clothing_item}"
    elif clothing_category in ["ethnic", "unique", "historical"]:
        clothing_str = f"dressed in {clothing_item}"
    elif clothing_category in ["uniform", "sports", "workwear", "religious"]:
        clothing_str = f"in {clothing_item}"
    else:
        clothing_str = f"wearing {clothing_item}"

    # if we are not using a stereotype, we can mix and match clothing items from different categories
    if np.random.rand() > sterotype_prob:
        clothing_items_list = [item for sublist in clothing_types_dict[clothing_category].values() for item in sublist]
        clothing_str = f"wearing {random.choice(clothing_items_list)}"
        return clothing_str

    return clothing_str

def get_random_modifier_string():

    modifier_str = ''
    if np.random.rand() < 0.2:
        modifier_str = modifier_str + 'wearing traditional attire, '
    if np.random.rand() < 0.1:
        modifier_str = modifier_str + 'casual pose, '
    if np.random.rand() < 0.3:
        modifier_str = modifier_str + np.random.choice(['photography, ', 'professional photography, ', 'photorealism, ', 'ultrarealistic uhd faces, '])
    if np.random.rand() < 0.2:
        modifier_str = modifier_str + np.random.choice(['hyper realism, ',  'realistic, ', 'ultra realistic, ',
                                                        'highly detailed, ', 'very detailed, ', 'hyper detailed, ', 'detailed, '])
    if np.random.rand() < 0.6:
        modifier_str = modifier_str + np.random.choice(['detailed skin, ', 'detailed skin texture, ', 'detailed skin pores, '])
    if np.random.rand() < 0.5:
        modifier_str = modifier_str + 'bokeh, '
    if np.random.rand() < 0.5:
        modifier_str = modifier_str + np.random.choice(['film, ', 'still from a film, ', 'raw candid cinema, ', 'cinematic movie still, '])
    if np.random.rand() < 0.2:
        modifier_str = modifier_str + np.random.choice(['head shot, ', 'medium shot, ', 'wide shot, ', 'zoomed out, '])
    if np.random.rand() < 0.2:
        modifier_str = modifier_str + np.random.choice(['triadic color scheme, ', 'vivid color, ', 'remarkable color, ', 'color graded, '])
    if np.random.rand() < 0.5:
        modifier_str = modifier_str + np.random.choice(['studio lighting, ', 'volumetric lighting, ', 'subsurface scatter, ', 'natural light, ', 'soft light, ', 'hard light, ',
                                                        'atmospheric lighting, ', 'cinematic lighting, ', 'dramatic lighting, ', 'hard rim lighting photography, '])
    if np.random.rand() < 0.3:
        modifier_str = modifier_str + np.random.choice(['4k, ', '8k, ', 'uhd, ', 'ultra hd, ', 'high quality, ', 'HDR, '])
    if np.random.rand() < 0.3:
        modifier_str = modifier_str + np.random.choice(['nikon d850, ', 'kodachrome 25, ', 'kodak ultra max 800, ', 'kodak portra 160, ', 'DSLR camera, ',
                                                        'canon eos r3, ', 'Ilford HP5 400, ', 'samsung nx300m, ', 'sony a6000, ', 'olympus om-d, ',
                                                        'panasonic lumix dmc-gx85, ', 'fujifilm x70, ', 'canon eos, ', 'color graded porta 400 film, '])
    if np.random.rand() < 0.1:
        modifier_str = modifier_str + np.random.choice(['120mm, ', '85mm, ', '50mm, ', '35mm, '])
    if np.random.rand() < 0.1:
        modifier_str = modifier_str + np.random.choice(['f/1.4, ', 'f/2.5, ', 'f/3.2, ', 'f/1.1, '])
    if np.random.rand() < 0.1:
        modifier_str = modifier_str + np.random.choice(['35mm film roll photo, ', 'film, ', 'porta 400 film, '])
    if np.random.rand() < 0.1:
        modifier_str = modifier_str + np.random.choice(['iso 120, ', 'iso 210, '])
    if np.random.rand() < 0.1:
        modifier_str = modifier_str + np.random.choice(['wide lens, ', 'lens flare, ', 'sharp focus, ', 'hasselblad, '])
    if np.random.rand() < 0.3:
        modifier_str = modifier_str + np.random.choice(['alluring, ', 'beautiful, ', 'breath-taking, ', 'captivating, ', 'addorable, ', 'intricate, ',
                                                        'chic, ', 'classy, ', 'curvaceous, ', 'breath-cute, ', 'fashionable, ', 'elegant, ',
                                                        'gorgeous, ', 'graceful, ', 'lovely, ', 'mesmerizing, ', 'petite, ', 'pretty, ', 'tall, ',
                                                        'radiant, ', 'ravishing, ', 'slim, ', 'stunning, ', 'stylish, ', 'sultry, ', 'sweet, ',
                                                        'affectionate, ', 'ardent, ', 'articulate, ', 'at ease, ', 'attentive, ', 'awake, ',
                                                        'aware, ', 'boyish, ', 'brave, ', 'broad-shouldered, ', 'calm, ', 'voluptuous, ',
                                                        'caring, ', 'centered, ', 'charming, ', 'chiseled cheekbones, ', 'sharp features, ',
                                                        'classic good looks, ', 'clean-shaven, ', 'clever, ', 'compassionate, ', 'candid, ', 'attractive, ',
                                                        'confident, ', 'conscious, ', 'considerate, ', 'content, ', 'cosmopolitan, ',
                                                        'courageous, ', 'courteous, ', 'cultured, ', 'dark skin, ', 'dashing, ',
                                                        'debonair, ', 'defined jawline, ', 'devoted, ', 'educated, ', 'eloquent, ',
                                                        'faithful, ', 'fearless, ', 'firm skin, ', 'focused, ', 'full lips, ',
                                                        'fully engaged, ', 'gentle, ', 'glowing skin, ', 'grounded, ', 'handsome, ',
                                                        'handsome features, ', 'in the moment, ', 'insightful, ', 'intelligent, ',
                                                        'intense, ', 'kind, ', 'loyal, ', 'mannerly, ', 'mischievous, ', 'muscular, ',
                                                        'olive skin, ', 'passionate, ', 'peaceful, ', 'polite, ', 'porcelain skin, ',
                                                        'present, ', 'refined, ', 'reliable, ', 'rugged, ', 'secure, ', 'self assured, ',
                                                        'sensitive, ', 'serene, ', 'smooth skin, ', 'soft skin, ', 'sophisticated, ',
                                                        'square-jawed, ', 'stable, ', 'strong, ', 'strong chin, ', 'suave, ',
                                                        'sun kissed skin, ', 'thick, ', 'trustworthy, ', 'twinkling eyes, ', 'urbane, ',
                                                        'well mannered, ', 'well spoken, ', 'well traveled, ', 'well-built, ', 'witty, ', 'worldly, '])
    if np.random.rand() < 0.2:
        modifier_str = modifier_str + np.random.choice(['subtle shadows, ', 'shadow, '])
    if np.random.rand() < 0.05:
        modifier_str = modifier_str + np.random.choice(['mist, ', 'wet, ', 'foggy background, '])
    if np.random.rand() < 0.05:
        modifier_str = modifier_str + 'golden ratio composition, '
    if np.random.rand() < 0.05:
        modifier_str = modifier_str + 'dramatic, '
    if np.random.rand() < 0.05:
        modifier_str = modifier_str + 'award winning photograph, '
    if np.random.rand() < 0.05:
        modifier_str = modifier_str + 'epic composition, '
    if np.random.rand() < 0.05:
        modifier_str = modifier_str + 'pexels, '
    if np.random.rand() < 0.05:
        modifier_str = modifier_str + 'high contrast, '

    return modifier_str[:-2]

def get_all_unique_dict_values(property_dict):
    all_unique_values = []
    for key, values in property_dict.items():
        all_unique_values.extend(values)
    all_unique_values = list(set(all_unique_values))

    return all_unique_values

def get_random_ethnicity(ethnicity_group=None, top_level_prob=0.5):    
    if ethnicity_group is None:
        ethnicity_group = np.random.choice(list(ethnicities_dict.keys()))

    if np.random.rand() < top_level_prob:
        ethnicity = ethnicity_group
    else:
        ethnicity = np.random.choice(ethnicities_dict[ethnicity_group])
    
    return ethnicity

def get_age_sex_ethnicity(ethnicity_group=None, sex_group=None, age_group=None):
    # Age group definitions
    age_groups = {
        "baby": (1, 18),          # 1-18 months
        "toddler": (1, 4),        # 1-4 years
        "child": (4, 12),         # 4-12 years
        "teenager": (13, 19),     # 13-19 years
        "young adult": (18, 38),  # 18-38 years
        "middle-aged": (30, 55),  # 30-55 years
        "elderly": (50, 100)      # 50-100 years
    }

    # sample age group if not provided
    if age_group is None:
        age_group = np.random.choice(list(age_groups.keys()))

    # sample sex group if not provided
    if sex_group is None:
        sex_group = np.random.choice(['male', 'female'])

    # sample ethnicity based on the cluster
    ethnicity = get_random_ethnicity(ethnicity_group)

    # sample age based on age group range
    age_min, age_max = age_groups[age_group]
    age = np.random.choice(range(age_min, age_max + 1))

    if age_group == 'baby':
        sex = 'boy' if sex_group == 'male' else 'girl'
        age_sex_ethnicity_str = f'{age} month old {ethnicity} baby {sex}'
    elif age_group == 'toddler':
        sex = 'boy' if sex_group == 'male' else 'girl'
        age_sex_ethnicity_str = f'{age} year old {ethnicity} toddler {sex}'
    elif age_group == 'child':
        sex = 'boy' if sex_group == 'male' else 'girl'
        age_sex_ethnicity_str = f'{age} year old {ethnicity} {sex}'
    elif age_group == 'teenager':
        sex = 'boy' if sex_group == 'male' else 'girl'
        age_sex_ethnicity_str = f'{age} year old {ethnicity} teenage {sex}'
    elif age_group == 'young adult':
        if sex_group == 'male':
            sex = np.random.choice(['man', 'guy', 'person', 'male', 'brother'])
        else:
            sex = np.random.choice(['woman', 'dame', 'lady', 'female', 'sister'])
        sex = f'young {sex}' if np.random.rand() < 0.25 else sex
        age_sex_ethnicity_str = f'{age} year old {ethnicity} {sex}'
    elif age_group == 'middle-aged':
        if sex_group == 'male':
            sex = np.random.choice(['man', 'guy', 'husband', 'person', 'male', 'father', 'gentleman'])
        else:
            sex = np.random.choice(['woman', 'dame', 'wife', 'lady', 'female', 'mother', 'gentlewoman'])
        sex = f'middle-aged {sex}' if np.random.rand() < 0.25 else sex
        age_sex_ethnicity_str = f'{age} year old {ethnicity} {sex}'
    elif age_group == 'elderly':
        if sex_group == 'male':
            sex = np.random.choice(['man', 'grandfather', 'grandpa', 'person', 'father', 'husband', 'gentleman'])
        else:
            sex = np.random.choice(['woman', 'grandmother', 'grandma', 'lady', 'mother', 'wife', 'gentlewoman'])
        sex = f'elderly {sex}' if np.random.rand() < 0.25 else sex
        age_sex_ethnicity_str = f'{age} year old {ethnicity} {sex}'

    return age_sex_ethnicity_str

def get_random_expression():
    return random.choice(expressions_list)

def get_lighting_atmosphere(lighting_category=None):
    if lighting_category is None:
        lighting_category = random.choice(list(lighting_descriptions_dict.keys()))
    return random.choice(lighting_descriptions_dict[lighting_category])

def get_hair_description(ethnicity_group=None, sterotype_prob=0.3):

    all_hair_colors = get_all_unique_dict_values(hair_colors_dict)
    if ethnicity_group is None or np.random.rand() > sterotype_prob:
        hair_colors_list = all_hair_colors
    else:
        hair_colors_list = hair_colors_dict.get(ethnicity_group, all_hair_colors)
    
    color = random.choice(hair_colors_list)
    style = random.choice(hair_styles_list)
    return f"with {color} {style}"

def get_random_face_pose():
    return random.choice(face_poses_list)


#%% main face prompt generator function

def generate_face_prompt(ethnicity_group=None, sex_group=None, age_group=None,
                         lighting_category=None, num_elements_to_add=None):
    
    # Sample demographic information
    if ethnicity_group is None:
        ethnicity_group = random.choice(list(ethnicities_dict.keys()))
    if sex_group is None:
        sex_group = np.random.choice(['male', 'female'], p=[0.5, 0.5])
    if age_group is None:
        age_group = np.random.choice(['baby', 'toddler', 'child', 'teenager', 'young adult', 'middle-aged', 'elderly'],
                                     p=[0.05, 0.05, 0.05, 0.2, 0.2, 0.2, 0.25])

    # Generate base prompt
    prompt_start = get_prompt_start()
    age_sex_ethnicity = get_age_sex_ethnicity(ethnicity_group, sex_group, age_group)
    base_prompt = f"{prompt_start} {age_sex_ethnicity}, "

    # Generate additional elements
    elements = [
        get_random_face_pose(),
        get_random_gaze_direction(),
        get_hair_description(ethnicity_group),
        get_eye_description(ethnicity_group),
        get_skin_description(ethnicity_group),
        get_clothing_description(sex_group),
        get_hats_and_headwear(sex_group),
        get_random_jewelry(sex_group),
        get_random_glasses(),
        get_random_time_of_day(),
        get_random_weather_condition(),
        get_weight_description(),
        get_random_modifier_string(),
        get_lighting_atmosphere(lighting_category),
    ]
    elements = elements + elements[-3:]  # repeat last 3 elements to increase their probability
    
    # add facial hair possibility when needed
    if sex_group == 'male' and age_group in ['teenager', 'young adult', 'middle-aged', 'elderly']:
        elements.append(ger_facial_hair_description())

    # add makeup possibility when needed
    if sex_group == 'female' and age_group in ['teenager', 'young adult', 'middle-aged', 'elderly']:
        elements.append(get_makeup_description())
        elements = elements + [elements[-1]] # repeat makeup description to increase its probability
        elements = elements + [elements[-1]] # repeat makeup description to increase its probability

    # Randomly select subset of elements
    if num_elements_to_add is None:
        num_elements_to_add = random.randint(4, 9)
    selected_elements = random.sample(elements, min(num_elements_to_add, len(elements)))
    selected_elements = list(set(selected_elements)) # remove duplicates
    selected_elements = [get_random_expression()] + selected_elements # always add expression at the beginning
    selected_elements.append(get_location_setting_background()) # always add location setting background at the end

    # Combine base prompt with selected elements
    full_prompt = f"{base_prompt} {', '.join(selected_elements)}"

    return full_prompt

def display_conditions(conditions_dict):
    print('Conditions:')
    print('-----------')
    for key, value in conditions_dict.items():
        print(f'  {key} = {value}')

def get_formatted_prompt_for_display(prompt, max_line_length=85):
    parts = [part.strip() for part in prompt.split(',')]
    formatted_prompt = ""
    current_line = ""

    for i, part in enumerate(parts):
        if len(current_line) + len(part) > max_line_length:
            if formatted_prompt:
                formatted_prompt += ',\n'
            formatted_prompt += current_line
            current_line = part
        else:
            if current_line:
                current_line += ", " + part
            else:
                current_line = part

    if current_line:
        if formatted_prompt:
            formatted_prompt += ',\n'
        formatted_prompt += current_line

    return formatted_prompt


#%% main test

if __name__ == "__main__":
    num_samples_per_type = 10
    show_conditional_sampling = True
    show_conditional_sampling = False
    print("Generating face prompts...\n")

    all_ethinicity_groups = list(ethnicities_dict.keys())
    all_lighting_categories = list(lighting_descriptions_dict.keys())
    all_age_groups = ['baby', 'toddler', 'child', 'teenager', 'young adult', 'middle-aged', 'elderly']
    all_sex_groups = ['male', 'female']

    print('=' * 100)
    print("1. Unconditioned sampling:")
    for i in range(num_samples_per_type):
        prompt = generate_face_prompt()
        print(f"Prompt {i+1}: \n----------")
        print(get_formatted_prompt_for_display(prompt))
        print()
    print('=' * 100)
    print('\n\n')

    if show_conditional_sampling:
        print('=' * 100)
        print("2. Partially Conditioned sampling:")
        conditions_dict_list = [
            {"ethnicity_group": random.choice(all_ethinicity_groups), 'sex_group': random.choice(all_sex_groups), 'age_group': random.choice(all_age_groups)},
            {"ethnicity_group": random.choice(all_ethinicity_groups), 'sex_group': random.choice(all_sex_groups)},
            {'age_group': random.choice(all_age_groups), "lighting_category": random.choice(all_lighting_categories)},
            {'sex_group': random.choice(all_sex_groups), 'age_group': random.choice(all_age_groups)},
            {'sex_group': random.choice(all_sex_groups), 'age_group': random.choice(all_age_groups)},
            {"lighting_category": random.choice(all_lighting_categories), "num_elements_to_add": np.random.randint(2, 5)},
            {"lighting_category": random.choice(all_lighting_categories), "num_elements_to_add": np.random.randint(4, 10)},
            {"lighting_category": random.choice(all_lighting_categories), "num_elements_to_add": np.random.randint(7, 12)},
        ]

        for i, conditions_dict in enumerate(conditions_dict_list):
            prompt = generate_face_prompt(**conditions_dict)
            print('=' * 80)
            display_conditions(conditions_dict)
            print('-' * 40)
            print(f"Prompt {i+1}: \n---------")
            print(get_formatted_prompt_for_display(prompt))
            print('=' * 80)
        print('=' * 100)
        print('\n\n')

        print('=' * 100)
        print("3. Fully Conditioned sampling:")
        for i in range(num_samples_per_type):
            ethnicity_group = random.choice(all_ethinicity_groups)
            sex_group = random.choice(all_sex_groups)
            age_group = random.choice(all_age_groups)
            lighting_category = random.choice(all_lighting_categories)
            num_elements_to_add = np.random.randint(1, 5)

            prompt = generate_face_prompt(
                ethnicity_group=ethnicity_group, 
                sex_group=sex_group,
                age_group=age_group,
                lighting_category=lighting_category,
                num_elements_to_add=num_elements_to_add
            )

            conditions_dict = {
                'ethnicity_group': ethnicity_group, 
                'sex_group': sex_group,
                'age_group': age_group,
                'lighting_category': lighting_category,
                'num_elements_to_add': num_elements_to_add
            }

            print('=' * 80)
            display_conditions(conditions_dict)
            print('-' * 40)
            print(f"Prompt {i+1}: \n---------")
            print(get_formatted_prompt_for_display(prompt))
            print('=' * 80)
        print('=' * 100)
        print('\n\n')

# %%

================================================================================
================================================================================
explore_dataset.py:
===================
#%% Imports 

import os
import glob
import torch
import open_clip
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from PIL import Image
from extract_pretrained_features import extract_pretrained_features, load_openclip_model
from extract_pretrained_features import collect_pretrained_features_from_folder

#%% Helper functions

def plot_model_distribution(df):
    model_counts = df['model_used'].value_counts().sort_values(ascending=False)
    
    fig, ax = plt.subplots(figsize=(12, 6))
    ax.bar(model_counts.index, model_counts.values)
    ax.set_title('Distribution of Images Across Models')
    ax.set_xlabel('Model')
    ax.set_ylabel('Number of Images')
    plt.xticks(rotation=45, ha='right')
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)

    # Add value labels on top of each bar
    for i, v in enumerate(model_counts.values):
        ax.text(i, v, str(v), ha='center', va='bottom')
    
    return fig

def plot_prompt_length_distribution(df):
    # Calculate prompt lengths
    df['prompt_chars'] = df['text_prompt'].str.len()
    df['prompt_words'] = df['text_prompt'].str.split().str.len()

    # Set up the plot
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))

    # Colors for each model
    models = df['model_used'].unique()
    colors = plt.cm.rainbow(np.linspace(0, 1, len(models)))

    # Plot character length distribution
    for model, color in zip(models, colors):
        model_data = df[df['model_used'] == model]['prompt_chars']
        ax1.hist(model_data, bins=50, alpha=0.5, label=model, color=color)
    
    ax1.set_title('Prompt Length in Characters', fontsize=15)
    ax1.set_xlabel('Number of Characters', fontsize=13)
    ax1.set_ylabel('Frequency', fontsize=13)
    # ax1.set_yscale('log')
    ax1.legend(fontsize=14)

    # Plot word length distribution
    for model, color in zip(models, colors):
        model_data = df[df['model_used'] == model]['prompt_words']
        ax2.hist(model_data, bins=20, alpha=0.5, label=model, color=color)
    
    ax2.set_title('Prompt Length in Words', fontsize=15)
    ax2.set_xlabel('Number of Words', fontsize=13)
    ax2.set_ylabel('Frequency', fontsize=13)
    # ax2.set_yscale('log')
    ax2.legend(fontsize=14)

    plt.tight_layout()
    return fig

def format_prompt(prompt, max_width=85, min_width=55):
    words = prompt.split()
    lines = []
    current_line = ""

    for word in words:
        if len(current_line) + len(word) + 1 > max_width:
            lines.append(current_line)
            current_line = word
        else:
            if current_line:
                current_line += " " + word
            else:
                current_line = word

            if len(current_line) >= min_width and (current_line.endswith(',') or current_line.endswith('.')):
                lines.append(current_line) 
                current_line = ""

    if current_line:
        lines.append(current_line)

    return '\n'.join(lines)

def remove_borders(ax):
    ax.set_xticks([])
    ax.set_yticks([])
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['bottom'].set_visible(False)
    ax.spines['left'].set_visible(False)

def display_single_random_image(df, image_folder, model_to_use=None):
    if model_to_use is not None:
        df_to_use = df[df['model_used'] == model_to_use]
    else:
        df_to_use = df
    
    random_row = df_to_use.sample(n=1).iloc[0]
    image_path = os.path.join(image_folder, random_row['image_filename'])
    
    img = Image.open(image_path)
    
    fig, ax = plt.subplots(figsize=(14, 11))
    plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.2)
    ax.imshow(img)
    
    title = f"Model: {random_row['model_used']}, filename = '{random_row['image_filename']}'"
    ax.set_title(title, fontsize=14)
    
    formatted_prompt = format_prompt(random_row['text_prompt'])
    ax.set_xlabel(formatted_prompt, fontsize=11)
    
    return fig

def display_single_model_images(df, image_folder, model_to_use=None):
    if model_to_use is not None:
        df_to_use = df[df['model_used'] == model_to_use]
    else:
        df_to_use = df

    if len(df_to_use) < 2:
        raise ValueError("Not enough images to display.")

    random_rows = df_to_use.sample(n=2)

    image_paths = []
    titles = []
    prompts = []
    for _, row in random_rows.iterrows():
        image_path = os.path.join(image_folder, row['image_filename'])
        image_paths.append(image_path)
        title = f"Model: {row['model_used']}\nfilename = '{row['image_filename']}'"
        titles.append(title)
        formatted_prompt = format_prompt(row['text_prompt'], max_width=80, min_width=55)
        prompts.append(formatted_prompt)

    fig, axes = plt.subplots(1, 2, figsize=(20, 13))
    fig.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.2, wspace=0.05)

    for ax, img_path, title, prompt in zip(axes, image_paths, titles, prompts):
        img = Image.open(img_path)
        ax.imshow(img)
        ax.set_title(title, fontsize=18)
        remove_borders(ax)
        ax.set_xlabel(prompt, fontsize=15, ha='center', va='top')

    return fig

def display_multi_model_images(df, image_folder, num_cols=5, models_names=None, display_prompt=False):
    if models_names is None:
        models_names = df['model_used'].unique().tolist()
    num_rows = len(models_names)
    
    if display_prompt:
        fig, axes = plt.subplots(num_rows, num_cols, figsize=(4*num_cols, 6*num_rows))
        fig.subplots_adjust(hspace=0.3, wspace=0.2, top=0.9)
    else:
        fig, axes = plt.subplots(num_rows, num_cols, figsize=(4*num_cols, 4*num_rows))
        fig.subplots_adjust(hspace=0.12, wspace=0.04)
    
    for row, model in enumerate(models_names):
        model_df = df[df['model_used'] == model]
        sample_df_rows = model_df.sample(n=min(num_cols, len(model_df)))
        
        for col, (_, sample_df_row) in enumerate(sample_df_rows.iterrows()):
            ax = axes[row, col] if num_rows > 1 else axes[col]
            
            image_path = os.path.join(image_folder, sample_df_row['image_filename'])
            img = Image.open(image_path)
            
            ax.imshow(img)
            remove_borders(ax)

            ax.set_title(sample_df_row['image_filename'], fontsize=10, wrap=True)
            
            if col == 0:
                ax.set_ylabel(model, fontsize=16)
            
            if display_prompt:
                formatted_prompt = format_prompt(sample_df_row['text_prompt'], max_width=40, min_width=20)
                ax.text(0, -0.1, formatted_prompt, fontsize=5, ha='left', va='top', 
                        wrap=True, transform=ax.transAxes)
    
    return fig

#%%

if __name__ == "__main__":

    #%% Set the style of the plots
    plt.style.use('dark_background')

    # Set the paths
    dataset_path = r"SFHQ_T2I_dataset"

    csv_path = os.path.join(dataset_path, "SFHQ_T2I_dataset.csv")
    image_folder = os.path.join(dataset_path, "images")

    # Load the dataset csv file
    df = pd.read_csv(csv_path)

    # print several randomly selected prompts to screen
    for i in range(10):
        random_row = df.sample(n=1).iloc[0]
        curr_row_prompt = random_row['text_prompt']
        num_chars = len(curr_row_prompt)
        num_words = len(curr_row_prompt.split())
        print('=' * 90)
        print(f'num chars: {num_chars}, num words: {num_words}')
        print('-' * 30)
        print(format_prompt(curr_row_prompt))
    print('=' * 90)

    # Plot the distribution of images across models
    fig_distribution = plot_model_distribution(df)

    # Plot the distribution of prompt lengths
    fig_prompt_lengths = plot_prompt_length_distribution(df)

    # Display two random images for each model
    model_to_use = 'FLUX1_pro'
    fig_single_pro = display_single_model_images(df, image_folder, model_to_use=model_to_use)

    model_to_use = 'FLUX1_dev'
    fig_single_dev = display_single_model_images(df, image_folder, model_to_use=model_to_use)

    model_to_use = 'FLUX1_schnell'
    fig_single_schnell = display_single_model_images(df, image_folder, model_to_use=model_to_use)

    model_to_use = 'SDXL'
    fig_single_sdxl = display_single_model_images(df, image_folder, model_to_use=model_to_use)

    model_to_use = 'DALLE3'
    fig_single_dalle3 = display_single_model_images(df, image_folder, model_to_use=model_to_use)

    # Display multiple random images for each model
    models_names = ['FLUX1_pro', 'FLUX1_dev', 'FLUX1_schnell', 'SDXL', 'DALLE3']
    num_cols = 8
    fig_all = display_multi_model_images(df, image_folder, num_cols, models_names=models_names)

    models_names = ['FLUX1_pro', 'FLUX1_dev', 'FLUX1_schnell', 'SDXL']
    num_cols = 6
    fig_good_ones = display_multi_model_images(df, image_folder, num_cols, models_names=models_names)

    models_names = ['FLUX1_pro', 'FLUX1_dev', 'FLUX1_schnell']
    num_cols = 4
    fig_flux = display_multi_model_images(df, image_folder, num_cols, models_names=models_names)

    models_names = ['FLUX1_schnell', 'SDXL']
    num_cols = 4
    fig_bulk = display_multi_model_images(df, image_folder, num_cols, models_names=models_names)

    #%% Optionally, save the figures

    # save_figures = False
    save_figures = True

    if save_figures:
        output_folder_path = "figures"
        os.makedirs(output_folder_path, exist_ok=True)

        fig_distribution.savefig(os.path.join(output_folder_path, 'model_distribution.jpg'), bbox_inches='tight')
        fig_prompt_lengths.savefig(os.path.join(output_folder_path, 'prompt_lengths_distribution.jpg'), bbox_inches='tight')

        fig_single_pro.savefig(os.path.join(output_folder_path, 'FLUX1_pro_images_with_prompts.jpg'), bbox_inches='tight')
        fig_single_dev.savefig(os.path.join(output_folder_path, 'FLUX1_dev_images_with_prompts.jpg'), bbox_inches='tight')
        fig_single_schnell.savefig(os.path.join(output_folder_path, 'FLUX1_schnell_images_with_prompts.jpg'), bbox_inches='tight')
        fig_single_sdxl.savefig(os.path.join(output_folder_path, 'SDXL_images_with_prompts.jpg'), bbox_inches='tight')
        fig_single_dalle3.savefig(os.path.join(output_folder_path, 'DALLE3_images_with_prompts.jpg'), bbox_inches='tight')

        fig_all.savefig(os.path.join(output_folder_path, 'all_model_images.jpg'), bbox_inches='tight')
        fig_good_ones.savefig(os.path.join(output_folder_path, 'good_model_images.jpg'), bbox_inches='tight')
        fig_flux.savefig(os.path.join(output_folder_path, 'flux_images.jpg'), bbox_inches='tight')
        fig_bulk.savefig(os.path.join(output_folder_path, 'FLUX1_schnell_SDXL_images.jpg'), bbox_inches='tight')

    #%% select open clip model to use for textual search

    model_type = 'OpenCLIP'
    model_type = 'SigLIP'

    if model_type == 'OpenCLIP':
        model_name = "OpenCLIP_ViT-H-14-378-quickgelu"
        base_model_name = "ViT-H-14-378-quickgelu"
    elif model_type == 'SigLIP':
        model_name = "OpenCLIP_ViT-SO400M-14-SigLIP-384"
        base_model_name = "ViT-SO400M-14-SigLIP"

    # Extract features
    extract_features = False
    # extract_features = True
    if extract_features:
        extract_pretrained_features(dataset_path, model_to_use=model_name)

    # Collect previously extracted features
    image_features, image_filename_map = collect_pretrained_features_from_folder(dataset_path, model_name, normalize_features=True)
    print(f"Image features shape: {image_features.shape}")

    #%% Load the OpenCLIP model for text encoding

    device = "cuda" if torch.cuda.is_available() else "cpu"
    model, preprocess = load_openclip_model(model_name, device)

    # define some text prompts for exploration
    text_prompts = {
        "age_male": ["baby boy", "boy todller", "child boy", "teenage boy", "adult male", "middle-aged adult male", "elderly male"],
        "age_female": ["baby girl", "girl todller", "child girl", "teenage girl", "adult female", "middle-aged adult female", "elderly female"],
        "expression": ["happy", "sad", "angry", "surprised", "neutral", "disgusted", "fearful", "tounge out"],
        "sex": ["male", "female", "non-binary person"],
        "ethnicity": ["Caucasian", "African", "Asian", "Hispanic", "Middle Eastern", "Scandinavian", "Native American"],
        "hair_color": ["black hair", "brown hair", "blonde hair", "red hair", "gray hair", "bald", "blue hair", "green hair", "pink hair"],
        "hats": ["baseball cap", "fedora", "beanie", "top hat", "cowboy hat", "sun hat"],
        "glasses": ["reading glasses", "sunglasses", "round glasses", "square glasses"],
        "accessories": ["earrings", "necklace", "bandana", "hat", "tie", "scarf", "headphones", "sunglasses"],
        "eye_color": ["blue eyes", "green eyes", "brown eyes", "yellow eyes", "hazel eyes", "red eyes"],
    }

    # define prefixes for each category
    prefixes = {
        "age_male": "A photo of a {prompt}",
        "age_female": "A photo of a {prompt}",
        "expression": "A photo of a person who is {prompt}",
        "sex": "A photo of a {prompt}",
        "ethnicity": "A photo of a {prompt} person",
        "hair_color": "A photo of a person with {prompt}",
        "hats": "A photo of a person wearing a {prompt}",
        "glasses": "A photo of a person wearing {prompt}",
        "accessories": "A photo of a person with {prompt}",
        "eye_color": "A photo of a person with {prompt}",
    }

    # encode the text prompts using the OpenCLIP text encoder
    tokenizer = open_clip.get_tokenizer(base_model_name)
    encoded_text_prompts = {}
    for category, prompts in text_prompts.items():
        prefix = prefixes.get(category, "A photo of a person with {prompt}")
        formatted_prompts = [prefix.format(prompt=prompt) for prompt in prompts]
        tokens = tokenizer(formatted_prompts).to(device)
        with torch.no_grad():
            encoded_text_prompts[category] = model.encode_text(tokens).float().cpu().numpy()
        encoded_text_prompts[category] /= np.linalg.norm(encoded_text_prompts[category], axis=1)[:, np.newaxis]

    # Calculate similarities between the texts and images and visualize results
    def visualize_category(category, image_features, encoded_prompts, prompts, num_cols=5, plot_boxplot=False):
        similarities = np.dot(image_features, encoded_prompts.T)
        
        if plot_boxplot:
            plt.figure(figsize=(12, 6))
            plt.boxplot(similarities)
            plt.title(f"Distribution of {category}")
            plt.xticks(range(1, len(prompts) + 1), prompts, rotation=45)
            plt.ylabel("Similarity Score")
            plt.tight_layout()
            plt.show()

        top_matches = np.argsort(similarities, axis=0)[::-1][:num_cols]
        
        fig = plt.figure(figsize=(3 * num_cols, 3 * len(prompts)))
        fig.patch.set_facecolor('black')
        
        for i, prompt in enumerate(prompts):
            for j in range(num_cols):
                ax = plt.subplot(len(prompts), num_cols, i * num_cols + j + 1)
                ax.set_facecolor('black')
                
                curr_image_filename = image_filename_map[top_matches[j, i]]
                base_filename = os.path.basename(curr_image_filename)
                img = Image.open(curr_image_filename)
                plt.imshow(img)
                plt.title(f"Match {j+1} for: {prompt}\n{base_filename}", fontsize=8, color='white')
                plt.axis('off')
        
        plt.tight_layout()

        return fig

    num_cols = 8
    for category, encoded_prompts in encoded_text_prompts.items():
        fig = visualize_category(category, image_features, encoded_prompts, text_prompts[category], num_cols=num_cols)

        if save_figures:
            figure_name_str = f'textual_search_1_{category}_top_{num_cols}_matches.jpg'
            fig.savefig(os.path.join(output_folder_path, figure_name_str), bbox_inches='tight')
        
    #%% make some textual searches on the dataset

    conditions_dict = {
        "Hair Color": {
            "text_prefix": "",
            "text_strings": ['white or gray hair', 'yellow or blond hair', 'green hair', 'blue hair', 'purple or pink hair', 'red or orange hair']
        },
        "Hair Style": {
            "text_prefix": "",
            "text_strings": ['straight hair', 'curly hair', 'high top hairstyle', 'bob-cut hairstyle', 'afro hairstyle']
        },
        "Hair Style x Sex": {
            "text_prefix": "woman with ",
            "text_strings": ['short blond hair', 'long blond hair', 'short red hair', 'long red hair', 'short black hair', 'long black hair']
        },
        "Makeup": {
            "text_prefix": "woman ",
            "text_strings": ['heavy makeup', 'without makeup', 'red lipstick', 'strong eyeliner', 'traditional makeup']
        },
        "Background Color": {
            "text_prefix": "",
            "text_strings": ['yellow background', 'green background', 'blue background', 'purple background', 'red background']
        },
        "Facial Features": {
            "text_prefix": "",
            "text_strings": ['reading glasses', 'sunglasses', 'bald', 'goatee', 'lipstick']
        },
        "Physical Characteristics": {
            "text_prefix": "",
            "text_strings": ['large or chiseled jaw', 'long white beard', 'fashionable beard', 'wide eyes', 'overweight or chubby']
        },
        "Expression": {
            "text_prefix": "",
            "text_strings": ['angry or enraged', 'surprised', 'smiling', 'sad or depressed', 'grim face', 'tounge out']
        },
        "Expression x Sex": {
            "text_prefix": "man ",
            "text_strings": ['angry or enraged', 'surprised', 'smiling', 'sad or depressed', 'grim face', 'tounge out']
        },
        "Ethnicity": {
            "text_prefix": "",
            "text_strings": ['asian', 'native american', 'african', 'persian', 'south-american', 'irish']
        },
        "Ethnicity x Age 1": {
            "text_prefix": "old age ",
            "text_strings": ['asian', 'native american', 'african', 'persian', 'south-american', 'irish']
        },
        "Ethnicity x Age 2": {
            "text_prefix": "typical adult ",
            "text_strings": ['asian', 'native american', 'african', 'persian', 'south-american', 'irish']
        },
        "Ethnicity x Age 3": {
            "text_prefix": "young child ",
            "text_strings": ['asian', 'native american', 'african', 'persian', 'south-american', 'irish']
        },
        "Age": {
            "text_prefix": "",
            "text_strings": ['10 month old baby', '2.5 year old toddler', 'small child', '16 year old teenager', '30 year old adult', 'wrinkly 70 year old senior']
        },
        "Age x Ethnicity x Sex 1": {
            "text_prefix": "asian female ",
            "text_strings": ['10 month old baby', '2.5 year old toddler', 'small child', '16 year old teenager', '30 year old adult', 'wrinkly 70 year old senior']
        },
        "Age x Ethnicity x Sex 2": {
            "text_prefix": "african male ",
            "text_strings": ['10 month old baby', '2.5 year old toddler', 'small child', '16 year old teenager', '30 year old adult', 'wrinkly 70 year old senior']
        },
        "Accessories": {
            "text_prefix": "person wearing ",
            "text_strings": ['earrings', 'necklace', 'bandana', 'hat', 'tie', 'scarf', 'headphones', 'sunglasses']
        },
        "Hats": {
            "text_prefix": "person wearing ",
            "text_strings": ['baseball cap', 'fedora', 'beanie', 'top hat', 'cowboy hat', 'sun hat']
        },
        "Jewelry": {
            "text_prefix": "person with ",
            "text_strings": ['gold chain', 'pearl necklace', 'earrings', 'diamond', 'crown']
        },
        "Face Pose": {
            "text_prefix": "person ",
            "text_strings": ['looking straight ahead', 'turned sideways', 'tilted upwards', 'tilted downwards', 'three-quarter view', 'profile view']
        },
        "Eye Gaze": {
            "text_prefix": "person ",
            "text_strings": ['looking directly at camera', 'looking to the left', 'looking up', 'looking down', 'eyes closed']
        },
        "Glasses Style": {
            "text_prefix": "person wearing ",
            "text_strings": ['round glasses', 'square glasses', 'cat-eye glasses', 'rimless glasses', 'aviator sunglasses', 'sport sunglasses']
        },
        "Facial Hair": {
            "text_prefix": "man with ",
            "text_strings": ['full beard', 'mustache', 'goatee', 'sideburns', 'stubble', 'shaved face']
        },
        "Lighting": {
            "text_prefix": "",
            "text_strings": ['side light with shadows', 'spotlight', 'soft lighting', 'back lighting', 'golden hour', 'blue hour lighting', 'studio lighting']
        },
        "Background": {
            "text_prefix": "",
            "text_strings": ['urban cityscape', 'natural landscape', 'stone wall background', 'wodden wall background', 'beach background', 'night background']
        },
        "Eye Color": {
            "text_prefix": "person with ",
            "text_strings": ['blue eyes', 'green eyes', 'brown eyes', 'yellow eyes', 'hazel eyes', 'red eyes']
        },
        "bad things": {
            "text_prefix": "",
            "text_strings": ['blurred', 'statue', 'two people', 'back of head', 'hand covering face', 'cat', 'dog', 'animal']
        },
    }

    for selected_condition in conditions_dict.keys():
        print(selected_condition)

        text_prefix = conditions_dict[selected_condition]['text_prefix']
        text_strings = conditions_dict[selected_condition]['text_strings']

        # will randomly display "num_top_images_to_show" among the top "num_top_image_candidates" best matching queries
        num_top_images_to_show = len(text_strings) + 4
        num_top_images_to_show = min(max(4, num_top_images_to_show), 10)
        num_top_image_candidates = int(1.0 * num_top_images_to_show)

        title_fontsize = 20

        # attach prefix and extract text features
        text_strings_full = [(text_prefix + x) for x in text_strings]
        tokenized_text_samples = tokenizer(text_strings_full).to(device)
        with torch.no_grad():
            openclip_text_features = model.encode_text(tokenized_text_samples).float().cpu().numpy()
        openclip_text_features /= np.linalg.norm(openclip_text_features, axis=1)[:, np.newaxis]  # normalize to unit norm

        # perform inner product to get image-text similarity score
        image_text_similarity = np.dot(image_features, openclip_text_features.T)

        num_rows = len(text_strings)
        num_cols = num_top_images_to_show

        fig, ax = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(5 * num_cols, 6 * num_rows))
        fig.patch.set_facecolor('0.0')
        fig.subplots_adjust(left=0.003, right=0.997, bottom=0.01, top=0.91, hspace=0.16, wspace=0.04)
        fig.suptitle(f'Image textual search using OpenCLIP features from synthetic dataset\nCondition: {selected_condition}\nPrefix: "{text_prefix}"', fontsize=30, color='white')

        all_basenames = []
        for row_ind, q_str in enumerate(text_strings):
            # get top "num_top_image_candidates" matching queries sorted from best matching downward
            query_best_inds = list(np.argsort(image_text_similarity[:,row_ind])[-num_top_image_candidates:])
            query_best_inds.reverse()
            # randomly select "num_top_images_to_show" from that list
            query_best_inds = np.random.choice(query_best_inds, size=num_top_images_to_show, replace=False)

            for col_ind in range(num_cols):
                curr_row_filename = image_filename_map[query_best_inds[col_ind]]
                curr_basename = os.path.basename(curr_row_filename)
                curr_image = Image.open(curr_row_filename).convert("RGB")
                ax[row_ind,col_ind].imshow(curr_image)
                ax[row_ind,col_ind].set_axis_off()
                ax[row_ind,col_ind].set_title(f"'{q_str}'\n{curr_basename}", fontsize=title_fontsize, color='white')

                all_basenames.append(curr_basename)

        if save_figures:
            figure_name_str = f'textual_search_2_{selected_condition}_top_{num_top_images_to_show}_matches.jpg'
            fig.savefig(os.path.join(output_folder_path, figure_name_str), bbox_inches='tight')


#%%





================================================================================

================================================================================
================================================================================
api_key_manager.py:
===================
import os
import dotenv

# API Key Configurations
API_KEYS = {
    'GEMINI_API_KEY': 'abcdefghijklmnopqrstuvwxyz1234567890abc',
}

def save_env_file(api_keys_dict=API_KEYS):
    """Create a .env file with API keys if it doesn't exist."""
    env_file = '.env'
    if not os.path.exists(env_file):
        print("Creating .env file...")
        with open(env_file, 'w') as f:
            for key, value in api_keys_dict.items():
                f.write(f"{key}={value}\n")
        print(f".env file created at {os.path.abspath(env_file)}")
        return False
    return True

def setup_api_keys():
    """Load API keys from .env file and set them as environment variables."""
    # Load the .env file
    dotenv.load_dotenv()
    
    # Check if all required keys are present
    missing_keys = [key for key in API_KEYS.keys() if not os.getenv(key)]
    
    if missing_keys:
        print(f"Error: The following API keys are missing in the .env file: {', '.join(missing_keys)}")
        print("Please add them to the .env file and run the script again.")
        return False
    else:
        print("API keys loaded successfully.")
        # Set environment variables
        for key in API_KEYS.keys():
            os.environ[key] = os.getenv(key)
        return True

def initialize_api_keys(api_keys_dict=None):

    # if we are given a dictionary of API keys, override the .env file
    if api_keys_dict is not None:
        env_file_exists = save_env_file(api_keys_dict)

    # check if .env file exists
    env_file_exists = os.path.exists('.env')

    if env_file_exists:
        return setup_api_keys()
    else:
        if save_env_file():
            return setup_api_keys()
    
    return False

def print_api_keys():
    """Print all loaded API keys to the screen."""
    for key in API_KEYS.keys():
        print(f'{key} = {os.environ.get(key, "Not set")}')

if __name__ == "__main__":
    if initialize_api_keys():
        print_api_keys()
    else:
        print("Failed to initialize API keys. Please check your .env file and try again.")

================================================================================
================================================================================
convert_repos_to_text_docs.py:
==============================
#%% Imports

import os
import time
from repo_to_text_core import convert_repos_to_text
from api_key_manager import initialize_api_keys, print_api_keys

#%% Main
if __name__ == "__main__":

    #%% Setup API keys

    API_KEYS = {
        'GEMINI_API_KEY': 'abcdefghijklmnopqrstuvwxyz1234567890abc',
    }

    initialize_api_keys(api_keys_dict=API_KEYS)
    print_api_keys()

    #%% Example usage for GitHub repositories

    github_repos_urls_list = [
        "https://github.com/SelfishGene/SFHQ-dataset",
        "https://github.com/SelfishGene/SFHQ-T2I-dataset",
        "https://github.com/SelfishGene/ImSME-dataset",
        "https://github.com/SelfishGene/neuron_as_deep_net",
        "https://github.com/SelfishGene/my_kaggle_notebooks",
        "https://github.com/SelfishGene/filter_and_fire_neuron",
        "https://github.com/SelfishGene/a_chatgpt_never_forgets",
        "https://github.com/SelfishGene/visual_taste_approximator",
        "https://github.com/anthropics/anthropic-quickstarts",
        "https://github.com/anthropics/anthropic-cookbook",
        "https://github.com/openai/openai-cookbook",
        "https://github.com/openai/openai-python",
    ]
    github_output_dir = r"data/repos_as_text_github_urls"
    
    start_time = time.time()
    print("Processing GitHub repositories...")
    print('---------------------------------')
    convert_repos_to_text(github_repos_urls_list, github_output_dir, is_github=True, 
                          describe_images=True, describe_csv_files=True, process_notebooks=True)

    duration_min = (time.time() - start_time) / 60
    print('---------------------------------------------------------')
    print(f"Finished processing {len(github_repos_urls_list)} GitHub repositories! Took {duration_min:.2f} minutes")
    print('---------------------------------------------------------')

    #%% Example usage for local folders

    local_folder_paths_list = [
        r"code/SFHQ-dataset",
        r"code/SFHQ-T2I-dataset",
        r"code/ImSME-dataset",
        r"code/neuron_as_deep_net",
        r"code/my_kaggle_notebooks",
        r"code/filter_and_fire_neuron",
        r"code/a_chatgpt_never_forgets",
        r"code/visual_taste_approximator",
    ]
    folders_output_dir = r"data/repos_as_text_local_folders"
    
    start_time = time.time()
    print("\nProcessing local folders...")
    print('-----------------------------')
    convert_repos_to_text(local_folder_paths_list, folders_output_dir, is_github=False, 
                          describe_images=True, describe_csv_files=True, process_notebooks=True)

    duration_min = (time.time() - start_time) / 60
    print('---------------------------------------------------------')
    print(f"Finished processing {len(local_folder_paths_list)} local folders! Took {duration_min:.2f} minutes")
    print('---------------------------------------------------------')

    #%% display the first 10000 chars of the file in the output directory of local folders

    filename_to_show = 'SFHQ-T2I-dataset.txt'
    file_path = os.path.join(folders_output_dir, filename_to_show)

    with open(file_path, 'r', encoding='utf-8') as file:
        content = file.read()

    print(content[:10000])

#%%
================================================================================
================================================================================
examples/ImSME-dataset.txt:
===========================
================================================================================
repo title: ImSME-dataset
repo link: https://github.com/SelfishGene/ImSME-dataset
date processed: 2024-10-21
================================================================================
================================================================================
repo file structure:
====================
    config.py
    explore_ImSME_dataset.ipynb
    README.md
    create_equations_dataframe.py
    LICENSE.txt
    generate_ImSME_dataset.py
    download_ImSME_dataset.py
    create_char_images_dataset.py
    requirements.txt
    utils.py
    sample_images/
        sample_image_with_segmented_main_equation_parts.png
        several_equations_with_description_labels.png
        an_equation_with_segmentation_labels.png
================================================================================
================================================================================
README.md:
==========
# ImSME - Images of Simple Math Equations Dataset

ImSME is a dataset of wide images containing simple math equations with detailed annotations. 
The annotations are both detailed textual descriptions as well as precise pixel coordinates of each equation part (at the digits/symbols, and arguments/operations hirarchies).  
This repository contains the scripts used to generate the dataset and perform basic exploratory data analysis.  
The dataset is hosted by kaggle and can be found at [ImSME dataset](https://www.kaggle.com/datasets/selfishgene/imsme-images-of-simple-math-equations)

![](https://github.com/SelfishGene/ImSME-dataset/blob/main/sample_images/sample_image_with_segmented_main_equation_parts.png)

## Dataset Description

- Images of simple math equations with varied widths and a fixed height of 128 pixels.
- Equations are of the form "{argument1} {operation} {argument2} {equal_sign} {result}".
- Characters are rendered using random fonts, sometimes with different fonts for each character.
- Detailed textual descriptions are provided for each equation, including information about fonts and character locations.
- Multiple versions of the dataset are available: tiny (~250MB), small (~1GB), and medium (~4GB).

## Repository Contents

1. `config.py`: Configuration parameters and fixed data dictionaries.
2. `utils.py`: Utility functions used across multiple scripts.
3. `create_char_images_dataset.py`: Script to create the character images dataset.
4. `create_equations_dataframe.py`: Script to create the equations dataframe.
5. `generate_ImSME_dataset.py`: Script to generate equation images from the equations dataframe.
6. `download_ImSME_dataset.py`: Script to download the ImSME dataset from Kaggle.
7. `explore_ImSME_dataset.ipynb`: Jupyter notebook for basic exploratory data analysis of the dataset.
8. `requirements.txt`: List of required Python packages.

### Example of several equations with textual description labels
![](https://github.com/SelfishGene/ImSME-dataset/blob/main/sample_images/several_equations_with_description_labels.png)

## Requirements

To install the required packages, run:

```
pip install -r requirements.txt
```

## Usage

You have two options: download the pre-generated dataset or generate the dataset from scratch.

### Option 1: Download and Explore the Pre-generated Dataset

1. Download the ImSME dataset:
   ```
   python download_ImSME_dataset.py
   ```

   One can also download directly from kaggle on the following link: [ImSME dataset](https://www.kaggle.com/datasets/selfishgene/imsme-images-of-simple-math-equations)

2. Explore the dataset:
   A Jupyter notebook with basic loading and visualization of the dataset is provided - `explore_ImSME_dataset.ipynb`

### Option 2: Generate the Dataset from Scratch

1. Create the character images dataset:
   ```
   python create_char_images_dataset.py
   ```

2. Create the equations dataframe:
   ```
   python create_equations_dataframe.py
   ```

3. Generate ImSME dataset:
   ```
   python generate_ImSME_dataset.py
   ```

4. Explore the dataset:
   A Jupyter notebook with basic loading and visualization of the dataset is provided - `explore_ImSME_dataset.ipynb` 

## Dataset Structure

- `math_equations_images_dataset_{tiny/small/medium}/`
  - `equation_images/`: Folder containing equation images
  - `label_images/`: Folder containing label images with pixel-precise annotations
  - `simple_math_equation_images__*.csv`: CSV file with detailed information about each image

- `simple_math_equations_dataset/`: Contains CSV files with equations in a convenient format
- `char_images_dataset/`: Contains 10,664 black and white character images (128x128)
- `char_images_dataset_tight/`: Similar to `char_images_dataset`, but with tighter cropping around the characters

## Citation

If you use this dataset in your research, please cite it as follows:

```
@dataset{ImSME_2024,
  title={ImSME: Images of Simple Math Equations Dataset},
  author={David Beniaguev},
  year={2024},
  url={https://github.com/SelfishGene/ImSME-dataset},
  DOI={10.34740/KAGGLE/DSV/9026395},
  publisher={GitHub},
}
```

## License

MIT License

### Example of an equation with segmentation labels
![](https://github.com/SelfishGene/ImSME-dataset/blob/main/sample_images/an_equation_with_segmentation_labels.png)

================================================================================
================================================================================
explore_ImSME_dataset.ipynb:
============================
------------------------------------------------------------
Cell index: 1
Input Cell Type: python
Input Text:
-----------
#%% Display a few samples from the dataset and their descriptions

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import random
import glob
from PIL import Image

# Set the path to your dataset
dataset_foldername = 'math_equations_images_dataset_medium'
curr_file_path = os.path.abspath(os.path.dirname(__file__))
# curr_file_path = r"C:\Users\david\Desktop\Code\ImSME-dataset"

downloaded_from_kaggle = True
if downloaded_from_kaggle:
    dataset_folder = os.path.join(curr_file_path, 'data', dataset_foldername, dataset_foldername)
else:
    dataset_folder = os.path.join(curr_file_path, 'data', dataset_foldername)

csv_file = glob.glob(os.path.join(dataset_folder, 'simple_math_equation_images__*.csv'))[0]

# Load the CSV file
df = pd.read_csv(os.path.join(dataset_folder, csv_file))

# Number of samples to display
num_samples_to_show = 6

# Randomly sample rows from the dataframe
sampled_rows = df.sample(n=num_samples_to_show)

# Create a figure with num_samples_to_show rows and 1 column
fig, axs = plt.subplots(num_samples_to_show, 1, figsize=(14, 3 * num_samples_to_show))

for i, (_, row) in enumerate(sampled_rows.iterrows()):
    # Load the image
    img_path = os.path.join(dataset_folder, 'equation_images', row['image_filename'])
    img = Image.open(img_path)
    
    # Display the image
    axs[i].imshow(img, cmap='gray')
    
    # Set the title (simple description + '\n' + additional description)
    axs[i].set_title(row['simple_description'] + '\n' + row['additional_description'], wrap=True)

plt.tight_layout()



Output Text:
------------
<Figure size 1400x1800 with 6 Axes>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image contains a series of six equations, each displayed as a separate image block.  Each equation image block shows a simple arithmetic calculation (multiplication, subtraction, division, or addition) involving two numbers and their result. The numbers and mathematical symbols are rendered using different fonts and sizes, resulting in a non-uniform and somewhat stylized appearance.

Each block includes descriptive text above the equation image. This text provides details about the equation's image size, the specific arithmetic operation used, and information regarding the font variations and the positional coordinates of the numbers, symbols, and the result within the image.  These coordinates appear to be relative to a pixel grid system within each image block. The descriptive text consistently mentions the number of digits in each argument and the result, providing a structured analysis of the visual layout.

The overall structure suggests a systematic presentation of example images, possibly for a research project or documentation related to optical character recognition (OCR) or image processing techniques. The emphasis on font variations and positional inconsistencies within the equation images highlights the challenges these factors pose in automatically interpreting such images.


------------------------------------------------------------
Cell index: 2
Input Cell Type: markdown
Input Text:
-----------
# Print basic summary statistics

Output Text:
------------


------------------------------------------------------------
Cell index: 3
Input Cell Type: python
Input Text:
-----------
# Print basic summary statistics

print('-' * 78)
print('Counts of unique values in the operation column sorted by frequency:')
print('-' * 68)
operation_counts = df['operation'].value_counts()
for op, count in operation_counts.items():
    print(f'"{op}": {count}')

for column in ['first argument', 'second argument', 'result']:
    print('-' * 78)
    print(f'4 most/least common {column} values and their counts:')
    print('-' * 60)
    value_counts = df[column].value_counts()
    for value, count in value_counts.head(4).items():
        print(f'"{value}": {count}')
    print('...')
    for value, count in value_counts.tail(4).items():
        print(f'"{value}": {count}')

print('-' * 78)


Output Text:
------------
------------------------------------------------------------------------------
Counts of unique values in the operation column sorted by frequency:
--------------------------------------------------------------------
"-": 65536
"*": 65536
"+": 65536
"/": 65536
------------------------------------------------------------------------------
4 most/least common first argument values and their counts:
------------------------------------------------------------
"0": 5136
"1": 3984
"2": 1976
"3": 1360
...
"7742": 8
"8118": 8
"8831": 8
"9308": 8
------------------------------------------------------------------------------
4 most/least common second argument values and their counts:
------------------------------------------------------------
"0": 36752
"1": 5120
"2": 3328
"3": 2496
...
"763": 48
"977": 48
"952": 48
"943": 40
------------------------------------------------------------------------------
4 most/least common result values and their counts:
------------------------------------------------------------
"0": 41544
"3": 1352
"6": 1328
"1": 1312
...
"397": 88
"465": 80
"459": 80
"559": 72
------------------------------------------------------------------------------


------------------------------------------------------------
Cell index: 4
Input Cell Type: markdown
Input Text:
-----------
# Display a single equation image with its labels

Output Text:
------------


------------------------------------------------------------
Cell index: 5
Input Cell Type: python
Input Text:
-----------
# Display a single equation image with its labels

# Randomly select a single row from the dataframe
selected_row = df.sample(n=1).iloc[0]

# Load the equation image
eq_img_path = os.path.join(dataset_folder, 'equation_images', selected_row['image_filename'])
eq_img = Image.open(eq_img_path)

# Load the labels image
label_img_path = os.path.join(dataset_folder, 'label_images', selected_row['image_filename'])
label_img = Image.open(label_img_path)

# Create the first figure with 2x1 subplot
fig1, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 6))

# Display the equation image
ax1.imshow(eq_img, cmap='gray')
ax1.set_title("Equation Image")
ax1.axis('off')

# Display the full labels image
ax2.imshow(label_img, cmap='gray')
ax2.set_title("Labels Image")
ax2.axis('off')

plt.tight_layout()
plt.show()


Output Text:
------------
<Figure size 1500x600 with 2 Axes>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a vertically stacked pair of horizontal black rectangles. The top rectangle contains a mathematical equation written in white, large, stylized font. The equation is "71 + 382 ÷ 453".  The numbers are clearly visible and well-spaced. The top rectangle is labeled "Equation Image".


The bottom rectangle is significantly less clear. It's filled with thin, horizontal, white lines of varying lengths and spacing, creating a somewhat chaotic or noisy appearance. These lines are not uniform in thickness or alignment, suggesting they may be a representation of data rather than a structured image. The bottom rectangle is labeled "Labels Image".

The overall impression is that the top rectangle shows the input image of a handwritten equation, while the bottom rectangle shows a corresponding output or processed representation, perhaps related to character segmentation or labeling in an optical character recognition (OCR) process.  The bottom image appears to represent character bounding boxes or similar data derived from the equation image.


------------------------------------------------------------
Cell index: 6
Input Cell Type: markdown
Input Text:
-----------
# Now create an additional figure with 4x1 subplot with all labels parts separated and annotated

Output Text:
------------


------------------------------------------------------------
Cell index: 7
Input Cell Type: python
Input Text:
-----------
# Randomly select a single row from the dataframe
selected_row = df.sample(n=1).iloc[0]

# Load the equation image
eq_img_path = os.path.join(dataset_folder, 'equation_images', selected_row['image_filename'])
eq_img = Image.open(eq_img_path)

# Load the labels image
label_img_path = os.path.join(dataset_folder, 'label_images', selected_row['image_filename'])
label_img = Image.open(label_img_path)

# Convert labels image to numpy array
label_array = np.array(label_img)


# Get the heights of each part
char_order = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '+', '-', '*', '/', '=', ' ']
section_order = ["argument 1", "operation", "argument 2", "equal sign", "result", "digit", "symbol", "space"]
font_order = [
    'ARIALN', 'times', 'verdanaz', 'georgiai', 'couri', 'tahomabd', 'impact', 'comicz',
    'calibriz', 'GARA', 'pala', 'BOOKOSBI', 'CENTURY', 'trebucbi', 'GILSANUB', 'BASKVILL',
    'ROCKEB', 'FRADMCN', 'cambriaz', 'consola', 'COPRGTB', 'PAPYRUS', 'BRUSHSCI', 'lucon',
    'BAUHS93', 'BOD_BLAI', 'AGENCYR', 'ALGER', 'BELLI', 'BRLNSB', 'BRITANIC', 'BROADW',
    'CALIFR', 'CALISTB', 'CASTELAR', 'CENTAUR', 'CHILLER', 'COLONNA', 'COOPBL', 'ELEPHNTI',
    'ENGR', 'ERASBD', 'FELIXTI', 'FORTE', 'FREESCPT', 'GIGI', 'GOUDOSI', 'HATTEN',
    'HARLOWSI', 'HTOWERT', 'JOKERMAN', 'KUNSTLER', 'LCALLIG', 'MAGNETOB', 'MAIAN', 'MATURASC',
    'MISTRAL', 'MOD20', 'MTCORSVA', 'NIAGENG', 'ONYX', 'PLAYBILL', 'POORICH', 'PRISTINA',
    'RAGE', 'RAVIE', 'SCRIPTBL', 'SHOWG', 'STENCIL', 'TEMPSITC', 'VINERITC', 'VIVALDII',
    'VLADIMIR', 'seguiemj', 'Candara', 'corbelb', 'Gabriola', 'Inkfree', 'javatext', 'LEELAWDB',
    'micross', 'mvboli', 'sylfaen', 'BERNHC', 'BKANT', 'BRADHITC', 'BRLNSDB', 'COPRGTL',
    'DUBAI-LIGHT', 'ERASDEMI', 'ERASLGHT', 'FRABKIT', 'FRAHV', 'FRAMDCN', 'FTLTLT', 'GLECB',
    'GOTHICI', 'HARNGTON', 'IMPRISHA', 'ITCBLKAD', 'ITCKRIST', 'LATINWD', 'LBRITEDI', 'LHANDW'
]

# from config import CHAR_ORDER, SECTION_ORDER, FONT_ORDER
# char_order = CHAR_ORDER
# section_order = SECTION_ORDER
# font_order = FONT_ORDER

# Split the labels image
char_height = len(char_order)
section_height = len(section_order)
font_height = len(font_order)

char_image = label_array[:char_height]
section_image = label_array[char_height:char_height+section_height]
font_image = label_array[char_height+section_height:]

# Create the main figure with 4 subplots
fig, axs = plt.subplots(4, 1, figsize=(20, 25), gridspec_kw={'height_ratios': [2, 2, 2, 9]})
plt.subplots_adjust(hspace=0.2)

# Plot the original equation image
axs[0].imshow(eq_img, cmap='gray', aspect='auto', interpolation='nearest')
axs[0].set_title(f'Equation Image: "{selected_row["full string"]}"', fontsize=16)

# Plot character labels
axs[1].imshow(char_image, aspect='auto', cmap='gray', interpolation='nearest')
axs[1].set_title("Character Labels", fontsize=16)
axs[1].set_yticks(range(len(char_order)))
axs[1].set_yticklabels([f'"{char}"' for char in char_order])

# Plot section labels
axs[2].imshow(section_image, aspect='auto', cmap='gray', interpolation='nearest')
axs[2].set_title("Section Labels", fontsize=16)
axs[2].set_yticks(range(len(section_order)))
axs[2].set_yticklabels([f'"{section}"' for section in section_order])

# Plot font labels
axs[3].imshow(font_image, aspect='auto', cmap='gray', interpolation='nearest')
axs[3].set_title("Font Labels", fontsize=16)
axs[3].set_yticks(range(len(font_order) + 1))
axs[3].set_yticklabels([f'"{font}"' for font in font_order + ["other"]])

# Add x-axis label to the bottom subplot
axs[3].set_xlabel("Time", fontsize=14)

plt.tight_layout()
plt.show()

# Print information about the selected equation
print(f"Equation: {selected_row['full string']}")
print(f"Simple description: {selected_row['simple_description']}")
print(f"Additional description: {selected_row['additional_description']}")

Output Text:
------------
<Figure size 2000x2500 with 4 Axes>
Equation: 2745 / 15 = 183
Simple description: An image of size 128x1156 of the equation "2745 / 15 = 183" (with the division operation), the digits and characters have varied vertical positions and have non-uniform widths, using many different mixed fonts (11)
Additional description: First argument has 4 digits, second argument has 2 digits, result has 3 digits. 1st argument is at timepoints [157, 452], operation symbol is at [485, 510], 2nd argument is at [544, 686], equal sign is at [711, 772], and the result at [796, 1013]


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image displays the results of an optical character recognition (OCR) and layout analysis process on an image of a handwritten mathematical equation: "2745 / 15 = 183".  The image is structured in four distinct sections.

The top section shows the original equation image itself. The second section provides a character-level labeling, indicating the location and type of each character (digit, symbol, etc.) within the equation. This is represented as a heatmap-like representation where white blocks show the presence of a character.  The third section presents a section-level labeling, showing the spatial extent of different sections like "argument 1," "operation," "argument 2," "equal sign," "result," etc. Again, this is a heatmap-like representation. Finally, the bottom section displays a font-level labeling, indicating the fonts potentially used for each part of the equation across time (horizontal axis). This section is also a heatmap showing the likelihood of different fonts being used.  Each row represents a different font.

In essence, the image visualizes the OCR's interpretation of the image's content at different levels of granularity: individual characters, equation sections, and potential fonts. This allows for a detailed analysis of the OCR process and its success in identifying the components and structure of the handwritten equation.


------------------------------------------------------------
Cell index: 8
Input Cell Type: markdown
Input Text:
-----------
# Display equation image with colored bounding boxes

Output Text:
------------


------------------------------------------------------------
Cell index: 9
Input Cell Type: python
Input Text:
-----------
# Display equation image with colored bounding boxes

# Randomly select a single row from the dataframe
selected_row = df.sample(n=1).iloc[0]

# Load the equation image
eq_img_path = os.path.join(dataset_folder, 'equation_images', selected_row['image_filename'])
eq_img = Image.open(eq_img_path)

# Create figure and axis
fig, ax = plt.subplots(figsize=(15, 7))

# Display the equation image
ax.imshow(eq_img, cmap='gray')

# Define colors for each part
colors = ['red', 'blue', 'green', 'orange', 'purple']
parts = ['argument1', 'operation', 'argument2', 'equal_sign', 'result']

# Create bounding boxes
for part, color in zip(parts, colors):
    start = selected_row[f'{part}_start']
    end = selected_row[f'{part}_end']
    rect = plt.Rectangle((start, 0), end - start, eq_img.height, 
                         fill=False, edgecolor=color, linewidth=4)
    ax.add_patch(rect)
    ax.text(start, eq_img.height + 5, part, color=color, 
            ha='left', va='top', rotation=40, fontsize=10)

# Set title and remove axis ticks
ax.set_title(f"Equation: \"{selected_row['full string']}\"", fontsize=16)
ax.set_xticks([])
ax.set_yticks([])

# Adjust layout and display
plt.tight_layout()
plt.show()

# Print information about the selected equation
print(f"Equation: \"{selected_row['full string']}\"")
print(f"Simple description: {selected_row['simple_description']}")
print(f"Additional description: {selected_row['additional_description']}")
print("\nBounding box locations:")
for part in parts:
    print(f"{part}: [{selected_row[f'{part}_start']}, {selected_row[f'{part}_end']}]")


Output Text:
------------
<Figure size 1500x700 with 1 Axes>
Equation: "2304 / 96 = 24"
Simple description: An image of size 128x1054 of the equation "2304 / 96 = 24" (with the division operation), the digits and characters have varied vertical positions and have non-uniform widths, using many different mixed fonts (10)
Additional description: First argument has 4 digits, second argument has 2 digits, result has 2 digits. 1st argument is at timepoints [158, 456], operation symbol is at [490, 539], 2nd argument is at [567, 729], equal sign is at [753, 824], and the result at [854, 924]

Bounding box locations:
argument1: [158, 456]
operation: [490, 539]
argument2: [567, 729]
equal_sign: [753, 824]
result: [854, 924]


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a horizontal rectangular banner showing a simple mathematical equation: "2304 / 96 = 24".  The equation is displayed in large, white numerals and symbols on a black background. Each element of the equation (the two numbers being divided, the division symbol, the equals sign, and the result) is clearly separated and framed with a different colored bounding box.

The bounding boxes are color-coded to identify each part: the first number (2304) is within a red box, labeled "argument1"; the division symbol (/) is in blue, labeled "operation"; the second number (96) is in green, labeled "argument2"; the equals sign (=) is in orange, labeled "equal_sign"; and the result (24) is enclosed in purple, labeled "result".  Above the equation, the text "Equation: "2304 / 96 = 24"" is displayed, confirming the equation being represented visually.  The color-coded boxes and labels provide a clear structural breakdown of the components of the equation.


------------------------------------------------------------
Cell index: 10
Input Cell Type: markdown
Input Text:
-----------
# Display another equation image with colored bounding boxes

Output Text:
------------


------------------------------------------------------------
Cell index: 11
Input Cell Type: python
Input Text:
-----------
# Display equation image with colored bounding boxes

# Randomly select a single row from the dataframe
selected_row = df.sample(n=1).iloc[0]

# Load the equation image
eq_img_path = os.path.join(dataset_folder, 'equation_images', selected_row['image_filename'])
eq_img = Image.open(eq_img_path)

# Create figure and axis
fig, ax = plt.subplots(figsize=(15, 7))

# Display the equation image
ax.imshow(eq_img, cmap='gray')

# Define colors for each part
colors = ['red', 'blue', 'green', 'orange', 'purple']
parts = ['argument1', 'operation', 'argument2', 'equal_sign', 'result']

# Create bounding boxes
for part, color in zip(parts, colors):
    start = selected_row[f'{part}_start']
    end = selected_row[f'{part}_end']
    rect = plt.Rectangle((start, 0), end - start, eq_img.height, 
                         fill=False, edgecolor=color, linewidth=4)
    ax.add_patch(rect)
    ax.text(start, eq_img.height + 5, part, color=color, 
            ha='left', va='top', rotation=40, fontsize=10)

# Set title and remove axis ticks
ax.set_title(f"Equation: \"{selected_row['full string']}\"", fontsize=16)
ax.set_xticks([])
ax.set_yticks([])

# Adjust layout and display
plt.tight_layout()
plt.show()

# Print information about the selected equation
print(f"Equation: \"{selected_row['full string']}\"")
print(f"Simple description: {selected_row['simple_description']}")
print(f"Additional description: {selected_row['additional_description']}")
print("\nBounding box locations:")
for part in parts:
    print(f"{part}: [{selected_row[f'{part}_start']}, {selected_row[f'{part}_end']}]")


Output Text:
------------
<Figure size 1500x700 with 1 Axes>
Equation: "806 - 27 = 779"
Simple description: An image of size 128x1099 of the equation "806 - 27 = 779" (with the subtraction operation), the digits and characters have varied vertical positions and have non-uniform widths, using many different mixed fonts (9)
Additional description: First argument has 3 digits, second argument has 2 digits, result has 3 digits. 1st argument is at timepoints [137, 320], operation symbol is at [361, 429], 2nd argument is at [472, 608], equal sign is at [641, 695], and the result at [735, 961]

Bounding box locations:
argument1: [137, 320]
operation: [361, 429]
argument2: [472, 608]
equal_sign: [641, 695]
result: [735, 961]


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a horizontal rectangular arrangement showing a mathematical equation: "806 - 27 = 779". Each component of the equation (the two arguments, the minus sign, the equals sign, and the result) is individually enclosed within a colored box. 


Specifically, the number "806" is in a red box, the minus sign ("-") is in a blue box, the number "27" is in a green box, the equals sign ("=") is in an orange box, and the number "779" is in a purple box. Below each box, a small text label identifies the component's role (argument1, operation, argument2, equal_sign, result). The background is black, making the white numbers and symbols stand out clearly. The equation itself is also displayed as text at the top of the image.


------------------------------------------------------------
Cell index: 12
Input Cell Type: python
Input Text:
-----------


Output Text:
------------


------------------------------------------------------------

================================================================================
================================================================================
requirements.txt:
=================
numpy
pandas
matplotlib
Pillow
tqdm
kaggle

================================================================================
================================================================================
config.py:
==========
# Configuration parameters and fixed data dictionaries/lists for ImSME dataset generation

# Character replacement dictionary
CHAR_REPLACEMENT_DICT = {
    '"': 'doublequote',
    "'": 'singlequote',
    '\\': 'backslash',
    '/': 'forwardslash',
    ':': 'colon',
    '*': 'asterisk',
    '?': 'questionmark',
    '<': 'lessthan',
    '>': 'greaterthan',
    '|': 'pipe',
    ' ': 'space',
    '(': 'leftparenthesis',
    ')': 'rightparenthesis',
    '[': 'leftbracket',
    ']': 'rightbracket',
    '{': 'leftbrace',
    '}': 'rightbrace',
    '#': 'hash',
    '%': 'percent',
    '&': 'ampersand',
    '^': 'caret',
    '@': 'at',
    ';': 'semicolon',
    ',': 'comma',
    '.': 'dot',
    '`': 'backtick',
    '=': 'equal',
    '!': 'exclamation',
    '+': 'plus',
    '-': 'minus',
    '_': 'underscore',
    '$': 'dollar',
    '~': 'tilde',
}

# Reverse character replacement dictionary
REVERSE_CHAR_REPLACEMENT_DICT = {v: k for k, v in CHAR_REPLACEMENT_DICT.items()}

# Character lists
BASE_ENGLISH_CHARACTERS_LIST = list('0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!"#$%&()*+-/:;<=>?@[\\]^{|}')
CHARACTERS_LIST = BASE_ENGLISH_CHARACTERS_LIST

# Char order for label generation
CHAR_ORDER = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '+', '-', '*', '/', '=', ' ']

# Section order for label generation
SECTION_ORDER = ["argument 1", "operation", "argument 2", "equal sign", "result", "digit", "symbol", "space"]

# Font order for label generation
FONT_ORDER = [
    'ARIALN', 'times', 'verdanaz', 'georgiai', 'couri', 'tahomabd', 'impact', 'comicz',
    'calibriz', 'GARA', 'pala', 'BOOKOSBI', 'CENTURY', 'trebucbi', 'GILSANUB', 'BASKVILL',
    'ROCKEB', 'FRADMCN', 'cambriaz', 'consola', 'COPRGTB', 'PAPYRUS', 'BRUSHSCI', 'lucon',
    'BAUHS93', 'BOD_BLAI', 'AGENCYR', 'ALGER', 'BELLI', 'BRLNSB', 'BRITANIC', 'BROADW',
    'CALIFR', 'CALISTB', 'CASTELAR', 'CENTAUR', 'CHILLER', 'COLONNA', 'COOPBL', 'ELEPHNTI',
    'ENGR', 'ERASBD', 'FELIXTI', 'FORTE', 'FREESCPT', 'GIGI', 'GOUDOSI', 'HATTEN',
    'HARLOWSI', 'HTOWERT', 'JOKERMAN', 'KUNSTLER', 'LCALLIG', 'MAGNETOB', 'MAIAN', 'MATURASC',
    'MISTRAL', 'MOD20', 'MTCORSVA', 'NIAGENG', 'ONYX', 'PLAYBILL', 'POORICH', 'PRISTINA',
    'RAGE', 'RAVIE', 'SCRIPTBL', 'SHOWG', 'STENCIL', 'TEMPSITC', 'VINERITC', 'VIVALDII',
    'VLADIMIR', 'seguiemj', 'Candara', 'corbelb', 'Gabriola', 'Inkfree', 'javatext', 'LEELAWDB',
    'micross', 'mvboli', 'sylfaen', 'BERNHC', 'BKANT', 'BRADHITC', 'BRLNSDB', 'COPRGTL',
    'DUBAI-LIGHT', 'ERASDEMI', 'ERASLGHT', 'FRABKIT', 'FRAHV', 'FRAMDCN', 'FTLTLT', 'GLECB',
    'GOTHICI', 'HARNGTON', 'IMPRISHA', 'ITCBLKAD', 'ITCKRIST', 'LATINWD', 'LBRITEDI', 'LHANDW'
]


# Section name mapping
SECTION_NAME_MAPPING = {
    'argument 1': 'argument1',
    'operation': 'operation',
    'argument 2': 'argument2',
    'equal sign': 'equal_sign',
    'result': 'result'
}

# Operations
OPERATIONS = ['+', '-', '*', '/']

# Symbol characters
SYMBOL_CHARS = set(['+', '-', '*', '/', '='])

# Digit characters
DIGIT_CHARS = set('0123456789')

# Font name to short path dictionary
NAME_SHORT_PATH_DICT = {
    'Franklin Gothic Demi': 'FRADM.TTF',
    'Goudy Old Style': 'GOUDOSI.TTF',
    'Cambria': 'cambriaz.ttf',
    'David': 'davidbd.ttf',
    'Informal Roman': 'INFROMAN.TTF',
    'Dubai': 'DUBAI-LIGHT.TTF',
    'Jokerman': 'JOKERMAN.TTF',
    'Broadway': 'BROADW.TTF',
    'Agency FB': 'AGENCYR.TTF',
    'Eras Bold ITC': 'ERASBD.TTF',
    'Franklin Gothic Heavy': 'FRAHV.TTF',
    'Times New Roman': 'times.ttf',
    'Viner Hand ITC': 'VINERITC.TTF',
    'Tahoma': 'tahomabd.ttf',
    'Stencil': 'STENCIL.TTF',
    'Chiller': 'CHILLER.TTF',
    'Segoe UI': 'segoeuil.ttf',
    'Rockwell': 'ROCK.TTF',
    'Trebuchet MS': 'trebucbi.ttf',
    'Lucida Fax': 'LFAXD.TTF',
    'Calisto MT': 'CALISTB.TTF',
    'Footlight MT Light': 'FTLTLT.TTF',
    'Bodoni MT': 'BOD_BLAI.TTF',
    'Microsoft PhagsPa': 'phagspa.ttf',
    'Franklin Gothic Book': 'FRABKIT.TTF',
    'Book Antiqua': 'BKANT.TTF',
    'Segoe Print': 'segoepr.ttf',
    'Century Gothic': 'GOTHICI.TTF',
    'Constantia': 'constani.ttf',
    'Bradley Hand ITC': 'BRADHITC.TTF',
    'Britannic Bold': 'BRITANIC.TTF',
    'Gill Sans Ultra Bold Condensed': 'GILLUBCD.TTF',
    'Lucida Bright': 'LBRITEDI.TTF',
    'Segoe Script': 'segoescb.ttf',
    'Ebrima': 'ebrimabd.ttf',
    'Century': 'CENTURY.TTF',
    'Berlin Sans FB': 'BRLNSB.TTF',
    'Calibri': 'calibriz.ttf',
    'Hadassah Friedlaender': 'HADASAH.TTF',
    'Palatino Linotype': 'pala.ttf',
    'Rockwell Condensed': 'ROCC____.TTF',
    'Lucida Sans': 'LSANS.TTF',
    'Candara': 'Candara.ttf',
    'Yu Gothic': 'YuGothM.ttc',
    'Lucida Sans Typewriter': 'LTYPEO.TTF',
    'Nirmala UI': 'NirmalaB.ttf',
    'Garamond': 'GARA.TTF',
    'Microsoft Tai Le': 'taile.ttf',
    'Perpetua': 'PERB____.TTF',
    'Californian FB': 'CALIFR.TTF',
    'Bookman Old Style': 'BOOKOSBI.TTF',
    'Microsoft JhengHei': 'msjh.ttc',
    'French Script MT': 'FRSCRIPT.TTF',
    'Palace Script MT': 'PALSCRI.TTF',
    'Aharoni': 'ahronbd.ttf',
    'Gigi': 'GIGI.TTF',
    'Gill Sans Ultra Bold': 'GILSANUB.TTF',
    'Comic Sans MS': 'comicz.ttf',
    'Courier New': 'couri.ttf',
    'Leelawadee': 'LEELAWDB.TTF',
    'Corbel': 'corbelb.ttf',
    'Elephant': 'ELEPHNTI.TTF',
    'Verdana': 'verdanaz.ttf',
    'Sitka': 'SitkaVF.ttf',
    'Segoe UI Emoji': 'seguiemj.ttf',
    'Leelawadee UI': 'LeelaUIb.ttf',
    'Bell MT': 'BELLI.TTF',
    'Georgia': 'georgiai.ttf',
    'Monotype Corsiva': 'MTCORSVA.TTF',
    'SimSun-ExtB': 'simsunb.ttf',
    'Consolas': 'consola.ttf',
    'Lucida Calligraphy': 'LCALLIG.TTF',
    'Lucida Handwriting': 'LHANDW.TTF',
    'Arial': 'ARIALN.TTF',
    'Levenim MT': 'lvnmbd.ttf',
    'Pristina': 'PRISTINA.TTF',
    'Gill Sans MT Condensed': 'GILC____.TTF',
    'Copperplate Gothic Light': 'COPRGTL.TTF',
    'Gill Sans MT': 'GIL_____.TTF',
    'High Tower Text': 'HTOWERT.TTF',
    'Segoe UI Variable': 'SegUIVar.ttf',
    'Bahnschrift': 'bahnschrift.ttf',
    'Vladimir Script': 'VLADIMIR.TTF',
    'SimSun': 'simsun.ttc',
    'Arial Rounded MT Bold': 'ARLRDBD.TTF',
    'MS Reference Sans Serif': 'REFSAN.TTF',
    'Microsoft YaHei': 'msyh.ttc',
    'Segoe UI Historic': 'seguihis.ttf',
    'Microsoft Himalaya': 'himalaya.ttf',
    'Algerian': 'ALGER.TTF',
    'Perpetua Titling MT': 'PERTIBD.TTF',
    'Harlow Solid Italic': 'HARLOWSI.TTF',
    'Niagara Solid': 'NIAGSOL.TTF',
    'Gisha': 'gishabd.ttf',
    'Centaur': 'CENTAUR.TTF',
    'Rockwell Extra Bold': 'ROCKEB.TTF',
    'Eras Medium ITC': 'ERASMD.TTF',
    'Rod': 'rod.ttf',
    'Kristen ITC': 'ITCKRIST.TTF',
    'Tw Cen MT': 'TCM_____.TTF',
    'Microsoft Uighur': 'MSUIGHUR.TTF',
    'Century Schoolbook': 'SCHLBKI.TTF',
    'Niagara Engraved': 'NIAGENG.TTF',
    'Playbill': 'PLAYBILL.TTF',
    'Juice ITC': 'JUICE___.TTF',
    'Forte': 'FORTE.TTF',
    'Segoe UI Symbol': 'seguisym.ttf',
    'Ink Free': 'Inkfree.ttf',
    'Blackadder ITC': 'ITCBLKAD.TTF',
    'Edwardian Script ITC': 'ITCEDSCR.TTF',
    'Sylfaen': 'sylfaen.ttf',
    'Kunstler Script': 'KUNSTLER.TTF',
    'Tw Cen MT Condensed': 'TCCB____.TTF',
    'Gabriola': 'Gabriola.ttf',
    'Eras Light ITC': 'ERASLGHT.TTF',
    'Tw Cen MT Condensed Extra Bold': 'TCCEB.TTF',
    'Wide Latin': 'LATINWD.TTF',
    'Mistral': 'MISTRAL.TTF',
    'Vivaldi': 'VIVALDII.TTF',
    'Engravers MT': 'ENGR.TTF',
    'Berlin Sans FB Demi': 'BRLNSDB.TTF',
    'Impact': 'impact.ttf',
    'Castellar': 'CASTELAR.TTF',
    'Microsoft Yi Baiti': 'msyi.ttf',
    'Snap ITC': 'SNAP____.TTF',
    'Lucida Sans Unicode': 'l_10646.ttf',
    'Eras Demi ITC': 'ERASDEMI.TTF',
    'Brush Script MT': 'BRUSHSCI.TTF',
    'Gill Sans MT Ext Condensed Bold': 'GLSNECB.TTF',
    'Rage Italic': 'RAGE.TTF',
    'Bauhaus 93': 'BAUHS93.TTF',
    'Script MT Bold': 'SCRIPTBL.TTF',
    'Myanmar Text': 'mmrtextb.ttf',
    'Mongolian Baiti': 'monbaiti.ttf',
    'Franklin Gothic Demi Cond': 'FRADMCN.TTF',
    'Microsoft New Tai Lue': 'ntailub.ttf',
    'Franklin Gothic Medium': 'framd.ttf',
    'Goudy Stout': 'GOUDYSTO.TTF',
    'Tempus Sans ITC': 'TEMPSITC.TTF',
    'Matura MT Script Capitals': 'MATURASC.TTF',
    'Cooper Black': 'COOPBL.TTF',
    'Baskerville Old Face': 'BASKVILL.TTF',
    'Ravie': 'RAVIE.TTF',
    'MingLiU-ExtB': 'mingliub.ttc',
    'Imprint MT Shadow': 'IMPRISHA.TTF',
    'Microsoft Sans Serif': 'micross.ttf',
    'Franklin Gothic Medium Cond': 'FRAMDCN.TTF',
    'Showcard Gothic': 'SHOWG.TTF',
    'Narkisim': 'nrkis.ttf',
    'Colonna MT': 'COLONNA.TTF',
    'Bernard MT Condensed': 'BERNHC.TTF',
    'Modern No. 20': 'MOD20.TTF',
    'Felix Titling': 'FELIXTI.TTF',
    'Freestyle Script': 'FREESCPT.TTF',
    'Copperplate Gothic Bold': 'COPRGTB.TTF',
    'OCR A Extended': 'OCRAEXT.TTF',
    'Miriam': 'mriam.ttf',
    'Gloucester MT Extra Condensed': 'GLECB.TTF',
    'Harrington': 'HARNGTON.TTF',
    'Maiandra GD': 'MAIAN.TTF',
    'Lucida Console': 'lucon.ttf',
    'Haettenschweiler': 'HATTEN.TTF',
    'Magneto': 'MAGNETOB.TTF',
    'MV Boli': 'mvboli.ttf',
    'Curlz MT': 'CURLZ___.TTF',
    'Javanese Text': 'javatext.ttf',
    'FrankRuehl': 'frank.ttf',
    'Onyx': 'ONYX.TTF',
    'Poor Richard': 'POORICH.TTF',
    'Gadugi': 'gadugib.ttf',
    'Papyrus': 'PAPYRUS.TTF',
    'MS Gothic': 'msgothic.ttc'
}

================================================================================
================================================================================
download_ImSME_dataset.py:
==========================
import os
from kaggle.api.kaggle_api_extended import KaggleApi
from urllib.parse import urlparse

# instructions for setting up Kaggle API:
# https://www.kaggle.com/docs/api

# 1. pip install kaggle
# 2. Go to your kaggle account settings, click on 'Create New API Token'
# 3. Save the kaggle.json file to C:\Users\<username>\.kaggle\kaggle.json (Windows)
# 3. Save the kaggle.json file to ~/.kaggle/kaggle.json (Linux)

def download_ImSME_dataset(output_dir):
    # Initialize and authenticate Kaggle API
    api = KaggleApi()
    api.authenticate()

    # ImSME dataset URL
    dataset_url = "https://www.kaggle.com/datasets/selfishgene/imsme-images-of-simple-math-equations"

    try:
        # Extract dataset info from URL
        path = urlparse(dataset_url).path
        parts = path.strip('/').split('/')
        if len(parts) < 3 or parts[0] != 'datasets':
            raise ValueError(f"Invalid Kaggle dataset URL: {dataset_url}")
        
        owner, dataset_name = parts[1], parts[2]
        dataset = f"{owner}/{dataset_name}"
        
        # Create output directory
        os.makedirs(output_dir, exist_ok=True)
        
        print(f"Downloading dataset: {dataset}")
        api.dataset_download_files(dataset, path=output_dir, unzip=True)
        print(f"Dataset {dataset} downloaded successfully to {output_dir}")
    
    except Exception as e:
        print(f"Error downloading {dataset_url}: {str(e)}")

if __name__ == "__main__":
    # Output directory
    output_dir = "data"
    
    download_ImSME_dataset(output_dir)

================================================================================
================================================================================
sample_images/sample_image_with_segmented_main_equation_parts.png:
==================================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a horizontal rectangular black banner displaying a simple arithmetic equation: "612 + 261 = 873".  The numbers and the plus and equals signs are rendered in a stylized serif font, appearing white against the black background. Each element of the equation (the two numbers, the plus sign, the equals sign, and the result) is individually enclosed within a colored box.

Specifically, the number 612 is contained in a red box, the "+" symbol is in a blue box, the number 261 is in a green box, the "=" sign is in an orange box, and the number 873 is in a purple box.  Beneath each box, a small text label identifies the element's role: "argument1," "operation," "argument2," "equal_sign," and "result."  The labels are colored to match their corresponding boxes, further clarifying the layout.  The overall arrangement provides a clear and visually organized representation of the equation's components.  At the top, the equation is also stated textually above the visual representation.

================================================================================
================================================================================
generate_ImSME_dataset.py:
==========================
import os
import random
import numpy as np
import pandas as pd
from PIL import Image
from tqdm import tqdm
from utils import equation_to_image, generate_indicators, indicators_to_images, generate_equation_descriptions
from config import (CHAR_ORDER, SECTION_ORDER, FONT_ORDER, OPERATIONS, SECTION_NAME_MAPPING)


def generate_and_save_images(df, num_equations, num_samples_per_equation, output_folder, char_images_folder, 
                             equations_df_name, space_padding_range, fraction_canonical, 
                             fraction_single_font, fraction_limited_font_set):
    
    # Create output folders
    equation_images_folder = os.path.join(output_folder, 'equation_images')
    label_images_folder = os.path.join(output_folder, 'label_images')
    os.makedirs(equation_images_folder, exist_ok=True)
    os.makedirs(label_images_folder, exist_ok=True)

    # Perform stratified sampling
    sampled_rows = pd.DataFrame()
    samples_per_operation = num_equations // len(OPERATIONS)
    for operation in OPERATIONS:
        operation_df = df[df['operation'] == operation]
        if len(operation_df) <= samples_per_operation:
            sampled_rows = pd.concat([sampled_rows, operation_df])
        else:
            sampled_rows = pd.concat([sampled_rows, operation_df.sample(n=samples_per_operation, replace=False)])

    sampled_rows = sampled_rows.sample(frac=1).reset_index(drop=True)

    new_rows = []
    num_digits_equation = len(str(len(sampled_rows) - 1))
    num_digits_sample = len(str(num_samples_per_equation))

    for equation_index, row in tqdm(sampled_rows.iterrows(), total=len(sampled_rows), desc="Generating equation images"):
        for sample_index in range(1, num_samples_per_equation + 1):
            new_row = row.copy()
            new_row['equation_index'] = equation_index
            new_row['sample_index'] = sample_index
            
            current_fraction_canonical = fraction_canonical * 2 if sample_index in [1, 2, 3] else fraction_canonical
            current_fraction_single_font = fraction_single_font * 2 if sample_index in [1, 2, 3] else fraction_single_font
            
            is_canonical = random.random() < current_fraction_canonical
            new_row['is_canonical'] = is_canonical

            rand_value = random.random()
            if rand_value < current_fraction_single_font:
                selected_font = random.choice(FONT_ORDER)
                fonts_pool = [selected_font]
                new_row['font_usage'] = selected_font
            elif rand_value < (current_fraction_single_font + fraction_limited_font_set):
                num_limited_fonts = random.randint(4, len(FONT_ORDER))
                fonts_pool = FONT_ORDER[:num_limited_fonts]
                new_row['font_usage'] = 'mixed_limited'
            else:
                fonts_pool = None
                new_row['font_usage'] = 'mixed_full'

            equation_string = row['full string']
            if is_canonical:
                equation_string = f"    {equation_string}    "
            else:
                equation_string = f"{' ' * random.randint(*space_padding_range)}{equation_string}{' ' * random.randint(*space_padding_range)}"

            equation_image, char_details = equation_to_image(equation_string, char_images_folder, 
                                                             use_random_augmentations=not is_canonical,
                                                             fonts_pool=fonts_pool)

            char_indicators, font_indicators, section_indicators = generate_indicators(char_details)

            for original_name, new_name in SECTION_NAME_MAPPING.items():
                indicator = section_indicators[original_name]
                non_zero = np.nonzero(indicator)[0]
                if len(non_zero) > 0:
                    start = non_zero[0]
                    end = non_zero[-1] + 1
                    new_row[f'{new_name}_start'] = start
                    new_row[f'{new_name}_end'] = end

            char_image, font_image, section_image = indicators_to_images(char_indicators, font_indicators, section_indicators,
                                                                         CHAR_ORDER, SECTION_ORDER, FONT_ORDER)

            total_height = char_image.height + section_image.height + font_image.height
            combined_label_image = Image.new('L', (equation_image.width, total_height))
            combined_label_image.paste(char_image, (0, 0))
            combined_label_image.paste(section_image, (0, char_image.height))
            combined_label_image.paste(font_image, (0, char_image.height + section_image.height))

            file_name = f"equation_{equation_index:0{num_digits_equation}d}_sample_{sample_index:0{num_digits_sample}d}.png"
            equation_image.save(os.path.join(equation_images_folder, file_name), 'PNG', compress_level=8)
            combined_label_image.save(os.path.join(label_images_folder, file_name), 'PNG', compress_level=8)
            
            new_row['image_filename'] = file_name

            simple_desc, additional_desc = generate_equation_descriptions(new_row, char_details, equation_image.height, equation_image.width)
            new_row['simple_description'] = simple_desc
            new_row['additional_description'] = additional_desc

            new_rows.append(new_row)

    new_df = pd.DataFrame(new_rows)

    columns = new_df.columns.tolist()
    for col in ['is_canonical', 'font_usage', 'sample_index', 'equation_index', 'image_filename']:
        columns.remove(col)
    new_column_order = ['image_filename', 'equation_index', 'sample_index', 'font_usage', 'is_canonical'] + columns
    new_df = new_df[new_column_order]

    actual_samples = len(new_df)
    new_filename = f"simple_math_equation_images__{equations_df_name.split('__')[1]}__{actual_samples}_rows.csv"
    new_df.to_csv(os.path.join(output_folder, new_filename), index=False)

    print('-' * 80)
    print(f'Image generation completed! \nNew dataframe saved as "{new_filename}"')
    print(f"Total number of equations processed: {len(sampled_rows)}")
    print(f"Total number of samples generated: {actual_samples}")
    print('-' * 40)
    print(f"Operation distribution in the generated dataset:")
    print(new_df['operation'].value_counts())
    print('-' * 40)
    print(f"Canonical vs Augmented distribution:")
    print(new_df['is_canonical'].value_counts(normalize=True))
    print('-' * 40)
    print(f"Font usage distribution:")
    print(new_df['font_usage'].value_counts(normalize=True))
    print('-' * 80)

    return new_df


if __name__ == "__main__":
    # if just_a_test is True, the script will generate a small dataset for testing purposes
    just_a_test = True

    # Dataset generation parameters
    OUTPUT_HEIGHT = 128
    RESIZE_HEIGHT_DIGIT = 112
    RESIZE_HEIGHT_SYMBOL = 72
    WIDTH_MULT_FACTORS = [0.25, 1.0]
    SPACE_PADDING_RANGE = (4, 7)
    FRACTION_CANONICAL = 0.2
    FRACTION_SINGLE_FONT = 0.2
    FRACTION_LIMITED_FONT_SET = 0.2

    # Folder paths
    equations_dataframe_folder = "data/simple_math_equations_dataset"
    char_images_folder = "data/char_images_dataset_tight"
    output_folder = "data/math_equations_images_dataset"

    # Choose the equations database to use
    equations_df_name = "simple_math_equations__4d_op_3d_eq_3d__1574610_rows.csv"
    
    # Load the equations dataframe
    df = pd.read_csv(os.path.join(equations_dataframe_folder, equations_df_name))

    # Generate datasets of different sizes
    if just_a_test:
        dataset_sizes = {
            "tiny": (4, 4),   # 4 equations, 4 samples each
            "small": (8, 4),  # 8 equations, 4 samples each
            "medium": (16, 4) # 16 equations, 4 samples each
        }
    else:
        dataset_sizes = {
            "tiny": (2048, 8),   # 2048 equations, 8 samples each
            "small": (8192, 8),  # 8192 equations, 8 samples each
            "medium": (32768, 8) # 32768 equations, 8 samples each
        }

    for size_name, (num_equations, num_samples) in dataset_sizes.items():
        print('-' * 80)
        print(f"\nGenerating {size_name} dataset...")
        print('-' * 30)
        size_output_folder = f"{output_folder}_{size_name}"
        generate_and_save_images(df, num_equations, num_samples, size_output_folder, char_images_folder, equations_df_name,
                                 space_padding_range=SPACE_PADDING_RANGE, fraction_canonical=FRACTION_CANONICAL, 
                                 fraction_single_font=FRACTION_SINGLE_FONT, fraction_limited_font_set=FRACTION_LIMITED_FONT_SET)
        print('-' * 80)

    print("\nImSME dataset generation completed.")

================================================================================
================================================================================
create_char_images_dataset.py:
==============================
import os
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from tqdm import tqdm
import matplotlib.font_manager as fm
from utils import create_char_image
from config import CHAR_REPLACEMENT_DICT, CHARACTERS_LIST, NAME_SHORT_PATH_DICT

def list_available_fonts():
    font_paths = fm.findSystemFonts()
    available_fonts = {}
    for font_path in font_paths:
        font_name = fm.FontProperties(fname=font_path).get_name()
        available_fonts[font_name] = font_path
    return available_fonts


def create_char_images_dataset(output_folder, fonts_to_use, image_size=(128, 128), return_tight_bbox=False):
    os.makedirs(output_folder, exist_ok=True)
    
    total_images = len([char for char in CHARACTERS_LIST]) * len(fonts_to_use)
    print(f'Total images to save: {total_images}')
    
    with tqdm(total=total_images, desc="Generating character images") as pbar:
        for char in CHARACTERS_LIST:
            for font_name, font_path in fonts_to_use.items():
                char_name = CHAR_REPLACEMENT_DICT.get(char, char)
                # use the shortened font path as the font name
                font_name_to_use = os.path.basename(font_path) if os.path.exists(font_path) else font_path.split('.')[0]
                filename = f'char_{char_name}_font_{font_name_to_use}.png'
                try:
                    image = create_char_image(char, font_path, final_image_size=image_size, return_tight_bbox=return_tight_bbox)
                    image.save(os.path.join(output_folder, filename))
                    pbar.update(1)
                except Exception as e:
                    print(f'Error saving image: "{filename}". Error: {str(e)}')
    
    print(f'Character images dataset created in: {output_folder}')


def display_sample_images(dataset_folder, num_samples=36):
    files = os.listdir(dataset_folder)
    sample_files = np.random.choice(files, num_samples, replace=False)
    
    num_rows = int(np.sqrt(num_samples))
    num_cols = int(np.ceil(num_samples / num_rows))
    
    plt.figure(figsize=(18, 20))
    
    for i, file in enumerate(sample_files):
        image = Image.open(os.path.join(dataset_folder, file))
        char = file.split('_font_')[0].split('char_')[1]
        font = file.split('_font_')[1].split('.')[0]
        
        plt.subplot(num_rows, num_cols, i + 1)
        plt.imshow(image, cmap='gray')
        plt.title(f'Char: "{char}"\nFont: "{font}"')
        plt.axis('off')
    
    plt.tight_layout()
    plt.show()


if __name__ == "__main__":
    # Flag to choose between using all available fonts or just NAME_SHORT_PATH_DICT
    USE_ALL_AVAILABLE_FONTS = False

    print('-' * 80)
    print("Creating character images dataset")
    print('-' * 80)

    # Set output folders
    output_folder = "data/char_images_dataset"
    output_folder_tight = "data/char_images_dataset_tight"
    
    # Get available fonts
    available_fonts = list_available_fonts()
    
    if USE_ALL_AVAILABLE_FONTS:
        fonts_to_use = available_fonts
        print(f"Using all {len(fonts_to_use)} available fonts")
    else:
        fonts_to_use = {}
        for font_name, short_path in NAME_SHORT_PATH_DICT.items():
            full_path = next((path for name, path in available_fonts.items() if path.lower().endswith(short_path.lower())), None)
            if full_path:
                fonts_to_use[font_name] = full_path
            else:
                print(f"Warning: Font {font_name} ({short_path}) not found in available fonts.")
        print(f"Using {len(fonts_to_use)} fonts from NAME_SHORT_PATH_DICT")
    
    print('-' * 80)
    
    # Create datasets
    print("Creating regular character images dataset:")
    create_char_images_dataset(output_folder, fonts_to_use, return_tight_bbox=False)
    print('-' * 80)
    
    print("Creating tight bounding box character images dataset:")
    create_char_images_dataset(output_folder_tight, fonts_to_use, return_tight_bbox=True)
    print('-' * 80)
    
    # Display sample images from both datasets
    print("Displaying sample images from regular dataset:")
    display_sample_images(output_folder)
    
    print("Displaying sample images from tight bounding box dataset:")
    display_sample_images(output_folder_tight)

    print('-' * 80)
    print("Character images dataset creation completed.")
    print('-' * 80)
================================================================================
================================================================================
create_equations_dataframe.py:
==============================
import os
import numpy as np
import pandas as pd
from tqdm import tqdm
from config import OPERATIONS


def generate_equations(first_argument_num_digits, second_argument_num_digits, result_num_digits):
    print(f'Generating equations with:')
    print(f'First argument: up to {first_argument_num_digits} digits')
    print(f'Second argument: up to {second_argument_num_digits} digits')
    print(f'Result: up to {result_num_digits} digits')
    print(f'Operations: {OPERATIONS}')

    first_argument_numbers = range(0, 10 ** first_argument_num_digits)
    second_argument_numbers = range(0, 10 ** second_argument_num_digits)
    max_result = 10 ** result_num_digits - 1

    data = []
    for arg1 in tqdm(first_argument_numbers, desc="Generating equations"):
        for arg2 in second_argument_numbers:
            for op in OPERATIONS:
                if op == '+':
                    result = arg1 + arg2
                elif op == '-':
                    result = arg1 - arg2
                elif op == '*':
                    result = arg1 * arg2
                elif op == '/':
                    if arg2 != 0 and arg1 % arg2 == 0:
                        result = arg1 // arg2
                    else:
                        continue
                
                if 0 <= result <= max_result:
                    full_string = f"{arg1} {op} {arg2} = {result}"
                    data.append([arg1, op, arg2, result, full_string])

    columns = ["first argument", "operation", "second argument", "result", "full string"]
    df = pd.DataFrame(data, columns=columns)
    
    print(f'Generated a total of {df.shape[0]} equations')
    return df


def save_dataframe(df, output_folder, filename):
    os.makedirs(output_folder, exist_ok=True)
    file_path = os.path.join(output_folder, filename)
    df.to_csv(file_path, index=False)
    print(f'Saved dataframe to: {file_path}')


def print_basic_stats(df):
    print("\nBasic Statistics:")
    print("-----------------")
    print("Operation distribution:")
    print(df['operation'].value_counts())
    print("\nNumber of unique values:")
    for column in ['first argument', 'second argument', 'result']:
        print(f"{column}: {df[column].nunique()}")
    print("\nMost common values:")
    for column in ['first argument', 'second argument', 'result']:
        print(f"\n{column}:")
        print(df[column].value_counts().head())


if __name__ == "__main__":
    output_folder = "data/simple_math_equations_dataset"
    
    # Generate different equation sets
    equation_sets = [
        (3, 3, 3),  # 3-digit op 3-digit = 3-digit
        (4, 3, 3),  # 4-digit op 3-digit = 3-digit
        (4, 4, 2),  # 4-digit op 4-digit = 2-digit
    ]
    
    for first_digits, second_digits, result_digits in equation_sets:
        print('-' * 80)
        print(f"\nGenerating equations: {first_digits}d op {second_digits}d = {result_digits}d")
        print('-' * 15)
        df = generate_equations(first_digits, second_digits, result_digits)
        
        filename = f"simple_math_equations__{first_digits}d_op_{second_digits}d_eq_{result_digits}d__{df.shape[0]}_rows.csv"
        save_dataframe(df, output_folder, filename)
        print('-' * 80)
        print_basic_stats(df)
        print('-' * 80)

    print("\nEquation dataframe creation completed.")

================================================================================
================================================================================
utils.py:
=========
import os
import random
import numpy as np
from PIL import Image, ImageDraw, ImageFont
from config import DIGIT_CHARS, SYMBOL_CHARS, CHAR_REPLACEMENT_DICT, SECTION_ORDER

OUTPUT_HEIGHT = 128
RESIZE_HEIGHT_DIGIT = 112
RESIZE_HEIGHT_SYMBOL = 72
WIDTH_MULT_FACTORS = [0.25, 1.0]


def create_char_image(char, selected_font, final_image_size=(128, 128), return_tight_bbox=False):
    image_size = (final_image_size[0] * 2, final_image_size[1] * 2)
    font_size = 100
    image = Image.new("L", image_size, "black")
    draw = ImageDraw.Draw(image)
    
    font = ImageFont.truetype(selected_font, font_size)
    text_bbox = draw.textbbox((0, 0), char, font=font)
    text_width, text_height = text_bbox[2] - text_bbox[0], text_bbox[3] - text_bbox[1]
    
    scale_H = (image_size[1] * 0.45) / text_height
    scale_W = (image_size[0] * 0.45) / text_width
    scale = np.array([scale_H, scale_W]).min()
    scaled_font = ImageFont.truetype(selected_font, int(font_size * scale))
    
    text_bbox = draw.textbbox((0, 0), char, font=scaled_font)
    position = (image_size[0] // 2, image_size[1] // 2)
    
    draw.text(position, char, font=scaled_font, fill="white", anchor="mm")

    if return_tight_bbox:
        non_zero_pixels = np.array(image).nonzero()
        top_left = (max(0, non_zero_pixels[0].min() - 10), max(0, non_zero_pixels[1].min() - 10))
        bottom_right = (min(image_size[0], non_zero_pixels[0].max() + 10), min(image_size[1], non_zero_pixels[1].max() + 10))
        image = image.crop((top_left[1], top_left[0], bottom_right[1], bottom_right[0]))
    else:
        crop_bbox = (image_size[0] // 32, image_size[1] // 32, 
                     31 * image_size[0] // 32, 31 * image_size[1] // 32)
        image = image.crop(crop_bbox)

    image = image.resize(final_image_size)
    return image


def equation_to_image(equation_string, char_images_folder, space_padding=48, output_height=OUTPUT_HEIGHT, 
                      resize_height_digit=RESIZE_HEIGHT_DIGIT, resize_height_symbol=RESIZE_HEIGHT_SYMBOL, 
                      width_mult_factors=WIDTH_MULT_FACTORS, use_random_augmentations=True, fonts_pool=None):
    char_images = []
    char_widths = []
    char_fonts = []
    char_offsets = []
    
    all_available_fonts = set(os.path.splitext(file)[0].split('_font_')[1] for file in os.listdir(char_images_folder) if file.startswith('char_'))
    
    if fonts_pool is not None:
        fonts_pool = set(fonts_pool).intersection(all_available_fonts)
        if not fonts_pool:
            print("Warning: None of the specified fonts in fonts_pool are available. Using all available fonts.")
            fonts_pool = all_available_fonts
    else:
        fonts_pool = all_available_fonts
    
    for char in equation_string:
        if char == ' ':
            space_width = space_padding if not use_random_augmentations else max(24, int(space_padding * random.uniform(*width_mult_factors)))
            space_image = Image.new('L', (space_width, output_height), color=0)
            char_images.append(space_image)
            char_widths.append(space_width)
            char_fonts.append('space')
            char_offsets.append(0)
        else:
            char_name = CHAR_REPLACEMENT_DICT.get(char, char)
            char_files = [f for f in os.listdir(char_images_folder) 
                          if f.startswith(f'char_{char_name}_font_') and 
                          f.split('_font_')[1].split('.')[0] in fonts_pool]
            
            if not char_files:
                char_files = [f for f in os.listdir(char_images_folder) 
                              if f.startswith(f'char_{char_name}_font_')]
                if char_files:
                    print(f"Warning: No image found for character '{char}' ('{char_name}') with the specified fonts. Falling back to all available fonts.")
            
            if char_files:
                chosen_file = random.choice(char_files)
                img_path = os.path.join(char_images_folder, chosen_file)
                char_img = Image.open(img_path).convert('L')
                
                resize_height = resize_height_digit if char in DIGIT_CHARS else resize_height_symbol if char in SYMBOL_CHARS else resize_height_digit
                
                aspect_ratio = char_img.width / char_img.height
                new_width = int(resize_height * aspect_ratio)
                if use_random_augmentations:
                    new_width = int(new_width * random.uniform(*width_mult_factors))
                char_img_resized = char_img.resize((new_width, resize_height), Image.LANCZOS)
                
                full_height_img = Image.new('L', (new_width, output_height), color=0)
                
                if char in SYMBOL_CHARS and use_random_augmentations:
                    offset = random.randint(int(0.25 * (output_height - resize_height)), int(0.75 * (output_height - resize_height)))
                elif char in DIGIT_CHARS and use_random_augmentations:
                    offset = random.randint(int(0.05 * (output_height - resize_height)), int(0.95 * (output_height - resize_height)))
                else:
                    offset = (output_height - resize_height) // 2

                full_height_img.paste(char_img_resized, (0, offset))
                
                char_images.append(full_height_img)
                char_widths.append(new_width)
                char_fonts.append(chosen_file.split('_font_')[1].split('.')[0])
                char_offsets.append(offset)
            else:
                print(f"Warning: No image found for character '{char}' ('{char_name}') in any available font.")
                char_widths.append(0)
                char_fonts.append('not_found')
                char_offsets.append(0)
    
    total_width = sum(img.width for img in char_images)
    equation_image = Image.new('L', (total_width, output_height), color=0)
    
    x_offset = 0
    for img in char_images:
        equation_image.paste(img, (x_offset, 0))
        x_offset += img.width
    
    char_details = {
        'chars': list(equation_string),
        'widths': char_widths,
        'fonts': char_fonts,
        'offsets': char_offsets
    }
    
    return equation_image, char_details


def generate_indicators(char_details):
    chars = char_details['chars']
    widths = char_details['widths']
    fonts = char_details['fonts']

    image_width = sum(widths)

    char_indicators = {}
    font_indicators = {}
    section_indicators = {section: np.zeros(image_width, dtype=int) for section in SECTION_ORDER}

    current_position = 0
    current_section = "argument 1"
    for char, width, font in zip(chars, widths, fonts):
        char_indicator = np.zeros(image_width, dtype=int)
        char_indicator[current_position:current_position + width] = 1
        
        if char not in char_indicators:
            char_indicators[char] = np.zeros(image_width, dtype=int)
        char_indicators[char] |= char_indicator

        if font not in font_indicators:
            font_indicators[font] = np.zeros(image_width, dtype=int)
        font_indicators[font] |= char_indicator

        if char in ['+', '-', '*', '/']:
            current_section = "operation"
            section_indicators["symbol"] |= char_indicator
        elif char == '=':
            current_section = "equal sign"
            section_indicators["symbol"] |= char_indicator
        elif current_section == "equal sign":
            current_section = "result"
        elif current_section == "operation":
            current_section = "argument 2"

        if char.isdigit():
            section_indicators["digit"] |= char_indicator
        elif char == ' ':
            section_indicators["space"] |= char_indicator

        section_indicators[current_section] |= char_indicator

        current_position += width

    space_mask = ~section_indicators["space"].astype(bool)
    for key in ["argument 1", "argument 2", "result", "operation", "equal sign"]:
        section_indicators[key] &= space_mask

    return char_indicators, font_indicators, section_indicators


def indicators_to_images(char_indicators, font_indicators, section_indicators, 
                         char_order, section_order, font_order):
    image_width = len(next(iter(char_indicators.values())))
    
    char_array = np.zeros((len(char_order), image_width), dtype=np.int32)
    font_array = np.zeros((len(font_order) + 1, image_width), dtype=np.int32)
    section_array = np.zeros((len(section_order), image_width), dtype=np.int32)
    
    for i, char in enumerate(char_order):
        if char in char_indicators:
            char_array[i] = char_indicators[char]
    
    for i, font in enumerate(font_order):
        if font in font_indicators:
            font_array[i] = font_indicators[font]
    
    other_fonts = set(font_indicators.keys()) - set(font_order)
    other_indicator = np.zeros(image_width, dtype=np.int32)
    for font in other_fonts:
        other_indicator |= font_indicators[font]
    font_array[-1] = other_indicator
    
    for i, section in enumerate(section_order):
        if section in section_indicators:
            section_array[i] = section_indicators[section]
    
    char_image = Image.fromarray((char_array * 255).astype(np.uint8))
    font_image = Image.fromarray((font_array * 255).astype(np.uint8))
    section_image = Image.fromarray((section_array * 255).astype(np.uint8))
    
    return char_image, font_image, section_image


def generate_equation_descriptions(row, char_details, image_height, image_width):
    equation_str = row['full string']
    is_canonical = row['is_canonical']
    font_usage = row['font_usage']
    
    operation_map = {'+': 'addition', '-': 'subtraction', '*': 'multiplication', '/': 'division'}
    operation = next(op for op in ['+', '-', '*', '/'] if op in equation_str)
    operation_name = operation_map[operation]

    simple_description = f'An image of size {image_height}x{image_width} of the equation "{equation_str}" (with the {operation_name} operation)'
    
    if is_canonical:
        simple_description += ", the digits and characters are in an aligned canonical form"
    else:
        simple_description += ", the digits and characters have varied vertical positions and have non-uniform widths"
    
    if font_usage == 'mixed_full' or font_usage == 'mixed_limited':
        unique_fonts = set(font for font in char_details['fonts'] if font != 'other' and font != 'space' and font != 'not_found')
        num_unique_fonts = len(unique_fonts)
        if font_usage == 'mixed_full':
            simple_description += f", using many different mixed fonts ({num_unique_fonts})"
        else:
            simple_description += f", using a limited set of {num_unique_fonts} mixed fonts"
    else:
        simple_description += f", using a single font ({font_usage})"

    args = equation_str.split()
    additional_description = f"First argument has {len(args[0])} digits, "
    additional_description += f"second argument has {len(args[2])} digits, "
    additional_description += f"result has {len(args[4])} digits. "
    
    section_orig_names = ['argument1', 'operation', 'argument2', 'equal_sign', 'result']
    section_names_to_use = ['1st argument', 'operation symbol', '2nd argument', 'equal sign', 'result']

    for k, (section, name_to_use) in enumerate(zip(section_orig_names, section_names_to_use)):
        start = row[f'{section}_start']
        end = row[f'{section}_end']
        if k == 0:
            additional_description += f"{name_to_use} is at timepoints [{start}, {end}], "
        elif k == len(section_names_to_use) - 1:
            additional_description += f"and the {name_to_use} at [{start}, {end}]"
        else:
            additional_description += f"{name_to_use} is at [{start}, {end}], "

    return simple_description, additional_description

================================================================================
================================================================================
sample_images/an_equation_with_segmentation_labels.png:
=======================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a visualization of the results of processing an equation image ("583 + 74 = 657").  It's structured vertically into four main sections.

The top section shows the original equation image itself. It's a black background with the equation rendered in white, showing different fonts and styles for the numbers and symbols.

The second section provides "Character Labels." This is a black and white representation where horizontal white bars indicate the temporal extent of each character in the equation.  The x-axis represents time, and the y-axis represents different characters (digits, operators, etc.).

The third section is titled "Section Labels".  Similar to the character labels, it uses horizontal white bars, but these represent higher-level sections within the equation, such as the "argument 1," "operation," "argument 2," "equal sign," and "result" sections.

The final and largest section is labeled "Font Labels." This section uses horizontal white bars to indicate the specific font used for each segment of the equation over time. The y-axis lists a large number of different fonts, and a white bar indicates the detected font for a given time period.  This section likely uses a font identification algorithm, which is why it's so much larger than the other sections.

In summary, the image presents a detailed breakdown of the equation image, analyzing its character composition, structural segmentation, and font usage over time.  The visualization is useful for understanding the results of an OCR or image analysis process.

================================================================================
================================================================================
LICENSE.txt:
============
MIT License

Copyright (c) 2022 David Beniaguev

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

================================================================================
================================================================================
sample_images/several_equations_with_description_labels.png:
============================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image contains six separate images, each displaying a mathematical equation rendered in different fonts and styles.  Each equation is accompanied by descriptive text providing details about its characteristics: the image size, the type of operation (addition, subtraction, multiplication, or division), the number of digits in each argument and the result, and the coordinates of the digits, operation symbols, and equal signs within the image.

The equations themselves are simple arithmetic problems: 70 + 538 = 608, 799 - 219 = 580, 7101 * 0 = 0, 1020 / 204 = 5, 249 + 542 = 791, and 412 * 0 = 0.  The variations in font, size, and positioning of the digits and symbols within each image showcase differences in how the equations might appear in different contexts or using different typesetting methods.

The accompanying text is consistently formatted, providing a structured and detailed analysis of each image's visual properties.  This detailed information, along with the visual display of the equations, suggests the image is likely part of a dataset or documentation used for research or development related to optical character recognition (OCR) or image processing techniques, particularly those dealing with varied font styles and digit positioning.

================================================================================

================================================================================
