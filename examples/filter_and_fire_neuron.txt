================================================================================
repo title: filter_and_fire_neuron
repo link: https://github.com/SelfishGene/filter_and_fire_neuron
date processed: 2024-10-21
================================================================================
================================================================================
repo file structure:
====================
    create_capacity_figure_Fig2.py
    create_explanatory_figure_Fig4.py
    create_hardware_saving_figure_Fig5.py
    create_intro_figure_Fig1.py
    create_MNIST_figure_Fig3.py
    FF_vs_IF_capacity_comparison_interactions.py
    MNIST_classification_LR_IF_FF_interactions.py
    README.md
    run_capacity_configs_on_cluster_slurm.py
    run_mnist_configs_on_cluster_slurm.py
    results_data_capacity/
        FF_vs_IF_capacity_comparision__num_axons_100__sim_duration_sec_120__num_mult_conn_5__rand_rep_12.pickle
        FF_vs_IF_capacity_comparision__num_axons_100__sim_duration_sec_120__num_mult_conn_6__rand_rep_18.pickle
        FF_vs_IF_capacity_comparision__num_axons_112__sim_duration_sec_120__num_mult_conn_6__rand_rep_18.pickle
        FF_vs_IF_capacity_comparision__num_axons_125__sim_duration_sec_120__num_mult_conn_6__rand_rep_18.pickle
        FF_vs_IF_capacity_comparision__num_axons_137__sim_duration_sec_120__num_mult_conn_6__rand_rep_18.pickle
        FF_vs_IF_capacity_comparision__num_axons_150__sim_duration_sec_120__num_mult_conn_6__rand_rep_18.pickle
        FF_vs_IF_capacity_comparision__num_axons_162__sim_duration_sec_120__num_mult_conn_6__rand_rep_18.pickle
        FF_vs_IF_capacity_comparision__num_axons_175__sim_duration_sec_120__num_mult_conn_6__rand_rep_18.pickle
        FF_vs_IF_capacity_comparision__num_axons_187__sim_duration_sec_120__num_mult_conn_6__rand_rep_18.pickle
        FF_vs_IF_capacity_comparision__num_axons_200__sim_duration_sec_120__num_mult_conn_5__rand_rep_12.pickle
        FF_vs_IF_capacity_comparision__num_axons_200__sim_duration_sec_120__num_mult_conn_6__rand_rep_18.pickle
        FF_vs_IF_capacity_comparision__num_axons_212__sim_duration_sec_120__num_mult_conn_6__rand_rep_18.pickle
        FF_vs_IF_capacity_comparision__num_axons_225__sim_duration_sec_120__num_mult_conn_6__rand_rep_18.pickle
        FF_vs_IF_capacity_comparision__num_axons_237__sim_duration_sec_120__num_mult_conn_6__rand_rep_18.pickle
        FF_vs_IF_capacity_comparision__num_axons_300__sim_duration_sec_120__num_mult_conn_5__rand_rep_12.pickle
        FF_vs_IF_capacity_comparision__num_axons_400__sim_duration_sec_120__num_mult_conn_5__rand_rep_12.pickle
        FF_vs_IF_capacity_comparision__num_axons_50__sim_duration_sec_120__num_mult_conn_5__rand_rep_12.pickle
    results_data_mnist/
        MNIST_classification_LR_FF_IF_5162_rows_12_cols.csv
    saved_figures/
        F&F_Axon_Reduction_Figure_5_4.png
        F&F_Capacity_Figure_2_3.png
        F&F_Explantion_Figure_4_2.png
        F&F_Introduction_Figure_1_11.png
        F&F_MNIST_Figure_3_7.png
================================================================================
================================================================================
README.md:
==========
# The Filter and Fire (F&F) Neuron Model
This repo contains the code behind the work  
[Multiple Synaptic Contacts combined with Dendritic Filtering enhance Spatio-Temporal Pattern Recognition capabilities of Single Neurons](https://www.biorxiv.org/content/10.1101/2022.01.28.478132v2)

## Multiple Synaptic Contacts combined with Dendritic Filtering <br > enhance Spatio-Temporal Pattern Recognition capabilities of Single Neurons  
David Beniaguev, Sapir Shapira, Idan Segev, Michael London

**Abstract**: *A cortical neuron typically makes multiple synaptic contacts on the dendrites of a post-synaptic target neuron. The functional implications of this apparent redundancy are unclear. The dendritic location of a synaptic contact affects the time-course of the somatic post-synaptic potential (PSP) due to dendritic cable filtering. Consequently, a single pre-synaptic axonal spike results with a PSP composed of multiple temporal profiles. Here, we developed a "filter-and-fire" (F&F) neuron model that captures these features and show that the memory capacity of this neuron is threefold larger than that of a leaky integrate-and-fire (I&F) neuron, when trained to emit precisely timed output spikes for specific input patterns. Furthermore, the F&F neuron can learn to recognize spatio-temporal input patterns, e.g., MNIST digits, where the I&F model completely fails. Multiple synaptic contacts between pairs of cortical neurons are therefore an important feature rather than a bug and can serve to reduce axonal wiring requirements.*

<img width="1161" alt="Overview_of_F F_neuron_model" src="https://user-images.githubusercontent.com/11506338/151635189-1e6bfe6f-78a5-4c7e-92a4-0599601697c3.PNG">

## Resources
Open Access version of Paper: [biorxiv.org/content/10.1101/2022.01.28.478132v2](https://www.biorxiv.org/content/10.1101/2022.01.28.478132v2)  
Data required for full replication of all results: [kaggle.com/selfishgene/fiter-and-fire-paper](https://www.kaggle.com/selfishgene/fiter-and-fire-paper)  
Introductory Notebook (Figure 1 in manuscript): [kaggle.com/selfishgene/f-f-introduction-figure-fig-1](https://www.kaggle.com/selfishgene/f-f-introduction-figure-fig-1)  
Notebook with replication of main results 1: [kaggle.com/selfishgene/f-f-capacity-figure-fig-2](https://www.kaggle.com/selfishgene/f-f-capacity-figure-fig-2)  
Notebook with replication of main results 2: [kaggle.com/selfishgene/f-f-mnist-figure-fig-3](https://www.kaggle.com/selfishgene/f-f-mnist-figure-fig-3)  
Notebooks for full replication of all figures: [kaggle.com/selfishgene/fiter-and-fire-paper/code](https://www.kaggle.com/selfishgene/fiter-and-fire-paper/code)  


## Increased capacity of F&F vs I&F 
<img width="1161" alt="Capacity_vs_multiple_contacts_compact" src="https://user-images.githubusercontent.com/11506338/151635194-af23b7d3-bb7a-48c9-aaeb-f05648cd4e64.PNG">

- Use `create_capacity_figure_Fig2.py` to replicate Figure 2 in the manuscript
  - All major parameters are documented inside the file using comments  
  - All necessary files are under the folder `results_data_capacity\`
- Use `FF_vs_IF_capacity_comparison_interactions.py` to recreate all files in `results_data_capacity\`
  - All major parameters are documented inside the file using comments  
  - Use `run_capacity_configs_on_cluster_slurm.py` to send jobs to a slurm cluster


## Single Neurons as Spatio-Temporal Pattern Recognizers
<img width="1003" alt="MNIST_classifying_digit_3_compact" src="https://user-images.githubusercontent.com/11506338/151635198-3b65239f-505c-46e3-8ec1-8ddc7931e52d.PNG">

- Use `create_MNIST_figure_Fig3.py` to replicate Figure 3 in the manuscript
  - All major parameters are documented inside the file using comments  
  - All necessary files are under the folder `results_data_mnist\`. large files are on [the dataset](https://www.kaggle.com/selfishgene/fiter-and-fire-paper) on kaggle
- Use `MNIST_classification_LR_IF_FF_interactions.py` to recreate all files in `results_data_mnist\`
  - All major parameters are documented inside the file using comments  
  - Use `run_mnist_configs_on_cluster_slurm.py` to send jobs to a slurm cluster

## PSPs of a realistic detailed biophysical Layer 5 Cortical Pyramidal Neuron
<img width="1040" alt="L5PC_morphology_PSPs" src="https://user-images.githubusercontent.com/11506338/151635200-a8288feb-0365-4c86-91ad-2d87dcc3e7b8.PNG">

- Visit [this link](https://www.kaggle.com/selfishgene/f-f-l5pc-psps-fig-s2) to replicate Supplementary Figure S2 in the manuscript
- All necessary simulation data for this figure are in the file `sim_results_excitatory.p` in [the dataset](https://www.kaggle.com/selfishgene/fiter-and-fire-paper) on kaggle 


## Acknowledgements
We thank all lab members of the Segev and London Labs for many fruitful discussions and valuable feedback regarding this work.
In particular we would like to thank [Sapir Shapira](https://github.com/ssapir) that skillfully collected all data and created Supplementary Figure S2 in the paper.


If you use this code or dataset, please cite the following work:  

1. David Beniaguev, Sapir Shapira, Idan Segev and Michael London. "Multiple Synaptic Contacts combined with Dendritic Filtering enhance Spatio-Temporal Pattern Recognition capabilities of Single Neurons
." bioRxiv 2022.01.28.478132; doi: https://doi.org/10.1101/2022.01.28.478132



================================================================================
================================================================================
create_intro_figure_Fig1.py:
============================
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
import matplotlib.gridspec as gridspec
from scipy import signal

matplotlib.rcParams['pdf.fonttype'] = 42
matplotlib.rcParams['svg.fonttype'] = 'none'

#%% script params

num_axons = 3
num_dendrites = 9

axon_colors = ['magenta', 'teal', 'purple']

num_spikes_per_axon = 3
experiment_time_ms  = 300

tau_rise_range  = [1,9]
tau_decay_range = [5,30]

v_reset     = -80
v_threshold = -55
current_to_voltage_mult_factor = 4
refreactory_time_constant = 25

save_figures = False
all_file_endings_to_use = ['.png', '.pdf', '.svg']

figure_folder = '/filter_and_fire_neuron/saved_figures/'

#%% helper functions


def create_single_PSP_profile(tau_rise, tau_decay, temporal_filter_length=50):

    if tau_rise >= tau_decay:
        tau_decay = tau_rise + 5

    exp_r = signal.exponential(M=temporal_filter_length, center=0, tau=tau_rise , sym=False)
    exp_d = signal.exponential(M=temporal_filter_length, center=0, tau=tau_decay, sym=False)

    post_syn_potential = exp_d - exp_r
    post_syn_potential /= post_syn_potential.max()

    return post_syn_potential


def construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec):

    num_synapses = tau_rise_vec.shape[0]
    temporal_filter_length = int(7 * tau_decay_vec.max()) + 1

    syn_filter = np.zeros((num_synapses, temporal_filter_length))

    for k, (tau_r, tau_d) in enumerate(zip(tau_rise_vec, tau_decay_vec)):
        syn_filter[k,:] = create_single_PSP_profile(tau_r, tau_d, temporal_filter_length=temporal_filter_length)

    return syn_filter


def simulate_filter_and_fire_cell_training(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                           refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):

    temporal_filter_length = int(5 * refreactory_time_constant) + 1
    refreactory_filter = signal.exponential(M=temporal_filter_length,center=0,tau=refreactory_time_constant,sym=False)[np.newaxis,:]

    # padd input and get all synaptic filters
    normlized_syn_filter = np.flipud(construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec))
    padded_input = np.hstack((np.zeros(normlized_syn_filter.shape), presynaptic_input_spikes))

    # calc local currents
    local_normlized_currents = np.zeros(presynaptic_input_spikes.shape, dtype=np.float16)
    for k in range(normlized_syn_filter.shape[0]):
        local_normlized_currents[k] = signal.convolve(padded_input[k], normlized_syn_filter[k], mode='valid')[1:]

    # multiply by weights to get the somatic current
    soma_current = signal.convolve(local_normlized_currents, synaptic_weights, mode='valid')

    # simulate the cell
    soma_voltage = v_reset + current_to_voltage_mult_factor * soma_current.ravel()
    output_spike_times_in_ms = []
    # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
    for t in range(len(soma_voltage)):
        # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
        if (soma_voltage[t] > v_threshold) and ((t + 1) < len(soma_voltage)):
            t_start = t + 1
            t_end = min(len(soma_voltage), t_start + temporal_filter_length)
            soma_voltage[t_start:t_end] -= (soma_voltage[t + 1] - v_reset) * refreactory_filter.ravel()[:(t_end - t_start)]
            output_spike_times_in_ms.append(t)

    return local_normlized_currents, soma_voltage, output_spike_times_in_ms


def add_offset_for_plotting(traces_matrix, offset_size=1.1):

    traces_matrix_with_offset = offset_size * np.kron(np.arange(traces_matrix.shape[0])[:,np.newaxis], np.ones((1,traces_matrix.shape[1])))
    traces_matrix_with_offset = traces_matrix_with_offset + traces_matrix

    return traces_matrix_with_offset


#%% simulate the cell

connections_per_axon = int(num_dendrites / num_axons)
num_synapses = num_dendrites

tau_rise_vec  = np.linspace(tau_rise_range[0] , tau_rise_range[1] , num_synapses)[:,np.newaxis]
tau_decay_vec = np.linspace(tau_decay_range[0], tau_decay_range[1], num_synapses)[:,np.newaxis]

# synapse learnable parameters
synaptic_weights_vec = 1.0 + 0.1 * np.random.uniform(size=(num_synapses, 1))

synaptic_filters = construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec)

#%% run once

# generate sample input
stimulus_duration_ms = experiment_time_ms

axon_input_spike_train = np.zeros((num_axons, stimulus_duration_ms))
for k in range(num_axons):
    curr_axon_spike_times = 20 + np.random.randint(stimulus_duration_ms -80, size=num_spikes_per_axon)
    axon_input_spike_train[k,curr_axon_spike_times] = 1.0

presynaptic_input_spikes = np.kron(np.ones((connections_per_axon,1), dtype=bool), axon_input_spike_train).astype(bool)

# simulate F&F cell with normlized currents
local_normlized_currents, soma_voltage, output_spike_times_in_ms = simulate_filter_and_fire_cell_training(presynaptic_input_spikes,
                                                                                                          synaptic_weights_vec, tau_rise_vec, tau_decay_vec,
                                                                                                          refreactory_time_constant=refreactory_time_constant,
                                                                                                          v_reset=v_reset, v_threshold=v_threshold,
                                                                                                          current_to_voltage_mult_factor=current_to_voltage_mult_factor)

local_normlized_currents = np.flipud(local_normlized_currents)

soma_voltage_with_spikes = soma_voltage
soma_voltage_with_spikes[output_spike_times_in_ms] = -25


max_local_added_voltage = add_offset_for_plotting(local_normlized_currents).T.max()

print('running once')


#%% run again until we have at least 1 spike

while len(output_spike_times_in_ms) != 1 or max_local_added_voltage > 10.15:
    axon_input_spike_train = np.zeros((num_axons, stimulus_duration_ms))
    for k in range(num_axons):
        curr_axon_spike_times = 30 + np.random.randint(stimulus_duration_ms -60, size=num_spikes_per_axon)
        axon_input_spike_train[k,curr_axon_spike_times] = 1.0

    presynaptic_input_spikes = np.kron(np.ones((connections_per_axon,1), dtype=bool), axon_input_spike_train).astype(bool)

    # simulate F&F cell with normlized currents
    local_normlized_currents, soma_voltage, output_spike_times_in_ms = simulate_filter_and_fire_cell_training(presynaptic_input_spikes,
                                                                                                              synaptic_weights_vec, tau_rise_vec, tau_decay_vec,
                                                                                                              refreactory_time_constant=refreactory_time_constant,
                                                                                                              v_reset=v_reset, v_threshold=v_threshold,
                                                                                                              current_to_voltage_mult_factor=current_to_voltage_mult_factor)

    local_normlized_currents = np.flipud(local_normlized_currents)

    soma_voltage_with_spikes = soma_voltage
    soma_voltage_with_spikes[output_spike_times_in_ms] = -15

    max_local_added_voltage = add_offset_for_plotting(local_normlized_currents).T.max()

print('there is at least 1 spike')

#%% run untill we have a changed enough spike location

min_spike_time_diff = 35
max_local_added_voltage = 0

while True:

    output_spike_times_in_ms =[]

    # generate input axons with a single spike
    while len(output_spike_times_in_ms) != 1 or max_local_added_voltage > 10.15:
        axon_input_spike_train = np.zeros((num_axons, stimulus_duration_ms))
        for k in range(num_axons):
            curr_axon_spike_times = 30 + np.random.randint(stimulus_duration_ms -60, size=num_spikes_per_axon)
            axon_input_spike_train[k,curr_axon_spike_times] = 1.0

        presynaptic_input_spikes = np.kron(np.ones((connections_per_axon,1), dtype=bool), axon_input_spike_train).astype(bool)

        # simulate F&F cell with normlized currents
        local_normlized_currents, soma_voltage, output_spike_times_in_ms = simulate_filter_and_fire_cell_training(presynaptic_input_spikes,
                                                                                                                  synaptic_weights_vec, tau_rise_vec, tau_decay_vec,
                                                                                                                  refreactory_time_constant=refreactory_time_constant,
                                                                                                                  v_reset=v_reset, v_threshold=v_threshold,
                                                                                                                  current_to_voltage_mult_factor=current_to_voltage_mult_factor)

        local_normlized_currents = np.flipud(local_normlized_currents)

        soma_voltage_with_spikes = soma_voltage
        soma_voltage_with_spikes[output_spike_times_in_ms] = -15

        max_local_added_voltage = add_offset_for_plotting(local_normlized_currents).T.max()

        if len(output_spike_times_in_ms) == 1 and (output_spike_times_in_ms[0] > 200 or output_spike_times_in_ms[0] < 120):
            output_spike_times_in_ms = []

    print('there is at least 1 spike, spike times: ', output_spike_times_in_ms)

    # generate a weights change that will move this spikes by a minimum amount
    mult_vector = np.random.permutation([0.25,0.5,0.5,0.75,1.25,1.5,1.75,1.75,2.0])[:,np.newaxis]
    synaptic_weights_vec_2 = mult_vector * synaptic_weights_vec

    # make sure only the a the middle axon weights are changed (for non cluttered visualization)
    synaptic_weights_vec_2[0::3] = synaptic_weights_vec[0::3]
    synaptic_weights_vec_2[2::3] = synaptic_weights_vec[2::3]
    mult_vector[0::3] = 1
    mult_vector[2::3] = 1

    # simulate F&F cell with normlized currents
    local_normlized_currents_2, soma_voltage_2, output_spike_times_in_ms_2 = simulate_filter_and_fire_cell_training(presynaptic_input_spikes,
                                                                                                                    synaptic_weights_vec_2, tau_rise_vec, tau_decay_vec,
                                                                                                                    refreactory_time_constant=refreactory_time_constant,
                                                                                                                    v_reset=v_reset, v_threshold=v_threshold,
                                                                                                                    current_to_voltage_mult_factor=current_to_voltage_mult_factor)

    local_normlized_currents_2 = np.flipud(local_normlized_currents_2)

    soma_voltage_with_spikes_2 = soma_voltage_2
    soma_voltage_with_spikes_2[output_spike_times_in_ms_2] = -15

    max_local_added_voltage_2 = add_offset_for_plotting(local_normlized_currents_2).T.max()

    local_normlized_currents_2 = mult_vector * local_normlized_currents_2

    if len(output_spike_times_in_ms_2) == 1 and ((output_spike_times_in_ms[0] - output_spike_times_in_ms_2[0]) <= -min_spike_time_diff):
        break

print('changed weights such that the spike changed location for more than %d ms' %(min_spike_time_diff))


#%% Display Input Axons, Synaptic Filters and Local Voltage Traces "Slopily"

plt.close('all')
fig = plt.figure(figsize=(20,8))
plt.subplots_adjust(left=0.03, right=0.97, top=0.95, bottom=0.05, wspace=0.25, hspace=0.35)

plt.subplot(1,3,1);
for k, axon_color in enumerate(axon_colors):
    plt.plot(add_offset_for_plotting(np.flipud(axon_input_spike_train)).T[:,k], color=axon_color); plt.title('Input Axons', fontsize=24);

plt.subplot(1,3,2);
for k, axon_color in enumerate(axon_colors):
    plt.plot(add_offset_for_plotting(synaptic_filters).T[:100,k::num_axons], color=axon_color); plt.title('Synaptic Filters', fontsize=24);

plt.subplot(1,3,3);
for k, axon_color in enumerate(axon_colors):
    plt.plot(add_offset_for_plotting(local_normlized_currents).T[:,k::num_axons], color=axon_color); plt.title('Synaptic Contact Voltage Contribution', fontsize=24);
    plt.plot(add_offset_for_plotting(local_normlized_currents_2).T[:,k::num_axons], color=axon_color, linestyle='dashed');

#%% plot one below each other

plt.close('all')
plt.figure(figsize=(10,25))
gs_figure = gridspec.GridSpec(nrows=11,ncols=1)
gs_figure.update(left=0.04, right=0.95, bottom=0.05, top=0.95, wspace=0.1, hspace=0.8)

ax_axons          = plt.subplot(gs_figure[:3,:])
ax_local_voltages = plt.subplot(gs_figure[3:9,:])
ax_soma_voltage   = plt.subplot(gs_figure[9:,:])

ax_axons.plot(add_offset_for_plotting(axon_input_spike_train).T); ax_axons.set_title('Input Axons', fontsize=24);
ax_local_voltages.plot(add_offset_for_plotting(local_normlized_currents).T); ax_local_voltages.set_title('Synaptic Contact Voltage Contribution', fontsize=24);
ax_local_voltages.plot(add_offset_for_plotting(local_normlized_currents_2).T, linestyle='dashed');
ax_soma_voltage.plot(soma_voltage_with_spikes); ax_soma_voltage.set_title('Somatic Voltage', fontsize=24);
ax_soma_voltage.plot(soma_voltage_with_spikes_2, linestyle='dashed');

#%% Display The Full Figure

plt.close('all')
fig = plt.figure(figsize=(25,18))
gs_figure = gridspec.GridSpec(nrows=11,ncols=30)
gs_figure.update(left=0.04, right=0.95, bottom=0.05, top=0.92, wspace=0.2, hspace=0.7)

ax_axons         = plt.subplot(gs_figure[2:6,:9])
ax_syn_filters   = plt.subplot(gs_figure[:8,13:17])
ax_local_voltges = plt.subplot(gs_figure[:8,21:])
ax_soma_voltage  = plt.subplot(gs_figure[8:,21:])


for k, axon_color in enumerate(axon_colors):
    ax_axons.plot(add_offset_for_plotting(np.flipud(axon_input_spike_train)).T[:,k], color=axon_color, lw=3);
ax_axons.set_yticks([])
ax_axons.set_xticks([])
ax_axons.set_title('Input Axons', fontsize=20);
ax_axons.spines['top'].set_visible(False)
ax_axons.spines['bottom'].set_visible(False)
ax_axons.spines['left'].set_visible(False)
ax_axons.spines['right'].set_visible(False)

for k, axon_color in enumerate(axon_colors):
    ax_syn_filters.plot(add_offset_for_plotting(synaptic_filters).T[:100,k::num_axons], color=axon_color, lw=3);
ax_syn_filters.set_yticks([])
ax_syn_filters.set_xticks([])
ax_syn_filters.set_title('Synaptic Filters', fontsize=20);
ax_syn_filters.spines['top'].set_visible(False)
ax_syn_filters.spines['bottom'].set_visible(False)
ax_syn_filters.spines['left'].set_visible(False)
ax_syn_filters.spines['right'].set_visible(False)

for k, axon_color in enumerate(axon_colors):
    ax_local_voltges.plot(add_offset_for_plotting(local_normlized_currents).T[:,k::num_axons], color=axon_color, lw=3);
    ax_local_voltges.plot(add_offset_for_plotting(local_normlized_currents_2).T[:,k::num_axons], color=axon_color, ls='dashed', lw=2.5);
ax_local_voltges.set_xticks([])
ax_local_voltges.set_yticks([])
ax_local_voltges.set_title('Synaptic Contact Voltage Contribution', fontsize=20);
ax_local_voltges.spines['top'].set_visible(False)
ax_local_voltges.spines['bottom'].set_visible(False)
ax_local_voltges.spines['left'].set_visible(False)
ax_local_voltges.spines['right'].set_visible(False)

ax_soma_voltage.plot(soma_voltage_with_spikes, color='0.1', lw=3, alpha=0.85);
ax_soma_voltage.plot(soma_voltage_with_spikes_2, color='dodgerblue', lw=3);
ax_soma_voltage.set_yticks([])
ax_soma_voltage.set_xticks([])
ax_soma_voltage.set_title('Somatic Voltage', fontsize=20);
ax_soma_voltage.spines['top'].set_visible(False)
ax_soma_voltage.spines['bottom'].set_visible(False)
ax_soma_voltage.spines['left'].set_visible(False)
ax_soma_voltage.spines['right'].set_visible(False)

# save figure
if save_figures:
    figure_name = 'F&F_A_Introduction_Figure_1_%d' %(np.random.randint(2000))
    for file_ending in all_file_endings_to_use:
        if file_ending == '.png':
            fig.savefig(figure_folder + figure_name + file_ending, bbox_inches='tight')
        else:
            fig.savefig(figure_folder + figure_name + file_ending, bbox_inches='tight')


#%%




================================================================================
================================================================================
saved_figures\F&F_Explantion_Figure_4_2.png:
============================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
This image presents a comprehensive analysis of postsynaptic potentials (PSPs) using several signal processing techniques. The top left panel displays a heatmap showing all PSPs, where each row represents a single PSP's temporal evolution. The adjacent panel shows the same data as individual traces, offering a different perspective.

The middle section focuses on Non-negative Matrix Factorization (NMF). The top panel shows the three basis functions identified by the NMF. The bottom panel depicts the variance explained as a function of the number of basis functions used, demonstrating that three basis functions capture nearly all the variance (99.93%).

The bottom left panel presents a heatmap of the Singular Value Decomposition (SVD) basis functions, providing another representation of the underlying temporal structure of the PSPs. The bottom right panel shows the results of a simulation comparing the ability of different models (optimal 3 PSPs, F&F, and I&F) to generate precisely timed output spikes given a varying number of multiple contacts.  The horizontal dashed red line indicates a performance level.  The figure demonstrates that the optimal 3 PSP model achieves superior performance at generating precisely timed spikes compared to other approaches.  The error bars show variability in the simulation results.
================================================================================
================================================================================
FF_vs_IF_capacity_comparison_interactions.py:
=============================================
import sys
import numpy as np
import time
import matplotlib.pyplot as plt
from scipy import signal
from sklearn import linear_model
from sklearn.metrics import roc_auc_score
import pickle

from warnings import simplefilter
from sklearn.exceptions import ConvergenceWarning
simplefilter("ignore", category=ConvergenceWarning)

#%% script params

start_time = time.time()

try:
    print('----------------------------')
    random_seed = int(sys.argv[1])
    num_axons = int(sys.argv[2])
    use_interaction_terms = (sys.argv[3] == 'True')
    interactions_degree = int(sys.argv[4])
    print('"random_seed" selected by user - %d' %(random_seed))
    print('"num_axons" selected by user - %d' %(num_axons))
    print('"use_interaction_terms" selected by user - %s' %(use_interaction_terms))
    print('"interactions_degree" selected by user - %d' %(interactions_degree))

    determine_internally = False
except:
    determine_internally = True
    try:
        random_seed = int(sys.argv[1])
        print('random seed selected by user - %d' %(random_seed))
    except:
        random_seed = np.random.randint(100000)
        print('randomly choose seed - %d' %(random_seed))

np.random.seed(random_seed)
print('----------------------------')


# input parameters
if determine_internally:
    num_axons = 100
    use_interaction_terms = True
    interactions_degree  = 2


# experiment parameters
stimulus_duration_sec = 90
min_time_between_spikes_ms = 110

instantanious_input_spike_probability = 0.004
almost_prefect_accuracy_AUC_threshold = 0.99

# full
model_type_list = ['F&F', 'I&F']
connections_per_axon_list = [1,3,5,10,15]
req_num_output_spikes_list = np.linspace(5,200,40).astype(int)
req_num_output_spikes_list = np.logspace(np.log10(11),np.log10(200),40).astype(int)
random_iterations_list = np.linspace(1,10,10).astype(int)

# neuron model parameters
v_reset     = -80
v_threshold = -55
current_to_voltage_mult_factor = 3
refreactory_time_constant = 15

tau_rise_range_FF  = [ 1,16]
tau_decay_range_FF = [ 8,30]
tau_rise_range_IF  = [ 1, 1]
tau_decay_range_IF = [30,30]

show_plots = True
show_plots = False

data_folder = '/filter_and_fire_neuron/results_data_capacity/'

#%% helper functions

def create_single_PSP_profile(tau_rise, tau_decay, temporal_filter_length=50):

    safety_factor = 1.5
    if tau_rise >= (tau_decay / safety_factor):
        tau_decay = safety_factor * tau_rise

    exp_r = signal.exponential(M=temporal_filter_length, center=0, tau=tau_rise , sym=False)
    exp_d = signal.exponential(M=temporal_filter_length, center=0, tau=tau_decay, sym=False)

    post_syn_potential = exp_d - exp_r
    post_syn_potential /= post_syn_potential.max()

    return post_syn_potential


def construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec):

    num_synapses = tau_rise_vec.shape[0]
    temporal_filter_length = int(4 * tau_decay_vec.max()) + 1

    syn_filter = np.zeros((num_synapses, temporal_filter_length))

    for k, (tau_r, tau_d) in enumerate(zip(tau_rise_vec, tau_decay_vec)):
        syn_filter[k,:] = create_single_PSP_profile(tau_r, tau_d, temporal_filter_length=temporal_filter_length)

    return syn_filter


def simulate_filter_and_fire_cell_training(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                           refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):

    temporal_filter_length = int(5 * refreactory_time_constant) + 1
    refreactory_filter = signal.exponential(M=temporal_filter_length,center=0,tau=refreactory_time_constant,sym=False)[np.newaxis,:]

    # padd input and get all synaptic filters
    normlized_syn_filter = np.flipud(construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec))
    padded_input = np.hstack((np.zeros(normlized_syn_filter.shape), presynaptic_input_spikes))

    # calc local currents
    local_normlized_currents = np.zeros(presynaptic_input_spikes.shape)
    for k in range(normlized_syn_filter.shape[0]):
        local_normlized_currents[k] = signal.convolve(padded_input[k], normlized_syn_filter[k], mode='valid')[1:]

    # multiply by weights to get the somatic current
    soma_current = signal.convolve(local_normlized_currents, synaptic_weights, mode='valid')

    # simulate the cell
    soma_voltage = v_reset + current_to_voltage_mult_factor * soma_current.ravel()
    output_spike_times_in_ms = []
    # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
    for t in range(len(soma_voltage)):
        # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
        if (soma_voltage[t] > v_threshold) and ((t + 1) < len(soma_voltage)):
            t_start = t + 1
            t_end = min(len(soma_voltage), t_start + temporal_filter_length)
            soma_voltage[t_start:t_end] -= (soma_voltage[t + 1] - v_reset) * refreactory_filter.ravel()[:(t_end - t_start)]
            output_spike_times_in_ms.append(t)

    return local_normlized_currents, soma_voltage, output_spike_times_in_ms


def simulate_filter_and_fire_cell_inference(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                            refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):

    temporal_filter_length = int(5 * refreactory_time_constant) + 1
    refreactory_filter = signal.exponential(M=temporal_filter_length,center=0,tau=refreactory_time_constant,sym=False)[np.newaxis,:]

    # padd input and get all synaptic filters
    normlized_syn_filter = construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec)
    padded_input = np.hstack((np.zeros(normlized_syn_filter.shape), presynaptic_input_spikes))

    # calc somatic current
    weighted_syn_filter  = synaptic_weights * normlized_syn_filter
    soma_current = signal.convolve(padded_input, weighted_syn_filter, mode='valid')[:,1:]

    # simulate the cell
    soma_voltage = v_reset + current_to_voltage_mult_factor * soma_current.ravel()
    output_spike_times_in_ms = []
    # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
    for t in range(len(soma_voltage)):

        # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
        if (soma_voltage[t] > v_threshold) and ((t + 1) < len(soma_voltage)):
            t_start = t + 1
            t_end = min(len(soma_voltage), t_start + temporal_filter_length)
            soma_voltage[t_start:t_end] -= (soma_voltage[t + 1] - v_reset) * refreactory_filter.ravel()[:(t_end - t_start)]
            output_spike_times_in_ms.append(t)

    return soma_voltage, output_spike_times_in_ms


# use local currents as "features" and fit a linear model to the data
def prepare_training_dataset(local_normlized_currents, desired_output_spikes, spike_safety_range_ms=10, negative_subsampling_fraction=0.1):

    # remove all "negative" time points that are too close to spikes
    desired_output_spikes_LPF = signal.convolve(desired_output_spikes, np.ones((spike_safety_range_ms,)), mode='same') > 0.1
    desired_timepoints = ~desired_output_spikes_LPF

    # massivly subsample the remaining timepoints
    desired_timepoints[np.random.rand(desired_timepoints.shape[0]) > negative_subsampling_fraction] = 0
    desired_timepoints[desired_output_spikes > 0.1] = 1

    X = local_normlized_currents.T[desired_timepoints,:]
    y = desired_output_spikes[desired_timepoints]

    return X, y


def apply_dendritic_interactions(normlized_synaptic_currents, interactions_degree=2, non_interaction_fraction=0.2):
    output_normlized_synaptic_currents = normlized_synaptic_currents.copy()

    # apply d times random interactions
    for degree in range(interactions_degree - 1):
        random_permutation = np.random.permutation(normlized_synaptic_currents.shape[0])
        output_normlized_synaptic_currents = output_normlized_synaptic_currents * normlized_synaptic_currents[random_permutation]

    # keep some fraction of only individual interactions
    random_permutation = np.random.permutation(normlized_synaptic_currents.shape[0])[:int(normlized_synaptic_currents.shape[0] * non_interaction_fraction)]
    output_normlized_synaptic_currents[random_permutation] = normlized_synaptic_currents[random_permutation]

    return output_normlized_synaptic_currents


#%% Fit I&F and F&F several times with variable multiple connections and different requested number of spikes

stimulus_duration_ms = 1000 * stimulus_duration_sec

all_results_dicts = []
print('------------------------------------------------------------------------------------------')
for model_type in model_type_list:
    for connections_per_axon in connections_per_axon_list:
        for requested_number_of_output_spikes in req_num_output_spikes_list:

            #print('------------------')
            #print('model type = "%s"' %(model_type))
            #print('number of connections per axon = %d' %(connections_per_axon))
            #print('number of requested output spikes = %d' %(requested_number_of_output_spikes))
            #print('------------------')

            full_AUC_list = []
            result_entry_dict = {}

            result_entry_dict['model type'] = model_type
            result_entry_dict['connections per axon'] = connections_per_axon
            result_entry_dict['num output spikes'] = requested_number_of_output_spikes

            for random_iter in random_iterations_list:

                # neuron model parameters
                num_synapses = connections_per_axon * num_axons

                # synapse non-learnable parameters
                if model_type == 'F&F':
                    tau_rise_range  = tau_rise_range_FF
                    tau_decay_range = tau_decay_range_FF
                elif model_type == 'I&F':
                    tau_rise_range  = tau_rise_range_IF
                    tau_decay_range = tau_decay_range_IF

                tau_rise_vec  = np.random.uniform(low=tau_rise_range[0] , high=tau_rise_range[1] , size=(num_synapses, 1))
                tau_decay_vec = np.random.uniform(low=tau_decay_range[0], high=tau_decay_range[1], size=(num_synapses, 1))

                # synapse learnable parameters
                synaptic_weights_vec = np.random.normal(size=(num_synapses, 1))

                # generate sample input
                axons_input_spikes = np.random.rand(num_axons, stimulus_duration_ms) < instantanious_input_spike_probability
                presynaptic_input_spikes = np.kron(np.ones((connections_per_axon,1)), axons_input_spikes)
                assert presynaptic_input_spikes.shape[0] == num_synapses, 'number of synapses doesnt match the number of presynaptic inputs'

                # generate desired pattern of output spikes

                desired_output_spike_times = min_time_between_spikes_ms * np.random.randint(int(stimulus_duration_ms / min_time_between_spikes_ms), size=requested_number_of_output_spikes)
                desired_output_spike_times = np.sort(np.unique(desired_output_spike_times))

                desired_output_spikes = np.zeros((stimulus_duration_ms,))
                desired_output_spikes[desired_output_spike_times] = 1.0

                # simulate cell with normlized currents
                local_normlized_currents, soma_voltage, output_spike_times_in_ms = simulate_filter_and_fire_cell_training(presynaptic_input_spikes,
                                                                                                                          synaptic_weights_vec, tau_rise_vec, tau_decay_vec,
                                                                                                                          refreactory_time_constant=refreactory_time_constant,
                                                                                                                          v_reset=v_reset, v_threshold=v_threshold,
                                                                                                                          current_to_voltage_mult_factor=current_to_voltage_mult_factor)


                # normlized_synaptic_currents = local_normlized_currents
                # plt.close('all')
                # plt.figure(figsize=(16,10))
                # plt.subplot(2,1,1); plt.imshow(normlized_synaptic_currents[:,:1000])
                # plt.subplot(2,1,2); plt.imshow(apply_dendritic_interactions(normlized_synaptic_currents, interactions_degree=2, non_interaction_fraction=0.001)[:,:1000])

                if use_interaction_terms:
                    non_interaction_fraction = num_axons / local_normlized_currents.shape[0]
                    local_normlized_currents = apply_dendritic_interactions(local_normlized_currents, interactions_degree=interactions_degree, non_interaction_fraction=non_interaction_fraction)

                # fit linear model to local currents
                logistic_reg_model = linear_model.LogisticRegression(C=100000, fit_intercept=False, penalty='l2', solver='lbfgs', verbose=False)

                spike_safety_range_ms = 5
                negative_subsampling_fraction = 0.5

                # use local currents as "features" and fit a linear model to the data
                X, y = prepare_training_dataset(local_normlized_currents, desired_output_spikes,
                                                spike_safety_range_ms=spike_safety_range_ms,
                                                negative_subsampling_fraction=negative_subsampling_fraction)
                #print('number of extracted data points for training = %d (%.2f%s positive class)' %(X.shape[0], 100 * y.mean(),'%'))

                # fit model
                logistic_reg_model.fit(X,y)

                # predict and calculate AUC on train data
                y_hat = logistic_reg_model.predict_proba(X)[:,1]
                train_AUC = roc_auc_score(y, y_hat)

                # predict and calculate AUC on full trace
                fitted_output_spike_prob = logistic_reg_model.predict_proba(local_normlized_currents.T)[:,1]
                full_AUC = roc_auc_score(desired_output_spikes, fitted_output_spike_prob)

                # print progress and save result
                #print('random iteration %d: (train AUC, full trace AUC) = (%.5f, %.5f)' %(random_iter, train_AUC, full_AUC))
                full_AUC_list.append(full_AUC)

            # convert to array:
            full_AUC_vec = np.array(full_AUC_list)
            mean_AUC = full_AUC_vec.mean()
            std_AUC = full_AUC_vec.std()

            result_entry_dict['full_AUC_vec'] = full_AUC_vec
            result_entry_dict['mean_AUC'] = mean_AUC
            result_entry_dict['std_AUC'] = std_AUC

            # append
            all_results_dicts.append(result_entry_dict)
            print(result_entry_dict)
            print('------------------------------------------------------------------------------------------')


#%% process results dict for showing capacity results

all_results_curves = {}


for model_type in model_type_list:
    for connections_per_axon in connections_per_axon_list:

        num_spikes_list = []
        mean_AUC_list = []
        std_AUC_list = []

        for res_dict in all_results_dicts:
            if model_type == res_dict['model type'] and connections_per_axon == res_dict['connections per axon']:
                num_spikes_list.append(res_dict['num output spikes'])
                mean_AUC_list.append(res_dict['mean_AUC'])
                std_AUC_list.append(res_dict['std_AUC'])

        num_spikes = np.array(num_spikes_list)
        mean_AUC = np.array(mean_AUC_list)
        std_AUC = np.array(std_AUC_list)

        sorted_inds = np.argsort(num_spikes)

        num_spikes = num_spikes[sorted_inds]
        mean_AUC = mean_AUC[sorted_inds]
        std_AUC = std_AUC[sorted_inds]

        try:
            almost_perfect_accuracy_inds = np.where(mean_AUC > almost_prefect_accuracy_AUC_threshold)
            num_almost_perfectly_placed_spikes = num_spikes[almost_perfect_accuracy_inds[0][-1]]

            dict_key = '%s, %d connections' %(model_type, connections_per_axon)
            all_results_curves[dict_key] = {}
            all_results_curves[dict_key]['num_spikes'] = num_spikes
            all_results_curves[dict_key]['mean_AUC'] = mean_AUC
            all_results_curves[dict_key]['std_AUC'] = std_AUC
            all_results_curves[dict_key]['model_type'] = model_type
            all_results_curves[dict_key]['connections_per_axon'] = connections_per_axon
            all_results_curves[dict_key]['num_almost_perfectly_placed_spikes'] = num_almost_perfectly_placed_spikes
        except:
            print('something wrong skipping')


#%% Accuracy as function of spikes for various conditions

if show_plots:
    plt.figure(figsize=(30,15))
    for key, value in all_results_curves.items():
        plt.errorbar(value['num_spikes'], value['mean_AUC'], yerr=value['std_AUC'], label=key)
    plt.legend(loc='upper right', fontsize=23, ncol=2)
    plt.title('learning to place precisly timed output spikes (random input, %d sec window, %d axons)' %(stimulus_duration_sec, num_axons), fontsize=24)
    plt.ylabel('accuracy at 1ms precision (AUC)', fontsize=24)
    plt.xlabel('num requested spikes to place', fontsize=24)

#%% Number of precisly placed spikes as function of num multiple connections

processed_res_curves = {}

for model_type in model_type_list:

    connections_per_axon = []
    num_almost_perfectly_placed_spikes = []

    for res_dict in all_results_curves.values():
        if model_type == res_dict['model_type']:
            connections_per_axon.append(res_dict['connections_per_axon'])
            num_almost_perfectly_placed_spikes.append(res_dict['num_almost_perfectly_placed_spikes'])

    connections_per_axon = np.array(connections_per_axon)
    num_almost_perfectly_placed_spikes = np.array(num_almost_perfectly_placed_spikes)

    sorted_inds = np.argsort(connections_per_axon)

    connections_per_axon = connections_per_axon[sorted_inds]
    num_almost_perfectly_placed_spikes = num_almost_perfectly_placed_spikes[sorted_inds]

    dict_key = model_type
    processed_res_curves[dict_key] = {}

    processed_res_curves[dict_key]['connections_per_axon'] = connections_per_axon
    processed_res_curves[dict_key]['num_almost_perfectly_placed_spikes'] = num_almost_perfectly_placed_spikes

    print('-----')
    print(model_type)
    print(processed_res_curves[dict_key])
    print('-------------------')


if show_plots:
    plt.figure(figsize=(30,12))
    for key, value in processed_res_curves.items():
        plt.plot(value['connections_per_axon'], value['num_almost_perfectly_placed_spikes'], label=key)
    plt.title('learning to place precisly timed output spikes (random input, %d sec window, %d axons)' %(stimulus_duration_sec, num_axons), fontsize=24)
    plt.xlabel('num multiple connections per axon', fontsize=24)
    plt.ylabel('num almost perfectly placed spikes', fontsize=24)
    plt.legend(loc='upper left', fontsize=23)


#%% Save results for Nicer presentation later on

# collect main script parameters
script_main_params = {}
script_main_params['num_axons']                     = num_axons
script_main_params['stimulus_duration_sec']         = stimulus_duration_sec
script_main_params['min_time_between_spikes_ms']    = min_time_between_spikes_ms
script_main_params['refreactory_time_constant']     = refreactory_time_constant
script_main_params['spike_safety_range_ms']         = spike_safety_range_ms
script_main_params['negative_subsampling_fraction'] = negative_subsampling_fraction
script_main_params['num_random_iter']               = len(random_iterations_list)
script_main_params['use_interaction_terms']         = use_interaction_terms
script_main_params['interactions_degree']           = interactions_degree

script_main_params['almost_prefect_accuracy_AUC_threshold'] = almost_prefect_accuracy_AUC_threshold
script_main_params['instantanious_input_spike_probability'] = instantanious_input_spike_probability
script_main_params['connections_per_axon_list']  = connections_per_axon_list
script_main_params['req_num_output_spikes_list'] = req_num_output_spikes_list
script_main_params['tau_rise_range_FF']  = tau_rise_range_FF
script_main_params['tau_decay_range_FF'] = tau_decay_range_FF
script_main_params['tau_rise_range_IF']  = tau_rise_range_IF
script_main_params['tau_decay_range_IF'] = tau_decay_range_IF
script_main_params['v_reset']     = v_reset
script_main_params['v_threshold'] = v_threshold
script_main_params['current_to_voltage_mult_factor'] = current_to_voltage_mult_factor

# collect main script results
script_results_dict = {}
script_results_dict['script_main_params']   = script_main_params
script_results_dict['all_results_dicts']    = all_results_dicts
script_results_dict['all_results_curves']   = all_results_curves
script_results_dict['processed_res_curves'] = processed_res_curves

# filename
if use_interaction_terms:
    results_filename = 'FF_vs_IF_capacity_comparision_interactions_degree_%d_num_axons_%d__sim_duration_sec_%d__num_mult_conn_%d__rand_rep_%d.pickle' %(interactions_degree, num_axons, stimulus_duration_sec, len(connections_per_axon_list), len(random_iterations_list))
else:
    results_filename = 'FF_vs_IF_capacity_comparision__num_axons_%d__sim_duration_sec_%d__num_mult_conn_%d__rand_rep_%d.pickle' %(num_axons, stimulus_duration_sec, len(connections_per_axon_list), len(random_iterations_list))

# pickle everythin
pickle.dump(script_results_dict, open(data_folder + results_filename, "wb"))

#%% Load the saved pickle just to check it's OK

loaded_script_results_dict = pickle.load(open(data_folder + results_filename, "rb" ))

print('-----------------------------------------------------------------------------------------------------------')
print('loaded_script_results_dict.keys():')
print('----------')
print(loaded_script_results_dict.keys())
print('-----------------------------------------------------------------------------------------------------------')
print('loaded_script_results_dict["script_main_params"].keys():')
print('----------')
[print(x) for x in loaded_script_results_dict['script_main_params'].keys()]
print('-----------------------------------------------------------------------------------------------------------')
print('num_axons =', loaded_script_results_dict['script_main_params']['num_axons'])
print('stimulus_duration_sec =', loaded_script_results_dict['script_main_params']['stimulus_duration_sec'])
print('min_time_between_spikes_ms =', loaded_script_results_dict['script_main_params']['min_time_between_spikes_ms'])
print('refreactory_time_constant =', loaded_script_results_dict['script_main_params']['refreactory_time_constant'])
print('num_random_iter =', loaded_script_results_dict['script_main_params']['num_random_iter'])
print('spike_safety_range_ms =', loaded_script_results_dict['script_main_params']['spike_safety_range_ms'])
print('negative_subsampling_fraction =', loaded_script_results_dict['script_main_params']['negative_subsampling_fraction'])
print('-----------------------------------------------------------------------------------------------------------')

script_duration_min = (time.time() - start_time) / 60
print('-----------------------------------')
print('finished script! took %.1f minutes' %(script_duration_min))
print('-----------------------------------')

================================================================================
================================================================================
create_capacity_figure_Fig2.py:
===============================
import numpy as np
import matplotlib.pyplot as plt
from scipy import signal
from sklearn import linear_model
from sklearn.metrics import roc_curve, roc_auc_score, auc
import pickle
import matplotlib
import matplotlib.gridspec as gridspec

matplotlib.rcParams['pdf.fonttype'] = 42
matplotlib.rcParams['svg.fonttype'] = 'none'

#%% main script params

# input parameters
num_axons = 100

# neuron model parameters
connections_per_axon = 5
num_synapses = connections_per_axon * num_axons

v_reset     = -80
v_threshold = -55
current_to_voltage_mult_factor = 3
refreactory_time_constant = 15

model_type = 'F&F'
#model_type = 'I&F'

# synapse non-learnable parameters
if model_type == 'F&F':
    tau_rise_range  = [1,16]
    tau_decay_range = [8,24]
elif model_type == 'I&F':
    tau_rise_range  = [1,1]
    tau_decay_range = [24,24]

tau_rise_vec  = np.random.uniform(low=tau_rise_range[0] , high=tau_rise_range[1] , size=(num_synapses, 1))
tau_decay_vec = np.random.uniform(low=tau_decay_range[0], high=tau_decay_range[1], size=(num_synapses, 1))

# synapse learnable parameters
synaptic_weights_vec = np.random.normal(size=(num_synapses, 1))

save_figures = True
save_figures = False
all_file_endings_to_use = ['.png', '.pdf', '.svg']

data_folder   = '/filter_and_fire_neuron/results_data_capacity/'
figure_folder = '/filter_and_fire_neuron/saved_figures/'

#%% helper functions

def create_single_PSP_profile(tau_rise, tau_decay, temporal_filter_length=50):

    safety_factor = 1.5
    if tau_rise >= (tau_decay / safety_factor):
        tau_decay = safety_factor * tau_rise

    exp_r = signal.exponential(M=temporal_filter_length, center=0, tau=tau_rise , sym=False)
    exp_d = signal.exponential(M=temporal_filter_length, center=0, tau=tau_decay, sym=False)

    post_syn_potential = exp_d - exp_r
    post_syn_potential /= post_syn_potential.max()

    return post_syn_potential


def construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec):

    num_synapses = tau_rise_vec.shape[0]
    temporal_filter_length = int(4 * tau_decay_vec.max()) + 1

    syn_filter = np.zeros((num_synapses, temporal_filter_length))

    for k, (tau_r, tau_d) in enumerate(zip(tau_rise_vec, tau_decay_vec)):
        syn_filter[k,:] = create_single_PSP_profile(tau_r, tau_d, temporal_filter_length=temporal_filter_length)

    return syn_filter


def simulate_filter_and_fire_cell_training(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                           refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):

    temporal_filter_length = int(5 * refreactory_time_constant) + 1
    refreactory_filter = signal.exponential(M=temporal_filter_length,center=0,tau=refreactory_time_constant,sym=False)[np.newaxis,:]

    # padd input and get all synaptic filters
    normlized_syn_filter = np.flipud(construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec))
    padded_input = np.hstack((np.zeros(normlized_syn_filter.shape), presynaptic_input_spikes))

    # calc local currents
    local_normlized_currents = np.zeros(presynaptic_input_spikes.shape)
    for k in range(normlized_syn_filter.shape[0]):
        local_normlized_currents[k] = signal.convolve(padded_input[k], normlized_syn_filter[k], mode='valid')[1:]

    # multiply by weights to get the somatic current
    soma_current = signal.convolve(local_normlized_currents, synaptic_weights, mode='valid')

    # simulate the cell
    soma_voltage = v_reset + current_to_voltage_mult_factor * soma_current.ravel()
    output_spike_times_in_ms = []
    # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
    for t in range(len(soma_voltage)):
        # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
        if (soma_voltage[t] > v_threshold) and ((t + 1) < len(soma_voltage)):
            t_start = t + 1
            t_end = min(len(soma_voltage), t_start + temporal_filter_length)
            soma_voltage[t_start:t_end] -= (soma_voltage[t + 1] - v_reset) * refreactory_filter.ravel()[:(t_end - t_start)]
            output_spike_times_in_ms.append(t)

    return local_normlized_currents, soma_voltage, output_spike_times_in_ms


def simulate_filter_and_fire_cell_inference(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                            refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):

    temporal_filter_length = int(5 * refreactory_time_constant) + 1
    refreactory_filter = signal.exponential(M=temporal_filter_length,center=0,tau=refreactory_time_constant,sym=False)[np.newaxis,:]

    # padd input and get all synaptic filters
    normlized_syn_filter = construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec)
    padded_input = np.hstack((np.zeros(normlized_syn_filter.shape), presynaptic_input_spikes))

    # calc somatic current
    weighted_syn_filter  = synaptic_weights * normlized_syn_filter
    soma_current = signal.convolve(padded_input, weighted_syn_filter, mode='valid')[:,1:]

    # simulate the cell
    soma_voltage = v_reset + current_to_voltage_mult_factor * soma_current.ravel()
    output_spike_times_in_ms = []
    # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
    for t in range(len(soma_voltage)):

        # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
        if (soma_voltage[t] > v_threshold) and ((t + 1) < len(soma_voltage)):
            t_start = t + 1
            t_end = min(len(soma_voltage), t_start + temporal_filter_length)
            soma_voltage[t_start:t_end] -= (soma_voltage[t + 1] - v_reset) * refreactory_filter.ravel()[:(t_end - t_start)]
            output_spike_times_in_ms.append(t)

    return soma_voltage, output_spike_times_in_ms


# use local currents as "features" and fit a linear model to the data
def prepare_training_dataset(local_normlized_currents, desired_output_spikes, spike_safety_range_ms=10, negative_subsampling_fraction=0.1):

    # remove all "negative" time points that are too close to spikes
    desired_output_spikes_LPF = signal.convolve(desired_output_spikes, np.ones((spike_safety_range_ms,)), mode='same') > 0.1
    desired_timepoints = ~desired_output_spikes_LPF

    # massivly subsample the remaining timepoints
    desired_timepoints[np.random.rand(desired_timepoints.shape[0]) > negative_subsampling_fraction] = 0
    desired_timepoints[desired_output_spikes > 0.1] = 1

    X = local_normlized_currents.T[desired_timepoints,:]
    y = desired_output_spikes[desired_timepoints]

    return X, y


#%% generate sample input

# generate sample input
stimulus_duration_ms = 60000

axons_input_spikes = np.random.rand(num_axons, stimulus_duration_ms) < 0.001

presynaptic_input_spikes = np.kron(np.ones((connections_per_axon,1)), axons_input_spikes)

assert presynaptic_input_spikes.shape[0] == num_synapses, 'number of synapses doesnt match the number of presynaptic inputs'

# generate desired pattern of output spikes
requested_number_of_output_spikes = 40
min_time_between_spikes_ms = 125

desired_output_spike_times = min_time_between_spikes_ms * np.random.randint(int(stimulus_duration_ms / min_time_between_spikes_ms), size=requested_number_of_output_spikes)
desired_output_spike_times = np.sort(np.unique(desired_output_spike_times))

desired_output_spikes = np.zeros((stimulus_duration_ms,))
desired_output_spikes[desired_output_spike_times] = 1.0

print('number of requested output spikes = %d' %(requested_number_of_output_spikes))

# simulate cell with normlized currents
local_normlized_currents, soma_voltage, output_spike_times_in_ms = simulate_filter_and_fire_cell_training(presynaptic_input_spikes,
                                                                                                          synaptic_weights_vec, tau_rise_vec, tau_decay_vec,
                                                                                                          refreactory_time_constant=refreactory_time_constant,
                                                                                                          v_reset=v_reset, v_threshold=v_threshold,
                                                                                                          current_to_voltage_mult_factor=current_to_voltage_mult_factor)

output_spikes = np.zeros((stimulus_duration_ms,))
try:
    output_spikes[np.array(output_spike_times_in_ms)] = 1.0
except:
    print('no output spikes created')

#%% fit linear model to local currents and display GT vs prediction

# fit linear model to local currents
logistic_reg_model = linear_model.LogisticRegression(C=30000, fit_intercept=True, penalty='l2', max_iter=3000)

spike_safety_range_ms = 1
negative_subsampling_fraction = 0.99

X, y = prepare_training_dataset(local_normlized_currents, desired_output_spikes,
                                spike_safety_range_ms=spike_safety_range_ms,
                                negative_subsampling_fraction=negative_subsampling_fraction)

# fit model
logistic_reg_model.fit(X,y)

print('number of data points = %d (%.2f%s positive class)' %(X.shape[0], 100 * y.mean(),'%'))

y_hat = logistic_reg_model.predict_proba(X)[:,1]

# calculate AUC
train_AUC = roc_auc_score(y, y_hat)

fitted_output_spike_prob = logistic_reg_model.predict_proba(local_normlized_currents.T)[:,1]
full_AUC = roc_auc_score(desired_output_spikes, fitted_output_spike_prob)

# get desired FP threshold
desired_false_positive_rate = 0.004

fpr, tpr, thresholds = roc_curve(desired_output_spikes, fitted_output_spike_prob)

desired_fp_ind = np.argmin(abs(fpr-desired_false_positive_rate))
if desired_fp_ind == 0:
    desired_fp_ind = 1

actual_false_positive_rate = fpr[desired_fp_ind]
true_positive_rate         = tpr[desired_fp_ind]
desired_fp_threshold       = thresholds[desired_fp_ind]

AUC_score = auc(fpr, tpr)

print('AUC = %.4f' %(AUC_score))
print('at %.4f FP rate, TP = %.4f' %(actual_false_positive_rate, true_positive_rate))

output_spikes_after_learning = fitted_output_spike_prob > desired_fp_threshold

plt.close('all')
plt.figure(figsize=(25,12))
plt.subplot(2,1,1);
plt.plot(1.05 * y - 0.025); plt.title('train AUC = %.5f' %(train_AUC))
plt.plot(y_hat); plt.xlabel('training samples'); plt.legend(['GT', 'prediction'])

plt.subplot(2,1,2);
plt.plot(1.05 * desired_output_spikes - 0.025); plt.title('full trace AUC = %.5f' %(full_AUC))
plt.plot(fitted_output_spike_prob); plt.xlabel('time [ms]'); plt.legend(['GT', 'prediction'])

#%% Display Input Raster, input and output before and after learning

plt.close('all')
fig = plt.figure(figsize=(20,16))
gs_figure = gridspec.GridSpec(nrows=9,ncols=1)
gs_figure.update(left=0.04, right=0.95, bottom=0.05, top=0.95, wspace=0.1, hspace=0.4)

ax_axons           = plt.subplot(gs_figure[:6,:])
ax_before_learning = plt.subplot(gs_figure[6,:])
ax_after_learning  = plt.subplot(gs_figure[7,:])
ax_desired_output  = plt.subplot(gs_figure[8,:])

syn_activation_time, syn_activation_index = np.nonzero(axons_input_spikes.T)
syn_activation_time = syn_activation_time / 1000

min_time_sec = 0
max_time_sec = stimulus_duration_ms / 1000

time_sec = np.linspace(min_time_sec, max_time_sec, output_spikes.shape[0])

ax_axons.scatter(syn_activation_time, syn_activation_index, s=2, c='k'); ax_axons.set_title('input axons raster', fontsize=15)
ax_axons.set_xlim(min_time_sec, max_time_sec);
ax_axons.set_xticks([])
ax_axons.spines['top'].set_visible(False)
ax_axons.spines['bottom'].set_visible(False)
ax_axons.spines['left'].set_visible(False)
ax_axons.spines['right'].set_visible(False)

ax_before_learning.plot(time_sec, output_spikes, c='k'); ax_before_learning.set_title('before learning', fontsize=15)
ax_before_learning.set_xlim(min_time_sec, max_time_sec);
ax_before_learning.set_xticks([])
ax_before_learning.set_yticks([])
ax_before_learning.spines['top'].set_visible(False)
ax_before_learning.spines['bottom'].set_visible(False)
ax_before_learning.spines['left'].set_visible(False)
ax_before_learning.spines['right'].set_visible(False)

ax_after_learning.plot(time_sec, output_spikes_after_learning, c='k'); ax_after_learning.set_title('after learning', fontsize=15)
ax_after_learning.set_xlim(min_time_sec, max_time_sec);
ax_after_learning.set_xticks([])
ax_after_learning.set_yticks([])
ax_after_learning.spines['top'].set_visible(False)
ax_after_learning.spines['bottom'].set_visible(False)
ax_after_learning.spines['left'].set_visible(False)
ax_after_learning.spines['right'].set_visible(False)

ax_desired_output.plot(time_sec, desired_output_spikes, c='k'); ax_desired_output.set_title('desired output (num spikes = %d)' %(requested_number_of_output_spikes), fontsize=15);
ax_desired_output.set_xlim(min_time_sec, max_time_sec);
ax_desired_output.set_yticks([]);
ax_desired_output.spines['top'].set_visible(False)
ax_desired_output.spines['bottom'].set_visible(False)
ax_desired_output.spines['left'].set_visible(False)
ax_desired_output.spines['right'].set_visible(False)

#%% Load pickle file with previously stored results and check that it's OK

results_filename = data_folder + 'FF_vs_IF_capacity_comparision__num_axons_200__sim_duration_sec_120__num_mult_conn_6__rand_rep_18.pickle'

loaded_script_results_dict = pickle.load(open(results_filename, "rb" ))

print('-----------------------------------------------------------------------------------------------------------')
print('loaded_script_results_dict.keys():')
print('----------')
print(loaded_script_results_dict.keys())
print('-----------------------------------------------------------------------------------------------------------')
print('loaded_script_results_dict["script_main_params"].keys():')
print('----------')
[print(x) for x in loaded_script_results_dict['script_main_params'].keys()]
print('-----------------------------------------------------------------------------------------------------------')
print('num_axons =', loaded_script_results_dict['script_main_params']['num_axons'])
print('stimulus_duration_sec =', loaded_script_results_dict['script_main_params']['stimulus_duration_sec'])
print('min_time_between_spikes_ms =', loaded_script_results_dict['script_main_params']['min_time_between_spikes_ms'])
print('refreactory_time_constant =', loaded_script_results_dict['script_main_params']['refreactory_time_constant'])
print('num_random_iter =', loaded_script_results_dict['script_main_params']['num_random_iter'])
print('spike_safety_range_ms =', loaded_script_results_dict['script_main_params']['spike_safety_range_ms'])
print('negative_subsampling_fraction =', loaded_script_results_dict['script_main_params']['negative_subsampling_fraction'])
print('-----------------------------------------------------------------------------------------------------------')

#%% extract params

processed_res_curves = loaded_script_results_dict['processed_res_curves']
all_results_curves   = loaded_script_results_dict['all_results_curves']

num_axons = loaded_script_results_dict['script_main_params']['num_axons']
stimulus_duration_sec = loaded_script_results_dict['script_main_params']['stimulus_duration_sec']

#%% Fig 2B

num_plots = len(all_results_curves.keys())

num_random_iterations = 5

fig = plt.figure(figsize=(30,15));
for key, value in all_results_curves.items():
    plt.errorbar(value['num_spikes'], value['mean_AUC'], yerr=value['std_AUC'] / np.sqrt(num_random_iterations), label=key, lw=4)
plt.legend(loc='lower left', fontsize=23, ncol=2)
plt.title('learning to place precisly timed output spikes (random input, %d sec window, %d axons)' %(stimulus_duration_sec, num_axons), fontsize=24)
plt.ylabel('accuracy at 1ms precision (AUC)', fontsize=24)
plt.xlabel('num requested spikes to place', fontsize=30);

#%% Fig 2C

filename_str = 'FF_vs_IF_capacity_comparision__num_axons_%d__sim_duration_sec_120__num_mult_conn_6__rand_rep_18.pickle'
num_axons_list = sorted([100, 112, 125, 137, 150, 162, 175, 187, 200, 212, 225, 237])
all_filenames_str = [filename_str %(x) for x in num_axons_list]

model_keys = list(loaded_script_results_dict['processed_res_curves'].keys())
connections_per_axon_2C = loaded_script_results_dict['processed_res_curves'][model_keys[0]]['connections_per_axon']

precisely_timed_spikes_per_axon_2C = {}
precisely_timed_spikes_per_axon_error_2C = {}
for key in model_keys:
    precisely_timed_spikes_per_axon_2C[key] = np.zeros((len(all_filenames_str), len(connections_per_axon_2C)))
    precisely_timed_spikes_per_axon_error_2C[key] = np.zeros((len(all_filenames_str), len(connections_per_axon_2C)))

for k, (curr_num_axons, curr_filename) in enumerate(zip(num_axons_list, all_filenames_str)):
    curr_results_filename = data_folder + curr_filename
    curr_loaded_results_dict = pickle.load(open(curr_results_filename, "rb" ))

    for key in model_keys:
        precisely_timed_spikes_per_axon_2C[key][k,:] = curr_loaded_results_dict['processed_res_curves'][key]['num_almost_perfectly_placed_spikes'] / curr_num_axons

        for j, (num_M_conn, num_spikes) in enumerate(zip(curr_loaded_results_dict['processed_res_curves'][key]['connections_per_axon'],
                                                         curr_loaded_results_dict['processed_res_curves'][key]['num_almost_perfectly_placed_spikes'])):

            model_connections_str = '%s, %d connections' %(key, num_M_conn)
            error_index = list(curr_loaded_results_dict['all_results_curves'][model_connections_str]['num_spikes']).index(num_spikes)
            error_scale = (curr_loaded_results_dict['all_results_curves'][model_connections_str]['num_spikes'][error_index + 1] -
                           curr_loaded_results_dict['all_results_curves'][model_connections_str]['num_spikes'][max(0, error_index - 1)])

            if error_index > 1:
                error_scale /= 2

            precisely_timed_spikes_per_axon_error_2C[key][k,j] = error_scale / curr_num_axons

num_plots = len(processed_res_curves.keys())

color_map = {}
color_map['I&F'] = '0.05'
color_map['F&F'] = 'orange'

fig = plt.figure(figsize=(30,12));
for key in processed_res_curves.keys():
    y_error = precisely_timed_spikes_per_axon_2C[key].std(axis=0)
    plt.errorbar(connections_per_axon_2C, precisely_timed_spikes_per_axon_2C[key].mean(axis=0), yerr=y_error, label=key, lw=4, color=color_map[key])
plt.title('learning to place precisely timed output spikes (random input, %d sec window, %d axons)' %(stimulus_duration_sec, num_axons), fontsize=24)
plt.xlabel('Number of Multiple Contacts - M', fontsize=24)
plt.ylabel('Number of Accuractly Timed Spikes\n per Input Axon', fontsize=24);
plt.legend(loc='upper left', fontsize=40)
plt.yticks([0.15,0.3,0.45])

#%% Fig 2D

filename_str = 'FF_vs_IF_capacity_comparision__num_axons_%d__sim_duration_sec_120__num_mult_conn_5__rand_rep_12.pickle'

num_axons_list = sorted([50,100,200,300,400])
num_multiple_conn_list = [1,3,5,10]

all_filenames_str = [filename_str %(x) for x in num_axons_list]

FF_num_placed_spikes = {}
IF_num_placed_spikes = {}

for num_M_conn in num_multiple_conn_list:

    FF_num_placed_spikes[num_M_conn] = {}
    FF_num_placed_spikes[num_M_conn]['num_accurately_placed_spikes'] = []
    FF_num_placed_spikes[num_M_conn]['num_accurately_placed_spikes_error'] = []

    IF_num_placed_spikes[num_M_conn] = {}
    IF_num_placed_spikes[num_M_conn]['num_accurately_placed_spikes'] = []
    IF_num_placed_spikes[num_M_conn]['num_accurately_placed_spikes_error'] = []

    for curr_num_axons, curr_filename in zip(num_axons_list, all_filenames_str):

        curr_results_filename = data_folder + curr_filename
        curr_loaded_results_dict = pickle.load(open(curr_results_filename, "rb" ))

        FF_ind = list(curr_loaded_results_dict['processed_res_curves']['F&F']['connections_per_axon']).index(num_M_conn)
        FF_num_spikes = curr_loaded_results_dict['processed_res_curves']['F&F']['num_almost_perfectly_placed_spikes'][FF_ind]

        FF_error_index = list(curr_loaded_results_dict['all_results_curves']['F&F, %d connections' %(num_M_conn)]['num_spikes']).index(FF_num_spikes)
        FF_error_scale = (curr_loaded_results_dict['all_results_curves']['F&F, %d connections' %(num_M_conn)]['num_spikes'][FF_error_index + 1] -
                          curr_loaded_results_dict['all_results_curves']['F&F, %d connections' %(num_M_conn)]['num_spikes'][max(0, FF_error_index - 1)])

        if FF_error_index > 1:
            FF_error_scale /= 2

        IF_ind = list(curr_loaded_results_dict['processed_res_curves']['I&F']['connections_per_axon']).index(num_M_conn)
        IF_num_spikes = curr_loaded_results_dict['processed_res_curves']['I&F']['num_almost_perfectly_placed_spikes'][IF_ind]

        IF_error_index = list(curr_loaded_results_dict['all_results_curves']['I&F, %d connections' %(num_M_conn)]['num_spikes']).index(IF_num_spikes)
        IF_error_scale = (curr_loaded_results_dict['all_results_curves']['F&F, %d connections' %(num_M_conn)]['num_spikes'][IF_error_index + 1] -
                          curr_loaded_results_dict['all_results_curves']['F&F, %d connections' %(num_M_conn)]['num_spikes'][max(0, IF_error_index - 1)])

        if IF_error_index > 1:
            IF_error_scale /= 2

        FF_num_placed_spikes[num_M_conn]['num_accurately_placed_spikes'].append(FF_num_spikes)
        FF_num_placed_spikes[num_M_conn]['num_accurately_placed_spikes_error'].append(FF_error_scale)
        IF_num_placed_spikes[num_M_conn]['num_accurately_placed_spikes'].append(IF_num_spikes)
        IF_num_placed_spikes[num_M_conn]['num_accurately_placed_spikes_error'].append(IF_error_scale)


fig = plt.figure(figsize=(25,16))

for M in [10,3]:
    plt.errorbar(num_axons_list, FF_num_placed_spikes[M]['num_accurately_placed_spikes'], yerr=FF_num_placed_spikes[M]['num_accurately_placed_spikes_error'], label='F&F (M = %d)' %(M), lw=5)
plt.errorbar(num_axons_list, IF_num_placed_spikes[1]['num_accurately_placed_spikes'], yerr=FF_num_placed_spikes[1]['num_accurately_placed_spikes_error'], label='I&F', lw=5)

plt.legend(loc='upper left', fontsize=30)
plt.title('Capacity Linearly scales with Number of Axons', fontsize=30)
plt.ylabel('Number of Accuratley Timed Spikes', fontsize=25)
plt.xlabel('Number of Input Axons', fontsize=25);

#%% Full Figure 2

plt.close('all')
fig = plt.figure(figsize=(25,16))
gs_figure = gridspec.GridSpec(nrows=13,ncols=6)
gs_figure.update(left=0.04, right=0.95, bottom=0.05, top=0.95, wspace=0.45, hspace=0.4)

ax_axons            = plt.subplot(gs_figure[:4,:4])
ax_before_learning  = plt.subplot(gs_figure[4,:4])
ax_after_learning   = plt.subplot(gs_figure[5,:4])
ax_desired_output   = plt.subplot(gs_figure[6,:4])
ax_acc_per_n_spikes = plt.subplot(gs_figure[8:,:4])

ax_n_spikes_m_cons  = plt.subplot(gs_figure[:6,4:])
ax_n_spikes_n_axons = plt.subplot(gs_figure[7:,4:])

# 2 A
before_color = '0.15'
after_color  = 'blue'
target_color = 'red'

syn_activation_time, syn_activation_index = np.nonzero(axons_input_spikes.T)
syn_activation_time = syn_activation_time / 1000

min_time_sec = 0
max_time_sec = stimulus_duration_ms / 1000

time_sec = np.linspace(min_time_sec, max_time_sec, output_spikes.shape[0])

ax_axons.scatter(syn_activation_time, syn_activation_index, s=3, c='k'); ax_axons.set_title('Input Axons Raster', fontsize=18)
ax_axons.set_xlim(min_time_sec, max_time_sec);
ax_axons.set_xticks([])
ax_axons.set_yticks([0,25,50,75,100])
ax_axons.set_yticklabels([0,25,50,75,100],fontsize=15)
ax_axons.spines['top'].set_visible(False)
ax_axons.spines['bottom'].set_visible(False)
ax_axons.spines['left'].set_visible(False)
ax_axons.spines['right'].set_visible(False)

ax_before_learning.plot(time_sec, output_spikes, c=before_color, lw=2.5);
ax_before_learning.set_title('Before Learning', fontsize=17, color=before_color)
ax_before_learning.set_xlim(min_time_sec, max_time_sec);
ax_before_learning.set_xticks([])
ax_before_learning.set_yticks([])
ax_before_learning.spines['top'].set_visible(False)
ax_before_learning.spines['bottom'].set_visible(False)
ax_before_learning.spines['left'].set_visible(False)
ax_before_learning.spines['right'].set_visible(False)

ax_after_learning.plot(time_sec, output_spikes_after_learning, c=after_color, lw=2.5);
ax_after_learning.set_title('After Learning', fontsize=17, color=after_color)
ax_after_learning.set_xlim(min_time_sec, max_time_sec);
ax_after_learning.set_xticks([])
ax_after_learning.set_yticks([])
ax_after_learning.spines['top'].set_visible(False)
ax_after_learning.spines['bottom'].set_visible(False)
ax_after_learning.spines['left'].set_visible(False)
ax_after_learning.spines['right'].set_visible(False)

ax_desired_output.plot(time_sec, desired_output_spikes, c=target_color, lw=2.5);
ax_desired_output.set_title('Desired Output (num spikes = %d)' %(requested_number_of_output_spikes), fontsize=17, color=target_color);
ax_desired_output.set_xlim(min_time_sec, max_time_sec);
ax_desired_output.set_yticks([]);
ax_desired_output.set_xticks([0,15,30,45,60])
ax_desired_output.set_xticklabels([0,15,30,45,60],fontsize=15)
ax_desired_output.spines['top'].set_visible(False)
ax_desired_output.spines['bottom'].set_visible(False)
ax_desired_output.spines['left'].set_visible(False)
ax_desired_output.spines['right'].set_visible(False)
ax_desired_output.set_xlabel('time (sec)', fontsize=17);


# 2 B
num_plots = len(all_results_curves.keys())

key_to_label_map = {
    'F&F, 1 connections' : 'F&F (M =  1)',
    'F&F, 2 connections' : 'F&F (M =  2)',
    'F&F, 3 connections' : 'F&F (M =  3)',
    'F&F, 5 connections' : 'F&F (M =  5)',
    'F&F, 10 connections': 'F&F (M = 10)',
    'F&F, 15 connections': 'F&F (M = 15)',
    'I&F, 1 connections' : 'I&F (M =  1)',
    'I&F, 2 connections' : 'I&F (M =  2)',
    'I&F, 3 connections' : 'I&F (M =  3)',
    'I&F, 5 connections' : 'I&F (M =  5)',
    'I&F, 10 connections': 'I&F (M = 10)',
    'I&F, 15 connections': 'I&F (M = 15)'}

key_to_color_map = {
    'F&F, 1 connections' : 'blue',
    'F&F, 2 connections' : 'orange',
    'F&F, 3 connections' : 'green',
    'F&F, 5 connections' : 'crimson',
    'F&F, 10 connections': 'brown',
    'F&F, 15 connections': 'purple',
    'I&F, 1 connections' : 'gray',
    'I&F, 2 connections' : 'gray',
    'I&F, 3 connections' : 'gray',
    'I&F, 5 connections' : 'gray',
    'I&F, 10 connections': 'gray',
    'I&F, 15 connections': 'gray'}

keys_ordering = ['I&F, 1 connections', 'I&F, 2 connections', 'I&F, 3 connections', 'I&F, 5 connections', 'I&F, 10 connections', 'I&F, 15 connections',
                 'F&F, 1 connections', 'F&F, 2 connections', 'F&F, 3 connections', 'F&F, 5 connections', 'F&F, 10 connections', 'F&F, 15 connections']

# for key, value in all_results_curves.items():
for key in keys_ordering:
    value = all_results_curves[key]
    curr_color = key_to_color_map[key]
    curr_label = key_to_label_map[key]
    if curr_color == 'gray':
        ax_acc_per_n_spikes.errorbar(value['num_spikes'], value['mean_AUC'], yerr=value['std_AUC'] / np.sqrt(num_random_iterations),
                                     label=curr_label, color=curr_color, lw=3, alpha=0.6)
    else:
        ax_acc_per_n_spikes.errorbar(value['num_spikes'], value['mean_AUC'], yerr=value['std_AUC'] / np.sqrt(num_random_iterations),
                                     label=curr_label, color=curr_color, lw=3)

ax_acc_per_n_spikes.legend(loc='lower left', fontsize=18, ncol=2)
ax_acc_per_n_spikes.set_title('Placing Precisely Timed output Spikes', fontsize=18)
ax_acc_per_n_spikes.set_ylabel('Accuracy at 1ms Precision (AUC)', fontsize=17)
ax_acc_per_n_spikes.set_xlabel('Number of Requried Precisely Timed Spikes', fontsize=17);
ax_acc_per_n_spikes.spines['top'].set_visible(False)
ax_acc_per_n_spikes.spines['right'].set_visible(False)
ax_acc_per_n_spikes.set_yticks([0.7,0.8,0.9,1.0])
ax_acc_per_n_spikes.set_yticklabels([0.7,0.8,0.9,1.0],fontsize=15)
ax_acc_per_n_spikes.set_xticks([0,50,100,150,200])
ax_acc_per_n_spikes.set_xticklabels([0,50,100,150,200],fontsize=15)


# 2 C
for key in processed_res_curves.keys():
    y_error = precisely_timed_spikes_per_axon_2C[key].std(axis=0)
    ax_n_spikes_m_cons.errorbar(connections_per_axon_2C, precisely_timed_spikes_per_axon_2C[key].mean(axis=0), yerr=y_error, label=key, lw=4, color=color_map[key])

ax_n_spikes_m_cons.legend(loc='upper left', fontsize=22)
ax_n_spikes_m_cons.set_title('Placing Precisely Timed output Spikes', fontsize=18)
ax_n_spikes_m_cons.set_xlabel('Number of Multiple Contacts - M', fontsize=17)
ax_n_spikes_m_cons.set_ylabel('Number of Precisely Timed Spikes / Input Axon', fontsize=17);
ax_n_spikes_m_cons.spines['top'].set_visible(False)
ax_n_spikes_m_cons.spines['right'].set_visible(False)
ax_n_spikes_m_cons.set_yticks([0.15,0.3,0.45])
ax_n_spikes_m_cons.set_ylim([0.08,0.53])
ax_n_spikes_m_cons.set_yticklabels([0.15,0.3,0.45], fontsize=15)
ax_n_spikes_m_cons.set_xticks([1,2,3,5,10,15])
ax_n_spikes_m_cons.set_xticklabels([1,2,3,5,10,15], fontsize=15)


# 2 D
for M in [10,3]:
    ax_n_spikes_n_axons.errorbar(num_axons_list, FF_num_placed_spikes[M]['num_accurately_placed_spikes'], yerr=FF_num_placed_spikes[M]['num_accurately_placed_spikes_error'], label='F&F (M = %d)' %(M), lw=5)
ax_n_spikes_n_axons.errorbar(num_axons_list, IF_num_placed_spikes[1]['num_accurately_placed_spikes'], yerr=FF_num_placed_spikes[1]['num_accurately_placed_spikes_error'], label='I&F', lw=5)

ax_n_spikes_n_axons.legend(loc='upper left', fontsize=22)
ax_n_spikes_n_axons.set_title('Capacity Linearly Scales with Number of Axons', fontsize=18)
ax_n_spikes_n_axons.set_ylabel('Number of Precisely Timed Spikes', fontsize=17)
ax_n_spikes_n_axons.set_xlabel('Number of Input Axons', fontsize=17);
ax_n_spikes_n_axons.set_yticks([0,100,200])
ax_n_spikes_n_axons.set_yticklabels([0,100,200], fontsize=15)
ax_n_spikes_n_axons.set_xticks(num_axons_list)
ax_n_spikes_n_axons.set_xticklabels(num_axons_list, fontsize=15)
ax_n_spikes_n_axons.spines['top'].set_visible(False)
ax_n_spikes_n_axons.spines['right'].set_visible(False)


# save figure
if save_figures:
    figure_name = 'F&F_capacity_Figure_2_%d' %(np.random.randint(200))
    for file_ending in all_file_endings_to_use:
        if file_ending == '.png':
            fig.savefig(figure_folder + figure_name + file_ending, bbox_inches='tight')
        else:
            fig.savefig(figure_folder + figure_name + file_ending, bbox_inches='tight')

================================================================================
================================================================================
create_MNIST_figure_Fig3.py:
============================
import numpy as np
import pandas as pd
import pickle
import matplotlib.pyplot as plt
from scipy import signal
from sklearn import linear_model
from sklearn.metrics import roc_curve, roc_auc_score
from tensorflow import keras
import glob
import matplotlib
import matplotlib.gridspec as gridspec

matplotlib.rcParams['pdf.fonttype'] = 42
matplotlib.rcParams['svg.fonttype'] = 'none'

#%% script params

save_figures = True
save_figures = False
all_file_endings_to_use = ['.png', '.pdf', '.svg']

data_folder   = '/filter_and_fire_neuron/results_data_mnist/'
figure_folder = '/filter_and_fire_neuron/saved_figures/'

build_dataframe_from_scratch = False

#%% helper functions


def create_single_PSP_profile(tau_rise, tau_decay, temporal_filter_length=50):

    safety_factor = 1.5
    if tau_rise >= (tau_decay / safety_factor):
        tau_decay = safety_factor * tau_rise

    exp_r = signal.exponential(M=temporal_filter_length, center=0, tau=tau_rise , sym=False)
    exp_d = signal.exponential(M=temporal_filter_length, center=0, tau=tau_decay, sym=False)

    post_syn_potential = exp_d - exp_r
    post_syn_potential /= post_syn_potential.max()

    return post_syn_potential


def construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec):

    num_synapses = tau_rise_vec.shape[0]
    temporal_filter_length = int(4 * tau_decay_vec.max()) + 1

    syn_filter = np.zeros((num_synapses, temporal_filter_length))

    for k, (tau_r, tau_d) in enumerate(zip(tau_rise_vec, tau_decay_vec)):
        syn_filter[k,:] = create_single_PSP_profile(tau_r, tau_d, temporal_filter_length=temporal_filter_length)

    return syn_filter


def simulate_filter_and_fire_cell_training(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                           refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):

    temporal_filter_length = int(5 * refreactory_time_constant) + 1
    refreactory_filter = signal.exponential(M=temporal_filter_length,center=0,tau=refreactory_time_constant,sym=False)[np.newaxis,:]

    # padd input and get all synaptic filters
    normlized_syn_filter = np.flipud(construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec))
    padded_input = np.hstack((np.zeros(normlized_syn_filter.shape), presynaptic_input_spikes))

    # calc local currents
    local_normlized_currents = np.zeros(presynaptic_input_spikes.shape, dtype=np.float16)
    for k in range(normlized_syn_filter.shape[0]):
        local_normlized_currents[k] = signal.convolve(padded_input[k], normlized_syn_filter[k], mode='valid')[1:]

    # multiply by weights to get the somatic current
    soma_current = signal.convolve(local_normlized_currents, synaptic_weights, mode='valid')

    # simulate the cell
    soma_voltage = v_reset + current_to_voltage_mult_factor * soma_current.ravel()
    output_spike_times_in_ms = []
    # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
    for t in range(len(soma_voltage)):
        # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
        if (soma_voltage[t] > v_threshold) and ((t + 1) < len(soma_voltage)):
            t_start = t + 1
            t_end = min(len(soma_voltage), t_start + temporal_filter_length)
            soma_voltage[t_start:t_end] -= (soma_voltage[t + 1] - v_reset) * refreactory_filter.ravel()[:(t_end - t_start)]
            output_spike_times_in_ms.append(t)

    return local_normlized_currents, soma_voltage, output_spike_times_in_ms


def simulate_filter_and_fire_cell_training_long(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                                refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):


    total_duration_ms = presynaptic_input_spikes.shape[1]
    max_duration_per_call_ms = 50000
    overlap_time_ms = 500

    if max_duration_per_call_ms >= total_duration_ms:
        local_normlized_currents, soma_voltage, output_spike_times_in_ms = simulate_filter_and_fire_cell_training(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                                                                                                  refreactory_time_constant=refreactory_time_constant, v_reset=v_reset, v_threshold=v_threshold,
                                                                                                                  current_to_voltage_mult_factor=current_to_voltage_mult_factor)
        return local_normlized_currents, soma_voltage, output_spike_times_in_ms


    local_normlized_currents = np.zeros(presynaptic_input_spikes.shape, dtype=np.float16)
    soma_voltage = np.zeros((total_duration_ms,))
    output_spike_times_in_ms = []

    num_sub_calls = int(np.ceil(total_duration_ms / (max_duration_per_call_ms - overlap_time_ms)))
    end_ind = overlap_time_ms
    for k in range(num_sub_calls):
        start_ind = end_ind - overlap_time_ms
        end_ind = min(start_ind + max_duration_per_call_ms, total_duration_ms)

        curr_loc_norm_c, curr_soma_v, curr_out_sp_t = simulate_filter_and_fire_cell_training(presynaptic_input_spikes[:,start_ind:end_ind], synaptic_weights, tau_rise_vec, tau_decay_vec,
                                                                                             refreactory_time_constant=refreactory_time_constant, v_reset=v_reset, v_threshold=v_threshold,
                                                                                             current_to_voltage_mult_factor=current_to_voltage_mult_factor)

        # update fields
        if k == 0:
            local_normlized_currents[:,start_ind:end_ind] = curr_loc_norm_c
            soma_voltage[start_ind:end_ind] = curr_soma_v
            output_spike_times_in_ms += curr_out_sp_t
        else:
            local_normlized_currents[:,(start_ind+overlap_time_ms):end_ind] = curr_loc_norm_c[:,overlap_time_ms:end_ind]
            soma_voltage[(start_ind+overlap_time_ms):end_ind] = curr_soma_v[overlap_time_ms:]
            curr_out_sp_t = [x for x in curr_out_sp_t if x >= (overlap_time_ms-1)]
            output_spike_times_in_ms += [(start_ind + x) for x in curr_out_sp_t]

    return local_normlized_currents, soma_voltage, output_spike_times_in_ms


def simulate_filter_and_fire_cell_inference(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                            refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):

    temporal_filter_length = int(5 * refreactory_time_constant) + 1
    refreactory_filter = signal.exponential(M=temporal_filter_length,center=0,tau=refreactory_time_constant,sym=False)[np.newaxis,:]

    # padd input and get all synaptic filters
    normlized_syn_filter = construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec)
    padded_input = np.hstack((np.zeros(normlized_syn_filter.shape), presynaptic_input_spikes))

    # calc somatic current
    weighted_syn_filter  = synaptic_weights * normlized_syn_filter
    soma_current = signal.convolve(padded_input, weighted_syn_filter, mode='valid')[:,1:]

    # simulate the cell
    soma_voltage = v_reset + current_to_voltage_mult_factor * soma_current.ravel()
    output_spike_times_in_ms = []
    # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
    for t in range(len(soma_voltage)):

        # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
        if (soma_voltage[t] > v_threshold) and ((t + 1) < len(soma_voltage)):
            t_start = t + 1
            t_end = min(len(soma_voltage), t_start + temporal_filter_length)
            soma_voltage[t_start:t_end] -= (soma_voltage[t + 1] - v_reset) * refreactory_filter.ravel()[:(t_end - t_start)]
            output_spike_times_in_ms.append(t)

    return soma_voltage, output_spike_times_in_ms


def simulate_filter_and_fire_cell_inference_long(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                                refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):

    total_duration_ms = presynaptic_input_spikes.shape[1]
    max_duration_per_call_ms = 50000
    overlap_time_ms = 500

    if max_duration_per_call_ms >= total_duration_ms:
        soma_voltage, output_spike_times_in_ms = simulate_filter_and_fire_cell_inference(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                                                                         refreactory_time_constant=refreactory_time_constant, v_reset=v_reset, v_threshold=v_threshold,
                                                                                         current_to_voltage_mult_factor=current_to_voltage_mult_factor)
        return soma_voltage, output_spike_times_in_ms


    soma_voltage = np.zeros((total_duration_ms,))
    output_spike_times_in_ms = []

    num_sub_calls = int(np.ceil(total_duration_ms / (max_duration_per_call_ms - overlap_time_ms)))
    end_ind = overlap_time_ms
    for k in range(num_sub_calls):
        start_ind = end_ind - overlap_time_ms
        end_ind = min(start_ind + max_duration_per_call_ms, total_duration_ms)

        curr_soma_v, curr_out_sp_t = simulate_filter_and_fire_cell_inference(presynaptic_input_spikes[:,start_ind:end_ind], synaptic_weights, tau_rise_vec, tau_decay_vec,
                                                                             refreactory_time_constant=refreactory_time_constant, v_reset=v_reset, v_threshold=v_threshold,
                                                                             current_to_voltage_mult_factor=current_to_voltage_mult_factor)

        # update fields
        if k == 0:
            soma_voltage[start_ind:end_ind] = curr_soma_v
            output_spike_times_in_ms += curr_out_sp_t
        else:
            soma_voltage[(start_ind+overlap_time_ms):end_ind] = curr_soma_v[overlap_time_ms:]
            curr_out_sp_t = [x for x in curr_out_sp_t if x >= (overlap_time_ms-1)]
            output_spike_times_in_ms += [(start_ind + x) for x in curr_out_sp_t]

    return soma_voltage, output_spike_times_in_ms


# use local currents as "features" and fit a linear model to the data
def prepare_training_dataset(local_normlized_currents, desired_output_spikes, spike_safety_range_ms=10, negative_subsampling_fraction=0.1):

    # remove all "negative" time points that are too close to spikes
    desired_output_spikes_LPF = signal.convolve(desired_output_spikes, np.ones((spike_safety_range_ms,)), mode='same') > 0.1
    desired_timepoints = ~desired_output_spikes_LPF

    # massivly subsample the remaining timepoints
    desired_timepoints[np.random.rand(desired_timepoints.shape[0]) > negative_subsampling_fraction] = 0
    desired_timepoints[desired_output_spikes > 0.1] = 1

    X = local_normlized_currents.T[desired_timepoints,:]
    y = desired_output_spikes[desired_timepoints]

    return X, y


#%% load MNIST dataset and show the data

(x_train_original, y_train), (x_test_original, y_test) = keras.datasets.mnist.load_data()

num_rows = 5
num_cols = 7

plt.close('all')
plt.figure(figsize=(20,15))
for k in range(num_rows * num_cols):
    rand_sample_ind = np.random.randint(x_train_original.shape[0])
    plt.subplot(num_rows, num_cols, k + 1);
    plt.imshow(x_train_original[k]); plt.title('digit "%s"' %(y_train[k]))

#%% Crop the data and binarize it

h_crop_range = [4,24]
w_crop_range = [4,24]

positive_threshold = 150

x_train_original = x_train_original[:,h_crop_range[0]:h_crop_range[1],w_crop_range[0]:w_crop_range[1]] > positive_threshold
x_test_original  = x_test_original[: ,h_crop_range[0]:h_crop_range[1],w_crop_range[0]:w_crop_range[1]] > positive_threshold

num_rows = 5
num_cols = 7

plt.close('all')
plt.figure(figsize=(20,15))
for k in range(num_rows * num_cols):
    rand_sample_ind = np.random.randint(x_train_original.shape[0])
    plt.subplot(num_rows, num_cols, k + 1);
    plt.imshow(x_train_original[k]); plt.title('digit "%s"' %(y_train[k]))

#%% Transform Xs to spatio-temporal spike trains

spatial_extent_factor = 5
temporal_extent_factor_numerator = 2
temporal_extent_factor_denumerator = 1

num_const_firing_channels = 20
temporal_silence_ms = 70

# extend according to "temporal_extent_factor"
kernel = np.ones((1, spatial_extent_factor, temporal_extent_factor_numerator), dtype=bool)

x_train = x_train_original.copy()
x_test  = x_test_original.copy()

# reshape X according to what is needed
x_train = np.kron(x_train, kernel)
x_test = np.kron(x_test, kernel)

# subsample according to "temporal_extent_factor_denumerator"
x_train = x_train[:,:,::temporal_extent_factor_denumerator]
x_test = x_test[:,:,::temporal_extent_factor_denumerator]

# padd with ones on top (for "bias" learning)
top_pad_train = np.ones((1, num_const_firing_channels, x_train.shape[2]), dtype=bool)
top_pad_test  = np.ones((1, num_const_firing_channels, x_test.shape[2] ), dtype=bool)

# add a few zero rows for clear seperation for visualization purpuses
top_pad_train[:,-5:,:] = 0
top_pad_test[:,-5:,:] = 0

x_train = np.concatenate((np.tile(top_pad_train, [x_train.shape[0],1,1]), x_train), axis=1)
x_test  = np.concatenate((np.tile(top_pad_test , [x_test.shape[0],1,1] ), x_test ), axis=1)

# pad with "temporal_silence_ms" zeros in the begining of each pattern (for silence between patterns)
left_pad_train = np.zeros((1, x_train.shape[1], temporal_silence_ms), dtype=bool)
left_pad_test  = np.zeros((1, x_test.shape[1] , temporal_silence_ms), dtype=bool)

x_train = np.concatenate((np.tile(left_pad_train, [x_train.shape[0],1,1]), x_train), axis=2)
x_test  = np.concatenate((np.tile(left_pad_test , [x_test.shape[0],1,1] ), x_test ), axis=2)

# add background activity
desired_background_activity_firing_rate_Hz = 10
background_activity_fraction = desired_background_activity_firing_rate_Hz / 1000

x_train[np.random.rand(x_train.shape[0], x_train.shape[1], x_train.shape[2]) < background_activity_fraction] = 1
x_test[ np.random.rand(x_test.shape[0] , x_test.shape[1] , x_test.shape[2] ) < background_activity_fraction] = 1

# subsample the input spikes
desired_average_input_firing_rate_Hz = 20
actual_mean_firing_rate_Hz = 1000 * x_train.mean()

fraction_of_spikes_to_eliminate = desired_average_input_firing_rate_Hz / actual_mean_firing_rate_Hz

x_train = x_train * (np.random.rand(x_train.shape[0], x_train.shape[1], x_train.shape[2]) < fraction_of_spikes_to_eliminate)
x_test  = x_test  * (np.random.rand(x_test.shape[0], x_test.shape[1], x_test.shape[2]) < fraction_of_spikes_to_eliminate)

final_mean_firing_rate_Hz = 1000 * x_train.mean()

# display the patterns
num_rows = 5
num_cols = 7

plt.close('all')
plt.figure(figsize=(20,15))
for k in range(num_rows * num_cols):
    rand_sample_ind = np.random.randint(x_train.shape[0])
    plt.subplot(num_rows, num_cols, k + 1);
    plt.imshow(x_train[k], cmap='gray'); plt.title('digit "%s"' %(y_train[k]))

#%% Create "one-vs-all" dataset

positive_digit = 3
num_train_positive_patterns = 7000

release_probability = 1.0
apply_release_prob_during_train = False
apply_releash_prob_during_test = False

y_train_binary = y_train == positive_digit
y_test_binary = y_test == positive_digit

num_train_positive_patterns = min(int(y_train_binary.sum()), num_train_positive_patterns)

num_train_negative_patterns = int(2.0 * num_train_positive_patterns)

positive_inds = np.where(y_train_binary)[0]
negative_inds = np.where(~y_train_binary)[0]

selected_train_positives = np.random.choice(positive_inds, size=num_train_positive_patterns)
selected_train_negatives = np.random.choice(negative_inds, size=num_train_negative_patterns)

all_selected = np.random.permutation(np.concatenate((selected_train_positives, selected_train_negatives)))

X_train_spikes = x_train[all_selected]
Y_train_spikes = y_train_binary[all_selected]

X_test_spikes = x_test.copy()
Y_test_spikes = y_test_binary.copy()

zero_pred_baseline_accuracy = 100 * (1 - Y_test_spikes.mean())

#%% Create a regularized logistic regression baseline

logistic_reg_model = linear_model.LogisticRegression(C=0.1, fit_intercept=False, penalty='l2',verbose=False)

# fit model
logistic_reg_model.fit(X_train_spikes.reshape([X_train_spikes.shape[0],-1]), Y_train_spikes)

# predict and calculate AUC on train data
Y_train_spikes_hat = logistic_reg_model.predict_proba(X_train_spikes.reshape([X_train_spikes.shape[0],-1]))[:,1]
Y_test_spikes_hat = logistic_reg_model.predict_proba(X_test_spikes.reshape([X_test_spikes.shape[0],-1]))[:,1]

train_AUC = roc_auc_score(Y_train_spikes, Y_train_spikes_hat)
test_AUC = roc_auc_score(Y_test_spikes, Y_test_spikes_hat)

print('for (# pos = %d, # neg = %d): (train AUC, test AUC) = (%.5f, %.5f)' %(num_train_positive_patterns, num_train_negative_patterns, train_AUC, test_AUC))

plt.close('all')
plt.figure(figsize=(8,8))
plt.imshow(logistic_reg_model.coef_.reshape([X_train_spikes.shape[1], X_train_spikes.shape[2]]));
plt.title('Learned Weights \n (spatio-temporal ("image") logistic regression)');

#%% Calculate and Display LogReg Accuracy

LL_false_positive_list, LL_true_positive_list, LL_thresholds_list = roc_curve(Y_test_spikes, Y_test_spikes_hat)

num_pos_class = int((Y_test_spikes == True).sum())
num_neg_class = int((Y_test_spikes == False).sum())

tp = LL_true_positive_list * num_pos_class
tn = (1 - LL_false_positive_list) * num_neg_class
LL_accuracy_list = (tp + tn) / (num_pos_class + num_neg_class)

LL_false_positive_list = LL_false_positive_list[LL_false_positive_list < 0.05]
LL_true_positive_list = LL_true_positive_list[:len(LL_false_positive_list)]
LL_thresholds_list = LL_thresholds_list[:len(LL_false_positive_list)]
LL_accuracy_list = LL_accuracy_list[:len(LL_false_positive_list)]

LL_false_positive_list = 100 * LL_false_positive_list
LL_true_positive_list = 100 * LL_true_positive_list
LL_accuracy_list = 100 * LL_accuracy_list

LL_accuracy_max = LL_accuracy_list.max()

LL_accuracy_subsampled = LL_accuracy_list[30::30]
LL_thresholds_subsampled = LL_thresholds_list[30::30]

acc_bar_x_axis = range(LL_accuracy_subsampled.shape[0])

plt.close('all')
plt.figure(figsize=(15,8));
plt.subplot(1,2,1); plt.bar(x=acc_bar_x_axis,height=LL_accuracy_subsampled);
plt.xticks(acc_bar_x_axis, LL_thresholds_subsampled, rotation='vertical');
plt.title('max accuracy = %.2f%s' %(LL_accuracy_max,'%'), fontsize=24)
plt.ylim(87.8,100); plt.xlabel('threshold', fontsize=20); plt.ylabel('Accuracy (%)', fontsize=20);
plt.plot([acc_bar_x_axis[0]-1, acc_bar_x_axis[-1]+1], [zero_pred_baseline_accuracy, zero_pred_baseline_accuracy], color='r')
plt.subplot(1,2,2); plt.plot(LL_false_positive_list, LL_true_positive_list);
plt.ylabel('True Positive (%)', fontsize=20); plt.xlabel('False Positive (%)', fontsize=20);


#%% Fit a F&F model

# main parameters
connections_per_axon = 5
model_type = 'F&F'
#model_type = 'I&F'

# neuron model parameters
num_axons = X_train_spikes[0].shape[0]
num_synapses = connections_per_axon * num_axons

v_reset     = -80
v_threshold = -55
current_to_voltage_mult_factor = 3
refreactory_time_constant = 15

# synapse non-learnable parameters
if model_type == 'F&F':
    tau_rise_range  = [1,18]
    tau_decay_range = [8,27]
elif model_type == 'I&F':
    tau_rise_range  = [1,1]
    tau_decay_range = [27,27]

tau_rise_vec  = np.random.uniform(low=tau_rise_range[0] , high=tau_rise_range[1] , size=(num_synapses, 1))
tau_decay_vec = np.random.uniform(low=tau_decay_range[0], high=tau_decay_range[1], size=(num_synapses, 1))

# synapse learnable parameters
synaptic_weights_vec = np.random.normal(size=(num_synapses, 1))

# prepare input spikes
axons_input_spikes = np.concatenate([X_train_spikes[k] for k in range(X_train_spikes.shape[0])],axis=1)

# prepare output spikes
pattern_duration_ms = X_train_spikes[0].shape[1]
output_spike_offset = 1
output_kernel = np.zeros((pattern_duration_ms,))
output_kernel[-output_spike_offset] = 1

desired_output_spikes = np.kron(Y_train_spikes, output_kernel)

plt.close('all')
plt.figure(figsize=(30,15));
plt.imshow(axons_input_spikes[:,:1101], cmap='gray')
plt.title('input axons raster', fontsize=22)
plt.ylabel('axon index', fontsize=22);
plt.xlabel('time [ms]', fontsize=22);

plt.figure(figsize=(30,1));
plt.plot(desired_output_spikes[:1101]); plt.xlim(0,1101)
plt.ylabel('output spike', fontsize=22)
plt.xlabel('time [ms]', fontsize=22);

#%% simulate cell with normlized currents

presynaptic_input_spikes = np.kron(np.ones((connections_per_axon,1), dtype=bool), axons_input_spikes).astype(bool)

local_normlized_currents, soma_voltage, output_spike_times_in_ms = simulate_filter_and_fire_cell_training_long(presynaptic_input_spikes,
                                                                                                               synaptic_weights_vec, tau_rise_vec, tau_decay_vec,
                                                                                                               refreactory_time_constant=refreactory_time_constant,
                                                                                                               v_reset=v_reset, v_threshold=v_threshold,
                                                                                                               current_to_voltage_mult_factor=current_to_voltage_mult_factor)

#%% fit linear model to local currents

filter_and_fire_model = linear_model.LogisticRegression(C=10000, fit_intercept=False, penalty='l2')

spike_safety_range_ms = 20
negative_subsampling_fraction = 0.5

X, y = prepare_training_dataset(local_normlized_currents, desired_output_spikes,
                                spike_safety_range_ms=spike_safety_range_ms,
                                negative_subsampling_fraction=negative_subsampling_fraction)

# fit model
filter_and_fire_model.fit(X, y)

print('number of data points = %d (%.2f%s positive class)' %(X.shape[0], 100 * y.mean(),'%'))

# calculate train AUC
y_hat = filter_and_fire_model.predict_proba(X)[:,1]
train_AUC = roc_auc_score(y, y_hat)

# display some training data predictions
num_timepoints_to_show = 10000
fitted_output_spike_prob = filter_and_fire_model.predict_proba(local_normlized_currents[:,:num_timepoints_to_show].T)[:,1]

plt.close('all')
plt.figure(figsize=(30,10))
plt.plot(1.05 * desired_output_spikes[:num_timepoints_to_show] - 0.025); plt.title('train AUC = %.5f' %(train_AUC), fontsize=22)
plt.plot(fitted_output_spike_prob[:num_timepoints_to_show]); plt.xlabel('time [ms]'); plt.legend(['GT', 'prediction'], fontsize=22);

#%% display learned weights

normlized_syn_filter = construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec)

# collect learned synaptic weights
FF_learned_synaptic_weights = np.fliplr(filter_and_fire_model.coef_).T
weighted_syn_filter = FF_learned_synaptic_weights * normlized_syn_filter

axon_spatio_temporal_pattern = np.zeros((num_axons, weighted_syn_filter.shape[1]))
for k in range(num_axons):
    axon_spatio_temporal_pattern[k] = weighted_syn_filter[k::num_axons].sum(axis=0)

axon_spatio_temporal_pattern_short = axon_spatio_temporal_pattern[:,:X_train_spikes.shape[2]]

plt.close('all')
plt.figure(figsize=(18,8))
plt.subplot(1,2,1); plt.imshow(logistic_reg_model.coef_.reshape([X_train_spikes.shape[1], X_train_spikes.shape[2]])); plt.title('logistic regression', fontsize=20)
plt.subplot(1,2,2); plt.imshow(np.flip(axon_spatio_temporal_pattern_short)); plt.title('filter and fire neuron', fontsize=20);

#%% Make a prediction on the entire test trace

num_test_patterns = X_test_spikes.shape[0]

# prepare test outputs
output_spike_tolorance_window_duration = 20
output_spike_tolorance_window_offset   = 5
output_kernel_test = np.zeros((X_test_spikes[0].shape[1],))
output_kernel_test[-output_spike_tolorance_window_duration:] = 1

desired_output_spikes_test = np.kron(Y_test_spikes[:num_test_patterns], output_kernel_test)
desired_output_spikes_test = np.concatenate((np.zeros((output_spike_tolorance_window_offset,)), desired_output_spikes_test[:-output_spike_tolorance_window_offset]))

# prepare test inputs
axons_input_spikes_test = np.concatenate([X_test_spikes[k] for k in range(num_test_patterns)],axis=1)
presynaptic_input_spikes_test = np.kron(np.ones((connections_per_axon,1), dtype=bool), axons_input_spikes_test).astype(bool)

# add synaptic unrelability ("release probability" that is not 100%)
if apply_releash_prob_during_test:
    presynaptic_input_spikes_test = presynaptic_input_spikes_test * (np.random.rand(presynaptic_input_spikes_test.shape[0], presynaptic_input_spikes_test.shape[1]) < release_probability)

FF_weight_mult_factors_list = [x for x in [1,2,3,4,5,6,9,12,20,50,120,250]]
FF_accuracy_list = []
FF_true_positive_list = []
FF_false_positive_list = []
for weight_mult_factor in FF_weight_mult_factors_list:

    # collect learned synaptic weights
    synaptic_weights_post_learning = weight_mult_factor * FF_learned_synaptic_weights

    soma_voltage_test, output_spike_times_in_ms_test = simulate_filter_and_fire_cell_inference_long(presynaptic_input_spikes_test,
                                                                                                    synaptic_weights_post_learning, tau_rise_vec, tau_decay_vec,
                                                                                                    refreactory_time_constant=refreactory_time_constant,
                                                                                                    v_reset=v_reset, v_threshold=v_threshold,
                                                                                                    current_to_voltage_mult_factor=current_to_voltage_mult_factor)


    output_spikes_test = np.zeros(soma_voltage_test.shape)
    try:
        output_spikes_test[np.array(output_spike_times_in_ms_test)] = 1.0
    except:
        print('no output spikes created')


    # calculate test accuracy
    compact_desired_output_test = Y_test_spikes[:num_test_patterns]

    compact_desired_output_test2 = np.zeros(compact_desired_output_test.shape, dtype=bool)
    compact_predicted_output_test = np.zeros(compact_desired_output_test.shape, dtype=bool)

    # go over all patterns and extract the prediction (depends if the spikes are in the desired window)
    for pattern_ind in range(num_test_patterns):
        start_ind = pattern_duration_ms * pattern_ind + output_spike_tolorance_window_offset
        end_ind = start_ind + pattern_duration_ms

        # extract prediction
        predicted_spike_train_for_pattern = output_spikes_test[start_ind:end_ind]
        desired_spike_train_for_pattern = desired_output_spikes_test[start_ind:end_ind]

        compact_desired_output_test2[pattern_ind]  = desired_spike_train_for_pattern.sum() > 0.1

        if Y_test_spikes[pattern_ind] == 1:
            # check if there is a spike in the desired window only
            compact_predicted_output_test[pattern_ind] = (desired_spike_train_for_pattern * predicted_spike_train_for_pattern).sum() > 0.1
        else:
            # check if there is any spike in the full pattern duration
            compact_predicted_output_test[pattern_ind] = predicted_spike_train_for_pattern.sum() > 0.1

    # small verificaiton
    assert((compact_desired_output_test == compact_desired_output_test2).sum() == num_test_patterns)

    # display accuracy
    percent_accuracy = 100 * (compact_desired_output_test == compact_predicted_output_test).mean()
    true_positive    = 100 * (np.logical_and(compact_desired_output_test == True , compact_predicted_output_test == True).sum() / (compact_desired_output_test == True).sum())
    false_positive   = 100 * (np.logical_and(compact_desired_output_test == False, compact_predicted_output_test == True).sum() / (compact_desired_output_test == False).sum())

    print('weights mult factor = %.1f: Accuracy = %.3f%s. (TP, FP) = (%.3f%s, %.3f%s)' %(weight_mult_factor, percent_accuracy,'%',true_positive,'%',false_positive,'%'))

    FF_accuracy_list.append(percent_accuracy)
    FF_true_positive_list.append(true_positive)
    FF_false_positive_list.append(false_positive)

#%% "after learning" Build the nice looking figure of before and after learning

# get the max accuracy weight matrix
max_accuracy_weight_mult_factor = FF_weight_mult_factors_list[np.argsort(np.array(FF_accuracy_list))[-1]]

synaptic_weights_vec_after_learning = max_accuracy_weight_mult_factor * FF_learned_synaptic_weights

# simulate the max accuracy output after learning
soma_voltage_test, output_spike_times_in_ms_test = simulate_filter_and_fire_cell_inference_long(presynaptic_input_spikes_test,
                                                                                                synaptic_weights_vec_after_learning, tau_rise_vec, tau_decay_vec,
                                                                                                refreactory_time_constant=refreactory_time_constant,
                                                                                                v_reset=v_reset, v_threshold=v_threshold,
                                                                                                current_to_voltage_mult_factor=current_to_voltage_mult_factor)


output_spikes_test_after_learning_full = np.zeros(soma_voltage_test.shape)
try:
    output_spikes_test_after_learning_full[np.array(output_spike_times_in_ms_test)] = 1.0
except:
    print('no output spikes created')

#%% "before learning" Simulate response to test set before learning

synaptic_weights_vec_before_learning = 0.01 + 0.3 * np.random.normal(size=(num_synapses, 1))

# simulate response to test set before learning (randomly permuted learned weights vector)
soma_voltage_test, output_spike_times_in_ms_test = simulate_filter_and_fire_cell_inference_long(presynaptic_input_spikes_test,
                                                                                                synaptic_weights_vec_before_learning, tau_rise_vec, tau_decay_vec,
                                                                                                refreactory_time_constant=refreactory_time_constant,
                                                                                                v_reset=v_reset, v_threshold=v_threshold,
                                                                                                current_to_voltage_mult_factor=current_to_voltage_mult_factor)


output_spikes_test_before_learning_full = np.zeros(soma_voltage_test.shape)
try:
    output_spikes_test_before_learning_full[np.array(output_spike_times_in_ms_test)] = 1.0
except:
    print('no output spikes created')


#%% organize everything into a figure

# test digit images
extention_kernel = np.ones((1, spatial_extent_factor, temporal_extent_factor_numerator), dtype=bool)
x_test_original_extended = np.kron(x_test_original, extention_kernel)
left_pad_test  = np.zeros((1, x_test_original_extended.shape[1] , temporal_silence_ms), dtype=bool)
x_test_original_extended  = np.concatenate((np.tile(left_pad_test , [x_test_original_extended.shape[0],1,1] ), x_test_original_extended ), axis=2)
x_test_axons_input_spikes = np.concatenate([x_test_original_extended[k] for k in range(x_test_original_extended.shape[0])],axis=1)

test_set_full_duration_ms = x_test_axons_input_spikes.shape[1]

# select a subset of time to display
num_digits_to_display = 9
start_time = x_test_original_extended.shape[2] * np.random.randint(int(test_set_full_duration_ms / x_test_original_extended.shape[2] - temporal_silence_ms))
end_time = start_time + 1 + x_test_original_extended.shape[2] * num_digits_to_display + temporal_silence_ms

num_spikes_in_window = 0
while num_spikes_in_window != 3:
    start_time = x_test_original_extended.shape[2] * np.random.randint(int(test_set_full_duration_ms / x_test_original_extended.shape[2] - temporal_silence_ms))
    end_time = start_time + 1 + x_test_original_extended.shape[2] * num_digits_to_display + temporal_silence_ms

    output_spike_tolorance_window_duration = 20
    output_spike_tolorance_window_offset   = 5

    output_kernel_test = np.zeros((X_test_spikes[0].shape[1],))
    output_kernel_test[-1] = 1
    desired_output_spikes_test_full = np.kron(Y_test_spikes[:num_test_patterns], output_kernel_test)
    desired_output_spikes_test = desired_output_spikes_test_full[start_time:end_time]

    num_spikes_in_window = desired_output_spikes_test.sum()

# make sure we have something decent to show (randomise start and end times untill we do)
output_spikes_test_after_learning = output_spikes_test_after_learning_full[start_time:end_time]
while output_spikes_test_after_learning.sum() < 2:
    start_time = x_test_original_extended.shape[2] * np.random.randint(int(test_set_full_duration_ms / x_test_original_extended.shape[2] - temporal_silence_ms))
    end_time = start_time + 1 + x_test_original_extended.shape[2] * num_digits_to_display + temporal_silence_ms
    output_spikes_test_after_learning = output_spikes_test_after_learning_full[start_time:end_time]

min_time_ms = 0
max_time_ms = end_time - start_time

time_sec = np.arange(min_time_ms, max_time_ms) / 1000
min_time_sec = min_time_ms / 1000
max_time_sec = max_time_ms / 1000


before_color = '0.15'
after_color = 'blue'
target_color = 'red'

# input digits
x_test_input_digits = x_test_axons_input_spikes[:,start_time:end_time]

# axon input raster
syn_activation_time, syn_activation_index = np.nonzero(axons_input_spikes_test[-x_test_axons_input_spikes.shape[0]:,start_time:end_time].T)
syn_activation_time = syn_activation_time / 1000
syn_activation_index = x_test_axons_input_spikes.shape[0] - syn_activation_index

# output before learning
output_spikes_test_before_learning = output_spikes_test_before_learning_full[start_time:end_time]

# output after learning
output_spikes_test_after_learning = output_spikes_test_after_learning_full[start_time:end_time]

# desired output
output_spike_tolorance_window_duration = 20
output_spike_tolorance_window_offset   = 5
output_kernel_test = np.zeros((X_test_spikes[0].shape[1],))
output_kernel_test[-1] = 1
desired_output_spikes_test_full = np.kron(Y_test_spikes[:num_test_patterns], output_kernel_test)
desired_output_spikes_test = desired_output_spikes_test_full[start_time:end_time]


# build full figure
plt.close('all')
fig = plt.figure(figsize=(19,22))
gs_figure = gridspec.GridSpec(nrows=4,ncols=3)
gs_figure.update(left=0.03, right=0.97, bottom=0.57, top=0.985, wspace=0.1, hspace=0.11)

ax_digits            = plt.subplot(gs_figure[0,:])
ax_axons             = plt.subplot(gs_figure[1:3,:])
ax_learning_outcomes = plt.subplot(gs_figure[3,:])

ax_digits.imshow(x_test_input_digits, cmap='gray'); ax_digits.set_title('Input Digits', fontsize=18)
ax_digits.set_xticks([])
ax_digits.set_yticks([])
ax_digits.spines['top'].set_visible(False)
ax_digits.spines['bottom'].set_visible(False)
ax_digits.spines['left'].set_visible(False)
ax_digits.spines['right'].set_visible(False)

ax_axons.scatter(syn_activation_time, syn_activation_index, s=6, c='k');
ax_axons.set_ylabel('Input Axons Raster', fontsize=18)
ax_axons.set_xlim(min_time_sec, max_time_sec);
ax_axons.set_xticks([])
ax_axons.set_yticks([])
ax_axons.spines['top'].set_visible(False)
ax_axons.spines['bottom'].set_visible(False)
ax_axons.spines['left'].set_visible(False)
ax_axons.spines['right'].set_visible(False)


ax_learning_outcomes.plot(time_sec, 2.2 + output_spikes_test_before_learning, c=before_color, lw=2.5);
ax_learning_outcomes.plot(time_sec, 1.1 + output_spikes_test_after_learning, c=after_color, lw=2.5);
ax_learning_outcomes.plot(time_sec, 0.0 + desired_output_spikes_test, c=target_color, lw=2.5);

ax_learning_outcomes.set_xlim(min_time_sec, max_time_sec);
ax_learning_outcomes.set_xticks([])
ax_learning_outcomes.set_yticks([])
ax_learning_outcomes.spines['top'].set_visible(False)
ax_learning_outcomes.spines['bottom'].set_visible(False)
ax_learning_outcomes.spines['left'].set_visible(False)
ax_learning_outcomes.spines['right'].set_visible(False)

ax_learning_outcomes.text(0.025,2.5, 'Before Learning', color=before_color, fontsize=20)
ax_learning_outcomes.text(0.025,1.4, 'After Learning', color=after_color, fontsize=20)
ax_learning_outcomes.text(0.025,0.3, 'Desired Output', color=target_color, fontsize=20)

# load data into dataframe

list_of_files = glob.glob(data_folder + 'MNIST_*.pickle')
print(len(list_of_files))

if build_dataframe_from_scratch:

    # Load one saved pickle
    filename_to_load = list_of_files[np.random.randint(len(list_of_files))]
    loaded_script_results_dict = pickle.load(open(filename_to_load, "rb" ))

    # display basic fields in the saved pickle file
    print('-----------------------------------------------------------------------------------------------------------')
    print('loaded_script_results_dict.keys():')
    print('----------')
    print(list(loaded_script_results_dict.keys()))
    print('-----------------------------------------------------------------------------------------------------------')
    print('loaded_script_results_dict["script_main_params"].keys():')
    print('----------')
    print(list(loaded_script_results_dict["script_main_params"].keys()))
    print('-----------------------------------------------------------------------------------------------------------')
    print('positive_digit =', loaded_script_results_dict['script_main_params']['positive_digit'])
    print('connections_per_axon =', loaded_script_results_dict['script_main_params']['connections_per_axon'])
    print('digit_sample_image_shape_cropped =', loaded_script_results_dict['script_main_params']['digit_sample_image_shape_cropped'])
    print('digit_sample_image_shape_expanded =', loaded_script_results_dict['script_main_params']['digit_sample_image_shape_expanded'])
    print('num_train_positive_patterns =', loaded_script_results_dict['script_main_params']['num_train_positive_patterns'])
    print('temporal_silence_ms =', loaded_script_results_dict['script_main_params']['temporal_silence_ms'])
    print('-----------------------------------------------------------------------------------------------------------')
    print('model_accuracy_LR =', loaded_script_results_dict['model_accuracy_LR'])
    print('model_accuracy_FF =', loaded_script_results_dict['model_accuracy_FF'])
    print('model_accuracy_IF =', loaded_script_results_dict['model_accuracy_IF'])
    print('model_accuracy_baseline =', loaded_script_results_dict['model_accuracy_baseline'])
    print('-----------------------------------------------------------------------------------------------------------')

    try:
        print('-----------------------------------------------------------------------------------------------------------')
        print('positive_digit =', loaded_script_results_dict['script_main_params']['positive_digit'])
        print('num_train_positive_patterns =', loaded_script_results_dict['script_main_params']['num_train_positive_patterns'])
        print('release_probability =', loaded_script_results_dict['script_main_params']['release_probability'])
        print('train_epochs =', loaded_script_results_dict['script_main_params']['train_epochs'])
        print('test_epochs =', loaded_script_results_dict['script_main_params']['test_epochs'])
        print('create_output_burst =', loaded_script_results_dict['script_main_params']['create_output_burst'])
        print('-----------------------------------------------------------------------------------------------------------')
    except:
        print('no prob release fields')

    # display the learned weights
    # plt.close('all')
    # plt.figure(figsize=(24,10))
    # plt.subplot(1,3,1); plt.imshow(loaded_script_results_dict['learned_weights_LR']); plt.title('logistic regression', fontsize=24)
    # plt.subplot(1,3,2); plt.imshow(loaded_script_results_dict['learned_weights_FF']); plt.title('filter and fire neuron', fontsize=24)
    # plt.subplot(1,3,3); plt.imshow(loaded_script_results_dict['learned_weights_IF']); plt.title('integrate and fire neuron', fontsize=24)

    # go over all files and insert into a large dataframe
    columns_to_use = ['digit','M_connections','N_axons', 'T', 'N_positive_samples',
                      'Accuracy LR', 'Accuracy FF', 'Accuracy IF', 'Accuracy baseline',
                      'release probability', 'train_epochs', 'test_epochs']
    results_df = pd.DataFrame(index=range(len(list_of_files)), columns=columns_to_use)

    for k, filename_to_load in enumerate(list_of_files):
        loaded_script_results_dict = pickle.load(open(filename_to_load, "rb" ))

        results_df.loc[k, 'digit']              = loaded_script_results_dict['script_main_params']['positive_digit']
        results_df.loc[k, 'M_connections']      = loaded_script_results_dict['script_main_params']['connections_per_axon']
        results_df.loc[k, 'N_axons']            = loaded_script_results_dict['script_main_params']['digit_sample_image_shape_expanded'][0]
        results_df.loc[k, 'T']                  = loaded_script_results_dict['script_main_params']['digit_sample_image_shape_expanded'][1]
        results_df.loc[k, 'N_positive_samples'] = loaded_script_results_dict['script_main_params']['num_train_positive_patterns']
        results_df.loc[k, 'Accuracy LR']        = loaded_script_results_dict['model_accuracy_LR']
        results_df.loc[k, 'Accuracy FF']        = loaded_script_results_dict['model_accuracy_FF']
        results_df.loc[k, 'Accuracy IF']        = loaded_script_results_dict['model_accuracy_IF']
        results_df.loc[k, 'Accuracy baseline']  = loaded_script_results_dict['model_accuracy_baseline']

        try:
            results_df.loc[k, 'release probability'] = loaded_script_results_dict['script_main_params']['release_probability']
            results_df.loc[k, 'train_epochs']        = loaded_script_results_dict['script_main_params']['train_epochs']
            results_df.loc[k, 'test_epochs']         = loaded_script_results_dict['script_main_params']['test_epochs']
        except:
            results_df.loc[k, 'release probability'] = 1.0
            results_df.loc[k, 'train_epochs']        = 1
            results_df.loc[k, 'test_epochs']         = 1

    print(results_df.shape)

    # save the dataframe
    filename = 'MNIST_classification_LR_FF_IF_%d_rows_%d_cols.csv' %(results_df.shape[0], results_df.shape[1])
    results_df.to_csv(data_folder + filename, index=False)
else:
    filename = 'MNIST_classification_LR_FF_IF_5162_rows_12_cols.csv'

# open previously saved file
results_df = pd.read_csv(data_folder + filename)
print(results_df.shape)

# display accuracy per digit at specific condition for (I&F, F&F, LR)
selected_T = 40
selected_M = 5
selected_N_axons = 100
selected_N_samples = 4000
selected_release_prob = 1.0

condition_rows = results_df.loc[:, 'T'] == selected_T
condition_rows = np.logical_and(condition_rows, results_df.loc[:, 'M_connections']       == selected_M)
condition_rows = np.logical_and(condition_rows, results_df.loc[:, 'N_axons']             == selected_N_axons)
condition_rows = np.logical_and(condition_rows, results_df.loc[:, 'N_positive_samples']  >= selected_N_samples)
condition_rows = np.logical_and(condition_rows, results_df.loc[:, 'release probability'] == selected_release_prob)

average_for_condition_per_digit_df = results_df.loc[condition_rows,:].groupby('digit').mean()
stddev_for_condition_per_digit_df  = results_df.loc[condition_rows,:].groupby('digit').std()
counts_for_condition_per_digit_df  = results_df.loc[condition_rows,:].groupby('digit').count().iloc[:,0]

digits_list = average_for_condition_per_digit_df.index.tolist()
LR_accuracy       = average_for_condition_per_digit_df['Accuracy LR'].tolist()
FF_accuracy       = average_for_condition_per_digit_df['Accuracy FF'].tolist()
IF_accuracy       = average_for_condition_per_digit_df['Accuracy IF'].tolist()
baseline_accuracy = average_for_condition_per_digit_df['Accuracy baseline'].tolist()

LR_stderr = stddev_for_condition_per_digit_df['Accuracy LR'].tolist()
FF_stderr = stddev_for_condition_per_digit_df['Accuracy FF'].tolist()
IF_stderr = stddev_for_condition_per_digit_df['Accuracy IF'].tolist()

# display plot
bar_plot_x_axis = 1.0 * np.arange(len(digits_list))
bar_widths = 0.7 / 3
x_tick_names = ['"%s"' %(str(x)) for x in digits_list]

gs_accuracy_per_digit = gridspec.GridSpec(nrows=2,ncols=1)
gs_accuracy_per_digit.update(left=0.045, right=0.675, bottom=0.32, top=0.52, wspace=0.1, hspace=0.1)
ax_accuracy_per_digit = plt.subplot(gs_accuracy_per_digit[:,:])

ax_accuracy_per_digit.bar(bar_plot_x_axis + 0 * bar_widths, IF_accuracy, bar_widths, yerr=IF_stderr, color='0.05'  , label='I&F')
ax_accuracy_per_digit.bar(bar_plot_x_axis + 1 * bar_widths, FF_accuracy, bar_widths, yerr=FF_stderr, color='orange', alpha=0.95, label='F&F')
ax_accuracy_per_digit.bar(bar_plot_x_axis + 2 * bar_widths, LR_accuracy, bar_widths, yerr=LR_stderr, color='0.45'  , label='Spatio\nTemporal LR')

for k in range(len(digits_list)):
    if k == 0:
        ax_accuracy_per_digit.plot([bar_plot_x_axis[k] - 0.7 * bar_widths, bar_plot_x_axis[k] + 2.7 * bar_widths], [baseline_accuracy[k], baseline_accuracy[k]], color='r', label='Baseline')
    else:
        ax_accuracy_per_digit.plot([bar_plot_x_axis[k] - 0.7 * bar_widths, bar_plot_x_axis[k] + 2.7 * bar_widths], [baseline_accuracy[k], baseline_accuracy[k]], color='r')

ax_accuracy_per_digit.set_title('Accuracy Comparison per digit (M = %d, T = %d (ms))' %(selected_M, selected_T), fontsize=22)
ax_accuracy_per_digit.set_yticks([90,92,94,96,98]);
ax_accuracy_per_digit.set_yticklabels([90,92,94,96,98], fontsize=16);
ax_accuracy_per_digit.set_ylim(87.9,98.9);
ax_accuracy_per_digit.set_xticks(bar_plot_x_axis + bar_widths);
ax_accuracy_per_digit.set_xticklabels(x_tick_names, rotation=0, fontsize=26);
ax_accuracy_per_digit.set_ylabel('Test accuracy (%)', fontsize=18)
ax_accuracy_per_digit.legend(fontsize=18, ncol=1);
ax_accuracy_per_digit.set_xlim(-0.5,10);
ax_accuracy_per_digit.spines['top'].set_visible(False)
ax_accuracy_per_digit.spines['right'].set_visible(False)

# display accuracy across all digits as function of pattern presentation duration for (I&F, F&F, LR)
selected_M = 5
selected_N_axons = 100
selected_N_samples = 4000
selected_release_prob = 1.0

condition_rows = results_df.loc[:, 'M_connections'] == selected_M
condition_rows = np.logical_and(condition_rows, results_df.loc[:, 'N_axons']             == selected_N_axons)
condition_rows = np.logical_and(condition_rows, results_df.loc[:, 'N_positive_samples']  >= selected_N_samples)
condition_rows = np.logical_and(condition_rows, results_df.loc[:, 'release probability'] == selected_release_prob)

all_T_values = sorted(results_df['T'].unique().tolist())
digits_list  = sorted(results_df['digit'].unique().tolist())
num_digits   = len(digits_list)

LR_acc_mean = []
FF_acc_mean = []
IF_acc_mean = []
baseline_acc_mean = []

LR_acc_std = []
FF_acc_std = []
IF_acc_std = []
baseline_acc_std = []

for selected_T in all_T_values:
    curr_rows = np.logical_and(condition_rows, results_df.loc[:, 'T'] == selected_T)

    average_acc_per_T_per_digit_df = results_df.loc[curr_rows,:].groupby('digit').mean()
    stddev_acc_per_T_per_digit_df  = results_df.loc[curr_rows,:].groupby('digit').std()
    # print(selected_T, ':\n', results_df.loc[curr_rows,:].groupby('digit').count().iloc[:,0])

    LR_acc_mean.append(average_acc_per_T_per_digit_df['Accuracy LR'].mean())
    FF_acc_mean.append(average_acc_per_T_per_digit_df['Accuracy FF'].mean())
    IF_acc_mean.append(average_acc_per_T_per_digit_df['Accuracy IF'].mean())
    baseline_acc_mean.append(average_acc_per_T_per_digit_df['Accuracy baseline'].mean())

    LR_acc_std.append(average_acc_per_T_per_digit_df['Accuracy LR'].std() / np.sqrt(num_digits))
    FF_acc_std.append(average_acc_per_T_per_digit_df['Accuracy FF'].std() / np.sqrt(num_digits))
    IF_acc_std.append(average_acc_per_T_per_digit_df['Accuracy IF'].std() / np.sqrt(num_digits))
    baseline_acc_std.append(average_acc_per_T_per_digit_df['Accuracy baseline'].std() / np.sqrt(num_digits))

gs_accuracy_vs_T = gridspec.GridSpec(nrows=2,ncols=1)
gs_accuracy_vs_T.update(left=0.045, right=0.3375, bottom=0.04, top=0.28, wspace=0.1, hspace=0.1)
ax_accuracy_vs_T = plt.subplot(gs_accuracy_vs_T[:,:])

ax_accuracy_vs_T.errorbar(all_T_values, LR_acc_mean, yerr=LR_acc_std, linewidth=4, color='0.45')
ax_accuracy_vs_T.errorbar(all_T_values, FF_acc_mean, yerr=FF_acc_std, linewidth=4, color='orange')
ax_accuracy_vs_T.errorbar(all_T_values, IF_acc_mean, yerr=IF_acc_std, linewidth=4, color='0.05')
ax_accuracy_vs_T.errorbar(all_T_values, baseline_acc_mean, yerr=baseline_acc_std, linewidth=1, color='red')
ax_accuracy_vs_T.set_xlabel('Pattern presentation duration - T (ms)', fontsize=18)
ax_accuracy_vs_T.set_ylabel('Test Accuracy (%)', fontsize=18)
ax_accuracy_vs_T.set_ylim(89.5,96.5)
ax_accuracy_vs_T.set_xticks(all_T_values)
ax_accuracy_vs_T.set_xticklabels(all_T_values, fontsize=15)
ax_accuracy_vs_T.set_yticks([90,92,94,96]);
ax_accuracy_vs_T.set_yticklabels([90,92,94,96], fontsize=15);
ax_accuracy_vs_T.spines['top'].set_visible(False)
ax_accuracy_vs_T.spines['right'].set_visible(False)


# display accuracy as function of M, for a specific digit and multiple release probabilities
selected_digit = 2
selected_N_axons = 100
selected_N_samples = 2048

condition_rows = results_df.loc[:, 'digit'] == selected_digit
condition_rows = np.logical_and(condition_rows, results_df.loc[:, 'N_axons']             == selected_N_axons)
condition_rows = np.logical_and(condition_rows, results_df.loc[:, 'N_positive_samples']  == selected_N_samples)

all_M_values = sorted(results_df.loc[condition_rows, 'M_connections'].unique().tolist())
all_P_values = sorted(results_df.loc[condition_rows, 'release probability'].unique().tolist())

all_M_values = [1,2,3,5,8]

results_dict_Acc_vs_M = {}
for selected_release_P in all_P_values:
    results_dict_Acc_vs_M[selected_release_P] = {}
    results_dict_Acc_vs_M[selected_release_P]['LR_acc_mean'] = []
    results_dict_Acc_vs_M[selected_release_P]['FF_acc_mean'] = []
    results_dict_Acc_vs_M[selected_release_P]['IF_acc_mean'] = []
    results_dict_Acc_vs_M[selected_release_P]['baseline_acc_mean'] = []

    results_dict_Acc_vs_M[selected_release_P]['LR_acc_std'] = []
    results_dict_Acc_vs_M[selected_release_P]['FF_acc_std'] = []
    results_dict_Acc_vs_M[selected_release_P]['IF_acc_std'] = []
    results_dict_Acc_vs_M[selected_release_P]['baseline_acc_std'] = []


for selected_M in all_M_values:
    curr_condition_rows = np.logical_and(condition_rows, results_df.loc[:, 'M_connections'] == selected_M)

    for selected_release_P in all_P_values:
        curr_rows = np.logical_and(curr_condition_rows, results_df.loc[:, 'release probability'] == selected_release_P)

        num_rows = curr_rows.sum()
        num_rows = 1
        results_dict_Acc_vs_M[selected_release_P]['LR_acc_mean'].append(results_df.loc[curr_rows,'Accuracy LR'].mean())
        results_dict_Acc_vs_M[selected_release_P]['FF_acc_mean'].append(results_df.loc[curr_rows,'Accuracy FF'].mean())
        results_dict_Acc_vs_M[selected_release_P]['IF_acc_mean'].append(results_df.loc[curr_rows,'Accuracy IF'].mean())
        results_dict_Acc_vs_M[selected_release_P]['baseline_acc_mean'].append(results_df.loc[curr_rows,'Accuracy baseline'].mean())

        results_dict_Acc_vs_M[selected_release_P]['LR_acc_std'].append(results_df.loc[curr_rows,'Accuracy LR'].std() / np.sqrt(num_rows))
        results_dict_Acc_vs_M[selected_release_P]['FF_acc_std'].append(results_df.loc[curr_rows,'Accuracy FF'].std() / np.sqrt(num_rows))
        results_dict_Acc_vs_M[selected_release_P]['IF_acc_std'].append(results_df.loc[curr_rows,'Accuracy IF'].std() / np.sqrt(num_rows))
        results_dict_Acc_vs_M[selected_release_P]['baseline_acc_std'].append(results_df.loc[curr_rows,'Accuracy baseline'].std() / np.sqrt(num_rows))

gs_accuracy_vs_M = gridspec.GridSpec(nrows=2,ncols=1)
gs_accuracy_vs_M.update(left=0.37, right=0.66, bottom=0.04, top=0.28, wspace=0.1, hspace=0.1)
ax_accuracy_vs_M = plt.subplot(gs_accuracy_vs_M[:,:])

selected_release_P = 1.0
ax_accuracy_vs_M.errorbar(all_M_values, results_dict_Acc_vs_M[selected_release_P]['LR_acc_mean'], yerr=results_dict_Acc_vs_M[selected_release_P]['LR_acc_std'], lw=4, color='0.45')
ax_accuracy_vs_M.errorbar(all_M_values, results_dict_Acc_vs_M[selected_release_P]['FF_acc_mean'], yerr=results_dict_Acc_vs_M[selected_release_P]['FF_acc_std'], lw=4, color='orange')
ax_accuracy_vs_M.errorbar(all_M_values, results_dict_Acc_vs_M[selected_release_P]['IF_acc_mean'], yerr=results_dict_Acc_vs_M[selected_release_P]['IF_acc_std'], lw=4, color='0.05')

selected_release_P = 0.5
ax_accuracy_vs_M.errorbar(all_M_values, results_dict_Acc_vs_M[selected_release_P]['LR_acc_mean'], yerr=results_dict_Acc_vs_M[selected_release_P]['LR_acc_std'], lw=4, ls=':', color='0.45')
ax_accuracy_vs_M.errorbar(all_M_values, results_dict_Acc_vs_M[selected_release_P]['FF_acc_mean'], yerr=results_dict_Acc_vs_M[selected_release_P]['FF_acc_std'], lw=4, ls=':', color='orange')
ax_accuracy_vs_M.errorbar(all_M_values, results_dict_Acc_vs_M[selected_release_P]['IF_acc_mean'], yerr=results_dict_Acc_vs_M[selected_release_P]['IF_acc_std'], lw=4, ls=':', color='0.05')

legend_list = ['release Prob = 1.0', 'release Prob = 1.0', 'release Prob = 1.0',
               'P = 0.5', 'P = 0.5', 'P = 0.5']
ax_accuracy_vs_M.legend(legend_list, ncol=2, mode='expand', fontsize=18)

ax_accuracy_vs_M.set_xlabel('Number of Multiple Contacts - M', fontsize=18)
ax_accuracy_vs_M.set_xticks(all_M_values)
ax_accuracy_vs_M.set_xticklabels(all_M_values, fontsize=15)
ax_accuracy_vs_M.set_yticks([91,93,95,97]);
ax_accuracy_vs_M.set_yticklabels([91,93,95,97], fontsize=15);
ax_accuracy_vs_M.set_ylim(90.5,98.2)
ax_accuracy_vs_M.spines['top'].set_visible(False)
ax_accuracy_vs_M.spines['right'].set_visible(False)
ax_accuracy_vs_M.set_xlim(0.8,8.2)


# display accuracy as function of number of positive training samples
selected_digit = 7
selected_T = 30
selected_M = 5
selected_N_axons = 100
max_N_samples = 1100
num_train_epochs = 15

condition_rows = results_df.loc[:, 'digit'] == selected_digit
condition_rows = np.logical_and(condition_rows, results_df.loc[:, 'T']                  == selected_T)
condition_rows = np.logical_and(condition_rows, results_df.loc[:, 'M_connections']      == selected_M)
condition_rows = np.logical_and(condition_rows, results_df.loc[:, 'N_axons']            == selected_N_axons)
condition_rows = np.logical_and(condition_rows, results_df.loc[:, 'N_positive_samples'] <= max_N_samples)

good_epoch_rows = np.logical_or(results_df.loc[:, 'release probability'] == 1.0, results_df.loc[:, 'train_epochs'] == num_train_epochs)
condition_rows  = np.logical_and(condition_rows, good_epoch_rows)

all_N_samples_values = sorted(results_df.loc[condition_rows, 'N_positive_samples'].unique().tolist())
all_P_values         = sorted(results_df.loc[condition_rows, 'release probability'].unique().tolist())

all_N_samples_values = [16, 32, 64, 128, 256, 512, 1024]

results_dict_Acc_vs_N_samples = {}
for selected_release_P in all_P_values:
    results_dict_Acc_vs_N_samples[selected_release_P] = {}
    results_dict_Acc_vs_N_samples[selected_release_P]['LR_acc_mean'] = []
    results_dict_Acc_vs_N_samples[selected_release_P]['FF_acc_mean'] = []
    results_dict_Acc_vs_N_samples[selected_release_P]['IF_acc_mean'] = []
    results_dict_Acc_vs_N_samples[selected_release_P]['baseline_acc_mean'] = []

    results_dict_Acc_vs_N_samples[selected_release_P]['LR_acc_std'] = []
    results_dict_Acc_vs_N_samples[selected_release_P]['FF_acc_std'] = []
    results_dict_Acc_vs_N_samples[selected_release_P]['IF_acc_std'] = []
    results_dict_Acc_vs_N_samples[selected_release_P]['baseline_acc_std'] = []


for selected_N_samples in all_N_samples_values:
    curr_condition_rows = np.logical_and(condition_rows, results_df.loc[:, 'N_positive_samples'] == selected_N_samples)

    for selected_release_P in all_P_values:
        curr_rows = np.logical_and(curr_condition_rows, results_df.loc[:, 'release probability'] == selected_release_P)
        num_rows = curr_rows.sum()

        results_dict_Acc_vs_N_samples[selected_release_P]['LR_acc_mean'].append(results_df.loc[curr_rows,'Accuracy LR'].mean())
        results_dict_Acc_vs_N_samples[selected_release_P]['FF_acc_mean'].append(results_df.loc[curr_rows,'Accuracy FF'].mean())
        results_dict_Acc_vs_N_samples[selected_release_P]['IF_acc_mean'].append(results_df.loc[curr_rows,'Accuracy IF'].mean())
        results_dict_Acc_vs_N_samples[selected_release_P]['baseline_acc_mean'].append(results_df.loc[curr_rows,'Accuracy baseline'].mean())

        results_dict_Acc_vs_N_samples[selected_release_P]['LR_acc_std'].append(results_df.loc[curr_rows,'Accuracy LR'].std() / np.sqrt(num_rows))
        results_dict_Acc_vs_N_samples[selected_release_P]['FF_acc_std'].append(results_df.loc[curr_rows,'Accuracy FF'].std() / np.sqrt(num_rows))
        results_dict_Acc_vs_N_samples[selected_release_P]['IF_acc_std'].append(results_df.loc[curr_rows,'Accuracy IF'].std() / np.sqrt(num_rows))
        results_dict_Acc_vs_N_samples[selected_release_P]['baseline_acc_std'].append(results_df.loc[curr_rows,'Accuracy baseline'].std() / np.sqrt(num_rows))


gs_accuracy_vs_N_samples = gridspec.GridSpec(nrows=2,ncols=1)
gs_accuracy_vs_N_samples.update(left=0.6925, right=0.97, bottom=0.04, top=0.28, wspace=0.1, hspace=0.1)
ax_accuracy_vs_N_samples = plt.subplot(gs_accuracy_vs_N_samples[:,:])

selected_release_P = 1.0
ax_accuracy_vs_N_samples.errorbar(all_N_samples_values, results_dict_Acc_vs_N_samples[selected_release_P]['LR_acc_mean'], yerr=results_dict_Acc_vs_N_samples[selected_release_P]['LR_acc_std'], lw=4, color='0.45')
ax_accuracy_vs_N_samples.errorbar(all_N_samples_values, results_dict_Acc_vs_N_samples[selected_release_P]['FF_acc_mean'], yerr=results_dict_Acc_vs_N_samples[selected_release_P]['FF_acc_std'], lw=4, color='orange')
ax_accuracy_vs_N_samples.errorbar(all_N_samples_values, results_dict_Acc_vs_N_samples[selected_release_P]['IF_acc_mean'], yerr=results_dict_Acc_vs_N_samples[selected_release_P]['IF_acc_std'], lw=4, color='0.05')

selected_release_P = 0.5
ax_accuracy_vs_N_samples.errorbar(all_N_samples_values, results_dict_Acc_vs_N_samples[selected_release_P]['LR_acc_mean'], yerr=results_dict_Acc_vs_N_samples[selected_release_P]['LR_acc_std'], lw=4, ls=':', color='0.45')
ax_accuracy_vs_N_samples.errorbar(all_N_samples_values, results_dict_Acc_vs_N_samples[selected_release_P]['FF_acc_mean'], yerr=results_dict_Acc_vs_N_samples[selected_release_P]['FF_acc_std'], lw=4, ls=':', color='orange')
ax_accuracy_vs_N_samples.errorbar(all_N_samples_values, results_dict_Acc_vs_N_samples[selected_release_P]['IF_acc_mean'], yerr=results_dict_Acc_vs_N_samples[selected_release_P]['IF_acc_std'], lw=4, ls=':', color='0.05')

ax_accuracy_vs_N_samples.set_xlabel('Number of Positive Training Patterns - N', fontsize=18)
ax_accuracy_vs_N_samples.set_xticks(all_N_samples_values)
ax_accuracy_vs_N_samples.set_xticklabels(all_N_samples_values, fontsize=15)
ax_accuracy_vs_N_samples.set_ylim(89.5,97.2)
ax_accuracy_vs_N_samples.set_yticks([90,92,94,96]);
ax_accuracy_vs_N_samples.set_yticklabels([90,92,94,96], fontsize=15);
ax_accuracy_vs_N_samples.spines['top'].set_visible(False)
ax_accuracy_vs_N_samples.spines['right'].set_visible(False)
ax_accuracy_vs_N_samples.set_xticks([16,128,256,512,1024]);
ax_accuracy_vs_N_samples.set_xticklabels([16,128,256,512,1024], fontsize=15);


# open digit 3 and display the learned weights of the 3 models for it
digit = 3
T = 50
N_axons = 100
M_connections = 5
temporal_silence_ms = 70

min_pos_samples = 5000

all_LR_weights = []
all_FF_weights = []
all_IF_weights = []

for k, filename_to_load in enumerate(list_of_files):
    loaded_script_results_dict = pickle.load(open(filename_to_load, "rb" ))

    digit_OK   = loaded_script_results_dict['script_main_params']['positive_digit'] == digit
    M_OK       = loaded_script_results_dict['script_main_params']['connections_per_axon'] == M_connections
    N_axons_OK = loaded_script_results_dict['script_main_params']['digit_sample_image_shape_expanded'][0] == N_axons
    T_OK       = loaded_script_results_dict['script_main_params']['digit_sample_image_shape_expanded'][1] == T
    N_pos_OK   = loaded_script_results_dict['script_main_params']['num_train_positive_patterns'] >= min_pos_samples
    ISS_OK     = loaded_script_results_dict['script_main_params']['temporal_silence_ms'] == temporal_silence_ms

    if digit_OK and M_OK and N_axons_OK and T_OK and N_pos_OK and ISS_OK:
        all_LR_weights.append(loaded_script_results_dict['learned_weights_LR'])
        all_FF_weights.append(loaded_script_results_dict['learned_weights_FF'])
        all_IF_weights.append(loaded_script_results_dict['learned_weights_IF'])

all_LR_weights = np.array(all_LR_weights)
all_FF_weights = np.array(all_FF_weights)
all_IF_weights = np.array(all_IF_weights)

# the first "num_const_firing_channels" axons are a "bias" term, so don't show them
h_start = loaded_script_results_dict['script_main_params']['num_const_firing_channels']
# the first "temporal_silence_ms" time points are silence, so they are boring
w_start = 39

rand_index = np.random.randint(all_LR_weights.shape[0])

single_trial_weights_LR = all_LR_weights[rand_index][h_start:,w_start:]
single_trial_weights_FF = all_FF_weights[rand_index][h_start:,w_start:]
single_trial_weights_IF = all_IF_weights[rand_index][h_start:,w_start:]

mean_weights_LR = all_LR_weights.mean(axis=0)[h_start:,w_start:]
mean_weights_FF = all_FF_weights.mean(axis=0)[h_start:,w_start:]
mean_weights_IF = all_IF_weights.mean(axis=0)[h_start:,w_start:]


def get_weight_symmetric_range(weights_matrix):
    top_value = np.percentile(weights_matrix, 99)
    bottom_value = np.percentile(weights_matrix, 1)
    symmetric_range = np.array([-1,1]) * max(np.abs(top_value), np.abs(bottom_value))

    return symmetric_range


symmetric_weight_range = get_weight_symmetric_range(single_trial_weights_LR)
symmetric_weight_range = get_weight_symmetric_range(mean_weights_LR)
colormap = 'viridis'
vmin = symmetric_weight_range[0]
vmax = symmetric_weight_range[1]

gs_learned_weights = gridspec.GridSpec(nrows=2,ncols=3)
gs_learned_weights.update(left=0.6925, right=0.97, bottom=0.32, top=0.529, wspace=0.06, hspace=0.06)

ax_learned_weights_00 = plt.subplot(gs_learned_weights[0,0])
ax_learned_weights_01 = plt.subplot(gs_learned_weights[0,1])
ax_learned_weights_02 = plt.subplot(gs_learned_weights[0,2])
ax_learned_weights_10 = plt.subplot(gs_learned_weights[1,0])
ax_learned_weights_11 = plt.subplot(gs_learned_weights[1,1])
ax_learned_weights_12 = plt.subplot(gs_learned_weights[1,2])

ax_learned_weights_00.set_title('Spatio-Temporal\n Logistic Regression', fontsize=13);
ax_learned_weights_00.set_ylabel('single trial', fontsize=14)
ax_learned_weights_00.imshow(single_trial_weights_LR, vmin=vmin, vmax=vmax, cmap=colormap);
ax_learned_weights_01.imshow(single_trial_weights_FF, vmin=vmin, vmax=vmax, cmap=colormap);
ax_learned_weights_01.set_title('Filter & Fire\n neuron', fontsize=13)
ax_learned_weights_02.imshow(single_trial_weights_IF, vmin=vmin, vmax=vmax, cmap=colormap);
ax_learned_weights_02.set_title('Integrate & Fire\n neuron', fontsize=13)

ax_learned_weights_10.set_ylabel('mean of %d trials' %(all_LR_weights.shape[0]), fontsize=14)
ax_learned_weights_10.imshow(mean_weights_LR, vmin=vmin, vmax=vmax, cmap=colormap);
ax_learned_weights_11.imshow(mean_weights_FF, vmin=vmin, vmax=vmax, cmap=colormap);
ax_learned_weights_12.imshow(mean_weights_IF, vmin=vmin, vmax=vmax, cmap=colormap);

def set_xy_ticks_ticklabels_to_None(ax_input):
    ax_input.set_xticks([])
    ax_input.set_xticklabels([])
    ax_input.set_yticks([])
    ax_input.set_yticklabels([])

set_xy_ticks_ticklabels_to_None(ax_learned_weights_00)
set_xy_ticks_ticklabels_to_None(ax_learned_weights_01)
set_xy_ticks_ticklabels_to_None(ax_learned_weights_02)
set_xy_ticks_ticklabels_to_None(ax_learned_weights_10)
set_xy_ticks_ticklabels_to_None(ax_learned_weights_11)
set_xy_ticks_ticklabels_to_None(ax_learned_weights_12)


# save figure
if save_figures:
    figure_name = 'F&F_MNIST_Figure_3_%d' %(np.random.randint(200))
    for file_ending in all_file_endings_to_use:
        if file_ending == '.png':
            fig.savefig(figure_folder + figure_name + file_ending, bbox_inches='tight')
        else:
            fig.savefig(figure_folder + figure_name + file_ending, bbox_inches='tight')

#%%


================================================================================
================================================================================
saved_figures\F&F_MNIST_Figure_3_7.png:
=======================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
This image presents a scientific study on the accuracy of different neural network models in recognizing handwritten digits.  The top section shows the input data: a raster plot of handwritten digits (0-9) and the corresponding neural network activity before and after learning.  A time series of desired output is also shown, indicating the expected neural response for each digit.

The main body of the image is organized into several subplots. The first major subplot is a bar chart comparing the test accuracy of four different models (Baseline, Integrate & Fire (I&F), Filter & Fire (F&F), and Spatio-Temporal Logistic Regression) for each digit.  Error bars represent variability.  A second subplot shows how test accuracy varies as a function of the pattern presentation duration (T) for each model. A third subplot displays the influence of the number of multiple contacts (M) on the test accuracy.  Finally, a fourth subplot illustrates the relationship between the number of positive training patterns (N) and the test accuracy for the three models.

The bottom right corner shows spatial-temporal heatmaps visualizing the neural activity patterns for each model, both for single trials and averaged across multiple trials.  These heatmaps provide a visual representation of how the different models process and represent the input digit data.  Overall, the image concisely displays a comprehensive evaluation of different neural network architectures in a pattern recognition task.

================================================================================
================================================================================
run_mnist_configs_on_cluster_slurm.py:
======================================
import os
import time


def mkdir_p(dir_path):
    '''make a directory (dir_path) if it doesn't exist'''
    if not os.path.exists(dir_path):
        os.mkdir(dir_path)


script_name    = 'MNIST_classification_LR_IF_FF_interactions.py'
output_log_dir = '/filter_and_fire_neuron/logs/'

# all experiment configs to run (will be form a grid of all combinations, so number of experiments will explode if not careful)
positive_digit_list = [0,1,2,3,4,5,6,7,8,9]
connections_per_axon_list = [1,2,5,10]
temporal_extent_factor_numerator_list = [1,2,3,4,5]
temporal_extent_factor_denumerator_list = [1,2]
release_probability_list = [0.5,1.0]
num_positive_training_samples_list = [16,32,64,128,256,512,1024,2048,4096]

num_random_seeds = 2
start_seed = 123456

partition_argument_str = "-p ss.q,elsc.q"
timelimit_argument_str = "-t 1-18:00:00"
CPU_argument_str = "-c 1"
RAM_argument_str = "--mem 64000"
CPU_exclude_nodes_str = "--exclude=ielsc-60,ielsc-108,ielsc-109"

temp_jobs_dir = os.path.join(output_log_dir, 'temp/')
mkdir_p(temp_jobs_dir)

random_seed = start_seed
for positive_digit in positive_digit_list:
    for connections_per_axon in connections_per_axon_list:
        for temporal_extent_factor_numerator in temporal_extent_factor_numerator_list:
            for temporal_extent_factor_denumerator in temporal_extent_factor_denumerator_list:
                for release_probability in release_probability_list:
                    for num_positive_training_samples in num_positive_training_samples_list:
                        for exp_index in range(num_random_seeds):
                            random_seed = random_seed + 1

                            # job and log names
                            digit_multconn_str = 'digit_%d_mult_connections_%d' %(positive_digit, connections_per_axon)
                            job_name = '%s_%s_randseed_%d' %(script_name[:-3], digit_multconn_str, random_seed)
                            log_filename = os.path.join(output_log_dir, "%s.log" %(job_name))
                            job_filename = os.path.join(temp_jobs_dir , "%s.job" %(job_name))

                            # write a job file and run it
                            with open(job_filename, 'w') as fh:
                                fh.writelines("#!/bin/bash\n")
                                fh.writelines("#SBATCH --job-name %s\n" %(job_name))
                                fh.writelines("#SBATCH -o %s\n" %(log_filename))
                                fh.writelines("#SBATCH %s\n" %(partition_argument_str))
                                fh.writelines("#SBATCH %s\n" %(timelimit_argument_str))
                                fh.writelines("#SBATCH %s\n" %(CPU_argument_str))
                                fh.writelines("#SBATCH %s\n" %(RAM_argument_str))
                                fh.writelines("#SBATCH %s\n" %(CPU_exclude_nodes_str))
                                fh.writelines("python3.6 -u %s %s %s %s %s %s %s %s\n" %(script_name, random_seed, positive_digit, connections_per_axon,
                                                                                         temporal_extent_factor_numerator, temporal_extent_factor_denumerator,
                                                                                         release_probability, num_positive_training_samples))

                            os.system("sbatch %s" %(job_filename))
                            time.sleep(0.1)

================================================================================
================================================================================
saved_figures\F&F_Capacity_Figure_2_3.png:
==========================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a comprehensive analysis of a spiking neural network's performance in precisely timing output spikes. It's structured into four main sections, each with multiple subplots:

The top-left section displays a raster plot showing the activity of input axons before and after learning.  A time series below illustrates the desired output spike train (red and blue) compared to the network's output before and after training. The black lines represent the input spike trains.  This section visually demonstrates the network's learning process, showing improvement in aligning the output spikes with the desired pattern.

The top-right section shows the relationship between the number of precisely timed output spikes and the number of multiple contacts (M) for two different neuron models: "F&F" (Feedforward) and "I&F" (Integrate-and-Fire).  Error bars indicate variability. The plot suggests that the F&F model consistently produces more precisely timed spikes than the I&F model, particularly as the number of multiple contacts increases.

The bottom-left section presents a set of curves depicting the accuracy (measured by the Area Under the Curve, AUC) of spike timing precision as a function of the number of required precisely timed output spikes.  Multiple lines represent different neuron models (F&F and I&F) and variations in the number of multiple contacts (M). This section provides a detailed comparison of the models' performance across different levels of difficulty (number of required spikes), showing that accuracy generally decreases as the task becomes more challenging.

Finally, the bottom-right section displays how the capacity (number of precisely timed spikes) scales linearly with the number of input axons for both F&F and I&F models with different values of M. This suggests that increasing the number of input axons enhances the network's capacity for precise spike timing.  The F&F model demonstrates greater capacity than the I&F model across various conditions.

In summary, this image provides a thorough quantitative and qualitative evaluation of a spiking neural network's ability to learn and generate precisely timed output spikes, comparing two neuron models and exploring the effect of network parameters such as the number of multiple contacts and input axons.

================================================================================
================================================================================
results_data_mnist\MNIST_classification_LR_FF_IF_5162_rows_12_cols.csv:
=======================================================================
CSV file with 5162 rows and 12 columns. First several column names are ['digit', 'M_connections', 'N_axons', 'T', 'N_positive_samples', 'Accuracy LR', 'Accuracy FF', 'Accuracy IF', 'Accuracy baseline', 'release probability']

This CSV file appears to document the results of a series of experiments, likely related to a machine learning model's performance on a digit classification task.  The structure is tabular, with each row representing a single experimental run.  The columns represent various parameters and metrics influencing or resulting from the experiment.  The parameters seem to involve aspects of the model's architecture (e.g., `M_connections`, `N_axons`), training data (`N_positive_samples`), and training process (`train_epochs`, `test_epochs`).  The metrics include different accuracy scores for various model types (`Accuracy LR`, `Accuracy FF`, `Accuracy IF`) and a baseline accuracy, as well as a `release probability` parameter whose exact meaning requires further context.

The potential purpose is to systematically evaluate the influence of different parameter settings on the accuracy of a digit recognition model.  The variation in parameters suggests a deliberate exploration of the model's sensitivity to these factors. The consistent presence of `N_axons = 100` suggests this is a fixed architectural parameter throughout the study.  The number of training epochs and testing epochs vary indicating testing under different training regimes.  The repeated digits indicate multiple runs for each digit with varying parameters, providing a measure of model robustness and performance variability.   Finally, the differing accuracy values across LR, FF, and IF classifiers suggests a comparison of different model architectures or training methodologies.

================================================================================
================================================================================
create_explanatory_figure_Fig4.py:
==================================
import numpy as np
import pickle
import matplotlib
import matplotlib.pyplot as plt
from scipy import signal
from sklearn import linear_model
from sklearn.metrics import roc_curve, roc_auc_score, auc
from sklearn.decomposition import TruncatedSVD, NMF
import matplotlib.gridspec as gridspec

matplotlib.rcParams['pdf.fonttype'] = 42
matplotlib.rcParams['svg.fonttype'] = 'none'

#%% script params

save_figures = True
save_figures = False
all_file_endings_to_use = ['.png', '.pdf', '.svg']

data_folder   = '/filter_and_fire_neuron/results_data_capacity/'
figure_folder = '/filter_and_fire_neuron/saved_figures/'

#%% helper functions

def create_single_PSP_profile(tau_rise, tau_decay, temporal_filter_length=50):

    safety_factor = 1.5
    if tau_rise >= (tau_decay / safety_factor):
        tau_decay = safety_factor * tau_rise

    exp_r = signal.exponential(M=temporal_filter_length, center=0, tau=tau_rise , sym=False)
    exp_d = signal.exponential(M=temporal_filter_length, center=0, tau=tau_decay, sym=False)

    post_syn_potential = exp_d - exp_r
    post_syn_potential /= post_syn_potential.max()

    return post_syn_potential


def construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec):

    num_synapses = tau_rise_vec.shape[0]
    temporal_filter_length = int(7 * tau_decay_vec.max()) + 1

    syn_filter = np.zeros((num_synapses, temporal_filter_length))

    for k, (tau_r, tau_d) in enumerate(zip(tau_rise_vec, tau_decay_vec)):
        syn_filter[k,:] = create_single_PSP_profile(tau_r, tau_d, temporal_filter_length=temporal_filter_length)

    return syn_filter


def simulate_filter_and_fire_cell_training(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                           refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):

    temporal_filter_length = int(5 * refreactory_time_constant) + 1
    refreactory_filter = signal.exponential(M=temporal_filter_length,center=0,tau=refreactory_time_constant,sym=False)[np.newaxis,:]

    # padd input and get all synaptic filters
    normlized_syn_filter = np.flipud(construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec))
    padded_input = np.hstack((np.zeros(normlized_syn_filter.shape), presynaptic_input_spikes))

    # calc local currents
    local_normlized_currents = np.zeros(presynaptic_input_spikes.shape)
    for k in range(normlized_syn_filter.shape[0]):
        local_normlized_currents[k] = signal.convolve(padded_input[k], normlized_syn_filter[k], mode='valid')[1:]

    # multiply by weights to get the somatic current
    soma_current = signal.convolve(local_normlized_currents, synaptic_weights, mode='valid')

    # simulate the cell
    soma_voltage = v_reset + current_to_voltage_mult_factor * soma_current.ravel()
    output_spike_times_in_ms = []
    # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
    for t in range(len(soma_voltage)):
        # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
        if (soma_voltage[t] > v_threshold) and ((t + 1) < len(soma_voltage)):
            t_start = t + 1
            t_end = min(len(soma_voltage), t_start + temporal_filter_length)
            soma_voltage[t_start:t_end] -= (soma_voltage[t + 1] - v_reset) * refreactory_filter.ravel()[:(t_end - t_start)]
            output_spike_times_in_ms.append(t)

    return local_normlized_currents, soma_voltage, output_spike_times_in_ms


def add_offset_for_plotting(traces_matrix, offset_size=1.1):

    traces_matrix_with_offset = offset_size * np.kron(np.arange(traces_matrix.shape[0])[:,np.newaxis], np.ones((1,traces_matrix.shape[1])))
    traces_matrix_with_offset = traces_matrix_with_offset + traces_matrix

    return traces_matrix_with_offset


#%% script params

# input parameters
num_values_per_param = 12

# neuron model parameters
connections_per_axon = 5
num_synapses = num_values_per_param * num_values_per_param

v_reset     = -80
v_threshold = -55
current_to_voltage_mult_factor = 3
refreactory_time_constant = 20

model_type = 'F&F'
#model_type = 'I&F'

time_limit_ms = 120

# synapse non-learnable parameters
if model_type == 'F&F':
    tau_rise_range  = [1,18]
    tau_decay_range = [7,25]
elif model_type == 'I&F':
    tau_rise_range  = [3,3]
    tau_decay_range = [25,25]

tau_rise_vec = np.linspace(tau_rise_range[0], tau_rise_range[1] , num_values_per_param)[:,np.newaxis]
tau_rise_vec = np.kron(np.ones((num_values_per_param,1)), tau_rise_vec)

tau_decay_vec = np.linspace(tau_decay_range[0], tau_decay_range[1] , num_values_per_param)[:,np.newaxis]
tau_decay_vec = np.kron(tau_decay_vec, np.ones((num_values_per_param,1)))

normlized_syn_filter_small = construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec)

offset_size = 0.15

plt.close('all')
plt.figure(figsize=(25,20));
plt.subplot(1,2,1); plt.imshow(normlized_syn_filter_small);
plt.title('normlized synaptic filters as heatmaps', fontsize=22); plt.xlabel('time [ms]', fontsize=22); plt.ylabel('synaptic filter index', fontsize=22); plt.xlim(0,time_limit_ms);
plt.subplot(1,2,2);

use_colors = False
if use_colors:
    colors = 'rgbymcrgbymc'

    end_ind = 0
    for k in range(num_values_per_param):
        start_ind = end_ind
        end_ind = start_ind + num_values_per_param
        print(start_ind, end_ind, colors[k])
        plt.plot(offset_size * k * num_values_per_param + add_offset_for_plotting(normlized_syn_filter_small[start_ind:end_ind], offset_size=offset_size).T, c=colors[k]);
else:
    plt.plot(add_offset_for_plotting(normlized_syn_filter_small, offset_size=offset_size).T, c='k');

plt.title('normlized synaptic filters as PSPs', fontsize=22); plt.xlabel('time [ms]', fontsize=22); plt.ylabel('normalized PSP (A.U)', fontsize=22); plt.xlim(-1,time_limit_ms);

#%% Create all possible combinations

# input parameters
num_values_per_param = 50

tau_rise_vec = np.linspace(tau_rise_range[0], tau_rise_range[1] , num_values_per_param)[:,np.newaxis]
tau_rise_vec = np.kron(np.ones((num_values_per_param,1)), tau_rise_vec)

tau_decay_vec = np.linspace(tau_decay_range[0], tau_decay_range[1] , num_values_per_param)[:,np.newaxis]
tau_decay_vec = np.kron(tau_decay_vec, np.ones((num_values_per_param,1)))

normlized_syn_filter_large = construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec)

plt.close('all')
plt.figure(figsize=(25,20));
plt.subplot(1,2,1); plt.imshow(normlized_syn_filter_large);
plt.title('normlized synaptic filters as heatmaps', fontsize=22); plt.xlabel('time [ms]', fontsize=22); plt.ylabel('synaptic filter index', fontsize=22)
plt.xlim(0,time_limit_ms);
plt.subplot(1,2,2); plt.plot(normlized_syn_filter_large.T, alpha=0.15);
plt.title('normlized synaptic filters as PSPs', fontsize=22); plt.xlabel('time [ms]', fontsize=22); plt.ylabel('normalized PSP (A.U)', fontsize=22);
plt.xlim(0,time_limit_ms);

#%% apply SVD and display

X = normlized_syn_filter_large
PSP_SVD_model = TruncatedSVD(n_components=100)
PSP_SVD_model.fit(X)

SVD_cutoff_ind = 3
max_SVD_basis_to_present = 18

plt.close('all')
plt.figure(figsize=(25,20));
plt.subplot(3,1,1); plt.imshow(PSP_SVD_model.components_[:max_SVD_basis_to_present]);
plt.title('normlized synaptic filters as heatmaps', fontsize=22); plt.xlabel('time [ms]', fontsize=22); plt.ylabel('synaptic filter index', fontsize=22); plt.xlim(0,time_limit_ms);
plt.subplot(3,1,2); plt.plot(PSP_SVD_model.components_[:SVD_cutoff_ind].T);
plt.title('first 3 basis functions', fontsize=22); plt.xlabel('time [ms]', fontsize=22); plt.ylabel('normalized PSP (A.U)', fontsize=22); plt.xlim(0,time_limit_ms);
plt.subplot(3,1,3); plt.plot(PSP_SVD_model.components_[SVD_cutoff_ind:max_SVD_basis_to_present].T);
plt.title('rest of the basis functions', fontsize=22); plt.xlabel('time [ms]', fontsize=22); plt.ylabel('normalized PSP (A.U)', fontsize=22); plt.xlim(0,time_limit_ms);

#%% show variance explained

num_basis_functions = PSP_SVD_model.explained_variance_ratio_.shape[0]
explained_var_percent = 100 * PSP_SVD_model.explained_variance_ratio_
cumsum_explained_var_percent = np.concatenate((np.array([0]), np.cumsum(explained_var_percent)))
dot_selected_ind = 3

plt.close('all')
plt.figure(figsize=(10,7));
plt.plot(np.arange(num_basis_functions + 1), cumsum_explained_var_percent, c='k')
plt.scatter(dot_selected_ind, cumsum_explained_var_percent[dot_selected_ind+1], c='r', s=200)
plt.xlabel('num basis functions', fontsize=16); plt.ylabel('explained %s' %('%'), fontsize=16);
plt.title('SVD cumulative explained percent \ntotal variance explained = %.2f%s' %(cumsum_explained_var_percent[dot_selected_ind+1],'%'), fontsize=18);
plt.ylim(-1,105); plt.xlim(-1,num_basis_functions+1);
plt.xlim(-0.3,12)

#%% Apply NMF

from warnings import simplefilter
from sklearn.exceptions import ConvergenceWarning
simplefilter("ignore", category=ConvergenceWarning)

# to avoid numberic instability, replicate the data and add some noise
noisy_data_for_NMF = np.tile(X,[3,1])
noisy_data_for_NMF = noisy_data_for_NMF + 0.0 * np.random.rand(noisy_data_for_NMF.shape[0], noisy_data_for_NMF.shape[1])

PSP_NMF_model = NMF(n_components=20)
PSP_NMF_model.fit(noisy_data_for_NMF)

NMF_cutoff_ind = 3
max_basis_to_present = 10

# normalize each basis vector to it's maximum (for presentation)
NMF_basis = PSP_NMF_model.components_
NMF_basis_norm = NMF_basis / np.tile(NMF_basis.max(axis=1, keepdims=True), [1, NMF_basis.shape[1]])

plt.close('all')
plt.figure(figsize=(25,20));
plt.subplot(3,1,1); plt.imshow(NMF_basis_norm[:max_basis_to_present]);
plt.title('normlized synaptic filters as heatmaps', fontsize=22); plt.xlabel('time [ms]', fontsize=22); plt.ylabel('synaptic filter index', fontsize=22); plt.xlim(0,time_limit_ms);
plt.subplot(3,1,2); plt.plot(NMF_basis_norm[:NMF_cutoff_ind].T);
plt.title('first 4 basis functions', fontsize=22); plt.xlabel('time [ms]', fontsize=22); plt.ylabel('normalized PSP (A.U)', fontsize=22); plt.xlim(0,time_limit_ms);
plt.subplot(3,1,3); plt.plot(NMF_basis_norm[NMF_cutoff_ind:max_basis_to_present].T);
plt.title('rest of the basis functions', fontsize=22); plt.xlabel('time [ms]', fontsize=22); plt.ylabel('normalized PSP (A.U)', fontsize=22); plt.xlim(0,time_limit_ms);

#%% load file for capacity plot

results_filename = data_folder + 'FF_vs_IF_capacity_comparision__num_axons_200__sim_duration_sec_120__num_mult_conn_6__rand_rep_18.pickle'

loaded_script_results_dict = pickle.load(open(results_filename, "rb" ))

processed_res_curves = loaded_script_results_dict['processed_res_curves']
all_results_curves   = loaded_script_results_dict['all_results_curves']

num_axons = loaded_script_results_dict['script_main_params']['num_axons']
stimulus_duration_sec = loaded_script_results_dict['script_main_params']['stimulus_duration_sec']

filename_str = 'FF_vs_IF_capacity_comparision__num_axons_%d__sim_duration_sec_120__num_mult_conn_6__rand_rep_18.pickle'
num_axons_list = sorted([100, 112, 125, 137, 150, 162, 175, 187, 200, 212, 225, 237])
all_filenames_str = [filename_str %(x) for x in num_axons_list]

model_keys = list(loaded_script_results_dict['processed_res_curves'].keys())
connections_per_axon_2C = loaded_script_results_dict['processed_res_curves'][model_keys[0]]['connections_per_axon']

precisely_timed_spikes_per_axon_2C = {}
precisely_timed_spikes_per_axon_error_2C = {}
for key in model_keys:
    precisely_timed_spikes_per_axon_2C[key] = np.zeros((len(all_filenames_str), len(connections_per_axon_2C)))
    precisely_timed_spikes_per_axon_error_2C[key] = np.zeros((len(all_filenames_str), len(connections_per_axon_2C)))

for k, (curr_num_axons, curr_filename) in enumerate(zip(num_axons_list, all_filenames_str)):
    curr_results_filename = data_folder + curr_filename
    curr_loaded_results_dict = pickle.load(open(curr_results_filename, "rb" ))

    for key in model_keys:
        precisely_timed_spikes_per_axon_2C[key][k,:] = curr_loaded_results_dict['processed_res_curves'][key]['num_almost_perfectly_placed_spikes'] / curr_num_axons

        for j, (num_M_conn, num_spikes) in enumerate(zip(curr_loaded_results_dict['processed_res_curves'][key]['connections_per_axon'],
                                                         curr_loaded_results_dict['processed_res_curves'][key]['num_almost_perfectly_placed_spikes'])):

            model_connections_str = '%s, %d connections' %(key, num_M_conn)
            error_index = list(curr_loaded_results_dict['all_results_curves'][model_connections_str]['num_spikes']).index(num_spikes)
            error_scale = (curr_loaded_results_dict['all_results_curves'][model_connections_str]['num_spikes'][error_index + 1] -
                           curr_loaded_results_dict['all_results_curves'][model_connections_str]['num_spikes'][max(0, error_index - 1)])

            if error_index > 1:
                error_scale /= 2

            precisely_timed_spikes_per_axon_error_2C[key][k,j] = error_scale / curr_num_axons


color_map = {}
color_map['I&F'] = '0.05'
color_map['F&F'] = 'orange'

#%% Calculate Capacity with optimal PSP profiles (m = 3)

optimal_basis_PSPs = NMF_basis_norm[:NMF_cutoff_ind]
print(optimal_basis_PSPs.shape)

num_axons = 3
normlized_syn_filter = np.kron(optimal_basis_PSPs, np.ones((num_axons,1)))

#%% Define several new helper functions (including two simulation functions)

def simulate_filter_and_fire_cell_training_PSPs(presynaptic_input_spikes, synaptic_weights, normlized_syn_filter,
                                                refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):

    temporal_filter_length = int(5 * refreactory_time_constant) + 1
    refreactory_filter = signal.exponential(M=temporal_filter_length,center=0,tau=refreactory_time_constant,sym=False)[np.newaxis,:]

    # padd input and get all synaptic filters
    padded_input = np.hstack((np.zeros(normlized_syn_filter.shape), presynaptic_input_spikes))

    # calc local currents
    local_normlized_currents = np.zeros(presynaptic_input_spikes.shape)
    for k in range(normlized_syn_filter.shape[0]):
        local_normlized_currents[k] = signal.convolve(padded_input[k], normlized_syn_filter[k], mode='valid')[1:]

    # multiply by weights to get the somatic current
    soma_current = signal.convolve(local_normlized_currents, synaptic_weights, mode='valid')

    # simulate the cell
    soma_voltage = v_reset + current_to_voltage_mult_factor * soma_current.ravel()
    output_spike_times_in_ms = []
    # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
    for t in range(len(soma_voltage)):
        # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
        if (soma_voltage[t] > v_threshold) and ((t + 1) < len(soma_voltage)):
            t_start = t + 1
            t_end = min(len(soma_voltage), t_start + temporal_filter_length)
            soma_voltage[t_start:t_end] -= (soma_voltage[t + 1] - v_reset) * refreactory_filter.ravel()[:(t_end - t_start)]
            output_spike_times_in_ms.append(t)

    return local_normlized_currents, soma_voltage, output_spike_times_in_ms


# use local currents as "features" and fit a linear model to the data
def prepare_training_dataset(local_normlized_currents, desired_output_spikes, spike_safety_range_ms=10, negative_subsampling_fraction=0.1):

    # remove all "negative" time points that are too close to spikes
    desired_output_spikes_LPF = signal.convolve(desired_output_spikes, np.ones((spike_safety_range_ms,)), mode='same') > 0.1
    desired_timepoints = ~desired_output_spikes_LPF

    # massivly subsample the remaining timepoints
    desired_timepoints[np.random.rand(desired_timepoints.shape[0]) > negative_subsampling_fraction] = 0
    desired_timepoints[desired_output_spikes > 0.1] = 1

    X = local_normlized_currents.T[desired_timepoints,:]
    y = desired_output_spikes[desired_timepoints]

    return X, y


#%% check if desired number of spikes is better than desired AUC score

requested_number_of_output_spikes = 93

optimal_basis_PSPs = NMF_basis_norm[:NMF_cutoff_ind]

# input parameters
num_axons = 200

# neuron model parameters
connections_per_axon = NMF_cutoff_ind
num_synapses = connections_per_axon * num_axons

v_reset     = -80
v_threshold = -55
current_to_voltage_mult_factor = 3
refreactory_time_constant = 15

model_type = 'F&F optimal'

# synapse learnable parameters
synaptic_weights_vec = np.random.normal(size=(num_synapses, 1))

# generate sample input
stimulus_duration_ms = 90000
instantanious_input_spike_probability = 0.004

axons_input_spikes = np.random.rand(num_axons, stimulus_duration_ms) < instantanious_input_spike_probability
presynaptic_input_spikes = np.kron(np.ones((connections_per_axon,1)), axons_input_spikes)
normlized_syn_filter = np.kron(optimal_basis_PSPs, np.ones((num_axons,1)))

assert presynaptic_input_spikes.shape[0] == num_synapses, 'number of synapses doesnt match the number of presynaptic inputs'

# generate desired pattern of output spikes
min_time_between_spikes_ms = 90

desired_output_spike_times = min_time_between_spikes_ms * np.random.randint(int(stimulus_duration_ms / min_time_between_spikes_ms), size=requested_number_of_output_spikes)
desired_output_spike_times = np.sort(np.unique(desired_output_spike_times))

desired_output_spikes = np.zeros((stimulus_duration_ms,))
desired_output_spikes[desired_output_spike_times] = 1.0

print('number of requested output spikes = %d' %(requested_number_of_output_spikes))

# simulate cell with normlized currents
local_normlized_currents, soma_voltage, output_spike_times_in_ms = simulate_filter_and_fire_cell_training_PSPs(presynaptic_input_spikes,
                                                                                                               synaptic_weights_vec, normlized_syn_filter,
                                                                                                               refreactory_time_constant=refreactory_time_constant,
                                                                                                               v_reset=v_reset, v_threshold=v_threshold,
                                                                                                               current_to_voltage_mult_factor=current_to_voltage_mult_factor)

output_spikes = np.zeros((stimulus_duration_ms,))
try:
    output_spikes[np.array(output_spike_times_in_ms)] = 1.0
except:
    print('no output spikes created')

#%% fit linear model to local currents

logistic_reg_model = linear_model.LogisticRegression(C=100000, fit_intercept=True, penalty='l2', max_iter=3000)

spike_safety_range_ms = 5
negative_subsampling_fraction = 0.5

X, y = prepare_training_dataset(local_normlized_currents, desired_output_spikes,
                                spike_safety_range_ms=spike_safety_range_ms,
                                negative_subsampling_fraction=negative_subsampling_fraction)

# fit model
logistic_reg_model.fit(X,y)

print('number of data points = %d (%.2f%s positive class)' %(X.shape[0], 100 * y.mean(),'%'))

y_hat = logistic_reg_model.predict_proba(X)[:,1]

# calculate AUC
train_AUC = roc_auc_score(y, y_hat)

fitted_output_spike_prob = logistic_reg_model.predict_proba(local_normlized_currents.T)[:,1]
full_AUC = roc_auc_score(desired_output_spikes, fitted_output_spike_prob)

# get desired FP threshold
desired_false_positive_rate = 0.004

fpr, tpr, thresholds = roc_curve(desired_output_spikes, fitted_output_spike_prob)

desired_fp_ind = np.argmin(abs(fpr-desired_false_positive_rate))
if desired_fp_ind == 0:
    desired_fp_ind = 1

actual_false_positive_rate = fpr[desired_fp_ind]
true_positive_rate         = tpr[desired_fp_ind]
desired_fp_threshold       = thresholds[desired_fp_ind]

AUC_score = auc(fpr, tpr)

print('AUC = %.4f' %(AUC_score))
print('at %.4f FP rate, TP = %.4f' %(actual_false_positive_rate, true_positive_rate))

output_spikes_after_learning = fitted_output_spike_prob > desired_fp_threshold

#%% Build the final figure

xy_label_fontsize = 16
title_fontsize = 21

plt.close('all')
fig = plt.figure(figsize=(20,18.5))
gs_figure = gridspec.GridSpec(nrows=8,ncols=5)
gs_figure.update(left=0.05, right=0.95, bottom=0.05, top=0.95, wspace=0.6, hspace=0.9)

ax_PSP_heatmap     = plt.subplot(gs_figure[ :6, :3])
ax_SVD_heatmap     = plt.subplot(gs_figure[6: , :3])
ax_PSP_traces      = plt.subplot(gs_figure[ :2,3: ])
ax_NMF_trance      = plt.subplot(gs_figure[2:4,3: ])
ax_explained_var   = plt.subplot(gs_figure[4:6,3: ])
ax_n_spikes_m_cons = plt.subplot(gs_figure[6: ,3: ])

interp_method_PSP = 'spline16'
interp_method_SVD = 'bilinear'
colormap = 'jet'

ax_PSP_heatmap.imshow(normlized_syn_filter_small, cmap=colormap, interpolation=interp_method_PSP);
ax_PSP_heatmap.set_xlim(0,time_limit_ms);
ax_PSP_heatmap.set_title('All PSPs as heatmap', fontsize=title_fontsize)
ax_PSP_heatmap.set_xlabel('Time (ms)', fontsize=xy_label_fontsize)
ax_PSP_heatmap.set_xticks([0,30,60,90,120])
ax_PSP_heatmap.set_xticklabels([0,30,60,90,120], fontsize=xy_label_fontsize)
ax_PSP_heatmap.set_ylabel('PSP index', fontsize=xy_label_fontsize)
ax_PSP_heatmap.set_yticks([0,24,48,72,96,120])
ax_PSP_heatmap.set_yticklabels([1,25,49,73,97,121], fontsize=xy_label_fontsize)

ax_SVD_heatmap.imshow(np.kron(PSP_SVD_model.components_[:max_SVD_basis_to_present], np.ones((2,1))), cmap=colormap, interpolation=interp_method_SVD);
ax_SVD_heatmap.set_xlim(0,time_limit_ms);
ax_SVD_heatmap.set_title('SVD basis functions as heatmap', fontsize=title_fontsize)
ax_SVD_heatmap.set_xlabel('Time (ms)', fontsize=xy_label_fontsize)
ax_SVD_heatmap.set_xticks([0,30,60,90,120])
ax_SVD_heatmap.set_xticklabels([0,30,60,90,120], fontsize=xy_label_fontsize)
ax_SVD_heatmap.set_ylabel('Basis function index', fontsize=xy_label_fontsize)
ax_SVD_heatmap.set_yticks([0,9,19,29])
ax_SVD_heatmap.set_yticklabels([1,10,20,30], fontsize=xy_label_fontsize)

ax_PSP_traces.plot(normlized_syn_filter_large.T, alpha=0.15);
ax_PSP_traces.set_xlim(-1,time_limit_ms);
ax_PSP_traces.set_title('All PSPs as traces', fontsize=title_fontsize)
ax_PSP_traces.set_ylabel('Magnitude (A.U.)', fontsize=xy_label_fontsize);
ax_PSP_traces.set_xlabel('Time (ms)', fontsize=xy_label_fontsize)
ax_PSP_traces.set_yticks([0.0,0.25,0.50,0.75,1.00])
ax_PSP_traces.set_yticklabels([0.0,0.25,0.50,0.75,1.00], fontsize=xy_label_fontsize)
ax_PSP_traces.set_xticks([0,30,60,90,120])
ax_PSP_traces.set_xticklabels([0,30,60,90,120], fontsize=xy_label_fontsize)

ax_NMF_trance.plot(NMF_basis_norm[:NMF_cutoff_ind].T);
ax_NMF_trance.set_xlim(-1,time_limit_ms);
ax_NMF_trance.set_title('NMF first %d basis functions' %(NMF_cutoff_ind), fontsize=title_fontsize)
ax_NMF_trance.set_ylabel('Magnitude  (A.U.)', fontsize=xy_label_fontsize);
ax_NMF_trance.set_xlabel('Time (ms)', fontsize=xy_label_fontsize)
ax_NMF_trance.set_yticks([0.0,0.25,0.50,0.75,1.00])
ax_NMF_trance.set_yticklabels([0.0,0.25,0.50,0.75,1.00], fontsize=xy_label_fontsize)
ax_NMF_trance.set_xticks([0,30,60,90,120])
ax_NMF_trance.set_xticklabels([0,30,60,90,120], fontsize=xy_label_fontsize)

ax_explained_var.plot(np.arange(num_basis_functions + 1), cumsum_explained_var_percent, c='k')

ax_explained_var.scatter(dot_selected_ind, cumsum_explained_var_percent[NMF_cutoff_ind + 1], c='r', s=200)
ax_explained_var.set_title('Variance explained = %.2f%s' %(cumsum_explained_var_percent[NMF_cutoff_ind + 1],'%'), fontsize=title_fontsize);
ax_explained_var.set_xlabel('Num basis functions', fontsize=xy_label_fontsize);
ax_explained_var.set_ylabel('Explained Percent (%s)' %('%'), fontsize=xy_label_fontsize);
ax_explained_var.set_ylim(-1,115);
ax_explained_var.set_yticks([0,25,50,75,100])
ax_explained_var.set_yticklabels([0,25,50,75,100], fontsize=xy_label_fontsize)
ax_explained_var.set_xlim(-0.3,12);
ax_explained_var.set_xticks([0,3,6,9,12])
ax_explained_var.set_xticklabels([0,3,6,9,12], fontsize=xy_label_fontsize)

for key in processed_res_curves.keys():
    y_error = precisely_timed_spikes_per_axon_2C[key].std(axis=0)
    ax_n_spikes_m_cons.errorbar(connections_per_axon_2C, precisely_timed_spikes_per_axon_2C[key].mean(axis=0), yerr=y_error, label=key, lw=4, color=color_map[key])

ax_n_spikes_m_cons.legend(loc='upper left', fontsize=22)
ax_n_spikes_m_cons.set_title('Placing Precisely Timed output Spikes', fontsize=title_fontsize)
ax_n_spikes_m_cons.set_xlabel('Number of Multiple Contacts - M', fontsize=xy_label_fontsize)
ax_n_spikes_m_cons.set_ylabel('Precisely Timed Spikes / Axon', fontsize=xy_label_fontsize);
ax_n_spikes_m_cons.spines['top'].set_visible(False)
ax_n_spikes_m_cons.spines['right'].set_visible(False)
ax_n_spikes_m_cons.set_yticks([0.15,0.3,0.45])
ax_n_spikes_m_cons.set_yticklabels([0.15,0.3,0.45], fontsize=xy_label_fontsize)
ax_n_spikes_m_cons.set_xticks([1,2,3,5,10,15])
ax_n_spikes_m_cons.set_xticklabels([1,2,3,5,10,15], fontsize=xy_label_fontsize)

# add the asimptote line
if AUC_score > 0.99:
    optimal_const_value = np.ones(connections_per_axon_2C.shape) * requested_number_of_output_spikes / num_axons
    ax_n_spikes_m_cons.plot(connections_per_axon_2C, optimal_const_value, label='Optimal 3 PSPs', ls='dashed', lw=2, color='red')
    ax_n_spikes_m_cons.scatter(3, optimal_const_value[0], label='Optimal 3 PSPs', s=200, color='red')
    ax_n_spikes_m_cons.set_ylim(0.09,1.06 * optimal_const_value[0])
    ax_n_spikes_m_cons.legend(loc='center right', fontsize=17)

# save figure
if save_figures:
    figure_name = 'F&F_explanatory_Figure_4_%d' %(np.random.randint(200))
    for file_ending in all_file_endings_to_use:
        if file_ending == '.png':
            fig.savefig(figure_folder + figure_name + file_ending, bbox_inches='tight')
        else:
            fig.savefig(figure_folder + figure_name + file_ending, bbox_inches='tight')


#%%

================================================================================
================================================================================
saved_figures\F&F_Introduction_Figure_1_11.png:
===============================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a diagram illustrating the process of synaptic transmission and summation in a neuron. It's divided into three main sections: input axons, synaptic filters, and synaptic contact voltage contribution, culminating in the somatic voltage.

The leftmost section shows three input axons firing action potentials at different times.  Each axon is represented by a distinct color (purple, teal, magenta). These action potentials represent the input signals to the neuron.

The central section displays "Synaptic Filters." This section shows how the action potentials from the input axons are filtered or modified by the synapses. Each colored line represents a different synapse, and the shape of the curve depicts the temporal filtering effect of each synapse on the incoming action potential.  The curves show that the signal is not directly transmitted but rather undergoes a characteristic change in shape and amplitude.

The right-hand section depicts "Synaptic Contact Voltage Contribution."  This section illustrates the individual contributions of each synapse to the overall voltage at the synapse.  The colored lines correspond to the synapses from the previous section and show how the filtered signal from each synapse affects the voltage at the point of contact.  Dashed lines are used in some cases, possibly representing subthreshold contributions that don't trigger a significant change in voltage.

Finally, at the bottom, the "Somatic Voltage" is shown. This is the sum of all the synaptic contributions, demonstrating the integration of multiple synaptic inputs at the neuron's soma. The resulting voltage waveform reflects the combined effect of all the individual synaptic potentials, showing how the neuron sums up the signals to determine whether or not it will fire its own action potential.  Two lines are shown here, likely representing the somatic voltage under different conditions or perhaps showing the voltage changes in different parts of the soma.

================================================================================
================================================================================
create_hardware_saving_figure_Fig5.py:
======================================
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from scipy import signal
from sklearn import linear_model
from sklearn.metrics import roc_curve, roc_auc_score, auc
from tensorflow import keras
import matplotlib.gridspec as gridspec

matplotlib.rcParams['pdf.fonttype'] = 42
matplotlib.rcParams['svg.fonttype'] = 'none'

#%% script params

# input parameters
num_axons_FF_cap = 100
time_delays_list_IF_cap = [250, 500]
num_time_delays_IF = len(time_delays_list_IF_cap) + 1
num_axons_IF = num_time_delays_IF * num_axons_FF_cap

stimulus_duration_ms = 10000
requested_number_of_output_spikes = 40
min_time_between_spikes_ms = 135

# neuron model parameters
v_reset     = -80
v_threshold = -55
current_to_voltage_mult_factor = 3
refreactory_time_constant = 15

# F&F neuron model parameters
connections_per_axon_FF = 5
num_synapses_FF = connections_per_axon_FF * num_axons_FF_cap

# synapse non-learnable parameters
tau_rise_range_FF  = [1,16]
tau_decay_range_FF = [8,24]

tau_rise_vec_FF  = np.random.uniform(low=tau_rise_range_FF[0] , high=tau_rise_range_FF[1] , size=(num_synapses_FF, 1))
tau_decay_vec_FF = np.random.uniform(low=tau_decay_range_FF[0], high=tau_decay_range_FF[1], size=(num_synapses_FF, 1))

# synapse learnable parameters
synaptic_weights_vec_FF = np.random.normal(size=(num_synapses_FF, 1))

# I&F neuron model parameters
connections_per_axon_IF = 1
num_synapses_IF = connections_per_axon_IF * num_axons_IF

# synapse non-learnable parameters
tau_rise_range_IF  = [1,1]
tau_decay_range_IF = [24,24]

tau_rise_vec_IF  = np.random.uniform(low=tau_rise_range_IF[0] , high=tau_rise_range_IF[1] , size=(num_synapses_IF, 1))
tau_decay_vec_IF = np.random.uniform(low=tau_decay_range_IF[0], high=tau_decay_range_IF[1], size=(num_synapses_IF, 1))

# synapse learnable parameters
synaptic_weights_vec_IF = np.random.normal(size=(num_synapses_IF, 1))

# book-keeping
save_figures = True
save_figures = False
all_file_endings_to_use = ['.png', '.pdf', '.svg']

figure_folder = '/filter_and_fire_neuron/saved_figures/'

#%% helper functions

def create_single_PSP_profile(tau_rise, tau_decay, temporal_filter_length=50):

    safety_factor = 1.5
    if tau_rise >= (tau_decay / safety_factor):
        tau_decay = safety_factor * tau_rise

    exp_r = signal.exponential(M=temporal_filter_length, center=0, tau=tau_rise , sym=False)
    exp_d = signal.exponential(M=temporal_filter_length, center=0, tau=tau_decay, sym=False)

    post_syn_potential = exp_d - exp_r
    post_syn_potential /= post_syn_potential.max()

    return post_syn_potential


def construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec):

    num_synapses = tau_rise_vec.shape[0]
    temporal_filter_length = int(4 * tau_decay_vec.max()) + 1

    syn_filter = np.zeros((num_synapses, temporal_filter_length))

    for k, (tau_r, tau_d) in enumerate(zip(tau_rise_vec, tau_decay_vec)):
        syn_filter[k,:] = create_single_PSP_profile(tau_r, tau_d, temporal_filter_length=temporal_filter_length)

    return syn_filter


def simulate_filter_and_fire_cell_training(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                           refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):

    temporal_filter_length = int(5 * refreactory_time_constant) + 1
    refreactory_filter = signal.exponential(M=temporal_filter_length,center=0,tau=refreactory_time_constant,sym=False)[np.newaxis,:]

    # padd input and get all synaptic filters
    normlized_syn_filter = np.flipud(construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec))
    padded_input = np.hstack((np.zeros(normlized_syn_filter.shape), presynaptic_input_spikes))

    # calc local currents
    local_normlized_currents = np.zeros(presynaptic_input_spikes.shape)
    for k in range(normlized_syn_filter.shape[0]):
        local_normlized_currents[k] = signal.convolve(padded_input[k], normlized_syn_filter[k], mode='valid')[1:]

    # multiply by weights to get the somatic current
    soma_current = signal.convolve(local_normlized_currents, synaptic_weights, mode='valid')

    # simulate the cell
    soma_voltage = v_reset + current_to_voltage_mult_factor * soma_current.ravel()
    output_spike_times_in_ms = []
    # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
    for t in range(len(soma_voltage)):
        # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
        if (soma_voltage[t] > v_threshold) and ((t + 1) < len(soma_voltage)):
            t_start = t + 1
            t_end = min(len(soma_voltage), t_start + temporal_filter_length)
            soma_voltage[t_start:t_end] -= (soma_voltage[t + 1] - v_reset) * refreactory_filter.ravel()[:(t_end - t_start)]
            output_spike_times_in_ms.append(t)

    return local_normlized_currents, soma_voltage, output_spike_times_in_ms


def simulate_filter_and_fire_cell_training_long(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                                refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):


    total_duration_ms = presynaptic_input_spikes.shape[1]
    max_duration_per_call_ms = 50000
    overlap_time_ms = 500

    if max_duration_per_call_ms >= total_duration_ms:
        local_normlized_currents, soma_voltage, output_spike_times_in_ms = simulate_filter_and_fire_cell_training(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                                                                                                  refreactory_time_constant=refreactory_time_constant, v_reset=v_reset, v_threshold=v_threshold,
                                                                                                                  current_to_voltage_mult_factor=current_to_voltage_mult_factor)
        return local_normlized_currents, soma_voltage, output_spike_times_in_ms


    local_normlized_currents = np.zeros(presynaptic_input_spikes.shape, dtype=np.float16)
    soma_voltage = np.zeros((total_duration_ms,))
    output_spike_times_in_ms = []

    num_sub_calls = int(np.ceil(total_duration_ms / (max_duration_per_call_ms - overlap_time_ms)))
    end_ind = overlap_time_ms
    for k in range(num_sub_calls):
        start_ind = end_ind - overlap_time_ms
        end_ind = min(start_ind + max_duration_per_call_ms, total_duration_ms)

        curr_loc_norm_c, curr_soma_v, curr_out_sp_t = simulate_filter_and_fire_cell_training(presynaptic_input_spikes[:,start_ind:end_ind], synaptic_weights, tau_rise_vec, tau_decay_vec,
                                                                                             refreactory_time_constant=refreactory_time_constant, v_reset=v_reset, v_threshold=v_threshold,
                                                                                             current_to_voltage_mult_factor=current_to_voltage_mult_factor)

        # update fields
        if k == 0:
            local_normlized_currents[:,start_ind:end_ind] = curr_loc_norm_c
            soma_voltage[start_ind:end_ind] = curr_soma_v
            output_spike_times_in_ms += curr_out_sp_t
        else:
            local_normlized_currents[:,(start_ind+overlap_time_ms):end_ind] = curr_loc_norm_c[:,overlap_time_ms:end_ind]
            soma_voltage[(start_ind+overlap_time_ms):end_ind] = curr_soma_v[overlap_time_ms:]
            curr_out_sp_t = [x for x in curr_out_sp_t if x >= (overlap_time_ms-1)]
            output_spike_times_in_ms += [(start_ind + x) for x in curr_out_sp_t]

    return local_normlized_currents, soma_voltage, output_spike_times_in_ms


def simulate_filter_and_fire_cell_inference(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                            refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):

    temporal_filter_length = int(5 * refreactory_time_constant) + 1
    refreactory_filter = signal.exponential(M=temporal_filter_length,center=0,tau=refreactory_time_constant,sym=False)[np.newaxis,:]

    # padd input and get all synaptic filters
    normlized_syn_filter = construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec)
    padded_input = np.hstack((np.zeros(normlized_syn_filter.shape), presynaptic_input_spikes))

    # calc somatic current
    weighted_syn_filter  = synaptic_weights * normlized_syn_filter
    soma_current = signal.convolve(padded_input, weighted_syn_filter, mode='valid')[:,1:]

    # simulate the cell
    soma_voltage = v_reset + current_to_voltage_mult_factor * soma_current.ravel()
    output_spike_times_in_ms = []
    # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
    for t in range(len(soma_voltage)):

        # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
        if (soma_voltage[t] > v_threshold) and ((t + 1) < len(soma_voltage)):
            t_start = t + 1
            t_end = min(len(soma_voltage), t_start + temporal_filter_length)
            soma_voltage[t_start:t_end] -= (soma_voltage[t + 1] - v_reset) * refreactory_filter.ravel()[:(t_end - t_start)]
            output_spike_times_in_ms.append(t)

    return soma_voltage, output_spike_times_in_ms


def simulate_filter_and_fire_cell_inference_long(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                                refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):

    total_duration_ms = presynaptic_input_spikes.shape[1]
    max_duration_per_call_ms = 50000
    overlap_time_ms = 500

    if max_duration_per_call_ms >= total_duration_ms:
        soma_voltage, output_spike_times_in_ms = simulate_filter_and_fire_cell_inference(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                                                                         refreactory_time_constant=refreactory_time_constant, v_reset=v_reset, v_threshold=v_threshold,
                                                                                         current_to_voltage_mult_factor=current_to_voltage_mult_factor)
        return soma_voltage, output_spike_times_in_ms


    soma_voltage = np.zeros((total_duration_ms,))
    output_spike_times_in_ms = []

    num_sub_calls = int(np.ceil(total_duration_ms / (max_duration_per_call_ms - overlap_time_ms)))
    end_ind = overlap_time_ms
    for k in range(num_sub_calls):
        start_ind = end_ind - overlap_time_ms
        end_ind = min(start_ind + max_duration_per_call_ms, total_duration_ms)

        curr_soma_v, curr_out_sp_t = simulate_filter_and_fire_cell_inference(presynaptic_input_spikes[:,start_ind:end_ind], synaptic_weights, tau_rise_vec, tau_decay_vec,
                                                                             refreactory_time_constant=refreactory_time_constant, v_reset=v_reset, v_threshold=v_threshold,
                                                                             current_to_voltage_mult_factor=current_to_voltage_mult_factor)

        # update fields
        if k == 0:
            soma_voltage[start_ind:end_ind] = curr_soma_v
            output_spike_times_in_ms += curr_out_sp_t
        else:
            soma_voltage[(start_ind+overlap_time_ms):end_ind] = curr_soma_v[overlap_time_ms:]
            curr_out_sp_t = [x for x in curr_out_sp_t if x >= (overlap_time_ms-1)]
            output_spike_times_in_ms += [(start_ind + x) for x in curr_out_sp_t]

    return soma_voltage, output_spike_times_in_ms


def prepare_training_dataset(local_normlized_currents, desired_output_spikes, spike_safety_range_ms=10, negative_subsampling_fraction=0.1):

    # remove all "negative" time points that are too close to spikes
    desired_output_spikes_LPF = signal.convolve(desired_output_spikes, np.ones((spike_safety_range_ms,)), mode='same') > 0.1
    desired_timepoints = ~desired_output_spikes_LPF

    # massivly subsample the remaining timepoints
    desired_timepoints[np.random.rand(desired_timepoints.shape[0]) > negative_subsampling_fraction] = 0
    desired_timepoints[desired_output_spikes > 0.1] = 1

    X = local_normlized_currents.T[desired_timepoints,:]
    y = desired_output_spikes[desired_timepoints]

    return X, y


#%% define random input for capacity plot

# generate sample input
axons_input_spikes_capacity = np.random.rand(num_axons_FF_cap, stimulus_duration_ms) < 0.0016

# F&F
presynaptic_input_spikes_FF = np.kron(np.ones((connections_per_axon_FF,1)), axons_input_spikes_capacity)
assert presynaptic_input_spikes_FF.shape[0] == num_synapses_FF, 'number of synapses doesnt match the number of presynaptic inputs'

# I&F
presynaptic_input_spikes_IF = axons_input_spikes_capacity.copy()
for delay_ms in time_delays_list_IF_cap:
    curr_delay_axons_input_spikes_IF = np.zeros(axons_input_spikes_capacity.shape)
    curr_delay_axons_input_spikes_IF[:,delay_ms:] = axons_input_spikes_capacity[:,:-delay_ms]
    presynaptic_input_spikes_IF = np.vstack((presynaptic_input_spikes_IF, curr_delay_axons_input_spikes_IF))
assert presynaptic_input_spikes_IF.shape[0] == num_synapses_IF, 'number of synapses doesnt match the number of presynaptic inputs'


# generate desired pattern of output spikes
desired_output_spike_times = min_time_between_spikes_ms * np.random.randint(int(stimulus_duration_ms / min_time_between_spikes_ms), size=requested_number_of_output_spikes)
desired_output_spike_times = np.sort(np.unique(desired_output_spike_times))

desired_output_spikes = np.zeros((stimulus_duration_ms,))
desired_output_spikes[desired_output_spike_times] = 1.0

print('number of requested output spikes = %d' %(requested_number_of_output_spikes))


#%% fit F&F model to the input

# simulate cell with normlized currents
local_normlized_currents_FF, _, _ = simulate_filter_and_fire_cell_training(presynaptic_input_spikes_FF,
                                                                           synaptic_weights_vec_FF, tau_rise_vec_FF, tau_decay_vec_FF,
                                                                           refreactory_time_constant=refreactory_time_constant,
                                                                           v_reset=v_reset, v_threshold=v_threshold,
                                                                           current_to_voltage_mult_factor=current_to_voltage_mult_factor)


# fit linear model to local currents
filter_and_fire_model = linear_model.LogisticRegression(C=30000, fit_intercept=True, penalty='l2', max_iter=3000)

spike_safety_range_ms = 1
negative_subsampling_fraction = 0.99

X, y = prepare_training_dataset(local_normlized_currents_FF, desired_output_spikes,
                                spike_safety_range_ms=spike_safety_range_ms,
                                negative_subsampling_fraction=negative_subsampling_fraction)

# fit model
filter_and_fire_model.fit(X,y)

print('number of data points = %d (%.2f%s positive class)' %(X.shape[0], 100 * y.mean(),'%'))

y_hat = filter_and_fire_model.predict_proba(X)[:,1]

# calculate AUC
train_AUC = roc_auc_score(y, y_hat)

fitted_output_spike_prob_FF = filter_and_fire_model.predict_proba(local_normlized_currents_FF.T)[:,1]
full_AUC = roc_auc_score(desired_output_spikes, fitted_output_spike_prob_FF)

# get desired FP threshold
desired_false_positive_rate = 0.004

fpr, tpr, thresholds = roc_curve(desired_output_spikes, fitted_output_spike_prob_FF)

desired_fp_ind = np.argmin(abs(fpr-desired_false_positive_rate))
if desired_fp_ind == 0:
    desired_fp_ind = 1

actual_false_positive_rate = fpr[desired_fp_ind]
true_positive_rate         = tpr[desired_fp_ind]
desired_fp_threshold       = thresholds[desired_fp_ind]

AUC_score = auc(fpr, tpr)

if AUC_score > 0.9995:
    desired_fp_threshold = 0.15

print('F&F fitting AUC = %.4f' %(AUC_score))
print('at %.4f FP rate, TP = %.4f' %(actual_false_positive_rate, true_positive_rate))

output_spikes_after_learning_FF = fitted_output_spike_prob_FF > desired_fp_threshold

#%% fit I&F to the input with 2 delayed versions of the same input

# simulate cell with normlized currents
local_normlized_currents_IF, _, _ = simulate_filter_and_fire_cell_training(presynaptic_input_spikes_IF,
                                                                           synaptic_weights_vec_IF, tau_rise_vec_IF, tau_decay_vec_IF,
                                                                           refreactory_time_constant=refreactory_time_constant,
                                                                           v_reset=v_reset, v_threshold=v_threshold,
                                                                           current_to_voltage_mult_factor=current_to_voltage_mult_factor)


# fit linear model to local currents
integrate_and_fire_model = linear_model.LogisticRegression(C=30000, fit_intercept=True, penalty='l2', max_iter=3000)

spike_safety_range_ms = 1
negative_subsampling_fraction = 0.99

X, y = prepare_training_dataset(local_normlized_currents_IF, desired_output_spikes,
                                spike_safety_range_ms=spike_safety_range_ms,
                                negative_subsampling_fraction=negative_subsampling_fraction)

# fit model
integrate_and_fire_model.fit(X,y)

print('number of data points = %d (%.2f%s positive class)' %(X.shape[0], 100 * y.mean(),'%'))

y_hat = integrate_and_fire_model.predict_proba(X)[:,1]

# calculate AUC
train_AUC = roc_auc_score(y, y_hat)

fitted_output_spike_prob_IF = integrate_and_fire_model.predict_proba(local_normlized_currents_IF.T)[:,1]
full_AUC = roc_auc_score(desired_output_spikes, fitted_output_spike_prob_IF)

# get desired FP threshold
desired_false_positive_rate = 0.004

fpr, tpr, thresholds = roc_curve(desired_output_spikes, fitted_output_spike_prob_IF)

desired_fp_ind = np.argmin(abs(fpr - desired_false_positive_rate))
if desired_fp_ind == 0:
    desired_fp_ind = 1

actual_false_positive_rate = fpr[desired_fp_ind]
true_positive_rate         = tpr[desired_fp_ind]
desired_fp_threshold       = thresholds[desired_fp_ind]

AUC_score = auc(fpr, tpr)

if AUC_score > 0.9995:
    desired_fp_threshold = 0.15

print('I&F fitting AUC = %.4f' %(AUC_score))
print('at %.4f FP rate, TP = %.4f' %(actual_false_positive_rate, true_positive_rate))

output_spikes_after_learning_IF = fitted_output_spike_prob_IF > desired_fp_threshold

#%% MNIST params

spatial_extent_factor = 5
temporal_extent_factor_numerator = 2
temporal_extent_factor_denumerator = 1

num_const_firing_channels = 20
temporal_silence_ms = 70

positive_digit = 7
num_train_positive_patterns = 2000

release_probability = 1.0
apply_release_prob_during_train = False
apply_releash_prob_during_test = False

output_spike_tolorance_window_duration = 20
output_spike_tolorance_window_offset   = 5

time_delays_list_IF = [12,24]
num_axons_FF = spatial_extent_factor * 20 + num_const_firing_channels
num_time_delays_IF = len(time_delays_list_IF) + 1
num_axons_IF = num_time_delays_IF * num_axons_FF

num_synapses_FF = connections_per_axon_FF * num_axons_FF
connections_per_axon_IF = 1
num_synapses_IF = connections_per_axon_IF * num_axons_IF

# synapse non-learnable parameters
tau_rise_range_FF  = [1,16]
tau_decay_range_FF = [8,24]

tau_rise_vec_FF  = np.random.uniform(low=tau_rise_range_FF[0] , high=tau_rise_range_FF[1] , size=(num_synapses_FF, 1))
tau_decay_vec_FF = np.random.uniform(low=tau_decay_range_FF[0], high=tau_decay_range_FF[1], size=(num_synapses_FF, 1))

# synapse learnable parameters
synaptic_weights_vec_FF = np.random.normal(size=(num_synapses_FF, 1))

# I&F neuron model parameters
connections_per_axon_IF = 1
num_synapses_IF = connections_per_axon_IF * num_axons_IF

# synapse non-learnable parameters
tau_rise_range_IF  = [1,1]
tau_decay_range_IF = [24,24]

tau_rise_vec_IF  = np.random.uniform(low=tau_rise_range_IF[0] , high=tau_rise_range_IF[1] , size=(num_synapses_IF, 1))
tau_decay_vec_IF = np.random.uniform(low=tau_decay_range_IF[0], high=tau_decay_range_IF[1], size=(num_synapses_IF, 1))

# synapse learnable parameters
synaptic_weights_vec_IF = np.random.normal(size=(num_synapses_IF, 1))

#%% load MNIST dataset and and transform into spikes

(x_train_original, y_train), (x_test_original, y_test) = keras.datasets.mnist.load_data()

# crop the data and binarize it
h_crop_range = [4,24]
w_crop_range = [4,24]

positive_threshold = 150

x_train_original = x_train_original[:,h_crop_range[0]:h_crop_range[1],w_crop_range[0]:w_crop_range[1]] > positive_threshold
x_test_original  = x_test_original[: ,h_crop_range[0]:h_crop_range[1],w_crop_range[0]:w_crop_range[1]] > positive_threshold

#%% Transform Xs to spatio-temporal spike trains

# extend according to "temporal_extent_factor"
kernel = np.ones((1, spatial_extent_factor, temporal_extent_factor_numerator), dtype=bool)

x_train = x_train_original.copy()
x_test  = x_test_original.copy()

# reshape X according to what is needed
x_train = np.kron(x_train, kernel)
x_test = np.kron(x_test, kernel)

# subsample according to "temporal_extent_factor_denumerator"
x_train = x_train[:,:,::temporal_extent_factor_denumerator]
x_test = x_test[:,:,::temporal_extent_factor_denumerator]

# padd with ones on top (for "bias" learning)
top_pad_train = np.ones((1, num_const_firing_channels, x_train.shape[2]), dtype=bool)
top_pad_test  = np.ones((1, num_const_firing_channels, x_test.shape[2] ), dtype=bool)

# add a few zero rows for clear seperation for visualization purpuses
top_pad_train[:,-5:,:] = 0
top_pad_test[:,-5:,:] = 0

x_train = np.concatenate((np.tile(top_pad_train, [x_train.shape[0],1,1]), x_train), axis=1)
x_test  = np.concatenate((np.tile(top_pad_test , [x_test.shape[0],1,1] ), x_test ), axis=1)

# pad with "temporal_silence_ms" zeros in the begining of each pattern (for silence between patterns)
left_pad_train = np.zeros((1, x_train.shape[1], temporal_silence_ms), dtype=bool)
left_pad_test  = np.zeros((1, x_test.shape[1] , temporal_silence_ms), dtype=bool)

x_train = np.concatenate((np.tile(left_pad_train, [x_train.shape[0],1,1]), x_train), axis=2)
x_test  = np.concatenate((np.tile(left_pad_test , [x_test.shape[0],1,1] ), x_test ), axis=2)

# add background activity
desired_background_activity_firing_rate_Hz = 10
background_activity_fraction = desired_background_activity_firing_rate_Hz / 1000

x_train[np.random.rand(x_train.shape[0], x_train.shape[1], x_train.shape[2]) < background_activity_fraction] = 1
x_test[ np.random.rand(x_test.shape[0] , x_test.shape[1] , x_test.shape[2] ) < background_activity_fraction] = 1

# subsample the input spikes
desired_average_input_firing_rate_Hz = 20
actual_mean_firing_rate_Hz = 1000 * x_train.mean()

fraction_of_spikes_to_eliminate = desired_average_input_firing_rate_Hz / actual_mean_firing_rate_Hz

x_train = x_train * (np.random.rand(x_train.shape[0], x_train.shape[1], x_train.shape[2]) < fraction_of_spikes_to_eliminate)
x_test  = x_test  * (np.random.rand(x_test.shape[0], x_test.shape[1], x_test.shape[2]) < fraction_of_spikes_to_eliminate)

final_mean_firing_rate_Hz = 1000 * x_train.mean()

#%% Create "one-vs-all" dataset

y_train_binary = y_train == positive_digit
y_test_binary  = y_test  == positive_digit

num_train_positive_patterns = min(int(y_train_binary.sum()), num_train_positive_patterns)

num_train_negative_patterns = int(2.0 * num_train_positive_patterns)

positive_inds = np.where(y_train_binary)[0]
negative_inds = np.where(~y_train_binary)[0]

selected_train_positives = np.random.choice(positive_inds, size=num_train_positive_patterns)
selected_train_negatives = np.random.choice(negative_inds, size=num_train_negative_patterns)

all_selected = np.random.permutation(np.concatenate((selected_train_positives, selected_train_negatives)))

X_train_spikes = x_train[all_selected]
Y_train_spikes = y_train_binary[all_selected]

X_test_spikes = x_test.copy()
Y_test_spikes = y_test_binary.copy()

zero_pred_baseline_accuracy = 100 * (1 - Y_test_spikes.mean())


#%% prepare input spikes for F&F and I&F training

axons_input_spikes = np.concatenate([X_train_spikes[k] for k in range(X_train_spikes.shape[0])],axis=1)

# prepare output spikes
pattern_duration_ms = X_train_spikes[0].shape[1]
output_spike_offset = 1
output_kernel = np.zeros((pattern_duration_ms,))
output_kernel[-output_spike_offset] = 1

desired_output_spikes_mnist = np.kron(Y_train_spikes, output_kernel)

# F&F
presynaptic_input_spikes_FF = np.kron(np.ones((connections_per_axon_FF,1)), axons_input_spikes)
assert presynaptic_input_spikes_FF.shape[0] == num_synapses_FF, 'number of synapses doesnt match the number of presynaptic inputs'

# I&F
presynaptic_input_spikes_IF = axons_input_spikes.copy()
for delay_ms in time_delays_list_IF:
    curr_delay_axons_input_spikes_IF = np.zeros(axons_input_spikes.shape)
    curr_delay_axons_input_spikes_IF[:,delay_ms:] = axons_input_spikes[:,:-delay_ms]
    presynaptic_input_spikes_IF = np.vstack((presynaptic_input_spikes_IF, curr_delay_axons_input_spikes_IF))
assert presynaptic_input_spikes_IF.shape[0] == num_synapses_IF, 'number of synapses doesnt match the number of presynaptic inputs'


#%% prepare input spikes for F&F and I&F testing

num_test_patterns = X_test_spikes.shape[0]

# prepare test outputs
output_kernel_test = np.zeros((X_test_spikes[0].shape[1],))
output_kernel_test[-output_spike_tolorance_window_duration:] = 1

desired_output_spikes_test = np.kron(Y_test_spikes[:num_test_patterns], output_kernel_test)
desired_output_spikes_test = np.concatenate((np.zeros((output_spike_tolorance_window_offset,)), desired_output_spikes_test[:-output_spike_tolorance_window_offset]))

# prepare test inputs
axons_input_spikes_test = np.concatenate([X_test_spikes[k] for k in range(num_test_patterns)],axis=1)

# F&F
presynaptic_input_spikes_test_FF = np.kron(np.ones((connections_per_axon_FF,1)), axons_input_spikes_test)
assert presynaptic_input_spikes_test_FF.shape[0] == num_synapses_FF, 'number of synapses doesnt match the number of presynaptic inputs'

# I&F
presynaptic_input_spikes_test_IF = axons_input_spikes_test.copy()
for delay_ms in time_delays_list_IF:
    curr_delay_axons_input_spikes_IF = np.zeros(axons_input_spikes_test.shape)
    curr_delay_axons_input_spikes_IF[:,delay_ms:] = axons_input_spikes_test[:,:-delay_ms]
    presynaptic_input_spikes_test_IF = np.vstack((presynaptic_input_spikes_test_IF, curr_delay_axons_input_spikes_IF))
assert presynaptic_input_spikes_test_IF.shape[0] == num_synapses_IF, 'number of synapses doesnt match the number of presynaptic inputs'


#%% simulate F&F cell with normlized currents on train

local_normlized_currents_FF, _, _ = simulate_filter_and_fire_cell_training_long(presynaptic_input_spikes_FF,
                                                                                synaptic_weights_vec_FF, tau_rise_vec_FF, tau_decay_vec_FF,
                                                                                refreactory_time_constant=refreactory_time_constant,
                                                                                v_reset=v_reset, v_threshold=v_threshold,
                                                                                current_to_voltage_mult_factor=current_to_voltage_mult_factor)

#%% fit linear model to local currents

filter_and_fire_model = linear_model.LogisticRegression(C=10000, fit_intercept=False, penalty='l2')

spike_safety_range_ms = 20
negative_subsampling_fraction = 0.5

X, y = prepare_training_dataset(local_normlized_currents_FF, desired_output_spikes_mnist,
                                spike_safety_range_ms=spike_safety_range_ms,
                                negative_subsampling_fraction=negative_subsampling_fraction)

# fit model
filter_and_fire_model.fit(X, y)

print('number of data points = %d (%.2f%s positive class)' %(X.shape[0], 100 * y.mean(),'%'))

# calculate train AUC
y_hat = filter_and_fire_model.predict_proba(X)[:,1]
train_AUC = roc_auc_score(y, y_hat)

FF_learned_synaptic_weights = np.fliplr(filter_and_fire_model.coef_).T

#%% find the best multiplicative factor for test prediction F&F

# FF_weight_mult_factors_list = [1,2,3,4,5,6,9,12,20,50,120,250]
FF_weight_mult_factors_list = [1,4,7,11,15]
FF_accuracy_list = []
FF_true_positive_list = []
FF_false_positive_list = []
for weight_mult_factor in FF_weight_mult_factors_list:

    # collect learned synaptic weights
    synaptic_weights_post_learning = weight_mult_factor * FF_learned_synaptic_weights

    soma_voltage_test, output_spike_times_in_ms_test = simulate_filter_and_fire_cell_inference_long(presynaptic_input_spikes_test_FF,
                                                                                                    synaptic_weights_post_learning, tau_rise_vec_FF, tau_decay_vec_FF,
                                                                                                    refreactory_time_constant=refreactory_time_constant,
                                                                                                    v_reset=v_reset, v_threshold=v_threshold,
                                                                                                    current_to_voltage_mult_factor=current_to_voltage_mult_factor)


    output_spikes_test = np.zeros(soma_voltage_test.shape)
    try:
        output_spikes_test[np.array(output_spike_times_in_ms_test)] = 1.0
    except:
        print('no output spikes created')


    # calculate test accuracy
    compact_desired_output_test = Y_test_spikes[:num_test_patterns]

    compact_desired_output_test2 = np.zeros(compact_desired_output_test.shape, dtype=bool)
    compact_predicted_output_test = np.zeros(compact_desired_output_test.shape, dtype=bool)

    # go over all patterns and extract the prediction (depends if the spikes are in the desired window)
    for pattern_ind in range(num_test_patterns):
        start_ind = pattern_duration_ms * pattern_ind + output_spike_tolorance_window_offset
        end_ind = start_ind + pattern_duration_ms

        # extract prediction
        predicted_spike_train_for_pattern = output_spikes_test[start_ind:end_ind]
        desired_spike_train_for_pattern = desired_output_spikes_test[start_ind:end_ind]

        compact_desired_output_test2[pattern_ind]  = desired_spike_train_for_pattern.sum() > 0.1

        if Y_test_spikes[pattern_ind] == 1:
            # check if there is a spike in the desired window only
            compact_predicted_output_test[pattern_ind] = (desired_spike_train_for_pattern * predicted_spike_train_for_pattern).sum() > 0.1
        else:
            # check if there is any spike in the full pattern duration
            compact_predicted_output_test[pattern_ind] = predicted_spike_train_for_pattern.sum() > 0.1

    # small verificaiton
    assert((compact_desired_output_test == compact_desired_output_test2).sum() == num_test_patterns)

    # display accuracy
    percent_accuracy = 100 * (compact_desired_output_test == compact_predicted_output_test).mean()
    true_positive    = 100 * (np.logical_and(compact_desired_output_test == True , compact_predicted_output_test == True).sum() / (compact_desired_output_test == True).sum())
    false_positive   = 100 * (np.logical_and(compact_desired_output_test == False, compact_predicted_output_test == True).sum() / (compact_desired_output_test == False).sum())

    print('weights mult factor = %.1f: Accuracy = %.3f%s. (TP, FP) = (%.3f%s, %.3f%s)' %(weight_mult_factor, percent_accuracy,'%',true_positive,'%',false_positive,'%'))

    FF_accuracy_list.append(percent_accuracy)
    FF_true_positive_list.append(true_positive)
    FF_false_positive_list.append(false_positive)

#%% make a final prediction on the test set F&F

# get the max accuracy weight matrix
max_accuracy_weight_mult_factor = FF_weight_mult_factors_list[np.argsort(np.array(FF_accuracy_list))[-1]]

synaptic_weights_vec_after_learning_FF = max_accuracy_weight_mult_factor * FF_learned_synaptic_weights

# simulate the max accuracy output after learning
soma_voltage_test, output_spike_times_in_ms_test_FF = simulate_filter_and_fire_cell_inference_long(presynaptic_input_spikes_test_FF,
                                                                                                   synaptic_weights_vec_after_learning_FF, tau_rise_vec_FF, tau_decay_vec_FF,
                                                                                                   refreactory_time_constant=refreactory_time_constant,
                                                                                                   v_reset=v_reset, v_threshold=v_threshold,
                                                                                                   current_to_voltage_mult_factor=current_to_voltage_mult_factor)


output_spikes_test_after_learning_full_FF = np.zeros(soma_voltage_test.shape)
try:
    output_spikes_test_after_learning_full_FF[np.array(output_spike_times_in_ms_test)] = 1.0
except:
    print('no output spikes created')


#%% simulate I&F cell with normlized currents on train

local_normlized_currents_IF, _, _ = simulate_filter_and_fire_cell_training_long(presynaptic_input_spikes_IF,
                                                                                synaptic_weights_vec_IF, tau_rise_vec_IF, tau_decay_vec_IF,
                                                                                refreactory_time_constant=refreactory_time_constant,
                                                                                v_reset=v_reset, v_threshold=v_threshold,
                                                                                current_to_voltage_mult_factor=current_to_voltage_mult_factor)

#%% fit linear model to local currents

integrate_and_fire_model = linear_model.LogisticRegression(C=10000, fit_intercept=False, penalty='l2')

spike_safety_range_ms = 20
negative_subsampling_fraction = 0.5

X, y = prepare_training_dataset(local_normlized_currents_IF, desired_output_spikes_mnist,
                                spike_safety_range_ms=spike_safety_range_ms,
                                negative_subsampling_fraction=negative_subsampling_fraction)

# fit model
integrate_and_fire_model.fit(X, y)

print('number of data points = %d (%.2f%s positive class)' %(X.shape[0], 100 * y.mean(),'%'))

# calculate train AUC
y_hat = integrate_and_fire_model.predict_proba(X)[:,1]
train_AUC = roc_auc_score(y, y_hat)

IF_learned_synaptic_weights = np.fliplr(integrate_and_fire_model.coef_).T

#%% find the best multiplicative factor for test prediction F&F

# FF_weight_mult_factors_list = [1,2,3,4,5,6,9,12,20,50,120,250]
IF_weight_mult_factors_list = [1,4,7,11,15]
IF_accuracy_list = []
IF_true_positive_list = []
IF_false_positive_list = []
for weight_mult_factor in IF_weight_mult_factors_list:

    # collect learned synaptic weights
    synaptic_weights_post_learning = weight_mult_factor * IF_learned_synaptic_weights

    soma_voltage_test, output_spike_times_in_ms_test = simulate_filter_and_fire_cell_inference_long(presynaptic_input_spikes_test_IF,
                                                                                                    synaptic_weights_post_learning, tau_rise_vec_IF, tau_decay_vec_IF,
                                                                                                    refreactory_time_constant=refreactory_time_constant,
                                                                                                    v_reset=v_reset, v_threshold=v_threshold,
                                                                                                    current_to_voltage_mult_factor=current_to_voltage_mult_factor)


    output_spikes_test = np.zeros(soma_voltage_test.shape)
    try:
        output_spikes_test[np.array(output_spike_times_in_ms_test)] = 1.0
    except:
        print('no output spikes created')


    # calculate test accuracy
    compact_desired_output_test = Y_test_spikes[:num_test_patterns]

    compact_desired_output_test2 = np.zeros(compact_desired_output_test.shape, dtype=bool)
    compact_predicted_output_test = np.zeros(compact_desired_output_test.shape, dtype=bool)

    # go over all patterns and extract the prediction (depends if the spikes are in the desired window)
    for pattern_ind in range(num_test_patterns):
        start_ind = pattern_duration_ms * pattern_ind + output_spike_tolorance_window_offset
        end_ind = start_ind + pattern_duration_ms

        # extract prediction
        predicted_spike_train_for_pattern = output_spikes_test[start_ind:end_ind]
        desired_spike_train_for_pattern = desired_output_spikes_test[start_ind:end_ind]

        compact_desired_output_test2[pattern_ind]  = desired_spike_train_for_pattern.sum() > 0.1

        if Y_test_spikes[pattern_ind] == 1:
            # check if there is a spike in the desired window only
            compact_predicted_output_test[pattern_ind] = (desired_spike_train_for_pattern * predicted_spike_train_for_pattern).sum() > 0.1
        else:
            # check if there is any spike in the full pattern duration
            compact_predicted_output_test[pattern_ind] = predicted_spike_train_for_pattern.sum() > 0.1

    # small verificaiton
    assert((compact_desired_output_test == compact_desired_output_test2).sum() == num_test_patterns)

    # display accuracy
    percent_accuracy = 100 * (compact_desired_output_test == compact_predicted_output_test).mean()
    true_positive    = 100 * (np.logical_and(compact_desired_output_test == True , compact_predicted_output_test == True).sum() / (compact_desired_output_test == True).sum())
    false_positive   = 100 * (np.logical_and(compact_desired_output_test == False, compact_predicted_output_test == True).sum() / (compact_desired_output_test == False).sum())

    print('weights mult factor = %.1f: Accuracy = %.3f%s. (TP, FP) = (%.3f%s, %.3f%s)' %(weight_mult_factor, percent_accuracy,'%',true_positive,'%',false_positive,'%'))

    IF_accuracy_list.append(percent_accuracy)
    IF_true_positive_list.append(true_positive)
    IF_false_positive_list.append(false_positive)

#%% make a final prediction on the test set F&F

# get the max accuracy weight matrix
max_accuracy_weight_mult_factor = IF_weight_mult_factors_list[np.argsort(np.array(IF_accuracy_list))[-1]]

synaptic_weights_vec_after_learning_IF = max_accuracy_weight_mult_factor * IF_learned_synaptic_weights

# simulate the max accuracy output after learning
soma_voltage_test, output_spike_times_in_ms_test_IF = simulate_filter_and_fire_cell_inference_long(presynaptic_input_spikes_test_IF,
                                                                                                   synaptic_weights_vec_after_learning_IF, tau_rise_vec_IF, tau_decay_vec_IF,
                                                                                                   refreactory_time_constant=refreactory_time_constant,
                                                                                                   v_reset=v_reset, v_threshold=v_threshold,
                                                                                                   current_to_voltage_mult_factor=current_to_voltage_mult_factor)


output_spikes_test_after_learning_full_IF = np.zeros(soma_voltage_test.shape)
try:
    output_spikes_test_after_learning_full_IF[np.array(output_spike_times_in_ms_test)] = 1.0
except:
    print('no output spikes created')


#%% Build the figure


plt.close('all')
fig = plt.figure(figsize=(12,22))
gs_figure = gridspec.GridSpec(nrows=17,ncols=1)
gs_figure.update(left=0.04, right=0.95, bottom=0.02, top=0.98, wspace=0.45, hspace=0.4)

ax_axons          = plt.subplot(gs_figure[:5,:])
ax_FF_spikes      = plt.subplot(gs_figure[5,:])
ax_IF_spikes      = plt.subplot(gs_figure[6,:])
ax_desired_output = plt.subplot(gs_figure[7,:])

ax_digits            = plt.subplot(gs_figure[9:11,:])
ax_axons_mnist       = plt.subplot(gs_figure[11:15,:])
ax_learning_outcomes = plt.subplot(gs_figure[15:,:])

FF_color = 'orange'
IF_color = '0.05'
target_color = 'red'

syn_activation_time, syn_activation_index = np.nonzero(presynaptic_input_spikes_IF.T)

syn_activation_time_1_cap, syn_activation_index_1_cap = np.nonzero(axons_input_spikes_capacity.T)
syn_activation_time_1_cap = syn_activation_time_1_cap / 1000

syn_activation_time_2_cap = syn_activation_time_1_cap + (time_delays_list_IF_cap[0] / 1000)
syn_activation_time_3_cap = syn_activation_time_1_cap + (time_delays_list_IF_cap[1] / 1000)
syn_activation_index_2_cap = syn_activation_index_1_cap + num_axons_FF_cap
syn_activation_index_3_cap = syn_activation_index_2_cap + num_axons_FF_cap

min_time_sec = -0.1
max_time_sec = stimulus_duration_ms / 1000
time_sec = np.linspace(0, max_time_sec, output_spikes_after_learning_FF.shape[0])

ax_axons.scatter(syn_activation_time_1_cap, syn_activation_index_1_cap, s=8, c='black');
ax_axons.scatter(syn_activation_time_2_cap, syn_activation_index_2_cap, s=8, c='chocolate');
ax_axons.scatter(syn_activation_time_3_cap, syn_activation_index_3_cap, s=8, c='purple');
ax_axons.set_xlim(min_time_sec, max_time_sec);
ax_axons.set_xticks([])
ax_axons.set_yticks([])
ax_axons.spines['top'].set_visible(False)
ax_axons.spines['bottom'].set_visible(False)
ax_axons.spines['left'].set_visible(False)
ax_axons.spines['right'].set_visible(False)

ax_FF_spikes.plot(time_sec, output_spikes_after_learning_FF, c=FF_color, lw=2.5);
ax_FF_spikes.set_title('F&F (M = 5)', fontsize=17, color=FF_color)
ax_FF_spikes.set_xlim(min_time_sec, max_time_sec);
ax_FF_spikes.set_xticks([])
ax_FF_spikes.set_yticks([])
ax_FF_spikes.spines['top'].set_visible(False)
ax_FF_spikes.spines['bottom'].set_visible(False)
ax_FF_spikes.spines['left'].set_visible(False)
ax_FF_spikes.spines['right'].set_visible(False)

ax_IF_spikes.plot(time_sec, output_spikes_after_learning_IF, c=IF_color, lw=2.5);
ax_IF_spikes.set_title('I&F (Orig Axons + 2 delayed)', fontsize=17, color=IF_color)
ax_IF_spikes.set_xlim(min_time_sec, max_time_sec);
ax_IF_spikes.set_xticks([])
ax_IF_spikes.set_yticks([])
ax_IF_spikes.spines['top'].set_visible(False)
ax_IF_spikes.spines['bottom'].set_visible(False)
ax_IF_spikes.spines['left'].set_visible(False)
ax_IF_spikes.spines['right'].set_visible(False)

ax_desired_output.plot(time_sec, desired_output_spikes, c=target_color, lw=2.5);
ax_desired_output.set_title('Desired Output (num spikes = %d)' %(requested_number_of_output_spikes), fontsize=17, color=target_color);
ax_desired_output.set_xlim(min_time_sec, max_time_sec);
ax_desired_output.set_xticks([]);
ax_desired_output.set_yticks([]);
ax_desired_output.spines['top'].set_visible(False)
ax_desired_output.spines['bottom'].set_visible(False)
ax_desired_output.spines['left'].set_visible(False)
ax_desired_output.spines['right'].set_visible(False)





##%% organize MNIST part into a figure

# test digit images
extention_kernel = np.ones((1, spatial_extent_factor, temporal_extent_factor_numerator), dtype=bool)
x_test_original_extended = np.kron(x_test_original, extention_kernel)
left_pad_test  = np.zeros((1, x_test_original_extended.shape[1] , temporal_silence_ms), dtype=bool)
x_test_original_extended  = np.concatenate((np.tile(left_pad_test , [x_test_original_extended.shape[0],1,1] ), x_test_original_extended ), axis=2)
x_test_axons_input_spikes = np.concatenate([x_test_original_extended[k] for k in range(x_test_original_extended.shape[0])],axis=1)

test_set_full_duration_ms = x_test_axons_input_spikes.shape[1]

# select a subset of time to display
num_digits_to_display = 9
start_time = x_test_original_extended.shape[2] * np.random.randint(int(test_set_full_duration_ms / x_test_original_extended.shape[2] - temporal_silence_ms))
end_time = start_time + 1 + x_test_original_extended.shape[2] * num_digits_to_display + temporal_silence_ms

# make sure we have something decent to show (randomise start and end times untill we do)
output_spikes_test_after_learning = output_spikes_test_after_learning_full_FF[start_time:end_time]
while output_spikes_test_after_learning.sum() < 2:
    start_time = x_test_original_extended.shape[2] * np.random.randint(int(test_set_full_duration_ms / x_test_original_extended.shape[2] - temporal_silence_ms))
    end_time = start_time + 1 + x_test_original_extended.shape[2] * num_digits_to_display + temporal_silence_ms
    output_spikes_test_after_learning = output_spikes_test_after_learning_full_FF[start_time:end_time]

min_time_ms = 0
max_time_ms = end_time - start_time

time_sec_mnist = np.arange(min_time_ms, max_time_ms) / 1000
min_time_sec_mnist = min_time_ms / 1000
max_time_sec_mnist = max_time_ms / 1000

# input digits
x_test_input_digits = x_test_axons_input_spikes[:,start_time:end_time]

syn_activation_time_1, syn_activation_index_1 = np.nonzero(axons_input_spikes_test[-x_test_axons_input_spikes.shape[0]:,start_time:end_time].T)
syn_activation_time_1 = syn_activation_time_1 / 1000
syn_activation_index_1 = x_test_axons_input_spikes.shape[0] - syn_activation_index_1

syn_activation_time_2 = syn_activation_time_1 + (time_delays_list_IF[0] / 1000)
syn_activation_time_3 = syn_activation_time_1 + (time_delays_list_IF[1] / 1000)
syn_activation_index_2 = syn_activation_index_1 + num_axons_FF
syn_activation_index_3 = syn_activation_index_2 + num_axons_FF


# output after learning
output_spikes_test_after_learning_FF = output_spikes_test_after_learning_full_FF[start_time:end_time]
output_spikes_test_after_learning_IF = output_spikes_test_after_learning_full_IF[start_time:end_time]

# desired output
output_kernel_test = np.zeros((X_test_spikes[0].shape[1],))
output_kernel_test[-1] = 1
desired_output_spikes_test_full = np.kron(Y_test_spikes[:num_test_patterns], output_kernel_test)
desired_output_spikes_test_mnist = desired_output_spikes_test_full[start_time:end_time]


# build figure
ax_digits.imshow(x_test_input_digits, cmap='gray');
ax_digits.set_xticks([])
ax_digits.set_yticks([])
ax_digits.spines['top'].set_visible(False)
ax_digits.spines['bottom'].set_visible(False)
ax_digits.spines['left'].set_visible(False)
ax_digits.spines['right'].set_visible(False)

ax_axons_mnist.scatter(syn_activation_time_1, syn_activation_index_1, s=8, c='black');
ax_axons_mnist.scatter(syn_activation_time_2, syn_activation_index_2, s=8, c='chocolate');
ax_axons_mnist.scatter(syn_activation_time_3, syn_activation_index_3, s=8, c='purple');
ax_axons_mnist.set_xlim(min_time_sec_mnist, max_time_sec_mnist);
ax_axons_mnist.set_xticks([])
ax_axons_mnist.set_yticks([])
ax_axons_mnist.spines['top'].set_visible(False)
ax_axons_mnist.spines['bottom'].set_visible(False)
ax_axons_mnist.spines['left'].set_visible(False)
ax_axons_mnist.spines['right'].set_visible(False)


ax_learning_outcomes.plot(time_sec_mnist, 2.2 + output_spikes_test_after_learning_FF, c=FF_color, lw=2.5);
ax_learning_outcomes.plot(time_sec_mnist, 1.1 + output_spikes_test_after_learning_IF, c=IF_color, lw=2.5);
ax_learning_outcomes.plot(time_sec_mnist, 0.0 + desired_output_spikes_test_mnist, c=target_color, lw=2.5);

ax_learning_outcomes.set_xlim(min_time_sec_mnist, max_time_sec_mnist);
ax_learning_outcomes.set_xticks([])
ax_learning_outcomes.set_yticks([])
ax_learning_outcomes.spines['top'].set_visible(False)
ax_learning_outcomes.spines['bottom'].set_visible(False)
ax_learning_outcomes.spines['left'].set_visible(False)
ax_learning_outcomes.spines['right'].set_visible(False)

ax_learning_outcomes.text(0.02,2.5, 'F&F (M=5)', color=FF_color, fontsize=20)
ax_learning_outcomes.text(0.02,1.4, 'I&F (Orig Axons + 2 delayed)', color=IF_color, fontsize=20)
ax_learning_outcomes.text(0.02,0.3, 'Desired Output', color=target_color, fontsize=20)


# save figure
if save_figures:
    figure_name = 'F&F_hardware_saving_Figure_5_%d' %(np.random.randint(200))
    for file_ending in all_file_endings_to_use:
        if file_ending == '.png':
            fig.savefig(figure_folder + figure_name + file_ending, bbox_inches='tight')
        else:
            fig.savefig(figure_folder + figure_name + file_ending, bbox_inches='tight')


================================================================================
================================================================================
MNIST_classification_LR_IF_FF_interactions.py:
==============================================
import os
import sys
import numpy as np
import time
import pickle
import matplotlib.pyplot as plt
from scipy import signal
from sklearn import linear_model
from sklearn.metrics import roc_curve, roc_auc_score
from tensorflow import keras

#%% script params

start_time = time.time()

try:
    print('----------------------------')
    print('----------------------------')
    random_seed = int(sys.argv[1])
    positive_digit = int(sys.argv[2])
    connections_per_axon = int(sys.argv[3])
    temporal_extent_factor_numerator = int(sys.argv[4])
    temporal_extent_factor_denumerator = int(sys.argv[5])
    release_probability = int(sys.argv[6])
    num_train_positive_patterns = int(sys.argv[7])
    print('"random_seed" selected by user - %d' %(random_seed))
    print('"positive_digit" selected by user - %d' %(positive_digit))
    print('"connections_per_axon" selected by user - %d' %(connections_per_axon))
    print('"temporal_extent_factor_numerator" selected by user - %d' %(temporal_extent_factor_numerator))
    print('"temporal_extent_factor_denumerator" selected by user - %d' %(temporal_extent_factor_denumerator))
    print('"release_probability" selected by user - %d' %(release_probability))
    print('"num_train_positive_patterns" selected by user - %d' %(num_train_positive_patterns))

    determine_internally = False
except:
    determine_internally = True
    try:
        random_seed = int(sys.argv[1])
        print('random seed selected by user - %d' %(random_seed))
    except:
        random_seed = np.random.randint(100000)
        print('randomly choose seed - %d' %(random_seed))

np.random.seed(random_seed)
print('----------------------------')
print('----------------------------')


if determine_internally:
    positive_digit = np.random.randint(10)
    connections_per_axon = np.random.choice([1,2,3,5,10], size=1)[0]
    temporal_extent_factor_numerator = np.random.choice([1,2,3,4,5], size=1)[0]
    temporal_extent_factor_denumerator = np.random.choice([1,2], size=1)[0]
    release_probability = np.random.choice([0.25, 0.5,0.5,0.5, 0.75, 1.0,1.0,1.0], size=1)[0]
    num_train_positive_patterns = np.random.choice([16,32,64,128,256,512,1024,2048,4096,5000], size=1)[0]

# interactions set to False
use_interaction_terms = False
interactions_degree  = 2

spatial_extent_factor = 5
num_const_firing_channels = 20
temporal_silence_ms = 70
#num_train_positive_patterns = 7000
num_train_negative_patterns_mult_factor = 5
spike_safety_range_ms = 20
negative_subsampling_fraction = 0.2

# release probability related params
# release_probability = 1.0
# release_probability = 0.5
train_epochs = 15
test_epochs  = 3

# what to consider as good prediction
output_spike_tolorance_window_duration = 30
output_spike_tolorance_window_offset   = 10

FF_weight_mult_factors_list = [0.01,0.03,0.07,0.1,0.3,0.5,0.8,1,1.3,2,3,4,5,7,10,25,50,120,250]
IF_weight_mult_factors_list = [0.01,0.03,0.07,0.1,0.3,0.5,0.8,1,1.3,2,3,4,5,7,10,25,50,120,250,1000,10000]

if use_interaction_terms is False:
    non_interaction_fraction_FF = 1.0
    non_interaction_fraction_IF = 1.0

# setting to create a 1 spike out or a 3 spike burst as supervising signal
create_output_burst = False
# create_output_burst = True

# setting for quick learning
#quick_test = True
quick_test = False

if quick_test:
    num_train_positive_patterns = 500
    num_train_negative_patterns_mult_factor = 2
    negative_subsampling_fraction = 0.1

    FF_weight_mult_factors_list = [0.25, 1, 2, 4, 8, 16, 64]
    IF_weight_mult_factors_list = [0.25, 1, 2, 4, 8, 16, 64]


show_plots = True
show_plots = False

data_folder = '/filter_and_fire_neuron/results_data_mnist/'

experiment_results_dict = {}
experiment_results_dict['script_main_params'] = {}
experiment_results_dict['script_main_params']['positive_digit'] = positive_digit
experiment_results_dict['script_main_params']['connections_per_axon'] = connections_per_axon
experiment_results_dict['script_main_params']['random_seed'] = random_seed
experiment_results_dict['script_main_params']['interactions_degree'] = interactions_degree

experiment_results_dict['script_main_params']['temporal_extent_factor_numerator'] = temporal_extent_factor_numerator
experiment_results_dict['script_main_params']['temporal_extent_factor_denumerator'] = temporal_extent_factor_denumerator
experiment_results_dict['script_main_params']['spatial_extent_factor'] = spatial_extent_factor
experiment_results_dict['script_main_params']['num_const_firing_channels'] = num_const_firing_channels
experiment_results_dict['script_main_params']['temporal_silence_ms'] = temporal_silence_ms

experiment_results_dict['script_main_params']['num_train_positive_patterns'] = num_train_positive_patterns
experiment_results_dict['script_main_params']['num_train_negative_patterns_mult_factor'] = num_train_negative_patterns_mult_factor
experiment_results_dict['script_main_params']['spike_safety_range_ms'] = spike_safety_range_ms
experiment_results_dict['script_main_params']['negative_subsampling_fraction'] = negative_subsampling_fraction

experiment_results_dict['script_main_params']['release_probability'] = release_probability
experiment_results_dict['script_main_params']['train_epochs'] = train_epochs
experiment_results_dict['script_main_params']['test_epochs'] = test_epochs
experiment_results_dict['script_main_params']['create_output_burst'] = create_output_burst

experiment_results_dict['script_main_params']['output_spike_tolorance_window_duration'] = output_spike_tolorance_window_duration
experiment_results_dict['script_main_params']['output_spike_tolorance_window_offset']   = output_spike_tolorance_window_offset
experiment_results_dict['script_main_params']['FF_weight_mult_factors_list'] = FF_weight_mult_factors_list
experiment_results_dict['script_main_params']['IF_weight_mult_factors_list'] = IF_weight_mult_factors_list


#%% helper functions


def create_single_PSP_profile(tau_rise, tau_decay, temporal_filter_length=50):

    if tau_rise >= tau_decay:
        tau_decay = tau_rise + 5

    exp_r = signal.exponential(M=temporal_filter_length, center=0, tau=tau_rise , sym=False)
    exp_d = signal.exponential(M=temporal_filter_length, center=0, tau=tau_decay, sym=False)

    post_syn_potential = exp_d - exp_r
    post_syn_potential /= post_syn_potential.max()

    return post_syn_potential


def construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec):

    num_synapses = tau_rise_vec.shape[0]
    temporal_filter_length = int(4 * tau_decay_vec.max()) + 1

    syn_filter = np.zeros((num_synapses, temporal_filter_length))

    for k, (tau_r, tau_d) in enumerate(zip(tau_rise_vec, tau_decay_vec)):
        syn_filter[k,:] = create_single_PSP_profile(tau_r, tau_d, temporal_filter_length=temporal_filter_length)

    return syn_filter


def simulate_filter_and_fire_cell_with_interactions(presynaptic_input_spikes, interactions_map, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                                    refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):

    temporal_filter_length = int(5 * refreactory_time_constant) + 1
    refreactory_filter = signal.exponential(M=temporal_filter_length,center=0,tau=refreactory_time_constant,sym=False)[np.newaxis,:]

    # padd input and get all synaptic filters
    normlized_syn_filter = np.flipud(construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec))
    padded_input = np.hstack((np.zeros(normlized_syn_filter.shape), presynaptic_input_spikes))

    # calc local currents
    local_normlized_currents = np.zeros(presynaptic_input_spikes.shape, dtype=np.float16)
    for k in range(normlized_syn_filter.shape[0]):
        local_normlized_currents[k] = signal.convolve(padded_input[k], normlized_syn_filter[k], mode='valid')[1:]

    # apply interactions
    local_normlized_currents = apply_dendritic_interactions(local_normlized_currents, interactions_map)

    # multiply by weights to get the somatic current
    soma_current = signal.convolve(local_normlized_currents, synaptic_weights, mode='valid')

    # simulate the cell
    soma_voltage = v_reset + current_to_voltage_mult_factor * soma_current.ravel()
    output_spike_times_in_ms = []
    # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
    for t in range(len(soma_voltage)):
        # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
        if (soma_voltage[t] > v_threshold) and ((t + 1) < len(soma_voltage)):
            t_start = t + 1
            t_end = min(len(soma_voltage), t_start + temporal_filter_length)
            soma_voltage[t_start:t_end] -= (soma_voltage[t + 1] - v_reset) * refreactory_filter.ravel()[:(t_end - t_start)]
            output_spike_times_in_ms.append(t)

    return local_normlized_currents, soma_voltage, output_spike_times_in_ms


def simulate_filter_and_fire_cell_with_interactions_long(presynaptic_input_spikes, interactions_map, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                                         refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):

    total_duration_ms = presynaptic_input_spikes.shape[1]
    max_duration_per_call_ms = 50000
    overlap_time_ms = 500

    if max_duration_per_call_ms >= total_duration_ms:
        local_normlized_currents, soma_voltage, output_spike_times_in_ms = simulate_filter_and_fire_cell_with_interactions(presynaptic_input_spikes, interactions_map, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                                                                                                           refreactory_time_constant=refreactory_time_constant, v_reset=v_reset, v_threshold=v_threshold,
                                                                                                                           current_to_voltage_mult_factor=current_to_voltage_mult_factor)
        return local_normlized_currents, soma_voltage, output_spike_times_in_ms


    local_normlized_currents = np.zeros(presynaptic_input_spikes.shape, dtype=np.float16)
    soma_voltage = np.zeros((total_duration_ms,))
    output_spike_times_in_ms = []

    num_sub_calls = int(np.ceil(total_duration_ms / (max_duration_per_call_ms - overlap_time_ms)))
    end_ind = overlap_time_ms
    for k in range(num_sub_calls):
        start_ind = end_ind - overlap_time_ms
        end_ind = min(start_ind + max_duration_per_call_ms, total_duration_ms)

        curr_loc_norm_c, curr_soma_v, curr_out_sp_t = simulate_filter_and_fire_cell_with_interactions(presynaptic_input_spikes[:,start_ind:end_ind], interactions_map, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                                                                                      refreactory_time_constant=refreactory_time_constant, v_reset=v_reset, v_threshold=v_threshold,
                                                                                                      current_to_voltage_mult_factor=current_to_voltage_mult_factor)

        # update fields
        if k == 0:
            local_normlized_currents[:,start_ind:end_ind] = curr_loc_norm_c
            soma_voltage[start_ind:end_ind] = curr_soma_v
            output_spike_times_in_ms += curr_out_sp_t
        else:
            local_normlized_currents[:,(start_ind + overlap_time_ms):end_ind] = curr_loc_norm_c[:,overlap_time_ms:end_ind]
            soma_voltage[(start_ind + overlap_time_ms):end_ind] = curr_soma_v[overlap_time_ms:]
            curr_out_sp_t = [x for x in curr_out_sp_t if x >= (overlap_time_ms - 1)]
            output_spike_times_in_ms += [(start_ind + x) for x in curr_out_sp_t]

    return local_normlized_currents, soma_voltage, output_spike_times_in_ms


# use local currents as "features" and fit a linear model to the data
def prepare_training_dataset(local_normlized_currents, desired_output_spikes, spike_safety_range_ms=10, negative_subsampling_fraction=0.1):

    # remove all "negative" time points that are too close to spikes
    desired_output_spikes_LPF = signal.convolve(desired_output_spikes, np.ones((spike_safety_range_ms,)), mode='same') > 0.1
    desired_timepoints = ~desired_output_spikes_LPF

    # massivly subsample the remaining timepoints
    desired_timepoints[np.random.rand(desired_timepoints.shape[0]) > negative_subsampling_fraction] = 0
    desired_timepoints[desired_output_spikes > 0.1] = 1

    X = local_normlized_currents.T[desired_timepoints,:]
    y = desired_output_spikes[desired_timepoints]

    return X, y


def generate_dendritic_interactions_map(num_synapses, interactions_degree=2, non_interaction_fraction=0.2):

    interactions_map = {}
    interactions_map['degree_permutations'] = {}

    for degree in range(interactions_degree - 1):
        interactions_map['degree_permutations'][degree] = np.random.permutation(num_synapses)
    interactions_map['non_interacting_indices'] = np.random.permutation(num_synapses)[:int(num_synapses * non_interaction_fraction)]

    return interactions_map


def apply_dendritic_interactions(normlized_synaptic_currents, interactions_map):
    output_normlized_synaptic_currents = normlized_synaptic_currents.copy()

    # apply d times random interactions
    for degree in range(interactions_degree - 1):
        output_normlized_synaptic_currents = output_normlized_synaptic_currents * normlized_synaptic_currents[interactions_map['degree_permutations'][degree]]

    # keep some fraction of only individual interactions
    output_normlized_synaptic_currents[interactions_map['non_interacting_indices']] = normlized_synaptic_currents[interactions_map['non_interacting_indices']]

    return output_normlized_synaptic_currents


#%% Load MNIST dataset and show the data

(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

if show_plots:
    num_rows = 5
    num_cols = 7

    plt.figure(figsize=(20,15))
    for k in range(num_rows * num_cols):
        rand_sample_ind = np.random.randint(x_train.shape[0])
        plt.subplot(num_rows, num_cols, k + 1)
        plt.imshow(x_train[k]); plt.title('digit "%s"' %(y_train[k]))

#%% display mean and std images, as well as histograms

mean_image = x_train.mean(axis=0)
std_image  = x_train.std(axis=0)

if show_plots:
    plt.figure(figsize=(21,14))
    plt.subplot(2,3,1); plt.imshow(mean_image); plt.title('mean image')
    plt.subplot(2,3,2); plt.bar(np.arange(mean_image.shape[0]), mean_image.sum(axis=0)); plt.title('"temporal" (columns) histogram (mean image)')
    plt.subplot(2,3,3); plt.bar(np.arange(mean_image.shape[0]), mean_image.sum(axis=1)); plt.title('"spatial" (rows) histogram (mean image)')

    plt.subplot(2,3,4); plt.imshow(std_image); plt.title('std image')
    plt.subplot(2,3,5); plt.bar(np.arange(std_image.shape[0]), std_image.sum(axis=0)); plt.title('"temporal" (columns) histogram (std image)')
    plt.subplot(2,3,6); plt.bar(np.arange(std_image.shape[0]), std_image.sum(axis=1)); plt.title('"spatial" (rows) histogram (std image)')


#%% Crop the data and binarize it

h_crop_range = [4,24]
w_crop_range = [4,24]

positive_threshold = 150

x_train = x_train[:,h_crop_range[0]:h_crop_range[1],w_crop_range[0]:w_crop_range[1]] > positive_threshold
x_test  = x_test[: ,h_crop_range[0]:h_crop_range[1],w_crop_range[0]:w_crop_range[1]] > positive_threshold

if show_plots:
    num_rows = 5
    num_cols = 7

    plt.figure(figsize=(20,15))
    for k in range(num_rows * num_cols):
        rand_sample_ind = np.random.randint(x_train.shape[0])
        plt.subplot(num_rows, num_cols, k + 1)
        plt.imshow(x_train[k]); plt.title('digit "%s"' %(y_train[k]))

experiment_results_dict['script_main_params']['digit_sample_image_shape_cropped'] = x_train[0].shape

#%% Transform Xs to spatio-temporal spike trains

# extend according to "spatial_extent_factor" and "temporal_extent_factor"
kernel = np.ones((1, spatial_extent_factor, temporal_extent_factor_numerator), dtype=bool)

# reshape X according to what is needed
x_train = np.kron(x_train, kernel)
x_test  = np.kron(x_test , kernel)

# subsample according to "temporal_extent_factor_denumerator"
x_train = x_train[:,:,::temporal_extent_factor_denumerator]
x_test  = x_test[:,:, ::temporal_extent_factor_denumerator]

experiment_results_dict['script_main_params']['digit_sample_image_shape_expanded'] = x_train[0].shape

# padd with ones on top (for "bias" learning)
top_pad_train = np.ones((1, num_const_firing_channels, x_train.shape[2]), dtype=bool)
top_pad_test  = np.ones((1, num_const_firing_channels, x_test.shape[2] ), dtype=bool)

# add a few zero rows for clear seperation for visualization purpuses
top_pad_train[:,-5:,:] = 0
top_pad_test[:,-5:,:] = 0

x_train = np.concatenate((np.tile(top_pad_train, [x_train.shape[0],1,1]), x_train), axis=1)
x_test  = np.concatenate((np.tile(top_pad_test , [x_test.shape[0],1,1] ), x_test ), axis=1)

# pad with "temporal_silence_ms" zeros in the begining of each pattern (for silence between patterns)
left_pad_train = np.zeros((1, x_train.shape[1], temporal_silence_ms), dtype=bool)
left_pad_test  = np.zeros((1, x_test.shape[1] , temporal_silence_ms), dtype=bool)

x_train = np.concatenate((np.tile(left_pad_train, [x_train.shape[0],1,1]), x_train), axis=2)
x_test  = np.concatenate((np.tile(left_pad_test , [x_test.shape[0],1,1] ), x_test ), axis=2)

# add background activity
desired_background_activity_firing_rate_Hz = 10
background_activity_fraction = desired_background_activity_firing_rate_Hz / 1000

x_train[np.random.rand(x_train.shape[0], x_train.shape[1], x_train.shape[2]) < background_activity_fraction] = 1
x_test[ np.random.rand(x_test.shape[0] , x_test.shape[1] , x_test.shape[2] ) < background_activity_fraction] = 1

# subsample the input spikes
desired_average_input_firing_rate_Hz = 20
actual_mean_firing_rate_Hz = 1000 * x_train.mean()

fraction_of_spikes_to_eliminate = desired_average_input_firing_rate_Hz / actual_mean_firing_rate_Hz

x_train = x_train * (np.random.rand(x_train.shape[0], x_train.shape[1], x_train.shape[2]) < fraction_of_spikes_to_eliminate)
x_test  = x_test  * (np.random.rand(x_test.shape[0] , x_test.shape[1] , x_test.shape[2])  < fraction_of_spikes_to_eliminate)

final_mean_firing_rate_Hz = 1000 * x_train.mean()

# display the patterns
if show_plots:
    num_rows = 5
    num_cols = 7

    plt.figure(figsize=(20,15))
    for k in range(num_rows * num_cols):
        rand_sample_ind = np.random.randint(x_train.shape[0])
        plt.subplot(num_rows, num_cols, k + 1)
        plt.imshow(x_train[k], cmap='gray'); plt.title('digit "%s"' %(y_train[k]))


#%% display distribution of number of spikes per pattern

if show_plots:
    plt.close('all')
    plt.figure(figsize=(12,8))
    plt.hist(x_train.sum(axis=2).sum(axis=1), bins=40); plt.title('distribution of number of spikes per pattern')
    plt.ylabel('number of patterns'); plt.xlabel('number of incoming spikes per pattern')

#%% Create "one-vs-all" dataset

y_train_binary = y_train == positive_digit
y_test_binary  = y_test  == positive_digit

num_train_positive_patterns = min(int(y_train_binary.sum()), num_train_positive_patterns)
num_train_negative_patterns = int(num_train_negative_patterns_mult_factor * num_train_positive_patterns)

positive_inds = np.where(y_train_binary)[0]
negative_inds = np.where(~y_train_binary)[0]

selected_train_positives = np.random.choice(positive_inds, size=num_train_positive_patterns)
selected_train_negatives = np.random.choice(negative_inds, size=num_train_negative_patterns)

all_selected = np.random.permutation(np.concatenate((selected_train_positives, selected_train_negatives)))

X_train_spikes = x_train[all_selected]
Y_train_spikes = y_train_binary[all_selected]

X_test_spikes = x_test.copy()
Y_test_spikes = y_test_binary.copy()

zero_pred_baseline_accuracy = 100 * (1 - Y_test_spikes.mean())

if release_probability < 1.0:
    # replicate train and test by corresponding factors (epochs)
    X_train_spikes = np.tile(X_train_spikes, (train_epochs, 1, 1))
    Y_train_spikes = np.tile(Y_train_spikes, (train_epochs, ))

    X_test_spikes = np.tile(X_test_spikes, (test_epochs, 1, 1))
    Y_test_spikes = np.tile(Y_test_spikes, (test_epochs, ))

    # add synaptic unrelability to all patterns after replication
    rand_matrix = np.random.rand(X_train_spikes.shape[0], X_train_spikes.shape[1], X_train_spikes.shape[2])
    X_train_spikes = X_train_spikes * (rand_matrix < release_probability)

    rand_matrix = np.random.rand(X_test_spikes.shape[0], X_test_spikes.shape[1], X_test_spikes.shape[2])
    X_test_spikes = X_test_spikes * (rand_matrix < release_probability)

experiment_results_dict['script_main_params']['num_train_positive_patterns'] = num_train_positive_patterns
experiment_results_dict['script_main_params']['num_train_negative_patterns_mult_factor'] = num_train_negative_patterns_mult_factor
experiment_results_dict['script_main_params']['release_probability'] = release_probability
experiment_results_dict['script_main_params']['train_epochs'] = train_epochs
experiment_results_dict['script_main_params']['test_epochs']  = test_epochs

#%% Create a regularized logistic regression baseline

logistic_reg_model = linear_model.LogisticRegression(C=0.1, fit_intercept=False, penalty='l2',verbose=False)

# fit model
logistic_reg_model.fit(X_train_spikes.reshape([X_train_spikes.shape[0],-1]), Y_train_spikes)

# predict and calculate AUC on train data
Y_train_spikes_hat = logistic_reg_model.predict_proba(X_train_spikes.reshape([X_train_spikes.shape[0],-1]))[:,1]
Y_test_spikes_hat = logistic_reg_model.predict_proba(X_test_spikes.reshape([X_test_spikes.shape[0],-1]))[:,1]

train_AUC = roc_auc_score(Y_train_spikes, Y_train_spikes_hat)
test_AUC = roc_auc_score(Y_test_spikes, Y_test_spikes_hat)

print('----------------------------')
print('----------------------------')
print('for (# pos = %d, # neg = %d): (train AUC, test AUC) = (%.5f, %.5f)' %(num_train_positive_patterns, num_train_negative_patterns, train_AUC, test_AUC))
print('----------------------------')

logistic_regression_learned_weights = logistic_reg_model.coef_.reshape([X_train_spikes.shape[1], X_train_spikes.shape[2]])

if show_plots:
    plt.figure(figsize=(8,8))
    plt.imshow(logistic_regression_learned_weights)
    plt.title('Learned Weights \n (spatio-temporal ("image") logistic regression)')

experiment_results_dict['learned_weights_LR'] = logistic_regression_learned_weights

#%% Calculate and Display LogReg Accuracy

LL_false_positive_list, LL_true_positive_list, LL_thresholds_list = roc_curve(Y_test_spikes, Y_test_spikes_hat)

num_pos_class = int((Y_test_spikes == True).sum())
num_neg_class = int((Y_test_spikes == False).sum())

tp = LL_true_positive_list * num_pos_class
tn = (1 - LL_false_positive_list) * num_neg_class
LL_accuracy_list = (tp + tn) / (num_pos_class + num_neg_class)

LL_false_positive_list = LL_false_positive_list[LL_false_positive_list < 0.05]
LL_true_positive_list = LL_true_positive_list[:len(LL_false_positive_list)]
LL_thresholds_list = LL_thresholds_list[:len(LL_false_positive_list)]
LL_accuracy_list = LL_accuracy_list[:len(LL_false_positive_list)]

LL_false_positive_list = 100 * LL_false_positive_list
LL_true_positive_list = 100 * LL_true_positive_list
LL_accuracy_list = 100 * LL_accuracy_list

LL_accuracy_max = LL_accuracy_list.max()

LL_accuracy_subsampled = LL_accuracy_list[30::30]
LL_thresholds_subsampled = LL_thresholds_list[30::30]

acc_bar_x_axis = range(LL_accuracy_subsampled.shape[0])

if show_plots:
    plt.figure(figsize=(15,8))
    plt.subplot(1,2,1); plt.bar(x=acc_bar_x_axis,height=LL_accuracy_subsampled)
    plt.xticks(acc_bar_x_axis, LL_thresholds_subsampled, rotation='vertical')
    plt.title('max accuracy = %.2f%s' %(LL_accuracy_max,'%'), fontsize=24)
    plt.ylim(87.8,100); plt.xlabel('threshold', fontsize=20); plt.ylabel('Accuracy (%)', fontsize=20)
    plt.plot([acc_bar_x_axis[0] - 1, acc_bar_x_axis[-1] + 1], [zero_pred_baseline_accuracy, zero_pred_baseline_accuracy], color='r')

    plt.subplot(1,2,2); plt.plot(LL_false_positive_list, LL_true_positive_list)
    plt.ylabel('True Positive (%)', fontsize=20); plt.xlabel('False Positive (%)', fontsize=20)

experiment_results_dict['model_accuracy_LR'] = LL_accuracy_max

#%% Fit a F&F model

# main parameters
# connections_per_axon = 5
model_type = 'F&F'
#model_type = 'I&F'

# neuron model parameters
num_axons = X_train_spikes[0].shape[0]
num_synapses = connections_per_axon * num_axons

v_reset     = -80
v_threshold = -55
current_to_voltage_mult_factor = 3
refreactory_time_constant = 15

# synapse non-learnable parameters
if model_type == 'F&F':
    tau_rise_range  = [1, 18]
    tau_decay_range = [8, 30]
    # tau_decay_range = [8,48]
elif model_type == 'I&F':
    tau_rise_range  = [ 1, 1]
    tau_decay_range = [30,30]

tau_rise_vec  = np.random.uniform(low=tau_rise_range[0] , high=tau_rise_range[1] , size=(num_synapses, 1))
tau_decay_vec = np.random.uniform(low=tau_decay_range[0], high=tau_decay_range[1], size=(num_synapses, 1))

experiment_results_dict['tau_rise_vec_FF']  = tau_rise_vec
experiment_results_dict['tau_decay_vec_FF'] = tau_decay_vec

# synapse learnable parameters
synaptic_weights_vec = np.random.normal(size=(num_synapses, 1))

# prepare input spikes
axons_input_spikes = np.concatenate([X_train_spikes[k] for k in range(X_train_spikes.shape[0])], axis=1)

# prepare output spikes
pattern_duration_ms = X_train_spikes[0].shape[1]
output_kernel = np.zeros((pattern_duration_ms,))
output_spike_offset = 1
output_kernel[-output_spike_offset] = 1

if create_output_burst:
    output_spike_offset = 6
    output_kernel[-output_spike_offset] = 1
    output_spike_offset = 11
    output_kernel[-output_spike_offset] = 1

desired_output_spikes = np.kron(Y_train_spikes, output_kernel)

if show_plots:
    plt.figure(figsize=(30,15))
    plt.imshow(axons_input_spikes[:,:1101], cmap='gray')
    plt.title('input axons raster', fontsize=22)
    plt.ylabel('axon index', fontsize=22)
    plt.xlabel('time [ms]', fontsize=22)

    plt.figure(figsize=(30,1))
    plt.plot(desired_output_spikes[:1101]); plt.xlim(0,1101)
    plt.ylabel('output spike', fontsize=22)
    plt.xlabel('time [ms]', fontsize=22)

#%%

presynaptic_input_spikes = np.kron(np.ones((connections_per_axon, 1), dtype=bool), axons_input_spikes).astype(bool)

if use_interaction_terms:
    non_interaction_fraction_FF = min(1.0, 2.5 * num_axons / num_synapses)
    non_interaction_fraction_FF = 0.40
interactions_map_FF = generate_dendritic_interactions_map(num_synapses, interactions_degree=interactions_degree, non_interaction_fraction=non_interaction_fraction_FF)

experiment_results_dict['script_main_params']['non_interaction_fraction_FF'] = non_interaction_fraction_FF
experiment_results_dict['interactions_map_FF'] = interactions_map_FF

# simulate cell with normlized currents
local_normlized_currents, soma_voltage, output_spike_times_in_ms = simulate_filter_and_fire_cell_with_interactions_long(presynaptic_input_spikes, interactions_map_FF,
                                                                                                                        synaptic_weights_vec, tau_rise_vec, tau_decay_vec,
                                                                                                                        refreactory_time_constant=refreactory_time_constant,
                                                                                                                        v_reset=v_reset, v_threshold=v_threshold,
                                                                                                                        current_to_voltage_mult_factor=current_to_voltage_mult_factor)


# fit linear model to local currents
filter_and_fire_model = linear_model.LogisticRegression(C=100000, fit_intercept=False, penalty='l2')

# spike_safety_range_ms = 20
# negative_subsampling_fraction = 0.25

X, y = prepare_training_dataset(local_normlized_currents, desired_output_spikes,
                                spike_safety_range_ms=spike_safety_range_ms,
                                negative_subsampling_fraction=negative_subsampling_fraction)

print('----------------------------')
print('number of data points = %d (%.2f%s positive class)' %(X.shape[0], 100 * y.mean(),'%'))
print('----------------------------')

# fit model
filter_and_fire_model.fit(X, y)

# calculate train AUC
y_hat = filter_and_fire_model.predict_proba(X)[:,1]
train_AUC = roc_auc_score(y, y_hat)

print('F&F train AUC = %.5f' %(train_AUC))

if show_plots:
    # display some training data predictions
    num_timepoints_to_show = 10000
    fitted_output_spike_prob = filter_and_fire_model.predict_proba(local_normlized_currents[:,:num_timepoints_to_show].T)[:,1]

    plt.figure(figsize=(30,10))
    plt.plot(1.05 * desired_output_spikes[:num_timepoints_to_show] - 0.025); plt.title('train AUC = %.5f' %(train_AUC), fontsize=22)
    plt.plot(fitted_output_spike_prob[:num_timepoints_to_show]); plt.xlabel('time [ms]'); plt.legend(['GT', 'prediction'], fontsize=22)

#%% display learned weights
normlized_syn_filter = construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec)

# collect learned synaptic weights
FF_learned_synaptic_weights = np.fliplr(filter_and_fire_model.coef_).T
weighted_syn_filter = FF_learned_synaptic_weights * normlized_syn_filter

axon_spatio_temporal_pattern = np.zeros((num_axons, weighted_syn_filter.shape[1]))
for k in range(num_axons):
    axon_spatio_temporal_pattern[k] = weighted_syn_filter[k::num_axons].sum(axis=0)

axon_spatio_temporal_pattern_short = axon_spatio_temporal_pattern[:,:X_train_spikes.shape[2]]

if show_plots:
    plt.figure(figsize=(18,8))
    plt.subplot(1,2,1); plt.imshow(logistic_reg_model.coef_.reshape([X_train_spikes.shape[1], X_train_spikes.shape[2]])); plt.title('logistic regression', fontsize=20)
    plt.subplot(1,2,2); plt.imshow(np.flip(axon_spatio_temporal_pattern_short)); plt.title('filter and fire neuron', fontsize=20)

experiment_results_dict['learned_weights_FF'] = np.flip(axon_spatio_temporal_pattern_short)

#%% Make a prediction on the entire test trace

num_test_patterns = X_test_spikes.shape[0]

# prepare test outputs
# output_spike_tolorance_window_duration = 20
# output_spike_tolorance_window_offset   = 5
output_kernel_test = np.zeros((X_test_spikes[0].shape[1],))
output_kernel_test[-output_spike_tolorance_window_duration:] = 1

desired_output_spikes_test = np.kron(Y_test_spikes[:num_test_patterns], output_kernel_test)
desired_output_spikes_test = np.concatenate((np.zeros((output_spike_tolorance_window_offset,)), desired_output_spikes_test[:-output_spike_tolorance_window_offset]))

# prepare test inputs
axons_input_spikes_test = np.concatenate([X_test_spikes[k] for k in range(num_test_patterns)],axis=1)
presynaptic_input_spikes_test = np.kron(np.ones((connections_per_axon, 1), dtype=bool), axons_input_spikes_test).astype(bool)

# FF_weight_mult_factors_list = [x for x in [2,3,4,5,6,9,20,50,120,250]]
FF_accuracy_list = []
FF_true_positive_list = []
FF_false_positive_list = []
for weight_mult_factor in FF_weight_mult_factors_list:

    # collect learned synaptic weights
    synaptic_weights_post_learning = weight_mult_factor * FF_learned_synaptic_weights

    _, soma_voltage_test, output_spike_times_in_ms_test = simulate_filter_and_fire_cell_with_interactions_long(presynaptic_input_spikes_test, interactions_map_FF,
                                                                                                               synaptic_weights_post_learning, tau_rise_vec, tau_decay_vec,
                                                                                                               refreactory_time_constant=refreactory_time_constant,
                                                                                                               v_reset=v_reset, v_threshold=v_threshold,
                                                                                                               current_to_voltage_mult_factor=current_to_voltage_mult_factor)


    output_spikes_test = np.zeros(soma_voltage_test.shape)
    try:
        output_spikes_test[np.array(output_spike_times_in_ms_test)] = 1.0
    except:
        print('no output spikes created')


    # calculate test accuracy
    compact_desired_output_test = Y_test_spikes[:num_test_patterns]

    compact_desired_output_test2 = np.zeros(compact_desired_output_test.shape, dtype=bool)
    compact_predicted_output_test = np.zeros(compact_desired_output_test.shape, dtype=bool)

    # go over all patterns and extract the prediction (depends if the spikes are in the desired window)
    for pattern_ind in range(num_test_patterns):
        start_ind = pattern_duration_ms * pattern_ind + output_spike_tolorance_window_offset
        end_ind = start_ind + pattern_duration_ms

        # extract prediction
        predicted_spike_train_for_pattern = output_spikes_test[start_ind:end_ind]
        desired_spike_train_for_pattern = desired_output_spikes_test[start_ind:end_ind]

        compact_desired_output_test2[pattern_ind]  = desired_spike_train_for_pattern.sum() > 0.1

        if Y_test_spikes[pattern_ind] == 1:
            # check if there is a spike in the desired window only
            compact_predicted_output_test[pattern_ind] = (desired_spike_train_for_pattern * predicted_spike_train_for_pattern).sum() > 0.1
        else:
            # check if there is any spike in the full pattern duration
            compact_predicted_output_test[pattern_ind] = predicted_spike_train_for_pattern.sum() > 0.1

    # small verificaiton
    assert((compact_desired_output_test == compact_desired_output_test2).sum() == num_test_patterns)

    # display accuracy
    percent_accuracy = 100 * (compact_desired_output_test == compact_predicted_output_test).mean()
    true_positive    = 100 * (np.logical_and(compact_desired_output_test == True , compact_predicted_output_test == True).sum() / (compact_desired_output_test == True).sum())
    false_positive   = 100 * (np.logical_and(compact_desired_output_test == False, compact_predicted_output_test == True).sum() / (compact_desired_output_test == False).sum())

    print('F&F: weights mult factor = %.1f: Accuracy = %.3f%s. (TP, FP) = (%.3f%s, %.3f%s)' %(weight_mult_factor, percent_accuracy,'%',true_positive,'%',false_positive,'%'))

    FF_accuracy_list.append(percent_accuracy)
    FF_true_positive_list.append(true_positive)
    FF_false_positive_list.append(false_positive)

experiment_results_dict['model_accuracy_FF'] = np.array(FF_accuracy_list).max()
experiment_results_dict['model_accuracy_baseline'] = zero_pred_baseline_accuracy

#%% Display accuracy results for F&F model

if show_plots:
    plt.figure(figsize=(15,8))
    plt.subplot(1,2,1); plt.bar(x=range(len(FF_accuracy_list)),height=FF_accuracy_list)
    plt.xticks(range(len(FF_accuracy_list)), FF_weight_mult_factors_list); plt.title('max accuracy = %.2f%s' %(np.array(FF_accuracy_list).max(),'%'), fontsize=24)
    plt.ylim(87.8,100); plt.xlabel('weight mult factor ("gain")', fontsize=20); plt.ylabel('Accuracy (%)', fontsize=20)
    plt.plot([-1, len(FF_accuracy_list)], [zero_pred_baseline_accuracy, zero_pred_baseline_accuracy], color='r')

    plt.subplot(1,2,2); plt.plot(FF_false_positive_list, FF_true_positive_list)
    plt.ylabel('True Positive (%)', fontsize=20); plt.xlabel('False Positive (%)', fontsize=20)

    plt.figure(figsize=(30,6))
    plt.plot(1.05 * desired_output_spikes_test[:25000] - 0.025)
    plt.plot(output_spikes_test[:25000])
    plt.legend(['desired outputs', 'actual outputs'])

    plt.figure(figsize=(30,6))
    plt.plot(1.05 * desired_output_spikes_test[2900:4600] - 0.025)
    plt.plot(output_spikes_test[2900:4600])
    plt.legend(['desired outputs', 'actual outputs'])

    plt.figure(figsize=(30,6))
    plt.plot(1.05 * desired_output_spikes_test[8500:9500] - 0.025)
    plt.plot(output_spikes_test[8500:9500])
    plt.legend(['desired outputs', 'actual outputs'])

#%% F&F model predictions

if show_plots:
    start_time = 100 * np.random.randint(int(axons_input_spikes_test.shape[1] / 100 - 20))
    end_time = start_time + 1 + 100 * 18

    plt.figure(figsize=(30,15))
    plt.subplot(3,1,1); plt.imshow(axons_input_spikes_test[:,start_time:end_time], cmap='gray'); plt.title('input axons raster (test set)', fontsize=22)
    plt.subplot(3,1,2); plt.plot(output_spikes_test[start_time:end_time]); plt.xlim(0,1 + 100 * 18); plt.ylabel('output spike', fontsize=22)
    plt.subplot(3,1,3); plt.plot(soma_voltage_test[start_time:end_time]); plt.xlim(0,1 + 100 * 18); plt.ylabel('soma voltage [mV]', fontsize=22)
    plt.xlabel('time [ms]', fontsize=22)

#%% Fit an I&F model

# main parameters
# connections_per_axon = 5
#model_type = 'F&F'
model_type = 'I&F'

# neuron model parameters
num_axons = X_train_spikes[0].shape[0]
num_synapses = connections_per_axon * num_axons

v_reset     = -80
v_threshold = -55
current_to_voltage_mult_factor = 3
refreactory_time_constant = 15

# synapse non-learnable parameters
if model_type == 'F&F':
    tau_rise_range  = [1, 18]
    tau_decay_range = [8, 30]
elif model_type == 'I&F':
    tau_rise_range  = [ 1, 1]
    tau_decay_range = [30,30]

tau_rise_vec_IF  = np.random.uniform(low=tau_rise_range[0] , high=tau_rise_range[1] , size=(num_synapses, 1))
tau_decay_vec_IF = np.random.uniform(low=tau_decay_range[0], high=tau_decay_range[1], size=(num_synapses, 1))

# synapse learnable parameters
synaptic_weights_vec_IF = np.random.normal(size=(num_synapses, 1))

#%%

if use_interaction_terms:
    non_interaction_fraction_IF = 1.0 * num_axons / num_synapses
    non_interaction_fraction_IF = 0.4
interactions_map_IF = generate_dendritic_interactions_map(num_synapses, interactions_degree=interactions_degree, non_interaction_fraction=non_interaction_fraction_IF)

experiment_results_dict['script_main_params']['non_interaction_fraction_IF'] = non_interaction_fraction_IF
experiment_results_dict['interactions_map_IF'] = interactions_map_IF

# simulate cell with normlized currents
local_normlized_currents_IF, soma_voltage_IF, output_spike_times_in_ms_IF = simulate_filter_and_fire_cell_with_interactions_long(presynaptic_input_spikes, interactions_map_IF,
                                                                                                                                 synaptic_weights_vec_IF, tau_rise_vec_IF, tau_decay_vec_IF,
                                                                                                                                 refreactory_time_constant=refreactory_time_constant,
                                                                                                                                 v_reset=v_reset, v_threshold=v_threshold,
                                                                                                                                 current_to_voltage_mult_factor=current_to_voltage_mult_factor)

# fit linear model to local currents
integrate_and_fire_model = linear_model.LogisticRegression(C=100000, fit_intercept=False, penalty='l2')

# spike_safety_range_ms = 20
# negative_subsampling_fraction = 0.25

X, y = prepare_training_dataset(local_normlized_currents_IF, desired_output_spikes,
                                spike_safety_range_ms=spike_safety_range_ms,
                                negative_subsampling_fraction=negative_subsampling_fraction)

print('----------------------------')
print('number of data points = %d (%.2f%s positive class)' %(X.shape[0], 100 * y.mean(),'%'))
print('----------------------------')

# fit model
integrate_and_fire_model.fit(X, y)

# calculate train AUC
y_hat = integrate_and_fire_model.predict_proba(X)[:,1]
train_AUC = roc_auc_score(y, y_hat)

print('I&F train AUC = %.5f' %(train_AUC))

if show_plots:
    # display some training data predictions
    num_timepoints_to_show = 10000
    fitted_output_spike_prob = integrate_and_fire_model.predict_proba(local_normlized_currents_IF[:,:num_timepoints_to_show].T)[:,1]

    plt.figure(figsize=(30,10))
    plt.plot(1.05 * desired_output_spikes[:num_timepoints_to_show] - 0.025); plt.title('train AUC = %.5f' %(train_AUC), fontsize=22)
    plt.plot(fitted_output_spike_prob[:num_timepoints_to_show]); plt.xlabel('time [ms]'); plt.legend(['GT', 'prediction'], fontsize=22)

#%% display learned weights

# display learned weights
normlized_syn_filter_IF = construct_normlized_synaptic_filter(tau_rise_vec_IF, tau_decay_vec_IF)

# collect learned synaptic weights
IF_learned_synaptic_weights = np.fliplr(integrate_and_fire_model.coef_).T
weighted_syn_filter_IF = IF_learned_synaptic_weights * normlized_syn_filter_IF

axon_spatio_temporal_pattern_IF = np.zeros((num_axons, weighted_syn_filter_IF.shape[1]))
for k in range(num_axons):
    axon_spatio_temporal_pattern_IF[k] = weighted_syn_filter_IF[k::num_axons].sum(axis=0)

axon_spatio_temporal_pattern_short_IF = axon_spatio_temporal_pattern_IF[:,:X_train_spikes.shape[2]]

if show_plots:
    plt.figure(figsize=(24,10))
    plt.subplot(1,3,1); plt.imshow(logistic_reg_model.coef_.reshape([X_train_spikes.shape[1], X_train_spikes.shape[2]])); plt.title('logistic regression', fontsize=24)
    plt.subplot(1,3,2); plt.imshow(np.flip(axon_spatio_temporal_pattern_short)); plt.title('filter and fire neuron', fontsize=24)
    plt.subplot(1,3,3); plt.imshow(np.flip(axon_spatio_temporal_pattern_short_IF)); plt.title('integrate and fire neuron', fontsize=24)

experiment_results_dict['learned_weights_IF'] = np.flip(axon_spatio_temporal_pattern_short_IF)

#%% Display I&F accuracy

# IF_weight_mult_factors_list = [x for x in [10,50,100,1000,10000]]
IF_accuracy_list = []
IF_true_positive_list = []
IF_false_positive_list = []

for weight_mult_factor in IF_weight_mult_factors_list:

    # collect learned synaptic weights
    synaptic_weights_post_learning = weight_mult_factor * IF_learned_synaptic_weights

    _, soma_voltage_test, output_spike_times_in_ms_test = simulate_filter_and_fire_cell_with_interactions_long(presynaptic_input_spikes_test, interactions_map_IF,
                                                                                                               synaptic_weights_post_learning, tau_rise_vec_IF, tau_decay_vec_IF,
                                                                                                               refreactory_time_constant=refreactory_time_constant,
                                                                                                               v_reset=v_reset, v_threshold=v_threshold,
                                                                                                               current_to_voltage_mult_factor=current_to_voltage_mult_factor)


    output_spikes_test = np.zeros(soma_voltage_test.shape)
    try:
        output_spikes_test[np.array(output_spike_times_in_ms_test)] = 1.0
    except:
        print('no output spikes created')


    # calculate test accuracy
    compact_desired_output_test = Y_test_spikes[:num_test_patterns]

    compact_desired_output_test2 = np.zeros(compact_desired_output_test.shape, dtype=bool)
    compact_predicted_output_test = np.zeros(compact_desired_output_test.shape, dtype=bool)

    # go over all patterns and extract the prediction (depends if the spikes are in the desired window)
    for pattern_ind in range(num_test_patterns):
        start_ind = pattern_duration_ms * pattern_ind + output_spike_tolorance_window_offset
        end_ind = start_ind + pattern_duration_ms

        # extract prediction
        predicted_spike_train_for_pattern = output_spikes_test[start_ind:end_ind]
        desired_spike_train_for_pattern = desired_output_spikes_test[start_ind:end_ind]

        compact_desired_output_test2[pattern_ind]  = desired_spike_train_for_pattern.sum() > 0.1

        if Y_test_spikes[pattern_ind] == 1:
            # check if there is a spike in the desired window only
            compact_predicted_output_test[pattern_ind] = (desired_spike_train_for_pattern * predicted_spike_train_for_pattern).sum() > 0.1
        else:
            # check if there is any spike in the full pattern duration
            compact_predicted_output_test[pattern_ind] = predicted_spike_train_for_pattern.sum() > 0.1

    # small verificaiton
    assert((compact_desired_output_test == compact_desired_output_test2).sum() == num_test_patterns)

    # display accuracy
    percent_accuracy = 100 * (compact_desired_output_test == compact_predicted_output_test).mean()
    true_positive    = 100 * (np.logical_and(compact_desired_output_test == True , compact_predicted_output_test == True).sum() / (compact_desired_output_test == True).sum())
    false_positive   = 100 * (np.logical_and(compact_desired_output_test == False, compact_predicted_output_test == True).sum() / (compact_desired_output_test == False).sum())

    print('I&F: weights mult factor = %.1f: Accuracy = %.3f%s. (TP, FP) = (%.3f%s, %.3f%s)' %(weight_mult_factor, percent_accuracy,'%',true_positive,'%',false_positive,'%'))

    IF_accuracy_list.append(percent_accuracy)
    IF_true_positive_list.append(true_positive)
    IF_false_positive_list.append(false_positive)

if show_plots:
    plt.figure(figsize=(15,8))
    plt.subplot(1,2,1); plt.bar(x=range(len(IF_accuracy_list)),height=IF_accuracy_list)
    plt.xticks(range(len(IF_accuracy_list)), IF_weight_mult_factors_list); plt.title('max accuracy = %.2f%s' %(np.array(IF_accuracy_list).max(),'%'), fontsize=24)
    plt.ylim(87.8,100); plt.xlabel('weight mult factor ("gain")', fontsize=20); plt.ylabel('Accuracy (%)', fontsize=20)
    plt.plot([-1, len(IF_accuracy_list)], [zero_pred_baseline_accuracy, zero_pred_baseline_accuracy], color='r')

    plt.subplot(1,2,2); plt.plot(IF_false_positive_list, IF_true_positive_list)
    plt.ylabel('True Positive (%)', fontsize=20); plt.xlabel('False Positive (%)', fontsize=20)


experiment_results_dict['model_accuracy_IF'] = np.array(IF_accuracy_list).max()


if show_plots:
    plt.figure(figsize=(30,6))
    plt.plot(1.05 * desired_output_spikes_test[:25000] - 0.025)
    plt.plot(output_spikes_test[:25000])
    plt.legend(['desired outputs', 'actual outputs'])

    plt.figure(figsize=(30,6))
    plt.plot(1.05 * desired_output_spikes_test[2900:4600] - 0.025)
    plt.plot(output_spikes_test[2900:4600])
    plt.legend(['desired outputs', 'actual outputs'])

    plt.figure(figsize=(30,6))
    plt.plot(1.05 * desired_output_spikes_test[8500:9500] - 0.025)
    plt.plot(output_spikes_test[8500:9500])
    plt.legend(['desired outputs', 'actual outputs'])

#%% I&F model predictions

if show_plots:
    start_time = 100 * np.random.randint(int(axons_input_spikes_test.shape[1] / 100 - 20))
    end_time = start_time + 1 + 100 * 18

    plt.figure(figsize=(30,15))
    plt.subplot(3,1,1); plt.imshow(axons_input_spikes_test[:,start_time:end_time], cmap='gray'); plt.title('input axons raster (test set)', fontsize=22)
    plt.subplot(3,1,2); plt.plot(output_spikes_test[start_time:end_time]); plt.xlim(0,1 + 100 * 18); plt.ylabel('output spike', fontsize=22)
    plt.subplot(3,1,3); plt.plot(soma_voltage_test[start_time:end_time]); plt.xlim(0,1 + 100 * 18); plt.ylabel('soma voltage [mV]', fontsize=22)
    plt.xlabel('time [ms]', fontsize=22)

#%% save results

if use_interaction_terms:
    results_filename = 'MNIST__interactions_%d__digit_%d__N_axons_%d__T_%d__M_%d__p_%0.3d__N_pos_samples_%d__randseed_%d.pickle' %(interactions_degree, positive_digit,
                                                                                                                                  experiment_results_dict['script_main_params']['digit_sample_image_shape_expanded'][0],
                                                                                                                                  experiment_results_dict['script_main_params']['digit_sample_image_shape_expanded'][1],
                                                                                                                                  experiment_results_dict['script_main_params']['connections_per_axon'],
                                                                                                                                  100 * experiment_results_dict['script_main_params']['release_probability'],
                                                                                                                                  num_train_positive_patterns, random_seed)
else:
    results_filename = 'MNIST__digit_%d__N_axons_%d__T_%d__M_%d__p_%0.3d__N_pos_samples_%d__randseed_%d.pickle' %(positive_digit,
                                                                                                                 experiment_results_dict['script_main_params']['digit_sample_image_shape_expanded'][0],
                                                                                                                 experiment_results_dict['script_main_params']['digit_sample_image_shape_expanded'][1],
                                                                                                                 experiment_results_dict['script_main_params']['connections_per_axon'],
                                                                                                                 100 * experiment_results_dict['script_main_params']['release_probability'],
                                                                                                                 num_train_positive_patterns, random_seed)

if not os.path.exists(data_folder):
    os.makedirs(data_folder)

# pickle everythin
pickle.dump(experiment_results_dict, open(data_folder + results_filename, "wb"))

#%% Load the saved pickle just to check it's OK

loaded_script_results_dict = pickle.load(open(data_folder + results_filename, "rb" ))

print('-----------------------------------------------------------------------------------------------------------')
print('loaded_script_results_dict.keys():')
print('----------')
print(list(loaded_script_results_dict.keys()))
print('-----------------------------------------------------------------------------------------------------------')
print('loaded_script_results_dict["script_main_params"].keys():')
print('----------')
print(list(loaded_script_results_dict["script_main_params"].keys()))
print('-----------------------------------------------------------------------------------------------------------')
print('interactions_degree =', loaded_script_results_dict['script_main_params']['interactions_degree'])
print('positive_digit =', loaded_script_results_dict['script_main_params']['positive_digit'])
print('connections_per_axon =', loaded_script_results_dict['script_main_params']['connections_per_axon'])
print('digit_sample_image_shape_cropped =', loaded_script_results_dict['script_main_params']['digit_sample_image_shape_cropped'])
print('digit_sample_image_shape_expanded =', loaded_script_results_dict['script_main_params']['digit_sample_image_shape_expanded'])
print('num_train_positive_patterns =', loaded_script_results_dict['script_main_params']['num_train_positive_patterns'])
print('temporal_silence_ms =', loaded_script_results_dict['script_main_params']['temporal_silence_ms'])
print('release_probability =', loaded_script_results_dict['script_main_params']['release_probability'])
print('train_epochs =', loaded_script_results_dict['script_main_params']['train_epochs'])
print('test_epochs =', loaded_script_results_dict['script_main_params']['test_epochs'])
print('-----------------------------------------------------------------------------------------------------------')
print('model_accuracy_LR =', loaded_script_results_dict['model_accuracy_LR'])
print('model_accuracy_FF =', loaded_script_results_dict['model_accuracy_FF'])
print('model_accuracy_IF =', loaded_script_results_dict['model_accuracy_IF'])
print('model_accuracy_baseline =', loaded_script_results_dict['model_accuracy_baseline'])
print('-----------------------------------------------------------------------------------------------------------')

if show_plots:
    plt.figure(figsize=(24,10))
    plt.subplot(1,3,1); plt.imshow(loaded_script_results_dict['learned_weights_LR']); plt.title('logistic regression', fontsize=24)
    plt.subplot(1,3,2); plt.imshow(loaded_script_results_dict['learned_weights_FF']); plt.title('filter and fire neuron', fontsize=24)
    plt.subplot(1,3,3); plt.imshow(loaded_script_results_dict['learned_weights_IF']); plt.title('integrate and fire neuron', fontsize=24)

script_duration_min = (time.time() - start_time) / 60
print('-----------------------------------')
print('finished script! took %.1f minutes' %(script_duration_min))
print('-----------------------------------')

================================================================================
================================================================================
run_capacity_configs_on_cluster_slurm.py:
=========================================
import os
import time


def mkdir_p(dir_path):
    '''make a directory (dir_path) if it doesn't exist'''
    if not os.path.exists(dir_path):
        os.mkdir(dir_path)


script_name    = 'FF_vs_IF_capacity_comparison_interactions.py'
output_log_dir = '/filter_and_fire_neuron/logs/'

# tiny experiment configs to run (will be form a grid of all combinations, so number of experiments will explode if not careful)
num_axons_list = [100]
use_interaction_terms_list = [False]
interactions_degree_list = [2]

# full experiment (axons, interaction degree)
num_axons_list = [50,100,200,300,400]
use_interaction_terms_list = [False]
interactions_degree_list = [2]

num_random_seeds = 1
start_seed = 123456

partition_argument_str = "-p ss.q,elsc.q"
timelimit_argument_str = "-t 0-22:00:00"
CPU_argument_str = "-c 1"
RAM_argument_str = "--mem 32000"
CPU_exclude_nodes_str = "--exclude=ielsc-58,ielsc-60,ielsc-108,ielsc-109"

temp_jobs_dir = os.path.join(output_log_dir, 'temp/')
mkdir_p(temp_jobs_dir)

random_seed = start_seed
for num_axons in num_axons_list:
    for use_interaction_terms in use_interaction_terms_list:
        for interactions_degree in interactions_degree_list:
            for exp_index in range(num_random_seeds):
                random_seed = random_seed + 1

                # job and log names
                axons_interactions_str = 'num_axons_%d_interactions_%s_degree_%d' %(num_axons, use_interaction_terms, interactions_degree)
                job_name = '%s_%s_randseed_%d' %(script_name[:-3], axons_interactions_str, random_seed)
                log_filename = os.path.join(output_log_dir, "%s.log" %(job_name))
                job_filename = os.path.join(temp_jobs_dir , "%s.job" %(job_name))

                # write a job file and run it
                with open(job_filename, 'w') as fh:
                    fh.writelines("#!/bin/bash\n")
                    fh.writelines("#SBATCH --job-name %s\n" %(job_name))
                    fh.writelines("#SBATCH -o %s\n" %(log_filename))
                    fh.writelines("#SBATCH %s\n" %(partition_argument_str))
                    fh.writelines("#SBATCH %s\n" %(timelimit_argument_str))
                    fh.writelines("#SBATCH %s\n" %(CPU_argument_str))
                    fh.writelines("#SBATCH %s\n" %(RAM_argument_str))
                    fh.writelines("#SBATCH %s\n" %(CPU_exclude_nodes_str))
                    fh.writelines("python3.6 -u %s %s %s %s %s\n" %(script_name, random_seed, num_axons, use_interaction_terms, interactions_degree))

                os.system("sbatch %s" %(job_filename))
                time.sleep(0.2)

================================================================================
================================================================================
saved_figures\F&F_Axon_Reduction_Figure_5_4.png:
================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image displays a visual representation of a neural network's functionality, likely focusing on spike-based or spiking neural networks (SNNs).  The top section shows a raster plot, a common visualization technique for SNNs, illustrating the firing patterns of neurons over time. Three distinct layers of neurons are represented by different colors (purple, orange, and black), suggesting different processing stages within the network. Each dot in the raster plot represents a single spike (a neural action potential).

The middle section presents a schematic representation of the network's input and output signals.  It depicts three lines representing different stages of processing: "F&F (M=5)" likely indicates a feedforward layer with a certain parameter (M=5); "I&F (Orig Axons + 2 delayed)" suggests an integrate-and-fire neuron layer with original axons and a delay mechanism; and "Desired Output" shows the target output spike pattern the network aims to achieve.  The vertical lines represent spikes, and their timing and frequency are crucial indicators of the network's performance. The numbers below this section seem to be a numerical representation of the input data.

The bottom section mirrors the top section's structure but on a smaller scale, likely showing a specific example or a smaller subset of the data processed by the network.  The three rows of handwritten digits (6, 5, 3, 0, 7, 2, 7, 4, 6) in purple, orange, and black dots correspond to the neuronal layers in the raster plot. The timing diagram below the digits shows the input and desired output for this specific example.  The consistency between the top and bottom sections suggests the image illustrates a general principle of the network's operation with the bottom section acting as a concrete example.

================================================================================
