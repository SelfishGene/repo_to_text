================================================================================
repo title: filter_and_fire_neuron
repo link: https://github.com/SelfishGene/filter_and_fire_neuron
date processed: 2024-10-21
================================================================================
================================================================================
repo file structure:
====================
    create_capacity_figure_Fig2.py
    create_hardware_saving_figure_Fig5.py
    create_MNIST_figure_Fig3.py
    create_intro_figure_Fig1.py
    README.md
    MNIST_classification_LR_IF_FF_interactions.py
    FF_vs_IF_capacity_comparison_interactions.py
    create_explanatory_figure_Fig4.py
    run_capacity_configs_on_cluster_slurm.py
    run_mnist_configs_on_cluster_slurm.py
    results_data_capacity/
        FF_vs_IF_capacity_comparision__num_axons_400__sim_duration_sec_120__num_mult_conn_5__rand_rep_12.pickle
        FF_vs_IF_capacity_comparision__num_axons_112__sim_duration_sec_120__num_mult_conn_6__rand_rep_18.pickle
        FF_vs_IF_capacity_comparision__num_axons_187__sim_duration_sec_120__num_mult_conn_6__rand_rep_18.pickle
        FF_vs_IF_capacity_comparision__num_axons_300__sim_duration_sec_120__num_mult_conn_5__rand_rep_12.pickle
        FF_vs_IF_capacity_comparision__num_axons_237__sim_duration_sec_120__num_mult_conn_6__rand_rep_18.pickle
        FF_vs_IF_capacity_comparision__num_axons_50__sim_duration_sec_120__num_mult_conn_5__rand_rep_12.pickle
        FF_vs_IF_capacity_comparision__num_axons_137__sim_duration_sec_120__num_mult_conn_6__rand_rep_18.pickle
        FF_vs_IF_capacity_comparision__num_axons_200__sim_duration_sec_120__num_mult_conn_6__rand_rep_18.pickle
        FF_vs_IF_capacity_comparision__num_axons_125__sim_duration_sec_120__num_mult_conn_6__rand_rep_18.pickle
        FF_vs_IF_capacity_comparision__num_axons_212__sim_duration_sec_120__num_mult_conn_6__rand_rep_18.pickle
        FF_vs_IF_capacity_comparision__num_axons_150__sim_duration_sec_120__num_mult_conn_6__rand_rep_18.pickle
        FF_vs_IF_capacity_comparision__num_axons_162__sim_duration_sec_120__num_mult_conn_6__rand_rep_18.pickle
        FF_vs_IF_capacity_comparision__num_axons_225__sim_duration_sec_120__num_mult_conn_6__rand_rep_18.pickle
        FF_vs_IF_capacity_comparision__num_axons_100__sim_duration_sec_120__num_mult_conn_5__rand_rep_12.pickle
        FF_vs_IF_capacity_comparision__num_axons_100__sim_duration_sec_120__num_mult_conn_6__rand_rep_18.pickle
        FF_vs_IF_capacity_comparision__num_axons_175__sim_duration_sec_120__num_mult_conn_6__rand_rep_18.pickle
        FF_vs_IF_capacity_comparision__num_axons_200__sim_duration_sec_120__num_mult_conn_5__rand_rep_12.pickle
    saved_figures/
        F&F_Capacity_Figure_2_3.png
        F&F_MNIST_Figure_3_7.png
        F&F_Explantion_Figure_4_2.png
        F&F_Axon_Reduction_Figure_5_4.png
        F&F_Introduction_Figure_1_11.png
    results_data_mnist/
        MNIST_classification_LR_FF_IF_5162_rows_12_cols.csv
================================================================================
================================================================================
README.md:
==========
# The Filter and Fire (F&F) Neuron Model
This repo contains the code behind the work  
[Multiple Synaptic Contacts combined with Dendritic Filtering enhance Spatio-Temporal Pattern Recognition capabilities of Single Neurons](https://www.biorxiv.org/content/10.1101/2022.01.28.478132v2)

## Multiple Synaptic Contacts combined with Dendritic Filtering <br > enhance Spatio-Temporal Pattern Recognition capabilities of Single Neurons  
David Beniaguev, Sapir Shapira, Idan Segev, Michael London

**Abstract**: *A cortical neuron typically makes multiple synaptic contacts on the dendrites of a post-synaptic target neuron. The functional implications of this apparent redundancy are unclear. The dendritic location of a synaptic contact affects the time-course of the somatic post-synaptic potential (PSP) due to dendritic cable filtering. Consequently, a single pre-synaptic axonal spike results with a PSP composed of multiple temporal profiles. Here, we developed a "filter-and-fire" (F&F) neuron model that captures these features and show that the memory capacity of this neuron is threefold larger than that of a leaky integrate-and-fire (I&F) neuron, when trained to emit precisely timed output spikes for specific input patterns. Furthermore, the F&F neuron can learn to recognize spatio-temporal input patterns, e.g., MNIST digits, where the I&F model completely fails. Multiple synaptic contacts between pairs of cortical neurons are therefore an important feature rather than a bug and can serve to reduce axonal wiring requirements.*

<img width="1161" alt="Overview_of_F F_neuron_model" src="https://user-images.githubusercontent.com/11506338/151635189-1e6bfe6f-78a5-4c7e-92a4-0599601697c3.PNG">

## Resources
Open Access version of Paper: [biorxiv.org/content/10.1101/2022.01.28.478132v2](https://www.biorxiv.org/content/10.1101/2022.01.28.478132v2)  
Data required for full replication of all results: [kaggle.com/selfishgene/fiter-and-fire-paper](https://www.kaggle.com/selfishgene/fiter-and-fire-paper)  
Introductory Notebook (Figure 1 in manuscript): [kaggle.com/selfishgene/f-f-introduction-figure-fig-1](https://www.kaggle.com/selfishgene/f-f-introduction-figure-fig-1)  
Notebook with replication of main results 1: [kaggle.com/selfishgene/f-f-capacity-figure-fig-2](https://www.kaggle.com/selfishgene/f-f-capacity-figure-fig-2)  
Notebook with replication of main results 2: [kaggle.com/selfishgene/f-f-mnist-figure-fig-3](https://www.kaggle.com/selfishgene/f-f-mnist-figure-fig-3)  
Notebooks for full replication of all figures: [kaggle.com/selfishgene/fiter-and-fire-paper/code](https://www.kaggle.com/selfishgene/fiter-and-fire-paper/code)  


## Increased capacity of F&F vs I&F 
<img width="1161" alt="Capacity_vs_multiple_contacts_compact" src="https://user-images.githubusercontent.com/11506338/151635194-af23b7d3-bb7a-48c9-aaeb-f05648cd4e64.PNG">

- Use `create_capacity_figure_Fig2.py` to replicate Figure 2 in the manuscript
  - All major parameters are documented inside the file using comments  
  - All necessary files are under the folder `results_data_capacity\`
- Use `FF_vs_IF_capacity_comparison_interactions.py` to recreate all files in `results_data_capacity\`
  - All major parameters are documented inside the file using comments  
  - Use `run_capacity_configs_on_cluster_slurm.py` to send jobs to a slurm cluster


## Single Neurons as Spatio-Temporal Pattern Recognizers
<img width="1003" alt="MNIST_classifying_digit_3_compact" src="https://user-images.githubusercontent.com/11506338/151635198-3b65239f-505c-46e3-8ec1-8ddc7931e52d.PNG">

- Use `create_MNIST_figure_Fig3.py` to replicate Figure 3 in the manuscript
  - All major parameters are documented inside the file using comments  
  - All necessary files are under the folder `results_data_mnist\`. large files are on [the dataset](https://www.kaggle.com/selfishgene/fiter-and-fire-paper) on kaggle
- Use `MNIST_classification_LR_IF_FF_interactions.py` to recreate all files in `results_data_mnist\`
  - All major parameters are documented inside the file using comments  
  - Use `run_mnist_configs_on_cluster_slurm.py` to send jobs to a slurm cluster

## PSPs of a realistic detailed biophysical Layer 5 Cortical Pyramidal Neuron
<img width="1040" alt="L5PC_morphology_PSPs" src="https://user-images.githubusercontent.com/11506338/151635200-a8288feb-0365-4c86-91ad-2d87dcc3e7b8.PNG">

- Visit [this link](https://www.kaggle.com/selfishgene/f-f-l5pc-psps-fig-s2) to replicate Supplementary Figure S2 in the manuscript
- All necessary simulation data for this figure are in the file `sim_results_excitatory.p` in [the dataset](https://www.kaggle.com/selfishgene/fiter-and-fire-paper) on kaggle 


## Acknowledgements
We thank all lab members of the Segev and London Labs for many fruitful discussions and valuable feedback regarding this work.
In particular we would like to thank [Sapir Shapira](https://github.com/ssapir) that skillfully collected all data and created Supplementary Figure S2 in the paper.


If you use this code or dataset, please cite the following work:  

1. David Beniaguev, Sapir Shapira, Idan Segev and Michael London. "Multiple Synaptic Contacts combined with Dendritic Filtering enhance Spatio-Temporal Pattern Recognition capabilities of Single Neurons
." bioRxiv 2022.01.28.478132; doi: https://doi.org/10.1101/2022.01.28.478132



================================================================================
================================================================================
run_mnist_configs_on_cluster_slurm.py:
======================================
import os
import time


def mkdir_p(dir_path):
    '''make a directory (dir_path) if it doesn't exist'''
    if not os.path.exists(dir_path):
        os.mkdir(dir_path)


script_name    = 'MNIST_classification_LR_IF_FF_interactions.py'
output_log_dir = '/filter_and_fire_neuron/logs/'

# all experiment configs to run (will be form a grid of all combinations, so number of experiments will explode if not careful)
positive_digit_list = [0,1,2,3,4,5,6,7,8,9]
connections_per_axon_list = [1,2,5,10]
temporal_extent_factor_numerator_list = [1,2,3,4,5]
temporal_extent_factor_denumerator_list = [1,2]
release_probability_list = [0.5,1.0]
num_positive_training_samples_list = [16,32,64,128,256,512,1024,2048,4096]

num_random_seeds = 2
start_seed = 123456

partition_argument_str = "-p ss.q,elsc.q"
timelimit_argument_str = "-t 1-18:00:00"
CPU_argument_str = "-c 1"
RAM_argument_str = "--mem 64000"
CPU_exclude_nodes_str = "--exclude=ielsc-60,ielsc-108,ielsc-109"

temp_jobs_dir = os.path.join(output_log_dir, 'temp/')
mkdir_p(temp_jobs_dir)

random_seed = start_seed
for positive_digit in positive_digit_list:
    for connections_per_axon in connections_per_axon_list:
        for temporal_extent_factor_numerator in temporal_extent_factor_numerator_list:
            for temporal_extent_factor_denumerator in temporal_extent_factor_denumerator_list:
                for release_probability in release_probability_list:
                    for num_positive_training_samples in num_positive_training_samples_list:
                        for exp_index in range(num_random_seeds):
                            random_seed = random_seed + 1

                            # job and log names
                            digit_multconn_str = 'digit_%d_mult_connections_%d' %(positive_digit, connections_per_axon)
                            job_name = '%s_%s_randseed_%d' %(script_name[:-3], digit_multconn_str, random_seed)
                            log_filename = os.path.join(output_log_dir, "%s.log" %(job_name))
                            job_filename = os.path.join(temp_jobs_dir , "%s.job" %(job_name))

                            # write a job file and run it
                            with open(job_filename, 'w') as fh:
                                fh.writelines("#!/bin/bash\n")
                                fh.writelines("#SBATCH --job-name %s\n" %(job_name))
                                fh.writelines("#SBATCH -o %s\n" %(log_filename))
                                fh.writelines("#SBATCH %s\n" %(partition_argument_str))
                                fh.writelines("#SBATCH %s\n" %(timelimit_argument_str))
                                fh.writelines("#SBATCH %s\n" %(CPU_argument_str))
                                fh.writelines("#SBATCH %s\n" %(RAM_argument_str))
                                fh.writelines("#SBATCH %s\n" %(CPU_exclude_nodes_str))
                                fh.writelines("python3.6 -u %s %s %s %s %s %s %s %s\n" %(script_name, random_seed, positive_digit, connections_per_axon,
                                                                                         temporal_extent_factor_numerator, temporal_extent_factor_denumerator,
                                                                                         release_probability, num_positive_training_samples))

                            os.system("sbatch %s" %(job_filename))
                            time.sleep(0.1)

================================================================================
================================================================================
create_capacity_figure_Fig2.py:
===============================
import numpy as np
import matplotlib.pyplot as plt
from scipy import signal
from sklearn import linear_model
from sklearn.metrics import roc_curve, roc_auc_score, auc
import pickle
import matplotlib
import matplotlib.gridspec as gridspec

matplotlib.rcParams['pdf.fonttype'] = 42
matplotlib.rcParams['svg.fonttype'] = 'none'

#%% main script params

# input parameters
num_axons = 100

# neuron model parameters
connections_per_axon = 5
num_synapses = connections_per_axon * num_axons

v_reset     = -80
v_threshold = -55
current_to_voltage_mult_factor = 3
refreactory_time_constant = 15

model_type = 'F&F'
#model_type = 'I&F'

# synapse non-learnable parameters
if model_type == 'F&F':
    tau_rise_range  = [1,16]
    tau_decay_range = [8,24]
elif model_type == 'I&F':
    tau_rise_range  = [1,1]
    tau_decay_range = [24,24]

tau_rise_vec  = np.random.uniform(low=tau_rise_range[0] , high=tau_rise_range[1] , size=(num_synapses, 1))
tau_decay_vec = np.random.uniform(low=tau_decay_range[0], high=tau_decay_range[1], size=(num_synapses, 1))

# synapse learnable parameters
synaptic_weights_vec = np.random.normal(size=(num_synapses, 1))

save_figures = True
save_figures = False
all_file_endings_to_use = ['.png', '.pdf', '.svg']

data_folder   = '/filter_and_fire_neuron/results_data_capacity/'
figure_folder = '/filter_and_fire_neuron/saved_figures/'

#%% helper functions

def create_single_PSP_profile(tau_rise, tau_decay, temporal_filter_length=50):

    safety_factor = 1.5
    if tau_rise >= (tau_decay / safety_factor):
        tau_decay = safety_factor * tau_rise

    exp_r = signal.exponential(M=temporal_filter_length, center=0, tau=tau_rise , sym=False)
    exp_d = signal.exponential(M=temporal_filter_length, center=0, tau=tau_decay, sym=False)

    post_syn_potential = exp_d - exp_r
    post_syn_potential /= post_syn_potential.max()

    return post_syn_potential


def construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec):

    num_synapses = tau_rise_vec.shape[0]
    temporal_filter_length = int(4 * tau_decay_vec.max()) + 1

    syn_filter = np.zeros((num_synapses, temporal_filter_length))

    for k, (tau_r, tau_d) in enumerate(zip(tau_rise_vec, tau_decay_vec)):
        syn_filter[k,:] = create_single_PSP_profile(tau_r, tau_d, temporal_filter_length=temporal_filter_length)

    return syn_filter


def simulate_filter_and_fire_cell_training(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                           refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):

    temporal_filter_length = int(5 * refreactory_time_constant) + 1
    refreactory_filter = signal.exponential(M=temporal_filter_length,center=0,tau=refreactory_time_constant,sym=False)[np.newaxis,:]

    # padd input and get all synaptic filters
    normlized_syn_filter = np.flipud(construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec))
    padded_input = np.hstack((np.zeros(normlized_syn_filter.shape), presynaptic_input_spikes))

    # calc local currents
    local_normlized_currents = np.zeros(presynaptic_input_spikes.shape)
    for k in range(normlized_syn_filter.shape[0]):
        local_normlized_currents[k] = signal.convolve(padded_input[k], normlized_syn_filter[k], mode='valid')[1:]

    # multiply by weights to get the somatic current
    soma_current = signal.convolve(local_normlized_currents, synaptic_weights, mode='valid')

    # simulate the cell
    soma_voltage = v_reset + current_to_voltage_mult_factor * soma_current.ravel()
    output_spike_times_in_ms = []
    # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
    for t in range(len(soma_voltage)):
        # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
        if (soma_voltage[t] > v_threshold) and ((t + 1) < len(soma_voltage)):
            t_start = t + 1
            t_end = min(len(soma_voltage), t_start + temporal_filter_length)
            soma_voltage[t_start:t_end] -= (soma_voltage[t + 1] - v_reset) * refreactory_filter.ravel()[:(t_end - t_start)]
            output_spike_times_in_ms.append(t)

    return local_normlized_currents, soma_voltage, output_spike_times_in_ms


def simulate_filter_and_fire_cell_inference(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                            refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):

    temporal_filter_length = int(5 * refreactory_time_constant) + 1
    refreactory_filter = signal.exponential(M=temporal_filter_length,center=0,tau=refreactory_time_constant,sym=False)[np.newaxis,:]

    # padd input and get all synaptic filters
    normlized_syn_filter = construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec)
    padded_input = np.hstack((np.zeros(normlized_syn_filter.shape), presynaptic_input_spikes))

    # calc somatic current
    weighted_syn_filter  = synaptic_weights * normlized_syn_filter
    soma_current = signal.convolve(padded_input, weighted_syn_filter, mode='valid')[:,1:]

    # simulate the cell
    soma_voltage = v_reset + current_to_voltage_mult_factor * soma_current.ravel()
    output_spike_times_in_ms = []
    # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
    for t in range(len(soma_voltage)):

        # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
        if (soma_voltage[t] > v_threshold) and ((t + 1) < len(soma_voltage)):
            t_start = t + 1
            t_end = min(len(soma_voltage), t_start + temporal_filter_length)
            soma_voltage[t_start:t_end] -= (soma_voltage[t + 1] - v_reset) * refreactory_filter.ravel()[:(t_end - t_start)]
            output_spike_times_in_ms.append(t)

    return soma_voltage, output_spike_times_in_ms


# use local currents as "features" and fit a linear model to the data
def prepare_training_dataset(local_normlized_currents, desired_output_spikes, spike_safety_range_ms=10, negative_subsampling_fraction=0.1):

    # remove all "negative" time points that are too close to spikes
    desired_output_spikes_LPF = signal.convolve(desired_output_spikes, np.ones((spike_safety_range_ms,)), mode='same') > 0.1
    desired_timepoints = ~desired_output_spikes_LPF

    # massivly subsample the remaining timepoints
    desired_timepoints[np.random.rand(desired_timepoints.shape[0]) > negative_subsampling_fraction] = 0
    desired_timepoints[desired_output_spikes > 0.1] = 1

    X = local_normlized_currents.T[desired_timepoints,:]
    y = desired_output_spikes[desired_timepoints]

    return X, y


#%% generate sample input

# generate sample input
stimulus_duration_ms = 60000

axons_input_spikes = np.random.rand(num_axons, stimulus_duration_ms) < 0.001

presynaptic_input_spikes = np.kron(np.ones((connections_per_axon,1)), axons_input_spikes)

assert presynaptic_input_spikes.shape[0] == num_synapses, 'number of synapses doesnt match the number of presynaptic inputs'

# generate desired pattern of output spikes
requested_number_of_output_spikes = 40
min_time_between_spikes_ms = 125

desired_output_spike_times = min_time_between_spikes_ms * np.random.randint(int(stimulus_duration_ms / min_time_between_spikes_ms), size=requested_number_of_output_spikes)
desired_output_spike_times = np.sort(np.unique(desired_output_spike_times))

desired_output_spikes = np.zeros((stimulus_duration_ms,))
desired_output_spikes[desired_output_spike_times] = 1.0

print('number of requested output spikes = %d' %(requested_number_of_output_spikes))

# simulate cell with normlized currents
local_normlized_currents, soma_voltage, output_spike_times_in_ms = simulate_filter_and_fire_cell_training(presynaptic_input_spikes,
                                                                                                          synaptic_weights_vec, tau_rise_vec, tau_decay_vec,
                                                                                                          refreactory_time_constant=refreactory_time_constant,
                                                                                                          v_reset=v_reset, v_threshold=v_threshold,
                                                                                                          current_to_voltage_mult_factor=current_to_voltage_mult_factor)

output_spikes = np.zeros((stimulus_duration_ms,))
try:
    output_spikes[np.array(output_spike_times_in_ms)] = 1.0
except:
    print('no output spikes created')

#%% fit linear model to local currents and display GT vs prediction

# fit linear model to local currents
logistic_reg_model = linear_model.LogisticRegression(C=30000, fit_intercept=True, penalty='l2', max_iter=3000)

spike_safety_range_ms = 1
negative_subsampling_fraction = 0.99

X, y = prepare_training_dataset(local_normlized_currents, desired_output_spikes,
                                spike_safety_range_ms=spike_safety_range_ms,
                                negative_subsampling_fraction=negative_subsampling_fraction)

# fit model
logistic_reg_model.fit(X,y)

print('number of data points = %d (%.2f%s positive class)' %(X.shape[0], 100 * y.mean(),'%'))

y_hat = logistic_reg_model.predict_proba(X)[:,1]

# calculate AUC
train_AUC = roc_auc_score(y, y_hat)

fitted_output_spike_prob = logistic_reg_model.predict_proba(local_normlized_currents.T)[:,1]
full_AUC = roc_auc_score(desired_output_spikes, fitted_output_spike_prob)

# get desired FP threshold
desired_false_positive_rate = 0.004

fpr, tpr, thresholds = roc_curve(desired_output_spikes, fitted_output_spike_prob)

desired_fp_ind = np.argmin(abs(fpr-desired_false_positive_rate))
if desired_fp_ind == 0:
    desired_fp_ind = 1

actual_false_positive_rate = fpr[desired_fp_ind]
true_positive_rate         = tpr[desired_fp_ind]
desired_fp_threshold       = thresholds[desired_fp_ind]

AUC_score = auc(fpr, tpr)

print('AUC = %.4f' %(AUC_score))
print('at %.4f FP rate, TP = %.4f' %(actual_false_positive_rate, true_positive_rate))

output_spikes_after_learning = fitted_output_spike_prob > desired_fp_threshold

plt.close('all')
plt.figure(figsize=(25,12))
plt.subplot(2,1,1);
plt.plot(1.05 * y - 0.025); plt.title('train AUC = %.5f' %(train_AUC))
plt.plot(y_hat); plt.xlabel('training samples'); plt.legend(['GT', 'prediction'])

plt.subplot(2,1,2);
plt.plot(1.05 * desired_output_spikes - 0.025); plt.title('full trace AUC = %.5f' %(full_AUC))
plt.plot(fitted_output_spike_prob); plt.xlabel('time [ms]'); plt.legend(['GT', 'prediction'])

#%% Display Input Raster, input and output before and after learning

plt.close('all')
fig = plt.figure(figsize=(20,16))
gs_figure = gridspec.GridSpec(nrows=9,ncols=1)
gs_figure.update(left=0.04, right=0.95, bottom=0.05, top=0.95, wspace=0.1, hspace=0.4)

ax_axons           = plt.subplot(gs_figure[:6,:])
ax_before_learning = plt.subplot(gs_figure[6,:])
ax_after_learning  = plt.subplot(gs_figure[7,:])
ax_desired_output  = plt.subplot(gs_figure[8,:])

syn_activation_time, syn_activation_index = np.nonzero(axons_input_spikes.T)
syn_activation_time = syn_activation_time / 1000

min_time_sec = 0
max_time_sec = stimulus_duration_ms / 1000

time_sec = np.linspace(min_time_sec, max_time_sec, output_spikes.shape[0])

ax_axons.scatter(syn_activation_time, syn_activation_index, s=2, c='k'); ax_axons.set_title('input axons raster', fontsize=15)
ax_axons.set_xlim(min_time_sec, max_time_sec);
ax_axons.set_xticks([])
ax_axons.spines['top'].set_visible(False)
ax_axons.spines['bottom'].set_visible(False)
ax_axons.spines['left'].set_visible(False)
ax_axons.spines['right'].set_visible(False)

ax_before_learning.plot(time_sec, output_spikes, c='k'); ax_before_learning.set_title('before learning', fontsize=15)
ax_before_learning.set_xlim(min_time_sec, max_time_sec);
ax_before_learning.set_xticks([])
ax_before_learning.set_yticks([])
ax_before_learning.spines['top'].set_visible(False)
ax_before_learning.spines['bottom'].set_visible(False)
ax_before_learning.spines['left'].set_visible(False)
ax_before_learning.spines['right'].set_visible(False)

ax_after_learning.plot(time_sec, output_spikes_after_learning, c='k'); ax_after_learning.set_title('after learning', fontsize=15)
ax_after_learning.set_xlim(min_time_sec, max_time_sec);
ax_after_learning.set_xticks([])
ax_after_learning.set_yticks([])
ax_after_learning.spines['top'].set_visible(False)
ax_after_learning.spines['bottom'].set_visible(False)
ax_after_learning.spines['left'].set_visible(False)
ax_after_learning.spines['right'].set_visible(False)

ax_desired_output.plot(time_sec, desired_output_spikes, c='k'); ax_desired_output.set_title('desired output (num spikes = %d)' %(requested_number_of_output_spikes), fontsize=15);
ax_desired_output.set_xlim(min_time_sec, max_time_sec);
ax_desired_output.set_yticks([]);
ax_desired_output.spines['top'].set_visible(False)
ax_desired_output.spines['bottom'].set_visible(False)
ax_desired_output.spines['left'].set_visible(False)
ax_desired_output.spines['right'].set_visible(False)

#%% Load pickle file with previously stored results and check that it's OK

results_filename = data_folder + 'FF_vs_IF_capacity_comparision__num_axons_200__sim_duration_sec_120__num_mult_conn_6__rand_rep_18.pickle'

loaded_script_results_dict = pickle.load(open(results_filename, "rb" ))

print('-----------------------------------------------------------------------------------------------------------')
print('loaded_script_results_dict.keys():')
print('----------')
print(loaded_script_results_dict.keys())
print('-----------------------------------------------------------------------------------------------------------')
print('loaded_script_results_dict["script_main_params"].keys():')
print('----------')
[print(x) for x in loaded_script_results_dict['script_main_params'].keys()]
print('-----------------------------------------------------------------------------------------------------------')
print('num_axons =', loaded_script_results_dict['script_main_params']['num_axons'])
print('stimulus_duration_sec =', loaded_script_results_dict['script_main_params']['stimulus_duration_sec'])
print('min_time_between_spikes_ms =', loaded_script_results_dict['script_main_params']['min_time_between_spikes_ms'])
print('refreactory_time_constant =', loaded_script_results_dict['script_main_params']['refreactory_time_constant'])
print('num_random_iter =', loaded_script_results_dict['script_main_params']['num_random_iter'])
print('spike_safety_range_ms =', loaded_script_results_dict['script_main_params']['spike_safety_range_ms'])
print('negative_subsampling_fraction =', loaded_script_results_dict['script_main_params']['negative_subsampling_fraction'])
print('-----------------------------------------------------------------------------------------------------------')

#%% extract params

processed_res_curves = loaded_script_results_dict['processed_res_curves']
all_results_curves   = loaded_script_results_dict['all_results_curves']

num_axons = loaded_script_results_dict['script_main_params']['num_axons']
stimulus_duration_sec = loaded_script_results_dict['script_main_params']['stimulus_duration_sec']

#%% Fig 2B

num_plots = len(all_results_curves.keys())

num_random_iterations = 5

fig = plt.figure(figsize=(30,15));
for key, value in all_results_curves.items():
    plt.errorbar(value['num_spikes'], value['mean_AUC'], yerr=value['std_AUC'] / np.sqrt(num_random_iterations), label=key, lw=4)
plt.legend(loc='lower left', fontsize=23, ncol=2)
plt.title('learning to place precisly timed output spikes (random input, %d sec window, %d axons)' %(stimulus_duration_sec, num_axons), fontsize=24)
plt.ylabel('accuracy at 1ms precision (AUC)', fontsize=24)
plt.xlabel('num requested spikes to place', fontsize=30);

#%% Fig 2C

filename_str = 'FF_vs_IF_capacity_comparision__num_axons_%d__sim_duration_sec_120__num_mult_conn_6__rand_rep_18.pickle'
num_axons_list = sorted([100, 112, 125, 137, 150, 162, 175, 187, 200, 212, 225, 237])
all_filenames_str = [filename_str %(x) for x in num_axons_list]

model_keys = list(loaded_script_results_dict['processed_res_curves'].keys())
connections_per_axon_2C = loaded_script_results_dict['processed_res_curves'][model_keys[0]]['connections_per_axon']

precisely_timed_spikes_per_axon_2C = {}
precisely_timed_spikes_per_axon_error_2C = {}
for key in model_keys:
    precisely_timed_spikes_per_axon_2C[key] = np.zeros((len(all_filenames_str), len(connections_per_axon_2C)))
    precisely_timed_spikes_per_axon_error_2C[key] = np.zeros((len(all_filenames_str), len(connections_per_axon_2C)))

for k, (curr_num_axons, curr_filename) in enumerate(zip(num_axons_list, all_filenames_str)):
    curr_results_filename = data_folder + curr_filename
    curr_loaded_results_dict = pickle.load(open(curr_results_filename, "rb" ))

    for key in model_keys:
        precisely_timed_spikes_per_axon_2C[key][k,:] = curr_loaded_results_dict['processed_res_curves'][key]['num_almost_perfectly_placed_spikes'] / curr_num_axons

        for j, (num_M_conn, num_spikes) in enumerate(zip(curr_loaded_results_dict['processed_res_curves'][key]['connections_per_axon'],
                                                         curr_loaded_results_dict['processed_res_curves'][key]['num_almost_perfectly_placed_spikes'])):

            model_connections_str = '%s, %d connections' %(key, num_M_conn)
            error_index = list(curr_loaded_results_dict['all_results_curves'][model_connections_str]['num_spikes']).index(num_spikes)
            error_scale = (curr_loaded_results_dict['all_results_curves'][model_connections_str]['num_spikes'][error_index + 1] -
                           curr_loaded_results_dict['all_results_curves'][model_connections_str]['num_spikes'][max(0, error_index - 1)])

            if error_index > 1:
                error_scale /= 2

            precisely_timed_spikes_per_axon_error_2C[key][k,j] = error_scale / curr_num_axons

num_plots = len(processed_res_curves.keys())

color_map = {}
color_map['I&F'] = '0.05'
color_map['F&F'] = 'orange'

fig = plt.figure(figsize=(30,12));
for key in processed_res_curves.keys():
    y_error = precisely_timed_spikes_per_axon_2C[key].std(axis=0)
    plt.errorbar(connections_per_axon_2C, precisely_timed_spikes_per_axon_2C[key].mean(axis=0), yerr=y_error, label=key, lw=4, color=color_map[key])
plt.title('learning to place precisely timed output spikes (random input, %d sec window, %d axons)' %(stimulus_duration_sec, num_axons), fontsize=24)
plt.xlabel('Number of Multiple Contacts - M', fontsize=24)
plt.ylabel('Number of Accuractly Timed Spikes\n per Input Axon', fontsize=24);
plt.legend(loc='upper left', fontsize=40)
plt.yticks([0.15,0.3,0.45])

#%% Fig 2D

filename_str = 'FF_vs_IF_capacity_comparision__num_axons_%d__sim_duration_sec_120__num_mult_conn_5__rand_rep_12.pickle'

num_axons_list = sorted([50,100,200,300,400])
num_multiple_conn_list = [1,3,5,10]

all_filenames_str = [filename_str %(x) for x in num_axons_list]

FF_num_placed_spikes = {}
IF_num_placed_spikes = {}

for num_M_conn in num_multiple_conn_list:

    FF_num_placed_spikes[num_M_conn] = {}
    FF_num_placed_spikes[num_M_conn]['num_accurately_placed_spikes'] = []
    FF_num_placed_spikes[num_M_conn]['num_accurately_placed_spikes_error'] = []

    IF_num_placed_spikes[num_M_conn] = {}
    IF_num_placed_spikes[num_M_conn]['num_accurately_placed_spikes'] = []
    IF_num_placed_spikes[num_M_conn]['num_accurately_placed_spikes_error'] = []

    for curr_num_axons, curr_filename in zip(num_axons_list, all_filenames_str):

        curr_results_filename = data_folder + curr_filename
        curr_loaded_results_dict = pickle.load(open(curr_results_filename, "rb" ))

        FF_ind = list(curr_loaded_results_dict['processed_res_curves']['F&F']['connections_per_axon']).index(num_M_conn)
        FF_num_spikes = curr_loaded_results_dict['processed_res_curves']['F&F']['num_almost_perfectly_placed_spikes'][FF_ind]

        FF_error_index = list(curr_loaded_results_dict['all_results_curves']['F&F, %d connections' %(num_M_conn)]['num_spikes']).index(FF_num_spikes)
        FF_error_scale = (curr_loaded_results_dict['all_results_curves']['F&F, %d connections' %(num_M_conn)]['num_spikes'][FF_error_index + 1] -
                          curr_loaded_results_dict['all_results_curves']['F&F, %d connections' %(num_M_conn)]['num_spikes'][max(0, FF_error_index - 1)])

        if FF_error_index > 1:
            FF_error_scale /= 2

        IF_ind = list(curr_loaded_results_dict['processed_res_curves']['I&F']['connections_per_axon']).index(num_M_conn)
        IF_num_spikes = curr_loaded_results_dict['processed_res_curves']['I&F']['num_almost_perfectly_placed_spikes'][IF_ind]

        IF_error_index = list(curr_loaded_results_dict['all_results_curves']['I&F, %d connections' %(num_M_conn)]['num_spikes']).index(IF_num_spikes)
        IF_error_scale = (curr_loaded_results_dict['all_results_curves']['F&F, %d connections' %(num_M_conn)]['num_spikes'][IF_error_index + 1] -
                          curr_loaded_results_dict['all_results_curves']['F&F, %d connections' %(num_M_conn)]['num_spikes'][max(0, IF_error_index - 1)])

        if IF_error_index > 1:
            IF_error_scale /= 2

        FF_num_placed_spikes[num_M_conn]['num_accurately_placed_spikes'].append(FF_num_spikes)
        FF_num_placed_spikes[num_M_conn]['num_accurately_placed_spikes_error'].append(FF_error_scale)
        IF_num_placed_spikes[num_M_conn]['num_accurately_placed_spikes'].append(IF_num_spikes)
        IF_num_placed_spikes[num_M_conn]['num_accurately_placed_spikes_error'].append(IF_error_scale)


fig = plt.figure(figsize=(25,16))

for M in [10,3]:
    plt.errorbar(num_axons_list, FF_num_placed_spikes[M]['num_accurately_placed_spikes'], yerr=FF_num_placed_spikes[M]['num_accurately_placed_spikes_error'], label='F&F (M = %d)' %(M), lw=5)
plt.errorbar(num_axons_list, IF_num_placed_spikes[1]['num_accurately_placed_spikes'], yerr=FF_num_placed_spikes[1]['num_accurately_placed_spikes_error'], label='I&F', lw=5)

plt.legend(loc='upper left', fontsize=30)
plt.title('Capacity Linearly scales with Number of Axons', fontsize=30)
plt.ylabel('Number of Accuratley Timed Spikes', fontsize=25)
plt.xlabel('Number of Input Axons', fontsize=25);

#%% Full Figure 2

plt.close('all')
fig = plt.figure(figsize=(25,16))
gs_figure = gridspec.GridSpec(nrows=13,ncols=6)
gs_figure.update(left=0.04, right=0.95, bottom=0.05, top=0.95, wspace=0.45, hspace=0.4)

ax_axons            = plt.subplot(gs_figure[:4,:4])
ax_before_learning  = plt.subplot(gs_figure[4,:4])
ax_after_learning   = plt.subplot(gs_figure[5,:4])
ax_desired_output   = plt.subplot(gs_figure[6,:4])
ax_acc_per_n_spikes = plt.subplot(gs_figure[8:,:4])

ax_n_spikes_m_cons  = plt.subplot(gs_figure[:6,4:])
ax_n_spikes_n_axons = plt.subplot(gs_figure[7:,4:])

# 2 A
before_color = '0.15'
after_color  = 'blue'
target_color = 'red'

syn_activation_time, syn_activation_index = np.nonzero(axons_input_spikes.T)
syn_activation_time = syn_activation_time / 1000

min_time_sec = 0
max_time_sec = stimulus_duration_ms / 1000

time_sec = np.linspace(min_time_sec, max_time_sec, output_spikes.shape[0])

ax_axons.scatter(syn_activation_time, syn_activation_index, s=3, c='k'); ax_axons.set_title('Input Axons Raster', fontsize=18)
ax_axons.set_xlim(min_time_sec, max_time_sec);
ax_axons.set_xticks([])
ax_axons.set_yticks([0,25,50,75,100])
ax_axons.set_yticklabels([0,25,50,75,100],fontsize=15)
ax_axons.spines['top'].set_visible(False)
ax_axons.spines['bottom'].set_visible(False)
ax_axons.spines['left'].set_visible(False)
ax_axons.spines['right'].set_visible(False)

ax_before_learning.plot(time_sec, output_spikes, c=before_color, lw=2.5);
ax_before_learning.set_title('Before Learning', fontsize=17, color=before_color)
ax_before_learning.set_xlim(min_time_sec, max_time_sec);
ax_before_learning.set_xticks([])
ax_before_learning.set_yticks([])
ax_before_learning.spines['top'].set_visible(False)
ax_before_learning.spines['bottom'].set_visible(False)
ax_before_learning.spines['left'].set_visible(False)
ax_before_learning.spines['right'].set_visible(False)

ax_after_learning.plot(time_sec, output_spikes_after_learning, c=after_color, lw=2.5);
ax_after_learning.set_title('After Learning', fontsize=17, color=after_color)
ax_after_learning.set_xlim(min_time_sec, max_time_sec);
ax_after_learning.set_xticks([])
ax_after_learning.set_yticks([])
ax_after_learning.spines['top'].set_visible(False)
ax_after_learning.spines['bottom'].set_visible(False)
ax_after_learning.spines['left'].set_visible(False)
ax_after_learning.spines['right'].set_visible(False)

ax_desired_output.plot(time_sec, desired_output_spikes, c=target_color, lw=2.5);
ax_desired_output.set_title('Desired Output (num spikes = %d)' %(requested_number_of_output_spikes), fontsize=17, color=target_color);
ax_desired_output.set_xlim(min_time_sec, max_time_sec);
ax_desired_output.set_yticks([]);
ax_desired_output.set_xticks([0,15,30,45,60])
ax_desired_output.set_xticklabels([0,15,30,45,60],fontsize=15)
ax_desired_output.spines['top'].set_visible(False)
ax_desired_output.spines['bottom'].set_visible(False)
ax_desired_output.spines['left'].set_visible(False)
ax_desired_output.spines['right'].set_visible(False)
ax_desired_output.set_xlabel('time (sec)', fontsize=17);


# 2 B
num_plots = len(all_results_curves.keys())

key_to_label_map = {
    'F&F, 1 connections' : 'F&F (M =  1)',
    'F&F, 2 connections' : 'F&F (M =  2)',
    'F&F, 3 connections' : 'F&F (M =  3)',
    'F&F, 5 connections' : 'F&F (M =  5)',
    'F&F, 10 connections': 'F&F (M = 10)',
    'F&F, 15 connections': 'F&F (M = 15)',
    'I&F, 1 connections' : 'I&F (M =  1)',
    'I&F, 2 connections' : 'I&F (M =  2)',
    'I&F, 3 connections' : 'I&F (M =  3)',
    'I&F, 5 connections' : 'I&F (M =  5)',
    'I&F, 10 connections': 'I&F (M = 10)',
    'I&F, 15 connections': 'I&F (M = 15)'}

key_to_color_map = {
    'F&F, 1 connections' : 'blue',
    'F&F, 2 connections' : 'orange',
    'F&F, 3 connections' : 'green',
    'F&F, 5 connections' : 'crimson',
    'F&F, 10 connections': 'brown',
    'F&F, 15 connections': 'purple',
    'I&F, 1 connections' : 'gray',
    'I&F, 2 connections' : 'gray',
    'I&F, 3 connections' : 'gray',
    'I&F, 5 connections' : 'gray',
    'I&F, 10 connections': 'gray',
    'I&F, 15 connections': 'gray'}

keys_ordering = ['I&F, 1 connections', 'I&F, 2 connections', 'I&F, 3 connections', 'I&F, 5 connections', 'I&F, 10 connections', 'I&F, 15 connections',
                 'F&F, 1 connections', 'F&F, 2 connections', 'F&F, 3 connections', 'F&F, 5 connections', 'F&F, 10 connections', 'F&F, 15 connections']

# for key, value in all_results_curves.items():
for key in keys_ordering:
    value = all_results_curves[key]
    curr_color = key_to_color_map[key]
    curr_label = key_to_label_map[key]
    if curr_color == 'gray':
        ax_acc_per_n_spikes.errorbar(value['num_spikes'], value['mean_AUC'], yerr=value['std_AUC'] / np.sqrt(num_random_iterations),
                                     label=curr_label, color=curr_color, lw=3, alpha=0.6)
    else:
        ax_acc_per_n_spikes.errorbar(value['num_spikes'], value['mean_AUC'], yerr=value['std_AUC'] / np.sqrt(num_random_iterations),
                                     label=curr_label, color=curr_color, lw=3)

ax_acc_per_n_spikes.legend(loc='lower left', fontsize=18, ncol=2)
ax_acc_per_n_spikes.set_title('Placing Precisely Timed output Spikes', fontsize=18)
ax_acc_per_n_spikes.set_ylabel('Accuracy at 1ms Precision (AUC)', fontsize=17)
ax_acc_per_n_spikes.set_xlabel('Number of Requried Precisely Timed Spikes', fontsize=17);
ax_acc_per_n_spikes.spines['top'].set_visible(False)
ax_acc_per_n_spikes.spines['right'].set_visible(False)
ax_acc_per_n_spikes.set_yticks([0.7,0.8,0.9,1.0])
ax_acc_per_n_spikes.set_yticklabels([0.7,0.8,0.9,1.0],fontsize=15)
ax_acc_per_n_spikes.set_xticks([0,50,100,150,200])
ax_acc_per_n_spikes.set_xticklabels([0,50,100,150,200],fontsize=15)


# 2 C
for key in processed_res_curves.keys():
    y_error = precisely_timed_spikes_per_axon_2C[key].std(axis=0)
    ax_n_spikes_m_cons.errorbar(connections_per_axon_2C, precisely_timed_spikes_per_axon_2C[key].mean(axis=0), yerr=y_error, label=key, lw=4, color=color_map[key])

ax_n_spikes_m_cons.legend(loc='upper left', fontsize=22)
ax_n_spikes_m_cons.set_title('Placing Precisely Timed output Spikes', fontsize=18)
ax_n_spikes_m_cons.set_xlabel('Number of Multiple Contacts - M', fontsize=17)
ax_n_spikes_m_cons.set_ylabel('Number of Precisely Timed Spikes / Input Axon', fontsize=17);
ax_n_spikes_m_cons.spines['top'].set_visible(False)
ax_n_spikes_m_cons.spines['right'].set_visible(False)
ax_n_spikes_m_cons.set_yticks([0.15,0.3,0.45])
ax_n_spikes_m_cons.set_ylim([0.08,0.53])
ax_n_spikes_m_cons.set_yticklabels([0.15,0.3,0.45], fontsize=15)
ax_n_spikes_m_cons.set_xticks([1,2,3,5,10,15])
ax_n_spikes_m_cons.set_xticklabels([1,2,3,5,10,15], fontsize=15)


# 2 D
for M in [10,3]:
    ax_n_spikes_n_axons.errorbar(num_axons_list, FF_num_placed_spikes[M]['num_accurately_placed_spikes'], yerr=FF_num_placed_spikes[M]['num_accurately_placed_spikes_error'], label='F&F (M = %d)' %(M), lw=5)
ax_n_spikes_n_axons.errorbar(num_axons_list, IF_num_placed_spikes[1]['num_accurately_placed_spikes'], yerr=FF_num_placed_spikes[1]['num_accurately_placed_spikes_error'], label='I&F', lw=5)

ax_n_spikes_n_axons.legend(loc='upper left', fontsize=22)
ax_n_spikes_n_axons.set_title('Capacity Linearly Scales with Number of Axons', fontsize=18)
ax_n_spikes_n_axons.set_ylabel('Number of Precisely Timed Spikes', fontsize=17)
ax_n_spikes_n_axons.set_xlabel('Number of Input Axons', fontsize=17);
ax_n_spikes_n_axons.set_yticks([0,100,200])
ax_n_spikes_n_axons.set_yticklabels([0,100,200], fontsize=15)
ax_n_spikes_n_axons.set_xticks(num_axons_list)
ax_n_spikes_n_axons.set_xticklabels(num_axons_list, fontsize=15)
ax_n_spikes_n_axons.spines['top'].set_visible(False)
ax_n_spikes_n_axons.spines['right'].set_visible(False)


# save figure
if save_figures:
    figure_name = 'F&F_capacity_Figure_2_%d' %(np.random.randint(200))
    for file_ending in all_file_endings_to_use:
        if file_ending == '.png':
            fig.savefig(figure_folder + figure_name + file_ending, bbox_inches='tight')
        else:
            fig.savefig(figure_folder + figure_name + file_ending, bbox_inches='tight')

================================================================================
================================================================================
saved_figures/F&F_Capacity_Figure_2_3.png:
==========================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a comprehensive analysis of a neural network's performance in generating precisely timed output spikes. It's structured into four main sections, each using a combination of raster plots, graphs, and legends to convey information.

The top-left section shows the input axon raster plot, depicting the spiking activity of input neurons before and after a learning process.  Below this, a zoomed-in timeline explicitly displays the desired output spikes (in red and blue) compared to the actual output after learning. This visual comparison highlights the network's success in learning to generate the desired precise timing of spikes.

The top-right and bottom-right sections contain bar and line graphs, respectively, which quantify the network's capacity. The top-right graph analyzes the number of precisely timed output spikes generated by two different neuron models ("F&F" and "I&F") as a function of the number of multiple contacts (M). The bottom-right graph shows how the number of precisely timed spikes scales with the number of input axons, again for both neuron models and different values of M. These graphs demonstrate the network's capacity and its scalability with different parameters.

Finally, the bottom-left section presents a detailed analysis of the network's accuracy.  Multiple lines depict accuracy (Area Under the Curve, or AUC) at 1ms precision for both neuron models ("F&F" and "I&F"), each with varying values of M, as a function of the number of required precisely timed spikes.  This graph provides a precise measure of the network's performance across different conditions.  Error bars are included to show variability.  In summary, the image uses multiple visualization techniques to thoroughly examine the performance and capabilities of a specific neural network model.

================================================================================
================================================================================
FF_vs_IF_capacity_comparison_interactions.py:
=============================================
import sys
import numpy as np
import time
import matplotlib.pyplot as plt
from scipy import signal
from sklearn import linear_model
from sklearn.metrics import roc_auc_score
import pickle

from warnings import simplefilter
from sklearn.exceptions import ConvergenceWarning
simplefilter("ignore", category=ConvergenceWarning)

#%% script params

start_time = time.time()

try:
    print('----------------------------')
    random_seed = int(sys.argv[1])
    num_axons = int(sys.argv[2])
    use_interaction_terms = (sys.argv[3] == 'True')
    interactions_degree = int(sys.argv[4])
    print('"random_seed" selected by user - %d' %(random_seed))
    print('"num_axons" selected by user - %d' %(num_axons))
    print('"use_interaction_terms" selected by user - %s' %(use_interaction_terms))
    print('"interactions_degree" selected by user - %d' %(interactions_degree))

    determine_internally = False
except:
    determine_internally = True
    try:
        random_seed = int(sys.argv[1])
        print('random seed selected by user - %d' %(random_seed))
    except:
        random_seed = np.random.randint(100000)
        print('randomly choose seed - %d' %(random_seed))

np.random.seed(random_seed)
print('----------------------------')


# input parameters
if determine_internally:
    num_axons = 100
    use_interaction_terms = True
    interactions_degree  = 2


# experiment parameters
stimulus_duration_sec = 90
min_time_between_spikes_ms = 110

instantanious_input_spike_probability = 0.004
almost_prefect_accuracy_AUC_threshold = 0.99

# full
model_type_list = ['F&F', 'I&F']
connections_per_axon_list = [1,3,5,10,15]
req_num_output_spikes_list = np.linspace(5,200,40).astype(int)
req_num_output_spikes_list = np.logspace(np.log10(11),np.log10(200),40).astype(int)
random_iterations_list = np.linspace(1,10,10).astype(int)

# neuron model parameters
v_reset     = -80
v_threshold = -55
current_to_voltage_mult_factor = 3
refreactory_time_constant = 15

tau_rise_range_FF  = [ 1,16]
tau_decay_range_FF = [ 8,30]
tau_rise_range_IF  = [ 1, 1]
tau_decay_range_IF = [30,30]

show_plots = True
show_plots = False

data_folder = '/filter_and_fire_neuron/results_data_capacity/'

#%% helper functions

def create_single_PSP_profile(tau_rise, tau_decay, temporal_filter_length=50):

    safety_factor = 1.5
    if tau_rise >= (tau_decay / safety_factor):
        tau_decay = safety_factor * tau_rise

    exp_r = signal.exponential(M=temporal_filter_length, center=0, tau=tau_rise , sym=False)
    exp_d = signal.exponential(M=temporal_filter_length, center=0, tau=tau_decay, sym=False)

    post_syn_potential = exp_d - exp_r
    post_syn_potential /= post_syn_potential.max()

    return post_syn_potential


def construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec):

    num_synapses = tau_rise_vec.shape[0]
    temporal_filter_length = int(4 * tau_decay_vec.max()) + 1

    syn_filter = np.zeros((num_synapses, temporal_filter_length))

    for k, (tau_r, tau_d) in enumerate(zip(tau_rise_vec, tau_decay_vec)):
        syn_filter[k,:] = create_single_PSP_profile(tau_r, tau_d, temporal_filter_length=temporal_filter_length)

    return syn_filter


def simulate_filter_and_fire_cell_training(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                           refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):

    temporal_filter_length = int(5 * refreactory_time_constant) + 1
    refreactory_filter = signal.exponential(M=temporal_filter_length,center=0,tau=refreactory_time_constant,sym=False)[np.newaxis,:]

    # padd input and get all synaptic filters
    normlized_syn_filter = np.flipud(construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec))
    padded_input = np.hstack((np.zeros(normlized_syn_filter.shape), presynaptic_input_spikes))

    # calc local currents
    local_normlized_currents = np.zeros(presynaptic_input_spikes.shape)
    for k in range(normlized_syn_filter.shape[0]):
        local_normlized_currents[k] = signal.convolve(padded_input[k], normlized_syn_filter[k], mode='valid')[1:]

    # multiply by weights to get the somatic current
    soma_current = signal.convolve(local_normlized_currents, synaptic_weights, mode='valid')

    # simulate the cell
    soma_voltage = v_reset + current_to_voltage_mult_factor * soma_current.ravel()
    output_spike_times_in_ms = []
    # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
    for t in range(len(soma_voltage)):
        # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
        if (soma_voltage[t] > v_threshold) and ((t + 1) < len(soma_voltage)):
            t_start = t + 1
            t_end = min(len(soma_voltage), t_start + temporal_filter_length)
            soma_voltage[t_start:t_end] -= (soma_voltage[t + 1] - v_reset) * refreactory_filter.ravel()[:(t_end - t_start)]
            output_spike_times_in_ms.append(t)

    return local_normlized_currents, soma_voltage, output_spike_times_in_ms


def simulate_filter_and_fire_cell_inference(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                            refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):

    temporal_filter_length = int(5 * refreactory_time_constant) + 1
    refreactory_filter = signal.exponential(M=temporal_filter_length,center=0,tau=refreactory_time_constant,sym=False)[np.newaxis,:]

    # padd input and get all synaptic filters
    normlized_syn_filter = construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec)
    padded_input = np.hstack((np.zeros(normlized_syn_filter.shape), presynaptic_input_spikes))

    # calc somatic current
    weighted_syn_filter  = synaptic_weights * normlized_syn_filter
    soma_current = signal.convolve(padded_input, weighted_syn_filter, mode='valid')[:,1:]

    # simulate the cell
    soma_voltage = v_reset + current_to_voltage_mult_factor * soma_current.ravel()
    output_spike_times_in_ms = []
    # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
    for t in range(len(soma_voltage)):

        # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
        if (soma_voltage[t] > v_threshold) and ((t + 1) < len(soma_voltage)):
            t_start = t + 1
            t_end = min(len(soma_voltage), t_start + temporal_filter_length)
            soma_voltage[t_start:t_end] -= (soma_voltage[t + 1] - v_reset) * refreactory_filter.ravel()[:(t_end - t_start)]
            output_spike_times_in_ms.append(t)

    return soma_voltage, output_spike_times_in_ms


# use local currents as "features" and fit a linear model to the data
def prepare_training_dataset(local_normlized_currents, desired_output_spikes, spike_safety_range_ms=10, negative_subsampling_fraction=0.1):

    # remove all "negative" time points that are too close to spikes
    desired_output_spikes_LPF = signal.convolve(desired_output_spikes, np.ones((spike_safety_range_ms,)), mode='same') > 0.1
    desired_timepoints = ~desired_output_spikes_LPF

    # massivly subsample the remaining timepoints
    desired_timepoints[np.random.rand(desired_timepoints.shape[0]) > negative_subsampling_fraction] = 0
    desired_timepoints[desired_output_spikes > 0.1] = 1

    X = local_normlized_currents.T[desired_timepoints,:]
    y = desired_output_spikes[desired_timepoints]

    return X, y


def apply_dendritic_interactions(normlized_synaptic_currents, interactions_degree=2, non_interaction_fraction=0.2):
    output_normlized_synaptic_currents = normlized_synaptic_currents.copy()

    # apply d times random interactions
    for degree in range(interactions_degree - 1):
        random_permutation = np.random.permutation(normlized_synaptic_currents.shape[0])
        output_normlized_synaptic_currents = output_normlized_synaptic_currents * normlized_synaptic_currents[random_permutation]

    # keep some fraction of only individual interactions
    random_permutation = np.random.permutation(normlized_synaptic_currents.shape[0])[:int(normlized_synaptic_currents.shape[0] * non_interaction_fraction)]
    output_normlized_synaptic_currents[random_permutation] = normlized_synaptic_currents[random_permutation]

    return output_normlized_synaptic_currents


#%% Fit I&F and F&F several times with variable multiple connections and different requested number of spikes

stimulus_duration_ms = 1000 * stimulus_duration_sec

all_results_dicts = []
print('------------------------------------------------------------------------------------------')
for model_type in model_type_list:
    for connections_per_axon in connections_per_axon_list:
        for requested_number_of_output_spikes in req_num_output_spikes_list:

            #print('------------------')
            #print('model type = "%s"' %(model_type))
            #print('number of connections per axon = %d' %(connections_per_axon))
            #print('number of requested output spikes = %d' %(requested_number_of_output_spikes))
            #print('------------------')

            full_AUC_list = []
            result_entry_dict = {}

            result_entry_dict['model type'] = model_type
            result_entry_dict['connections per axon'] = connections_per_axon
            result_entry_dict['num output spikes'] = requested_number_of_output_spikes

            for random_iter in random_iterations_list:

                # neuron model parameters
                num_synapses = connections_per_axon * num_axons

                # synapse non-learnable parameters
                if model_type == 'F&F':
                    tau_rise_range  = tau_rise_range_FF
                    tau_decay_range = tau_decay_range_FF
                elif model_type == 'I&F':
                    tau_rise_range  = tau_rise_range_IF
                    tau_decay_range = tau_decay_range_IF

                tau_rise_vec  = np.random.uniform(low=tau_rise_range[0] , high=tau_rise_range[1] , size=(num_synapses, 1))
                tau_decay_vec = np.random.uniform(low=tau_decay_range[0], high=tau_decay_range[1], size=(num_synapses, 1))

                # synapse learnable parameters
                synaptic_weights_vec = np.random.normal(size=(num_synapses, 1))

                # generate sample input
                axons_input_spikes = np.random.rand(num_axons, stimulus_duration_ms) < instantanious_input_spike_probability
                presynaptic_input_spikes = np.kron(np.ones((connections_per_axon,1)), axons_input_spikes)
                assert presynaptic_input_spikes.shape[0] == num_synapses, 'number of synapses doesnt match the number of presynaptic inputs'

                # generate desired pattern of output spikes

                desired_output_spike_times = min_time_between_spikes_ms * np.random.randint(int(stimulus_duration_ms / min_time_between_spikes_ms), size=requested_number_of_output_spikes)
                desired_output_spike_times = np.sort(np.unique(desired_output_spike_times))

                desired_output_spikes = np.zeros((stimulus_duration_ms,))
                desired_output_spikes[desired_output_spike_times] = 1.0

                # simulate cell with normlized currents
                local_normlized_currents, soma_voltage, output_spike_times_in_ms = simulate_filter_and_fire_cell_training(presynaptic_input_spikes,
                                                                                                                          synaptic_weights_vec, tau_rise_vec, tau_decay_vec,
                                                                                                                          refreactory_time_constant=refreactory_time_constant,
                                                                                                                          v_reset=v_reset, v_threshold=v_threshold,
                                                                                                                          current_to_voltage_mult_factor=current_to_voltage_mult_factor)


                # normlized_synaptic_currents = local_normlized_currents
                # plt.close('all')
                # plt.figure(figsize=(16,10))
                # plt.subplot(2,1,1); plt.imshow(normlized_synaptic_currents[:,:1000])
                # plt.subplot(2,1,2); plt.imshow(apply_dendritic_interactions(normlized_synaptic_currents, interactions_degree=2, non_interaction_fraction=0.001)[:,:1000])

                if use_interaction_terms:
                    non_interaction_fraction = num_axons / local_normlized_currents.shape[0]
                    local_normlized_currents = apply_dendritic_interactions(local_normlized_currents, interactions_degree=interactions_degree, non_interaction_fraction=non_interaction_fraction)

                # fit linear model to local currents
                logistic_reg_model = linear_model.LogisticRegression(C=100000, fit_intercept=False, penalty='l2', solver='lbfgs', verbose=False)

                spike_safety_range_ms = 5
                negative_subsampling_fraction = 0.5

                # use local currents as "features" and fit a linear model to the data
                X, y = prepare_training_dataset(local_normlized_currents, desired_output_spikes,
                                                spike_safety_range_ms=spike_safety_range_ms,
                                                negative_subsampling_fraction=negative_subsampling_fraction)
                #print('number of extracted data points for training = %d (%.2f%s positive class)' %(X.shape[0], 100 * y.mean(),'%'))

                # fit model
                logistic_reg_model.fit(X,y)

                # predict and calculate AUC on train data
                y_hat = logistic_reg_model.predict_proba(X)[:,1]
                train_AUC = roc_auc_score(y, y_hat)

                # predict and calculate AUC on full trace
                fitted_output_spike_prob = logistic_reg_model.predict_proba(local_normlized_currents.T)[:,1]
                full_AUC = roc_auc_score(desired_output_spikes, fitted_output_spike_prob)

                # print progress and save result
                #print('random iteration %d: (train AUC, full trace AUC) = (%.5f, %.5f)' %(random_iter, train_AUC, full_AUC))
                full_AUC_list.append(full_AUC)

            # convert to array:
            full_AUC_vec = np.array(full_AUC_list)
            mean_AUC = full_AUC_vec.mean()
            std_AUC = full_AUC_vec.std()

            result_entry_dict['full_AUC_vec'] = full_AUC_vec
            result_entry_dict['mean_AUC'] = mean_AUC
            result_entry_dict['std_AUC'] = std_AUC

            # append
            all_results_dicts.append(result_entry_dict)
            print(result_entry_dict)
            print('------------------------------------------------------------------------------------------')


#%% process results dict for showing capacity results

all_results_curves = {}


for model_type in model_type_list:
    for connections_per_axon in connections_per_axon_list:

        num_spikes_list = []
        mean_AUC_list = []
        std_AUC_list = []

        for res_dict in all_results_dicts:
            if model_type == res_dict['model type'] and connections_per_axon == res_dict['connections per axon']:
                num_spikes_list.append(res_dict['num output spikes'])
                mean_AUC_list.append(res_dict['mean_AUC'])
                std_AUC_list.append(res_dict['std_AUC'])

        num_spikes = np.array(num_spikes_list)
        mean_AUC = np.array(mean_AUC_list)
        std_AUC = np.array(std_AUC_list)

        sorted_inds = np.argsort(num_spikes)

        num_spikes = num_spikes[sorted_inds]
        mean_AUC = mean_AUC[sorted_inds]
        std_AUC = std_AUC[sorted_inds]

        try:
            almost_perfect_accuracy_inds = np.where(mean_AUC > almost_prefect_accuracy_AUC_threshold)
            num_almost_perfectly_placed_spikes = num_spikes[almost_perfect_accuracy_inds[0][-1]]

            dict_key = '%s, %d connections' %(model_type, connections_per_axon)
            all_results_curves[dict_key] = {}
            all_results_curves[dict_key]['num_spikes'] = num_spikes
            all_results_curves[dict_key]['mean_AUC'] = mean_AUC
            all_results_curves[dict_key]['std_AUC'] = std_AUC
            all_results_curves[dict_key]['model_type'] = model_type
            all_results_curves[dict_key]['connections_per_axon'] = connections_per_axon
            all_results_curves[dict_key]['num_almost_perfectly_placed_spikes'] = num_almost_perfectly_placed_spikes
        except:
            print('something wrong skipping')


#%% Accuracy as function of spikes for various conditions

if show_plots:
    plt.figure(figsize=(30,15))
    for key, value in all_results_curves.items():
        plt.errorbar(value['num_spikes'], value['mean_AUC'], yerr=value['std_AUC'], label=key)
    plt.legend(loc='upper right', fontsize=23, ncol=2)
    plt.title('learning to place precisly timed output spikes (random input, %d sec window, %d axons)' %(stimulus_duration_sec, num_axons), fontsize=24)
    plt.ylabel('accuracy at 1ms precision (AUC)', fontsize=24)
    plt.xlabel('num requested spikes to place', fontsize=24)

#%% Number of precisly placed spikes as function of num multiple connections

processed_res_curves = {}

for model_type in model_type_list:

    connections_per_axon = []
    num_almost_perfectly_placed_spikes = []

    for res_dict in all_results_curves.values():
        if model_type == res_dict['model_type']:
            connections_per_axon.append(res_dict['connections_per_axon'])
            num_almost_perfectly_placed_spikes.append(res_dict['num_almost_perfectly_placed_spikes'])

    connections_per_axon = np.array(connections_per_axon)
    num_almost_perfectly_placed_spikes = np.array(num_almost_perfectly_placed_spikes)

    sorted_inds = np.argsort(connections_per_axon)

    connections_per_axon = connections_per_axon[sorted_inds]
    num_almost_perfectly_placed_spikes = num_almost_perfectly_placed_spikes[sorted_inds]

    dict_key = model_type
    processed_res_curves[dict_key] = {}

    processed_res_curves[dict_key]['connections_per_axon'] = connections_per_axon
    processed_res_curves[dict_key]['num_almost_perfectly_placed_spikes'] = num_almost_perfectly_placed_spikes

    print('-----')
    print(model_type)
    print(processed_res_curves[dict_key])
    print('-------------------')


if show_plots:
    plt.figure(figsize=(30,12))
    for key, value in processed_res_curves.items():
        plt.plot(value['connections_per_axon'], value['num_almost_perfectly_placed_spikes'], label=key)
    plt.title('learning to place precisly timed output spikes (random input, %d sec window, %d axons)' %(stimulus_duration_sec, num_axons), fontsize=24)
    plt.xlabel('num multiple connections per axon', fontsize=24)
    plt.ylabel('num almost perfectly placed spikes', fontsize=24)
    plt.legend(loc='upper left', fontsize=23)


#%% Save results for Nicer presentation later on

# collect main script parameters
script_main_params = {}
script_main_params['num_axons']                     = num_axons
script_main_params['stimulus_duration_sec']         = stimulus_duration_sec
script_main_params['min_time_between_spikes_ms']    = min_time_between_spikes_ms
script_main_params['refreactory_time_constant']     = refreactory_time_constant
script_main_params['spike_safety_range_ms']         = spike_safety_range_ms
script_main_params['negative_subsampling_fraction'] = negative_subsampling_fraction
script_main_params['num_random_iter']               = len(random_iterations_list)
script_main_params['use_interaction_terms']         = use_interaction_terms
script_main_params['interactions_degree']           = interactions_degree

script_main_params['almost_prefect_accuracy_AUC_threshold'] = almost_prefect_accuracy_AUC_threshold
script_main_params['instantanious_input_spike_probability'] = instantanious_input_spike_probability
script_main_params['connections_per_axon_list']  = connections_per_axon_list
script_main_params['req_num_output_spikes_list'] = req_num_output_spikes_list
script_main_params['tau_rise_range_FF']  = tau_rise_range_FF
script_main_params['tau_decay_range_FF'] = tau_decay_range_FF
script_main_params['tau_rise_range_IF']  = tau_rise_range_IF
script_main_params['tau_decay_range_IF'] = tau_decay_range_IF
script_main_params['v_reset']     = v_reset
script_main_params['v_threshold'] = v_threshold
script_main_params['current_to_voltage_mult_factor'] = current_to_voltage_mult_factor

# collect main script results
script_results_dict = {}
script_results_dict['script_main_params']   = script_main_params
script_results_dict['all_results_dicts']    = all_results_dicts
script_results_dict['all_results_curves']   = all_results_curves
script_results_dict['processed_res_curves'] = processed_res_curves

# filename
if use_interaction_terms:
    results_filename = 'FF_vs_IF_capacity_comparision_interactions_degree_%d_num_axons_%d__sim_duration_sec_%d__num_mult_conn_%d__rand_rep_%d.pickle' %(interactions_degree, num_axons, stimulus_duration_sec, len(connections_per_axon_list), len(random_iterations_list))
else:
    results_filename = 'FF_vs_IF_capacity_comparision__num_axons_%d__sim_duration_sec_%d__num_mult_conn_%d__rand_rep_%d.pickle' %(num_axons, stimulus_duration_sec, len(connections_per_axon_list), len(random_iterations_list))

# pickle everythin
pickle.dump(script_results_dict, open(data_folder + results_filename, "wb"))

#%% Load the saved pickle just to check it's OK

loaded_script_results_dict = pickle.load(open(data_folder + results_filename, "rb" ))

print('-----------------------------------------------------------------------------------------------------------')
print('loaded_script_results_dict.keys():')
print('----------')
print(loaded_script_results_dict.keys())
print('-----------------------------------------------------------------------------------------------------------')
print('loaded_script_results_dict["script_main_params"].keys():')
print('----------')
[print(x) for x in loaded_script_results_dict['script_main_params'].keys()]
print('-----------------------------------------------------------------------------------------------------------')
print('num_axons =', loaded_script_results_dict['script_main_params']['num_axons'])
print('stimulus_duration_sec =', loaded_script_results_dict['script_main_params']['stimulus_duration_sec'])
print('min_time_between_spikes_ms =', loaded_script_results_dict['script_main_params']['min_time_between_spikes_ms'])
print('refreactory_time_constant =', loaded_script_results_dict['script_main_params']['refreactory_time_constant'])
print('num_random_iter =', loaded_script_results_dict['script_main_params']['num_random_iter'])
print('spike_safety_range_ms =', loaded_script_results_dict['script_main_params']['spike_safety_range_ms'])
print('negative_subsampling_fraction =', loaded_script_results_dict['script_main_params']['negative_subsampling_fraction'])
print('-----------------------------------------------------------------------------------------------------------')

script_duration_min = (time.time() - start_time) / 60
print('-----------------------------------')
print('finished script! took %.1f minutes' %(script_duration_min))
print('-----------------------------------')

================================================================================
================================================================================
saved_figures/F&F_MNIST_Figure_3_7.png:
=======================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
This image presents a comprehensive analysis of a spiking neural network's performance in recognizing handwritten digits.  The top section showcases the input data: a sequence of handwritten digits ("0" through "9") represented as raster plots. Below, the network's response before and after learning is shown, along with the desired output.  This visually demonstrates the learning process.

The main body of the image contains several graphs and subplots that analyze the network's accuracy. One graph compares the accuracy across different digits for various network models (Integrate & Fire, Filter & Fire, Spatio-Temporal Logistic Regression), highlighting the strengths and weaknesses of each.  Other graphs explore the effects of varying parameters like pattern presentation duration (T), the number of multiple contacts (M), and the number of positive training patterns (N) on the network's test accuracy.  Error bars are included to indicate the variability in the results.

Finally, the image includes a set of raster plots illustrating the spatio-temporal activity of the different neuron models (Filter & Fire, Integrate & Fire, Spatio-Temporal Logistic Regression) at both the single trial and averaged levels.  These plots provide a visual representation of how different models process the input data and contribute to the overall classification accuracy.  The combination of these visual and quantitative results offers a thorough evaluation of the spiking neural network's capabilities in digit recognition.

================================================================================
================================================================================
create_explanatory_figure_Fig4.py:
==================================
import numpy as np
import pickle
import matplotlib
import matplotlib.pyplot as plt
from scipy import signal
from sklearn import linear_model
from sklearn.metrics import roc_curve, roc_auc_score, auc
from sklearn.decomposition import TruncatedSVD, NMF
import matplotlib.gridspec as gridspec

matplotlib.rcParams['pdf.fonttype'] = 42
matplotlib.rcParams['svg.fonttype'] = 'none'

#%% script params

save_figures = True
save_figures = False
all_file_endings_to_use = ['.png', '.pdf', '.svg']

data_folder   = '/filter_and_fire_neuron/results_data_capacity/'
figure_folder = '/filter_and_fire_neuron/saved_figures/'

#%% helper functions

def create_single_PSP_profile(tau_rise, tau_decay, temporal_filter_length=50):

    safety_factor = 1.5
    if tau_rise >= (tau_decay / safety_factor):
        tau_decay = safety_factor * tau_rise

    exp_r = signal.exponential(M=temporal_filter_length, center=0, tau=tau_rise , sym=False)
    exp_d = signal.exponential(M=temporal_filter_length, center=0, tau=tau_decay, sym=False)

    post_syn_potential = exp_d - exp_r
    post_syn_potential /= post_syn_potential.max()

    return post_syn_potential


def construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec):

    num_synapses = tau_rise_vec.shape[0]
    temporal_filter_length = int(7 * tau_decay_vec.max()) + 1

    syn_filter = np.zeros((num_synapses, temporal_filter_length))

    for k, (tau_r, tau_d) in enumerate(zip(tau_rise_vec, tau_decay_vec)):
        syn_filter[k,:] = create_single_PSP_profile(tau_r, tau_d, temporal_filter_length=temporal_filter_length)

    return syn_filter


def simulate_filter_and_fire_cell_training(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                           refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):

    temporal_filter_length = int(5 * refreactory_time_constant) + 1
    refreactory_filter = signal.exponential(M=temporal_filter_length,center=0,tau=refreactory_time_constant,sym=False)[np.newaxis,:]

    # padd input and get all synaptic filters
    normlized_syn_filter = np.flipud(construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec))
    padded_input = np.hstack((np.zeros(normlized_syn_filter.shape), presynaptic_input_spikes))

    # calc local currents
    local_normlized_currents = np.zeros(presynaptic_input_spikes.shape)
    for k in range(normlized_syn_filter.shape[0]):
        local_normlized_currents[k] = signal.convolve(padded_input[k], normlized_syn_filter[k], mode='valid')[1:]

    # multiply by weights to get the somatic current
    soma_current = signal.convolve(local_normlized_currents, synaptic_weights, mode='valid')

    # simulate the cell
    soma_voltage = v_reset + current_to_voltage_mult_factor * soma_current.ravel()
    output_spike_times_in_ms = []
    # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
    for t in range(len(soma_voltage)):
        # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
        if (soma_voltage[t] > v_threshold) and ((t + 1) < len(soma_voltage)):
            t_start = t + 1
            t_end = min(len(soma_voltage), t_start + temporal_filter_length)
            soma_voltage[t_start:t_end] -= (soma_voltage[t + 1] - v_reset) * refreactory_filter.ravel()[:(t_end - t_start)]
            output_spike_times_in_ms.append(t)

    return local_normlized_currents, soma_voltage, output_spike_times_in_ms


def add_offset_for_plotting(traces_matrix, offset_size=1.1):

    traces_matrix_with_offset = offset_size * np.kron(np.arange(traces_matrix.shape[0])[:,np.newaxis], np.ones((1,traces_matrix.shape[1])))
    traces_matrix_with_offset = traces_matrix_with_offset + traces_matrix

    return traces_matrix_with_offset


#%% script params

# input parameters
num_values_per_param = 12

# neuron model parameters
connections_per_axon = 5
num_synapses = num_values_per_param * num_values_per_param

v_reset     = -80
v_threshold = -55
current_to_voltage_mult_factor = 3
refreactory_time_constant = 20

model_type = 'F&F'
#model_type = 'I&F'

time_limit_ms = 120

# synapse non-learnable parameters
if model_type == 'F&F':
    tau_rise_range  = [1,18]
    tau_decay_range = [7,25]
elif model_type == 'I&F':
    tau_rise_range  = [3,3]
    tau_decay_range = [25,25]

tau_rise_vec = np.linspace(tau_rise_range[0], tau_rise_range[1] , num_values_per_param)[:,np.newaxis]
tau_rise_vec = np.kron(np.ones((num_values_per_param,1)), tau_rise_vec)

tau_decay_vec = np.linspace(tau_decay_range[0], tau_decay_range[1] , num_values_per_param)[:,np.newaxis]
tau_decay_vec = np.kron(tau_decay_vec, np.ones((num_values_per_param,1)))

normlized_syn_filter_small = construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec)

offset_size = 0.15

plt.close('all')
plt.figure(figsize=(25,20));
plt.subplot(1,2,1); plt.imshow(normlized_syn_filter_small);
plt.title('normlized synaptic filters as heatmaps', fontsize=22); plt.xlabel('time [ms]', fontsize=22); plt.ylabel('synaptic filter index', fontsize=22); plt.xlim(0,time_limit_ms);
plt.subplot(1,2,2);

use_colors = False
if use_colors:
    colors = 'rgbymcrgbymc'

    end_ind = 0
    for k in range(num_values_per_param):
        start_ind = end_ind
        end_ind = start_ind + num_values_per_param
        print(start_ind, end_ind, colors[k])
        plt.plot(offset_size * k * num_values_per_param + add_offset_for_plotting(normlized_syn_filter_small[start_ind:end_ind], offset_size=offset_size).T, c=colors[k]);
else:
    plt.plot(add_offset_for_plotting(normlized_syn_filter_small, offset_size=offset_size).T, c='k');

plt.title('normlized synaptic filters as PSPs', fontsize=22); plt.xlabel('time [ms]', fontsize=22); plt.ylabel('normalized PSP (A.U)', fontsize=22); plt.xlim(-1,time_limit_ms);

#%% Create all possible combinations

# input parameters
num_values_per_param = 50

tau_rise_vec = np.linspace(tau_rise_range[0], tau_rise_range[1] , num_values_per_param)[:,np.newaxis]
tau_rise_vec = np.kron(np.ones((num_values_per_param,1)), tau_rise_vec)

tau_decay_vec = np.linspace(tau_decay_range[0], tau_decay_range[1] , num_values_per_param)[:,np.newaxis]
tau_decay_vec = np.kron(tau_decay_vec, np.ones((num_values_per_param,1)))

normlized_syn_filter_large = construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec)

plt.close('all')
plt.figure(figsize=(25,20));
plt.subplot(1,2,1); plt.imshow(normlized_syn_filter_large);
plt.title('normlized synaptic filters as heatmaps', fontsize=22); plt.xlabel('time [ms]', fontsize=22); plt.ylabel('synaptic filter index', fontsize=22)
plt.xlim(0,time_limit_ms);
plt.subplot(1,2,2); plt.plot(normlized_syn_filter_large.T, alpha=0.15);
plt.title('normlized synaptic filters as PSPs', fontsize=22); plt.xlabel('time [ms]', fontsize=22); plt.ylabel('normalized PSP (A.U)', fontsize=22);
plt.xlim(0,time_limit_ms);

#%% apply SVD and display

X = normlized_syn_filter_large
PSP_SVD_model = TruncatedSVD(n_components=100)
PSP_SVD_model.fit(X)

SVD_cutoff_ind = 3
max_SVD_basis_to_present = 18

plt.close('all')
plt.figure(figsize=(25,20));
plt.subplot(3,1,1); plt.imshow(PSP_SVD_model.components_[:max_SVD_basis_to_present]);
plt.title('normlized synaptic filters as heatmaps', fontsize=22); plt.xlabel('time [ms]', fontsize=22); plt.ylabel('synaptic filter index', fontsize=22); plt.xlim(0,time_limit_ms);
plt.subplot(3,1,2); plt.plot(PSP_SVD_model.components_[:SVD_cutoff_ind].T);
plt.title('first 3 basis functions', fontsize=22); plt.xlabel('time [ms]', fontsize=22); plt.ylabel('normalized PSP (A.U)', fontsize=22); plt.xlim(0,time_limit_ms);
plt.subplot(3,1,3); plt.plot(PSP_SVD_model.components_[SVD_cutoff_ind:max_SVD_basis_to_present].T);
plt.title('rest of the basis functions', fontsize=22); plt.xlabel('time [ms]', fontsize=22); plt.ylabel('normalized PSP (A.U)', fontsize=22); plt.xlim(0,time_limit_ms);

#%% show variance explained

num_basis_functions = PSP_SVD_model.explained_variance_ratio_.shape[0]
explained_var_percent = 100 * PSP_SVD_model.explained_variance_ratio_
cumsum_explained_var_percent = np.concatenate((np.array([0]), np.cumsum(explained_var_percent)))
dot_selected_ind = 3

plt.close('all')
plt.figure(figsize=(10,7));
plt.plot(np.arange(num_basis_functions + 1), cumsum_explained_var_percent, c='k')
plt.scatter(dot_selected_ind, cumsum_explained_var_percent[dot_selected_ind+1], c='r', s=200)
plt.xlabel('num basis functions', fontsize=16); plt.ylabel('explained %s' %('%'), fontsize=16);
plt.title('SVD cumulative explained percent \ntotal variance explained = %.2f%s' %(cumsum_explained_var_percent[dot_selected_ind+1],'%'), fontsize=18);
plt.ylim(-1,105); plt.xlim(-1,num_basis_functions+1);
plt.xlim(-0.3,12)

#%% Apply NMF

from warnings import simplefilter
from sklearn.exceptions import ConvergenceWarning
simplefilter("ignore", category=ConvergenceWarning)

# to avoid numberic instability, replicate the data and add some noise
noisy_data_for_NMF = np.tile(X,[3,1])
noisy_data_for_NMF = noisy_data_for_NMF + 0.0 * np.random.rand(noisy_data_for_NMF.shape[0], noisy_data_for_NMF.shape[1])

PSP_NMF_model = NMF(n_components=20)
PSP_NMF_model.fit(noisy_data_for_NMF)

NMF_cutoff_ind = 3
max_basis_to_present = 10

# normalize each basis vector to it's maximum (for presentation)
NMF_basis = PSP_NMF_model.components_
NMF_basis_norm = NMF_basis / np.tile(NMF_basis.max(axis=1, keepdims=True), [1, NMF_basis.shape[1]])

plt.close('all')
plt.figure(figsize=(25,20));
plt.subplot(3,1,1); plt.imshow(NMF_basis_norm[:max_basis_to_present]);
plt.title('normlized synaptic filters as heatmaps', fontsize=22); plt.xlabel('time [ms]', fontsize=22); plt.ylabel('synaptic filter index', fontsize=22); plt.xlim(0,time_limit_ms);
plt.subplot(3,1,2); plt.plot(NMF_basis_norm[:NMF_cutoff_ind].T);
plt.title('first 4 basis functions', fontsize=22); plt.xlabel('time [ms]', fontsize=22); plt.ylabel('normalized PSP (A.U)', fontsize=22); plt.xlim(0,time_limit_ms);
plt.subplot(3,1,3); plt.plot(NMF_basis_norm[NMF_cutoff_ind:max_basis_to_present].T);
plt.title('rest of the basis functions', fontsize=22); plt.xlabel('time [ms]', fontsize=22); plt.ylabel('normalized PSP (A.U)', fontsize=22); plt.xlim(0,time_limit_ms);

#%% load file for capacity plot

results_filename = data_folder + 'FF_vs_IF_capacity_comparision__num_axons_200__sim_duration_sec_120__num_mult_conn_6__rand_rep_18.pickle'

loaded_script_results_dict = pickle.load(open(results_filename, "rb" ))

processed_res_curves = loaded_script_results_dict['processed_res_curves']
all_results_curves   = loaded_script_results_dict['all_results_curves']

num_axons = loaded_script_results_dict['script_main_params']['num_axons']
stimulus_duration_sec = loaded_script_results_dict['script_main_params']['stimulus_duration_sec']

filename_str = 'FF_vs_IF_capacity_comparision__num_axons_%d__sim_duration_sec_120__num_mult_conn_6__rand_rep_18.pickle'
num_axons_list = sorted([100, 112, 125, 137, 150, 162, 175, 187, 200, 212, 225, 237])
all_filenames_str = [filename_str %(x) for x in num_axons_list]

model_keys = list(loaded_script_results_dict['processed_res_curves'].keys())
connections_per_axon_2C = loaded_script_results_dict['processed_res_curves'][model_keys[0]]['connections_per_axon']

precisely_timed_spikes_per_axon_2C = {}
precisely_timed_spikes_per_axon_error_2C = {}
for key in model_keys:
    precisely_timed_spikes_per_axon_2C[key] = np.zeros((len(all_filenames_str), len(connections_per_axon_2C)))
    precisely_timed_spikes_per_axon_error_2C[key] = np.zeros((len(all_filenames_str), len(connections_per_axon_2C)))

for k, (curr_num_axons, curr_filename) in enumerate(zip(num_axons_list, all_filenames_str)):
    curr_results_filename = data_folder + curr_filename
    curr_loaded_results_dict = pickle.load(open(curr_results_filename, "rb" ))

    for key in model_keys:
        precisely_timed_spikes_per_axon_2C[key][k,:] = curr_loaded_results_dict['processed_res_curves'][key]['num_almost_perfectly_placed_spikes'] / curr_num_axons

        for j, (num_M_conn, num_spikes) in enumerate(zip(curr_loaded_results_dict['processed_res_curves'][key]['connections_per_axon'],
                                                         curr_loaded_results_dict['processed_res_curves'][key]['num_almost_perfectly_placed_spikes'])):

            model_connections_str = '%s, %d connections' %(key, num_M_conn)
            error_index = list(curr_loaded_results_dict['all_results_curves'][model_connections_str]['num_spikes']).index(num_spikes)
            error_scale = (curr_loaded_results_dict['all_results_curves'][model_connections_str]['num_spikes'][error_index + 1] -
                           curr_loaded_results_dict['all_results_curves'][model_connections_str]['num_spikes'][max(0, error_index - 1)])

            if error_index > 1:
                error_scale /= 2

            precisely_timed_spikes_per_axon_error_2C[key][k,j] = error_scale / curr_num_axons


color_map = {}
color_map['I&F'] = '0.05'
color_map['F&F'] = 'orange'

#%% Calculate Capacity with optimal PSP profiles (m = 3)

optimal_basis_PSPs = NMF_basis_norm[:NMF_cutoff_ind]
print(optimal_basis_PSPs.shape)

num_axons = 3
normlized_syn_filter = np.kron(optimal_basis_PSPs, np.ones((num_axons,1)))

#%% Define several new helper functions (including two simulation functions)

def simulate_filter_and_fire_cell_training_PSPs(presynaptic_input_spikes, synaptic_weights, normlized_syn_filter,
                                                refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):

    temporal_filter_length = int(5 * refreactory_time_constant) + 1
    refreactory_filter = signal.exponential(M=temporal_filter_length,center=0,tau=refreactory_time_constant,sym=False)[np.newaxis,:]

    # padd input and get all synaptic filters
    padded_input = np.hstack((np.zeros(normlized_syn_filter.shape), presynaptic_input_spikes))

    # calc local currents
    local_normlized_currents = np.zeros(presynaptic_input_spikes.shape)
    for k in range(normlized_syn_filter.shape[0]):
        local_normlized_currents[k] = signal.convolve(padded_input[k], normlized_syn_filter[k], mode='valid')[1:]

    # multiply by weights to get the somatic current
    soma_current = signal.convolve(local_normlized_currents, synaptic_weights, mode='valid')

    # simulate the cell
    soma_voltage = v_reset + current_to_voltage_mult_factor * soma_current.ravel()
    output_spike_times_in_ms = []
    # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
    for t in range(len(soma_voltage)):
        # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
        if (soma_voltage[t] > v_threshold) and ((t + 1) < len(soma_voltage)):
            t_start = t + 1
            t_end = min(len(soma_voltage), t_start + temporal_filter_length)
            soma_voltage[t_start:t_end] -= (soma_voltage[t + 1] - v_reset) * refreactory_filter.ravel()[:(t_end - t_start)]
            output_spike_times_in_ms.append(t)

    return local_normlized_currents, soma_voltage, output_spike_times_in_ms


# use local currents as "features" and fit a linear model to the data
def prepare_training_dataset(local_normlized_currents, desired_output_spikes, spike_safety_range_ms=10, negative_subsampling_fraction=0.1):

    # remove all "negative" time points that are too close to spikes
    desired_output_spikes_LPF = signal.convolve(desired_output_spikes, np.ones((spike_safety_range_ms,)), mode='same') > 0.1
    desired_timepoints = ~desired_output_spikes_LPF

    # massivly subsample the remaining timepoints
    desired_timepoints[np.random.rand(desired_timepoints.shape[0]) > negative_subsampling_fraction] = 0
    desired_timepoints[desired_output_spikes > 0.1] = 1

    X = local_normlized_currents.T[desired_timepoints,:]
    y = desired_output_spikes[desired_timepoints]

    return X, y


#%% check if desired number of spikes is better than desired AUC score

requested_number_of_output_spikes = 93

optimal_basis_PSPs = NMF_basis_norm[:NMF_cutoff_ind]

# input parameters
num_axons = 200

# neuron model parameters
connections_per_axon = NMF_cutoff_ind
num_synapses = connections_per_axon * num_axons

v_reset     = -80
v_threshold = -55
current_to_voltage_mult_factor = 3
refreactory_time_constant = 15

model_type = 'F&F optimal'

# synapse learnable parameters
synaptic_weights_vec = np.random.normal(size=(num_synapses, 1))

# generate sample input
stimulus_duration_ms = 90000
instantanious_input_spike_probability = 0.004

axons_input_spikes = np.random.rand(num_axons, stimulus_duration_ms) < instantanious_input_spike_probability
presynaptic_input_spikes = np.kron(np.ones((connections_per_axon,1)), axons_input_spikes)
normlized_syn_filter = np.kron(optimal_basis_PSPs, np.ones((num_axons,1)))

assert presynaptic_input_spikes.shape[0] == num_synapses, 'number of synapses doesnt match the number of presynaptic inputs'

# generate desired pattern of output spikes
min_time_between_spikes_ms = 90

desired_output_spike_times = min_time_between_spikes_ms * np.random.randint(int(stimulus_duration_ms / min_time_between_spikes_ms), size=requested_number_of_output_spikes)
desired_output_spike_times = np.sort(np.unique(desired_output_spike_times))

desired_output_spikes = np.zeros((stimulus_duration_ms,))
desired_output_spikes[desired_output_spike_times] = 1.0

print('number of requested output spikes = %d' %(requested_number_of_output_spikes))

# simulate cell with normlized currents
local_normlized_currents, soma_voltage, output_spike_times_in_ms = simulate_filter_and_fire_cell_training_PSPs(presynaptic_input_spikes,
                                                                                                               synaptic_weights_vec, normlized_syn_filter,
                                                                                                               refreactory_time_constant=refreactory_time_constant,
                                                                                                               v_reset=v_reset, v_threshold=v_threshold,
                                                                                                               current_to_voltage_mult_factor=current_to_voltage_mult_factor)

output_spikes = np.zeros((stimulus_duration_ms,))
try:
    output_spikes[np.array(output_spike_times_in_ms)] = 1.0
except:
    print('no output spikes created')

#%% fit linear model to local currents

logistic_reg_model = linear_model.LogisticRegression(C=100000, fit_intercept=True, penalty='l2', max_iter=3000)

spike_safety_range_ms = 5
negative_subsampling_fraction = 0.5

X, y = prepare_training_dataset(local_normlized_currents, desired_output_spikes,
                                spike_safety_range_ms=spike_safety_range_ms,
                                negative_subsampling_fraction=negative_subsampling_fraction)

# fit model
logistic_reg_model.fit(X,y)

print('number of data points = %d (%.2f%s positive class)' %(X.shape[0], 100 * y.mean(),'%'))

y_hat = logistic_reg_model.predict_proba(X)[:,1]

# calculate AUC
train_AUC = roc_auc_score(y, y_hat)

fitted_output_spike_prob = logistic_reg_model.predict_proba(local_normlized_currents.T)[:,1]
full_AUC = roc_auc_score(desired_output_spikes, fitted_output_spike_prob)

# get desired FP threshold
desired_false_positive_rate = 0.004

fpr, tpr, thresholds = roc_curve(desired_output_spikes, fitted_output_spike_prob)

desired_fp_ind = np.argmin(abs(fpr-desired_false_positive_rate))
if desired_fp_ind == 0:
    desired_fp_ind = 1

actual_false_positive_rate = fpr[desired_fp_ind]
true_positive_rate         = tpr[desired_fp_ind]
desired_fp_threshold       = thresholds[desired_fp_ind]

AUC_score = auc(fpr, tpr)

print('AUC = %.4f' %(AUC_score))
print('at %.4f FP rate, TP = %.4f' %(actual_false_positive_rate, true_positive_rate))

output_spikes_after_learning = fitted_output_spike_prob > desired_fp_threshold

#%% Build the final figure

xy_label_fontsize = 16
title_fontsize = 21

plt.close('all')
fig = plt.figure(figsize=(20,18.5))
gs_figure = gridspec.GridSpec(nrows=8,ncols=5)
gs_figure.update(left=0.05, right=0.95, bottom=0.05, top=0.95, wspace=0.6, hspace=0.9)

ax_PSP_heatmap     = plt.subplot(gs_figure[ :6, :3])
ax_SVD_heatmap     = plt.subplot(gs_figure[6: , :3])
ax_PSP_traces      = plt.subplot(gs_figure[ :2,3: ])
ax_NMF_trance      = plt.subplot(gs_figure[2:4,3: ])
ax_explained_var   = plt.subplot(gs_figure[4:6,3: ])
ax_n_spikes_m_cons = plt.subplot(gs_figure[6: ,3: ])

interp_method_PSP = 'spline16'
interp_method_SVD = 'bilinear'
colormap = 'jet'

ax_PSP_heatmap.imshow(normlized_syn_filter_small, cmap=colormap, interpolation=interp_method_PSP);
ax_PSP_heatmap.set_xlim(0,time_limit_ms);
ax_PSP_heatmap.set_title('All PSPs as heatmap', fontsize=title_fontsize)
ax_PSP_heatmap.set_xlabel('Time (ms)', fontsize=xy_label_fontsize)
ax_PSP_heatmap.set_xticks([0,30,60,90,120])
ax_PSP_heatmap.set_xticklabels([0,30,60,90,120], fontsize=xy_label_fontsize)
ax_PSP_heatmap.set_ylabel('PSP index', fontsize=xy_label_fontsize)
ax_PSP_heatmap.set_yticks([0,24,48,72,96,120])
ax_PSP_heatmap.set_yticklabels([1,25,49,73,97,121], fontsize=xy_label_fontsize)

ax_SVD_heatmap.imshow(np.kron(PSP_SVD_model.components_[:max_SVD_basis_to_present], np.ones((2,1))), cmap=colormap, interpolation=interp_method_SVD);
ax_SVD_heatmap.set_xlim(0,time_limit_ms);
ax_SVD_heatmap.set_title('SVD basis functions as heatmap', fontsize=title_fontsize)
ax_SVD_heatmap.set_xlabel('Time (ms)', fontsize=xy_label_fontsize)
ax_SVD_heatmap.set_xticks([0,30,60,90,120])
ax_SVD_heatmap.set_xticklabels([0,30,60,90,120], fontsize=xy_label_fontsize)
ax_SVD_heatmap.set_ylabel('Basis function index', fontsize=xy_label_fontsize)
ax_SVD_heatmap.set_yticks([0,9,19,29])
ax_SVD_heatmap.set_yticklabels([1,10,20,30], fontsize=xy_label_fontsize)

ax_PSP_traces.plot(normlized_syn_filter_large.T, alpha=0.15);
ax_PSP_traces.set_xlim(-1,time_limit_ms);
ax_PSP_traces.set_title('All PSPs as traces', fontsize=title_fontsize)
ax_PSP_traces.set_ylabel('Magnitude (A.U.)', fontsize=xy_label_fontsize);
ax_PSP_traces.set_xlabel('Time (ms)', fontsize=xy_label_fontsize)
ax_PSP_traces.set_yticks([0.0,0.25,0.50,0.75,1.00])
ax_PSP_traces.set_yticklabels([0.0,0.25,0.50,0.75,1.00], fontsize=xy_label_fontsize)
ax_PSP_traces.set_xticks([0,30,60,90,120])
ax_PSP_traces.set_xticklabels([0,30,60,90,120], fontsize=xy_label_fontsize)

ax_NMF_trance.plot(NMF_basis_norm[:NMF_cutoff_ind].T);
ax_NMF_trance.set_xlim(-1,time_limit_ms);
ax_NMF_trance.set_title('NMF first %d basis functions' %(NMF_cutoff_ind), fontsize=title_fontsize)
ax_NMF_trance.set_ylabel('Magnitude  (A.U.)', fontsize=xy_label_fontsize);
ax_NMF_trance.set_xlabel('Time (ms)', fontsize=xy_label_fontsize)
ax_NMF_trance.set_yticks([0.0,0.25,0.50,0.75,1.00])
ax_NMF_trance.set_yticklabels([0.0,0.25,0.50,0.75,1.00], fontsize=xy_label_fontsize)
ax_NMF_trance.set_xticks([0,30,60,90,120])
ax_NMF_trance.set_xticklabels([0,30,60,90,120], fontsize=xy_label_fontsize)

ax_explained_var.plot(np.arange(num_basis_functions + 1), cumsum_explained_var_percent, c='k')

ax_explained_var.scatter(dot_selected_ind, cumsum_explained_var_percent[NMF_cutoff_ind + 1], c='r', s=200)
ax_explained_var.set_title('Variance explained = %.2f%s' %(cumsum_explained_var_percent[NMF_cutoff_ind + 1],'%'), fontsize=title_fontsize);
ax_explained_var.set_xlabel('Num basis functions', fontsize=xy_label_fontsize);
ax_explained_var.set_ylabel('Explained Percent (%s)' %('%'), fontsize=xy_label_fontsize);
ax_explained_var.set_ylim(-1,115);
ax_explained_var.set_yticks([0,25,50,75,100])
ax_explained_var.set_yticklabels([0,25,50,75,100], fontsize=xy_label_fontsize)
ax_explained_var.set_xlim(-0.3,12);
ax_explained_var.set_xticks([0,3,6,9,12])
ax_explained_var.set_xticklabels([0,3,6,9,12], fontsize=xy_label_fontsize)

for key in processed_res_curves.keys():
    y_error = precisely_timed_spikes_per_axon_2C[key].std(axis=0)
    ax_n_spikes_m_cons.errorbar(connections_per_axon_2C, precisely_timed_spikes_per_axon_2C[key].mean(axis=0), yerr=y_error, label=key, lw=4, color=color_map[key])

ax_n_spikes_m_cons.legend(loc='upper left', fontsize=22)
ax_n_spikes_m_cons.set_title('Placing Precisely Timed output Spikes', fontsize=title_fontsize)
ax_n_spikes_m_cons.set_xlabel('Number of Multiple Contacts - M', fontsize=xy_label_fontsize)
ax_n_spikes_m_cons.set_ylabel('Precisely Timed Spikes / Axon', fontsize=xy_label_fontsize);
ax_n_spikes_m_cons.spines['top'].set_visible(False)
ax_n_spikes_m_cons.spines['right'].set_visible(False)
ax_n_spikes_m_cons.set_yticks([0.15,0.3,0.45])
ax_n_spikes_m_cons.set_yticklabels([0.15,0.3,0.45], fontsize=xy_label_fontsize)
ax_n_spikes_m_cons.set_xticks([1,2,3,5,10,15])
ax_n_spikes_m_cons.set_xticklabels([1,2,3,5,10,15], fontsize=xy_label_fontsize)

# add the asimptote line
if AUC_score > 0.99:
    optimal_const_value = np.ones(connections_per_axon_2C.shape) * requested_number_of_output_spikes / num_axons
    ax_n_spikes_m_cons.plot(connections_per_axon_2C, optimal_const_value, label='Optimal 3 PSPs', ls='dashed', lw=2, color='red')
    ax_n_spikes_m_cons.scatter(3, optimal_const_value[0], label='Optimal 3 PSPs', s=200, color='red')
    ax_n_spikes_m_cons.set_ylim(0.09,1.06 * optimal_const_value[0])
    ax_n_spikes_m_cons.legend(loc='center right', fontsize=17)

# save figure
if save_figures:
    figure_name = 'F&F_explanatory_Figure_4_%d' %(np.random.randint(200))
    for file_ending in all_file_endings_to_use:
        if file_ending == '.png':
            fig.savefig(figure_folder + figure_name + file_ending, bbox_inches='tight')
        else:
            fig.savefig(figure_folder + figure_name + file_ending, bbox_inches='tight')


#%%

================================================================================
================================================================================
run_capacity_configs_on_cluster_slurm.py:
=========================================
import os
import time


def mkdir_p(dir_path):
    '''make a directory (dir_path) if it doesn't exist'''
    if not os.path.exists(dir_path):
        os.mkdir(dir_path)


script_name    = 'FF_vs_IF_capacity_comparison_interactions.py'
output_log_dir = '/filter_and_fire_neuron/logs/'

# tiny experiment configs to run (will be form a grid of all combinations, so number of experiments will explode if not careful)
num_axons_list = [100]
use_interaction_terms_list = [False]
interactions_degree_list = [2]

# full experiment (axons, interaction degree)
num_axons_list = [50,100,200,300,400]
use_interaction_terms_list = [False]
interactions_degree_list = [2]

num_random_seeds = 1
start_seed = 123456

partition_argument_str = "-p ss.q,elsc.q"
timelimit_argument_str = "-t 0-22:00:00"
CPU_argument_str = "-c 1"
RAM_argument_str = "--mem 32000"
CPU_exclude_nodes_str = "--exclude=ielsc-58,ielsc-60,ielsc-108,ielsc-109"

temp_jobs_dir = os.path.join(output_log_dir, 'temp/')
mkdir_p(temp_jobs_dir)

random_seed = start_seed
for num_axons in num_axons_list:
    for use_interaction_terms in use_interaction_terms_list:
        for interactions_degree in interactions_degree_list:
            for exp_index in range(num_random_seeds):
                random_seed = random_seed + 1

                # job and log names
                axons_interactions_str = 'num_axons_%d_interactions_%s_degree_%d' %(num_axons, use_interaction_terms, interactions_degree)
                job_name = '%s_%s_randseed_%d' %(script_name[:-3], axons_interactions_str, random_seed)
                log_filename = os.path.join(output_log_dir, "%s.log" %(job_name))
                job_filename = os.path.join(temp_jobs_dir , "%s.job" %(job_name))

                # write a job file and run it
                with open(job_filename, 'w') as fh:
                    fh.writelines("#!/bin/bash\n")
                    fh.writelines("#SBATCH --job-name %s\n" %(job_name))
                    fh.writelines("#SBATCH -o %s\n" %(log_filename))
                    fh.writelines("#SBATCH %s\n" %(partition_argument_str))
                    fh.writelines("#SBATCH %s\n" %(timelimit_argument_str))
                    fh.writelines("#SBATCH %s\n" %(CPU_argument_str))
                    fh.writelines("#SBATCH %s\n" %(RAM_argument_str))
                    fh.writelines("#SBATCH %s\n" %(CPU_exclude_nodes_str))
                    fh.writelines("python3.6 -u %s %s %s %s %s\n" %(script_name, random_seed, num_axons, use_interaction_terms, interactions_degree))

                os.system("sbatch %s" %(job_filename))
                time.sleep(0.2)

================================================================================
================================================================================
saved_figures/F&F_Explantion_Figure_4_2.png:
============================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
This image presents a comprehensive analysis of postsynaptic potentials (PSPs), likely from a neuronal recording. The analysis uses several techniques to decompose and understand the underlying patterns in the data.

The top left panel shows a heatmap of all PSPs, illustrating their temporal evolution.  Each horizontal line represents a single PSP, revealing a consistent pattern across the different recordings.  To the right of this, a line plot shows the same data, with each line representing an individual PSP trace. This provides another visual representation of the temporal dynamics, especially showing the similarities and differences more clearly. The next section shows the results of Non-negative Matrix Factorization (NMF), a dimensionality reduction technique.  The top plot shows the three basis functions (temporal patterns) identified by NMF, suggesting three primary temporal components that contribute to the observed PSP shapes. The bottom plot in this section displays the variance explained by using a varying number of basis functions, highlighting that three basis functions explain almost all of the variance (99.93%).  The bottom left panel presents a heatmap of Singular Value Decomposition (SVD) basis functions, offering an alternative decomposition of the PSP data.  Finally, the bottom right panel presents the results of a simulation or experiment, showing how the number of "multiple contacts" affects the precision of timed output spikes.  The plot compares the performance using an optimal three-PSP model against two other models, labeled F&F and I&F. The horizontal dashed line represents a target level of precision.
================================================================================
================================================================================
MNIST_classification_LR_IF_FF_interactions.py:
==============================================
import os
import sys
import numpy as np
import time
import pickle
import matplotlib.pyplot as plt
from scipy import signal
from sklearn import linear_model
from sklearn.metrics import roc_curve, roc_auc_score
from tensorflow import keras

#%% script params

start_time = time.time()

try:
    print('----------------------------')
    print('----------------------------')
    random_seed = int(sys.argv[1])
    positive_digit = int(sys.argv[2])
    connections_per_axon = int(sys.argv[3])
    temporal_extent_factor_numerator = int(sys.argv[4])
    temporal_extent_factor_denumerator = int(sys.argv[5])
    release_probability = int(sys.argv[6])
    num_train_positive_patterns = int(sys.argv[7])
    print('"random_seed" selected by user - %d' %(random_seed))
    print('"positive_digit" selected by user - %d' %(positive_digit))
    print('"connections_per_axon" selected by user - %d' %(connections_per_axon))
    print('"temporal_extent_factor_numerator" selected by user - %d' %(temporal_extent_factor_numerator))
    print('"temporal_extent_factor_denumerator" selected by user - %d' %(temporal_extent_factor_denumerator))
    print('"release_probability" selected by user - %d' %(release_probability))
    print('"num_train_positive_patterns" selected by user - %d' %(num_train_positive_patterns))

    determine_internally = False
except:
    determine_internally = True
    try:
        random_seed = int(sys.argv[1])
        print('random seed selected by user - %d' %(random_seed))
    except:
        random_seed = np.random.randint(100000)
        print('randomly choose seed - %d' %(random_seed))

np.random.seed(random_seed)
print('----------------------------')
print('----------------------------')


if determine_internally:
    positive_digit = np.random.randint(10)
    connections_per_axon = np.random.choice([1,2,3,5,10], size=1)[0]
    temporal_extent_factor_numerator = np.random.choice([1,2,3,4,5], size=1)[0]
    temporal_extent_factor_denumerator = np.random.choice([1,2], size=1)[0]
    release_probability = np.random.choice([0.25, 0.5,0.5,0.5, 0.75, 1.0,1.0,1.0], size=1)[0]
    num_train_positive_patterns = np.random.choice([16,32,64,128,256,512,1024,2048,4096,5000], size=1)[0]

# interactions set to False
use_interaction_terms = False
interactions_degree  = 2

spatial_extent_factor = 5
num_const_firing_channels = 20
temporal_silence_ms = 70
#num_train_positive_patterns = 7000
num_train_negative_patterns_mult_factor = 5
spike_safety_range_ms = 20
negative_subsampling_fraction = 0.2

# release probability related params
# release_probability = 1.0
# release_probability = 0.5
train_epochs = 15
test_epochs  = 3

# what to consider as good prediction
output_spike_tolorance_window_duration = 30
output_spike_tolorance_window_offset   = 10

FF_weight_mult_factors_list = [0.01,0.03,0.07,0.1,0.3,0.5,0.8,1,1.3,2,3,4,5,7,10,25,50,120,250]
IF_weight_mult_factors_list = [0.01,0.03,0.07,0.1,0.3,0.5,0.8,1,1.3,2,3,4,5,7,10,25,50,120,250,1000,10000]

if use_interaction_terms is False:
    non_interaction_fraction_FF = 1.0
    non_interaction_fraction_IF = 1.0

# setting to create a 1 spike out or a 3 spike burst as supervising signal
create_output_burst = False
# create_output_burst = True

# setting for quick learning
#quick_test = True
quick_test = False

if quick_test:
    num_train_positive_patterns = 500
    num_train_negative_patterns_mult_factor = 2
    negative_subsampling_fraction = 0.1

    FF_weight_mult_factors_list = [0.25, 1, 2, 4, 8, 16, 64]
    IF_weight_mult_factors_list = [0.25, 1, 2, 4, 8, 16, 64]


show_plots = True
show_plots = False

data_folder = '/filter_and_fire_neuron/results_data_mnist/'

experiment_results_dict = {}
experiment_results_dict['script_main_params'] = {}
experiment_results_dict['script_main_params']['positive_digit'] = positive_digit
experiment_results_dict['script_main_params']['connections_per_axon'] = connections_per_axon
experiment_results_dict['script_main_params']['random_seed'] = random_seed
experiment_results_dict['script_main_params']['interactions_degree'] = interactions_degree

experiment_results_dict['script_main_params']['temporal_extent_factor_numerator'] = temporal_extent_factor_numerator
experiment_results_dict['script_main_params']['temporal_extent_factor_denumerator'] = temporal_extent_factor_denumerator
experiment_results_dict['script_main_params']['spatial_extent_factor'] = spatial_extent_factor
experiment_results_dict['script_main_params']['num_const_firing_channels'] = num_const_firing_channels
experiment_results_dict['script_main_params']['temporal_silence_ms'] = temporal_silence_ms

experiment_results_dict['script_main_params']['num_train_positive_patterns'] = num_train_positive_patterns
experiment_results_dict['script_main_params']['num_train_negative_patterns_mult_factor'] = num_train_negative_patterns_mult_factor
experiment_results_dict['script_main_params']['spike_safety_range_ms'] = spike_safety_range_ms
experiment_results_dict['script_main_params']['negative_subsampling_fraction'] = negative_subsampling_fraction

experiment_results_dict['script_main_params']['release_probability'] = release_probability
experiment_results_dict['script_main_params']['train_epochs'] = train_epochs
experiment_results_dict['script_main_params']['test_epochs'] = test_epochs
experiment_results_dict['script_main_params']['create_output_burst'] = create_output_burst

experiment_results_dict['script_main_params']['output_spike_tolorance_window_duration'] = output_spike_tolorance_window_duration
experiment_results_dict['script_main_params']['output_spike_tolorance_window_offset']   = output_spike_tolorance_window_offset
experiment_results_dict['script_main_params']['FF_weight_mult_factors_list'] = FF_weight_mult_factors_list
experiment_results_dict['script_main_params']['IF_weight_mult_factors_list'] = IF_weight_mult_factors_list


#%% helper functions


def create_single_PSP_profile(tau_rise, tau_decay, temporal_filter_length=50):

    if tau_rise >= tau_decay:
        tau_decay = tau_rise + 5

    exp_r = signal.exponential(M=temporal_filter_length, center=0, tau=tau_rise , sym=False)
    exp_d = signal.exponential(M=temporal_filter_length, center=0, tau=tau_decay, sym=False)

    post_syn_potential = exp_d - exp_r
    post_syn_potential /= post_syn_potential.max()

    return post_syn_potential


def construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec):

    num_synapses = tau_rise_vec.shape[0]
    temporal_filter_length = int(4 * tau_decay_vec.max()) + 1

    syn_filter = np.zeros((num_synapses, temporal_filter_length))

    for k, (tau_r, tau_d) in enumerate(zip(tau_rise_vec, tau_decay_vec)):
        syn_filter[k,:] = create_single_PSP_profile(tau_r, tau_d, temporal_filter_length=temporal_filter_length)

    return syn_filter


def simulate_filter_and_fire_cell_with_interactions(presynaptic_input_spikes, interactions_map, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                                    refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):

    temporal_filter_length = int(5 * refreactory_time_constant) + 1
    refreactory_filter = signal.exponential(M=temporal_filter_length,center=0,tau=refreactory_time_constant,sym=False)[np.newaxis,:]

    # padd input and get all synaptic filters
    normlized_syn_filter = np.flipud(construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec))
    padded_input = np.hstack((np.zeros(normlized_syn_filter.shape), presynaptic_input_spikes))

    # calc local currents
    local_normlized_currents = np.zeros(presynaptic_input_spikes.shape, dtype=np.float16)
    for k in range(normlized_syn_filter.shape[0]):
        local_normlized_currents[k] = signal.convolve(padded_input[k], normlized_syn_filter[k], mode='valid')[1:]

    # apply interactions
    local_normlized_currents = apply_dendritic_interactions(local_normlized_currents, interactions_map)

    # multiply by weights to get the somatic current
    soma_current = signal.convolve(local_normlized_currents, synaptic_weights, mode='valid')

    # simulate the cell
    soma_voltage = v_reset + current_to_voltage_mult_factor * soma_current.ravel()
    output_spike_times_in_ms = []
    # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
    for t in range(len(soma_voltage)):
        # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
        if (soma_voltage[t] > v_threshold) and ((t + 1) < len(soma_voltage)):
            t_start = t + 1
            t_end = min(len(soma_voltage), t_start + temporal_filter_length)
            soma_voltage[t_start:t_end] -= (soma_voltage[t + 1] - v_reset) * refreactory_filter.ravel()[:(t_end - t_start)]
            output_spike_times_in_ms.append(t)

    return local_normlized_currents, soma_voltage, output_spike_times_in_ms


def simulate_filter_and_fire_cell_with_interactions_long(presynaptic_input_spikes, interactions_map, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                                         refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):

    total_duration_ms = presynaptic_input_spikes.shape[1]
    max_duration_per_call_ms = 50000
    overlap_time_ms = 500

    if max_duration_per_call_ms >= total_duration_ms:
        local_normlized_currents, soma_voltage, output_spike_times_in_ms = simulate_filter_and_fire_cell_with_interactions(presynaptic_input_spikes, interactions_map, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                                                                                                           refreactory_time_constant=refreactory_time_constant, v_reset=v_reset, v_threshold=v_threshold,
                                                                                                                           current_to_voltage_mult_factor=current_to_voltage_mult_factor)
        return local_normlized_currents, soma_voltage, output_spike_times_in_ms


    local_normlized_currents = np.zeros(presynaptic_input_spikes.shape, dtype=np.float16)
    soma_voltage = np.zeros((total_duration_ms,))
    output_spike_times_in_ms = []

    num_sub_calls = int(np.ceil(total_duration_ms / (max_duration_per_call_ms - overlap_time_ms)))
    end_ind = overlap_time_ms
    for k in range(num_sub_calls):
        start_ind = end_ind - overlap_time_ms
        end_ind = min(start_ind + max_duration_per_call_ms, total_duration_ms)

        curr_loc_norm_c, curr_soma_v, curr_out_sp_t = simulate_filter_and_fire_cell_with_interactions(presynaptic_input_spikes[:,start_ind:end_ind], interactions_map, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                                                                                      refreactory_time_constant=refreactory_time_constant, v_reset=v_reset, v_threshold=v_threshold,
                                                                                                      current_to_voltage_mult_factor=current_to_voltage_mult_factor)

        # update fields
        if k == 0:
            local_normlized_currents[:,start_ind:end_ind] = curr_loc_norm_c
            soma_voltage[start_ind:end_ind] = curr_soma_v
            output_spike_times_in_ms += curr_out_sp_t
        else:
            local_normlized_currents[:,(start_ind + overlap_time_ms):end_ind] = curr_loc_norm_c[:,overlap_time_ms:end_ind]
            soma_voltage[(start_ind + overlap_time_ms):end_ind] = curr_soma_v[overlap_time_ms:]
            curr_out_sp_t = [x for x in curr_out_sp_t if x >= (overlap_time_ms - 1)]
            output_spike_times_in_ms += [(start_ind + x) for x in curr_out_sp_t]

    return local_normlized_currents, soma_voltage, output_spike_times_in_ms


# use local currents as "features" and fit a linear model to the data
def prepare_training_dataset(local_normlized_currents, desired_output_spikes, spike_safety_range_ms=10, negative_subsampling_fraction=0.1):

    # remove all "negative" time points that are too close to spikes
    desired_output_spikes_LPF = signal.convolve(desired_output_spikes, np.ones((spike_safety_range_ms,)), mode='same') > 0.1
    desired_timepoints = ~desired_output_spikes_LPF

    # massivly subsample the remaining timepoints
    desired_timepoints[np.random.rand(desired_timepoints.shape[0]) > negative_subsampling_fraction] = 0
    desired_timepoints[desired_output_spikes > 0.1] = 1

    X = local_normlized_currents.T[desired_timepoints,:]
    y = desired_output_spikes[desired_timepoints]

    return X, y


def generate_dendritic_interactions_map(num_synapses, interactions_degree=2, non_interaction_fraction=0.2):

    interactions_map = {}
    interactions_map['degree_permutations'] = {}

    for degree in range(interactions_degree - 1):
        interactions_map['degree_permutations'][degree] = np.random.permutation(num_synapses)
    interactions_map['non_interacting_indices'] = np.random.permutation(num_synapses)[:int(num_synapses * non_interaction_fraction)]

    return interactions_map


def apply_dendritic_interactions(normlized_synaptic_currents, interactions_map):
    output_normlized_synaptic_currents = normlized_synaptic_currents.copy()

    # apply d times random interactions
    for degree in range(interactions_degree - 1):
        output_normlized_synaptic_currents = output_normlized_synaptic_currents * normlized_synaptic_currents[interactions_map['degree_permutations'][degree]]

    # keep some fraction of only individual interactions
    output_normlized_synaptic_currents[interactions_map['non_interacting_indices']] = normlized_synaptic_currents[interactions_map['non_interacting_indices']]

    return output_normlized_synaptic_currents


#%% Load MNIST dataset and show the data

(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

if show_plots:
    num_rows = 5
    num_cols = 7

    plt.figure(figsize=(20,15))
    for k in range(num_rows * num_cols):
        rand_sample_ind = np.random.randint(x_train.shape[0])
        plt.subplot(num_rows, num_cols, k + 1)
        plt.imshow(x_train[k]); plt.title('digit "%s"' %(y_train[k]))

#%% display mean and std images, as well as histograms

mean_image = x_train.mean(axis=0)
std_image  = x_train.std(axis=0)

if show_plots:
    plt.figure(figsize=(21,14))
    plt.subplot(2,3,1); plt.imshow(mean_image); plt.title('mean image')
    plt.subplot(2,3,2); plt.bar(np.arange(mean_image.shape[0]), mean_image.sum(axis=0)); plt.title('"temporal" (columns) histogram (mean image)')
    plt.subplot(2,3,3); plt.bar(np.arange(mean_image.shape[0]), mean_image.sum(axis=1)); plt.title('"spatial" (rows) histogram (mean image)')

    plt.subplot(2,3,4); plt.imshow(std_image); plt.title('std image')
    plt.subplot(2,3,5); plt.bar(np.arange(std_image.shape[0]), std_image.sum(axis=0)); plt.title('"temporal" (columns) histogram (std image)')
    plt.subplot(2,3,6); plt.bar(np.arange(std_image.shape[0]), std_image.sum(axis=1)); plt.title('"spatial" (rows) histogram (std image)')


#%% Crop the data and binarize it

h_crop_range = [4,24]
w_crop_range = [4,24]

positive_threshold = 150

x_train = x_train[:,h_crop_range[0]:h_crop_range[1],w_crop_range[0]:w_crop_range[1]] > positive_threshold
x_test  = x_test[: ,h_crop_range[0]:h_crop_range[1],w_crop_range[0]:w_crop_range[1]] > positive_threshold

if show_plots:
    num_rows = 5
    num_cols = 7

    plt.figure(figsize=(20,15))
    for k in range(num_rows * num_cols):
        rand_sample_ind = np.random.randint(x_train.shape[0])
        plt.subplot(num_rows, num_cols, k + 1)
        plt.imshow(x_train[k]); plt.title('digit "%s"' %(y_train[k]))

experiment_results_dict['script_main_params']['digit_sample_image_shape_cropped'] = x_train[0].shape

#%% Transform Xs to spatio-temporal spike trains

# extend according to "spatial_extent_factor" and "temporal_extent_factor"
kernel = np.ones((1, spatial_extent_factor, temporal_extent_factor_numerator), dtype=bool)

# reshape X according to what is needed
x_train = np.kron(x_train, kernel)
x_test  = np.kron(x_test , kernel)

# subsample according to "temporal_extent_factor_denumerator"
x_train = x_train[:,:,::temporal_extent_factor_denumerator]
x_test  = x_test[:,:, ::temporal_extent_factor_denumerator]

experiment_results_dict['script_main_params']['digit_sample_image_shape_expanded'] = x_train[0].shape

# padd with ones on top (for "bias" learning)
top_pad_train = np.ones((1, num_const_firing_channels, x_train.shape[2]), dtype=bool)
top_pad_test  = np.ones((1, num_const_firing_channels, x_test.shape[2] ), dtype=bool)

# add a few zero rows for clear seperation for visualization purpuses
top_pad_train[:,-5:,:] = 0
top_pad_test[:,-5:,:] = 0

x_train = np.concatenate((np.tile(top_pad_train, [x_train.shape[0],1,1]), x_train), axis=1)
x_test  = np.concatenate((np.tile(top_pad_test , [x_test.shape[0],1,1] ), x_test ), axis=1)

# pad with "temporal_silence_ms" zeros in the begining of each pattern (for silence between patterns)
left_pad_train = np.zeros((1, x_train.shape[1], temporal_silence_ms), dtype=bool)
left_pad_test  = np.zeros((1, x_test.shape[1] , temporal_silence_ms), dtype=bool)

x_train = np.concatenate((np.tile(left_pad_train, [x_train.shape[0],1,1]), x_train), axis=2)
x_test  = np.concatenate((np.tile(left_pad_test , [x_test.shape[0],1,1] ), x_test ), axis=2)

# add background activity
desired_background_activity_firing_rate_Hz = 10
background_activity_fraction = desired_background_activity_firing_rate_Hz / 1000

x_train[np.random.rand(x_train.shape[0], x_train.shape[1], x_train.shape[2]) < background_activity_fraction] = 1
x_test[ np.random.rand(x_test.shape[0] , x_test.shape[1] , x_test.shape[2] ) < background_activity_fraction] = 1

# subsample the input spikes
desired_average_input_firing_rate_Hz = 20
actual_mean_firing_rate_Hz = 1000 * x_train.mean()

fraction_of_spikes_to_eliminate = desired_average_input_firing_rate_Hz / actual_mean_firing_rate_Hz

x_train = x_train * (np.random.rand(x_train.shape[0], x_train.shape[1], x_train.shape[2]) < fraction_of_spikes_to_eliminate)
x_test  = x_test  * (np.random.rand(x_test.shape[0] , x_test.shape[1] , x_test.shape[2])  < fraction_of_spikes_to_eliminate)

final_mean_firing_rate_Hz = 1000 * x_train.mean()

# display the patterns
if show_plots:
    num_rows = 5
    num_cols = 7

    plt.figure(figsize=(20,15))
    for k in range(num_rows * num_cols):
        rand_sample_ind = np.random.randint(x_train.shape[0])
        plt.subplot(num_rows, num_cols, k + 1)
        plt.imshow(x_train[k], cmap='gray'); plt.title('digit "%s"' %(y_train[k]))


#%% display distribution of number of spikes per pattern

if show_plots:
    plt.close('all')
    plt.figure(figsize=(12,8))
    plt.hist(x_train.sum(axis=2).sum(axis=1), bins=40); plt.title('distribution of number of spikes per pattern')
    plt.ylabel('number of patterns'); plt.xlabel('number of incoming spikes per pattern')

#%% Create "one-vs-all" dataset

y_train_binary = y_train == positive_digit
y_test_binary  = y_test  == positive_digit

num_train_positive_patterns = min(int(y_train_binary.sum()), num_train_positive_patterns)
num_train_negative_patterns = int(num_train_negative_patterns_mult_factor * num_train_positive_patterns)

positive_inds = np.where(y_train_binary)[0]
negative_inds = np.where(~y_train_binary)[0]

selected_train_positives = np.random.choice(positive_inds, size=num_train_positive_patterns)
selected_train_negatives = np.random.choice(negative_inds, size=num_train_negative_patterns)

all_selected = np.random.permutation(np.concatenate((selected_train_positives, selected_train_negatives)))

X_train_spikes = x_train[all_selected]
Y_train_spikes = y_train_binary[all_selected]

X_test_spikes = x_test.copy()
Y_test_spikes = y_test_binary.copy()

zero_pred_baseline_accuracy = 100 * (1 - Y_test_spikes.mean())

if release_probability < 1.0:
    # replicate train and test by corresponding factors (epochs)
    X_train_spikes = np.tile(X_train_spikes, (train_epochs, 1, 1))
    Y_train_spikes = np.tile(Y_train_spikes, (train_epochs, ))

    X_test_spikes = np.tile(X_test_spikes, (test_epochs, 1, 1))
    Y_test_spikes = np.tile(Y_test_spikes, (test_epochs, ))

    # add synaptic unrelability to all patterns after replication
    rand_matrix = np.random.rand(X_train_spikes.shape[0], X_train_spikes.shape[1], X_train_spikes.shape[2])
    X_train_spikes = X_train_spikes * (rand_matrix < release_probability)

    rand_matrix = np.random.rand(X_test_spikes.shape[0], X_test_spikes.shape[1], X_test_spikes.shape[2])
    X_test_spikes = X_test_spikes * (rand_matrix < release_probability)

experiment_results_dict['script_main_params']['num_train_positive_patterns'] = num_train_positive_patterns
experiment_results_dict['script_main_params']['num_train_negative_patterns_mult_factor'] = num_train_negative_patterns_mult_factor
experiment_results_dict['script_main_params']['release_probability'] = release_probability
experiment_results_dict['script_main_params']['train_epochs'] = train_epochs
experiment_results_dict['script_main_params']['test_epochs']  = test_epochs

#%% Create a regularized logistic regression baseline

logistic_reg_model = linear_model.LogisticRegression(C=0.1, fit_intercept=False, penalty='l2',verbose=False)

# fit model
logistic_reg_model.fit(X_train_spikes.reshape([X_train_spikes.shape[0],-1]), Y_train_spikes)

# predict and calculate AUC on train data
Y_train_spikes_hat = logistic_reg_model.predict_proba(X_train_spikes.reshape([X_train_spikes.shape[0],-1]))[:,1]
Y_test_spikes_hat = logistic_reg_model.predict_proba(X_test_spikes.reshape([X_test_spikes.shape[0],-1]))[:,1]

train_AUC = roc_auc_score(Y_train_spikes, Y_train_spikes_hat)
test_AUC = roc_auc_score(Y_test_spikes, Y_test_spikes_hat)

print('----------------------------')
print('----------------------------')
print('for (# pos = %d, # neg = %d): (train AUC, test AUC) = (%.5f, %.5f)' %(num_train_positive_patterns, num_train_negative_patterns, train_AUC, test_AUC))
print('----------------------------')

logistic_regression_learned_weights = logistic_reg_model.coef_.reshape([X_train_spikes.shape[1], X_train_spikes.shape[2]])

if show_plots:
    plt.figure(figsize=(8,8))
    plt.imshow(logistic_regression_learned_weights)
    plt.title('Learned Weights \n (spatio-temporal ("image") logistic regression)')

experiment_results_dict['learned_weights_LR'] = logistic_regression_learned_weights

#%% Calculate and Display LogReg Accuracy

LL_false_positive_list, LL_true_positive_list, LL_thresholds_list = roc_curve(Y_test_spikes, Y_test_spikes_hat)

num_pos_class = int((Y_test_spikes == True).sum())
num_neg_class = int((Y_test_spikes == False).sum())

tp = LL_true_positive_list * num_pos_class
tn = (1 - LL_false_positive_list) * num_neg_class
LL_accuracy_list = (tp + tn) / (num_pos_class + num_neg_class)

LL_false_positive_list = LL_false_positive_list[LL_false_positive_list < 0.05]
LL_true_positive_list = LL_true_positive_list[:len(LL_false_positive_list)]
LL_thresholds_list = LL_thresholds_list[:len(LL_false_positive_list)]
LL_accuracy_list = LL_accuracy_list[:len(LL_false_positive_list)]

LL_false_positive_list = 100 * LL_false_positive_list
LL_true_positive_list = 100 * LL_true_positive_list
LL_accuracy_list = 100 * LL_accuracy_list

LL_accuracy_max = LL_accuracy_list.max()

LL_accuracy_subsampled = LL_accuracy_list[30::30]
LL_thresholds_subsampled = LL_thresholds_list[30::30]

acc_bar_x_axis = range(LL_accuracy_subsampled.shape[0])

if show_plots:
    plt.figure(figsize=(15,8))
    plt.subplot(1,2,1); plt.bar(x=acc_bar_x_axis,height=LL_accuracy_subsampled)
    plt.xticks(acc_bar_x_axis, LL_thresholds_subsampled, rotation='vertical')
    plt.title('max accuracy = %.2f%s' %(LL_accuracy_max,'%'), fontsize=24)
    plt.ylim(87.8,100); plt.xlabel('threshold', fontsize=20); plt.ylabel('Accuracy (%)', fontsize=20)
    plt.plot([acc_bar_x_axis[0] - 1, acc_bar_x_axis[-1] + 1], [zero_pred_baseline_accuracy, zero_pred_baseline_accuracy], color='r')

    plt.subplot(1,2,2); plt.plot(LL_false_positive_list, LL_true_positive_list)
    plt.ylabel('True Positive (%)', fontsize=20); plt.xlabel('False Positive (%)', fontsize=20)

experiment_results_dict['model_accuracy_LR'] = LL_accuracy_max

#%% Fit a F&F model

# main parameters
# connections_per_axon = 5
model_type = 'F&F'
#model_type = 'I&F'

# neuron model parameters
num_axons = X_train_spikes[0].shape[0]
num_synapses = connections_per_axon * num_axons

v_reset     = -80
v_threshold = -55
current_to_voltage_mult_factor = 3
refreactory_time_constant = 15

# synapse non-learnable parameters
if model_type == 'F&F':
    tau_rise_range  = [1, 18]
    tau_decay_range = [8, 30]
    # tau_decay_range = [8,48]
elif model_type == 'I&F':
    tau_rise_range  = [ 1, 1]
    tau_decay_range = [30,30]

tau_rise_vec  = np.random.uniform(low=tau_rise_range[0] , high=tau_rise_range[1] , size=(num_synapses, 1))
tau_decay_vec = np.random.uniform(low=tau_decay_range[0], high=tau_decay_range[1], size=(num_synapses, 1))

experiment_results_dict['tau_rise_vec_FF']  = tau_rise_vec
experiment_results_dict['tau_decay_vec_FF'] = tau_decay_vec

# synapse learnable parameters
synaptic_weights_vec = np.random.normal(size=(num_synapses, 1))

# prepare input spikes
axons_input_spikes = np.concatenate([X_train_spikes[k] for k in range(X_train_spikes.shape[0])], axis=1)

# prepare output spikes
pattern_duration_ms = X_train_spikes[0].shape[1]
output_kernel = np.zeros((pattern_duration_ms,))
output_spike_offset = 1
output_kernel[-output_spike_offset] = 1

if create_output_burst:
    output_spike_offset = 6
    output_kernel[-output_spike_offset] = 1
    output_spike_offset = 11
    output_kernel[-output_spike_offset] = 1

desired_output_spikes = np.kron(Y_train_spikes, output_kernel)

if show_plots:
    plt.figure(figsize=(30,15))
    plt.imshow(axons_input_spikes[:,:1101], cmap='gray')
    plt.title('input axons raster', fontsize=22)
    plt.ylabel('axon index', fontsize=22)
    plt.xlabel('time [ms]', fontsize=22)

    plt.figure(figsize=(30,1))
    plt.plot(desired_output_spikes[:1101]); plt.xlim(0,1101)
    plt.ylabel('output spike', fontsize=22)
    plt.xlabel('time [ms]', fontsize=22)

#%%

presynaptic_input_spikes = np.kron(np.ones((connections_per_axon, 1), dtype=bool), axons_input_spikes).astype(bool)

if use_interaction_terms:
    non_interaction_fraction_FF = min(1.0, 2.5 * num_axons / num_synapses)
    non_interaction_fraction_FF = 0.40
interactions_map_FF = generate_dendritic_interactions_map(num_synapses, interactions_degree=interactions_degree, non_interaction_fraction=non_interaction_fraction_FF)

experiment_results_dict['script_main_params']['non_interaction_fraction_FF'] = non_interaction_fraction_FF
experiment_results_dict['interactions_map_FF'] = interactions_map_FF

# simulate cell with normlized currents
local_normlized_currents, soma_voltage, output_spike_times_in_ms = simulate_filter_and_fire_cell_with_interactions_long(presynaptic_input_spikes, interactions_map_FF,
                                                                                                                        synaptic_weights_vec, tau_rise_vec, tau_decay_vec,
                                                                                                                        refreactory_time_constant=refreactory_time_constant,
                                                                                                                        v_reset=v_reset, v_threshold=v_threshold,
                                                                                                                        current_to_voltage_mult_factor=current_to_voltage_mult_factor)


# fit linear model to local currents
filter_and_fire_model = linear_model.LogisticRegression(C=100000, fit_intercept=False, penalty='l2')

# spike_safety_range_ms = 20
# negative_subsampling_fraction = 0.25

X, y = prepare_training_dataset(local_normlized_currents, desired_output_spikes,
                                spike_safety_range_ms=spike_safety_range_ms,
                                negative_subsampling_fraction=negative_subsampling_fraction)

print('----------------------------')
print('number of data points = %d (%.2f%s positive class)' %(X.shape[0], 100 * y.mean(),'%'))
print('----------------------------')

# fit model
filter_and_fire_model.fit(X, y)

# calculate train AUC
y_hat = filter_and_fire_model.predict_proba(X)[:,1]
train_AUC = roc_auc_score(y, y_hat)

print('F&F train AUC = %.5f' %(train_AUC))

if show_plots:
    # display some training data predictions
    num_timepoints_to_show = 10000
    fitted_output_spike_prob = filter_and_fire_model.predict_proba(local_normlized_currents[:,:num_timepoints_to_show].T)[:,1]

    plt.figure(figsize=(30,10))
    plt.plot(1.05 * desired_output_spikes[:num_timepoints_to_show] - 0.025); plt.title('train AUC = %.5f' %(train_AUC), fontsize=22)
    plt.plot(fitted_output_spike_prob[:num_timepoints_to_show]); plt.xlabel('time [ms]'); plt.legend(['GT', 'prediction'], fontsize=22)

#%% display learned weights
normlized_syn_filter = construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec)

# collect learned synaptic weights
FF_learned_synaptic_weights = np.fliplr(filter_and_fire_model.coef_).T
weighted_syn_filter = FF_learned_synaptic_weights * normlized_syn_filter

axon_spatio_temporal_pattern = np.zeros((num_axons, weighted_syn_filter.shape[1]))
for k in range(num_axons):
    axon_spatio_temporal_pattern[k] = weighted_syn_filter[k::num_axons].sum(axis=0)

axon_spatio_temporal_pattern_short = axon_spatio_temporal_pattern[:,:X_train_spikes.shape[2]]

if show_plots:
    plt.figure(figsize=(18,8))
    plt.subplot(1,2,1); plt.imshow(logistic_reg_model.coef_.reshape([X_train_spikes.shape[1], X_train_spikes.shape[2]])); plt.title('logistic regression', fontsize=20)
    plt.subplot(1,2,2); plt.imshow(np.flip(axon_spatio_temporal_pattern_short)); plt.title('filter and fire neuron', fontsize=20)

experiment_results_dict['learned_weights_FF'] = np.flip(axon_spatio_temporal_pattern_short)

#%% Make a prediction on the entire test trace

num_test_patterns = X_test_spikes.shape[0]

# prepare test outputs
# output_spike_tolorance_window_duration = 20
# output_spike_tolorance_window_offset   = 5
output_kernel_test = np.zeros((X_test_spikes[0].shape[1],))
output_kernel_test[-output_spike_tolorance_window_duration:] = 1

desired_output_spikes_test = np.kron(Y_test_spikes[:num_test_patterns], output_kernel_test)
desired_output_spikes_test = np.concatenate((np.zeros((output_spike_tolorance_window_offset,)), desired_output_spikes_test[:-output_spike_tolorance_window_offset]))

# prepare test inputs
axons_input_spikes_test = np.concatenate([X_test_spikes[k] for k in range(num_test_patterns)],axis=1)
presynaptic_input_spikes_test = np.kron(np.ones((connections_per_axon, 1), dtype=bool), axons_input_spikes_test).astype(bool)

# FF_weight_mult_factors_list = [x for x in [2,3,4,5,6,9,20,50,120,250]]
FF_accuracy_list = []
FF_true_positive_list = []
FF_false_positive_list = []
for weight_mult_factor in FF_weight_mult_factors_list:

    # collect learned synaptic weights
    synaptic_weights_post_learning = weight_mult_factor * FF_learned_synaptic_weights

    _, soma_voltage_test, output_spike_times_in_ms_test = simulate_filter_and_fire_cell_with_interactions_long(presynaptic_input_spikes_test, interactions_map_FF,
                                                                                                               synaptic_weights_post_learning, tau_rise_vec, tau_decay_vec,
                                                                                                               refreactory_time_constant=refreactory_time_constant,
                                                                                                               v_reset=v_reset, v_threshold=v_threshold,
                                                                                                               current_to_voltage_mult_factor=current_to_voltage_mult_factor)


    output_spikes_test = np.zeros(soma_voltage_test.shape)
    try:
        output_spikes_test[np.array(output_spike_times_in_ms_test)] = 1.0
    except:
        print('no output spikes created')


    # calculate test accuracy
    compact_desired_output_test = Y_test_spikes[:num_test_patterns]

    compact_desired_output_test2 = np.zeros(compact_desired_output_test.shape, dtype=bool)
    compact_predicted_output_test = np.zeros(compact_desired_output_test.shape, dtype=bool)

    # go over all patterns and extract the prediction (depends if the spikes are in the desired window)
    for pattern_ind in range(num_test_patterns):
        start_ind = pattern_duration_ms * pattern_ind + output_spike_tolorance_window_offset
        end_ind = start_ind + pattern_duration_ms

        # extract prediction
        predicted_spike_train_for_pattern = output_spikes_test[start_ind:end_ind]
        desired_spike_train_for_pattern = desired_output_spikes_test[start_ind:end_ind]

        compact_desired_output_test2[pattern_ind]  = desired_spike_train_for_pattern.sum() > 0.1

        if Y_test_spikes[pattern_ind] == 1:
            # check if there is a spike in the desired window only
            compact_predicted_output_test[pattern_ind] = (desired_spike_train_for_pattern * predicted_spike_train_for_pattern).sum() > 0.1
        else:
            # check if there is any spike in the full pattern duration
            compact_predicted_output_test[pattern_ind] = predicted_spike_train_for_pattern.sum() > 0.1

    # small verificaiton
    assert((compact_desired_output_test == compact_desired_output_test2).sum() == num_test_patterns)

    # display accuracy
    percent_accuracy = 100 * (compact_desired_output_test == compact_predicted_output_test).mean()
    true_positive    = 100 * (np.logical_and(compact_desired_output_test == True , compact_predicted_output_test == True).sum() / (compact_desired_output_test == True).sum())
    false_positive   = 100 * (np.logical_and(compact_desired_output_test == False, compact_predicted_output_test == True).sum() / (compact_desired_output_test == False).sum())

    print('F&F: weights mult factor = %.1f: Accuracy = %.3f%s. (TP, FP) = (%.3f%s, %.3f%s)' %(weight_mult_factor, percent_accuracy,'%',true_positive,'%',false_positive,'%'))

    FF_accuracy_list.append(percent_accuracy)
    FF_true_positive_list.append(true_positive)
    FF_false_positive_list.append(false_positive)

experiment_results_dict['model_accuracy_FF'] = np.array(FF_accuracy_list).max()
experiment_results_dict['model_accuracy_baseline'] = zero_pred_baseline_accuracy

#%% Display accuracy results for F&F model

if show_plots:
    plt.figure(figsize=(15,8))
    plt.subplot(1,2,1); plt.bar(x=range(len(FF_accuracy_list)),height=FF_accuracy_list)
    plt.xticks(range(len(FF_accuracy_list)), FF_weight_mult_factors_list); plt.title('max accuracy = %.2f%s' %(np.array(FF_accuracy_list).max(),'%'), fontsize=24)
    plt.ylim(87.8,100); plt.xlabel('weight mult factor ("gain")', fontsize=20); plt.ylabel('Accuracy (%)', fontsize=20)
    plt.plot([-1, len(FF_accuracy_list)], [zero_pred_baseline_accuracy, zero_pred_baseline_accuracy], color='r')

    plt.subplot(1,2,2); plt.plot(FF_false_positive_list, FF_true_positive_list)
    plt.ylabel('True Positive (%)', fontsize=20); plt.xlabel('False Positive (%)', fontsize=20)

    plt.figure(figsize=(30,6))
    plt.plot(1.05 * desired_output_spikes_test[:25000] - 0.025)
    plt.plot(output_spikes_test[:25000])
    plt.legend(['desired outputs', 'actual outputs'])

    plt.figure(figsize=(30,6))
    plt.plot(1.05 * desired_output_spikes_test[2900:4600] - 0.025)
    plt.plot(output_spikes_test[2900:4600])
    plt.legend(['desired outputs', 'actual outputs'])

    plt.figure(figsize=(30,6))
    plt.plot(1.05 * desired_output_spikes_test[8500:9500] - 0.025)
    plt.plot(output_spikes_test[8500:9500])
    plt.legend(['desired outputs', 'actual outputs'])

#%% F&F model predictions

if show_plots:
    start_time = 100 * np.random.randint(int(axons_input_spikes_test.shape[1] / 100 - 20))
    end_time = start_time + 1 + 100 * 18

    plt.figure(figsize=(30,15))
    plt.subplot(3,1,1); plt.imshow(axons_input_spikes_test[:,start_time:end_time], cmap='gray'); plt.title('input axons raster (test set)', fontsize=22)
    plt.subplot(3,1,2); plt.plot(output_spikes_test[start_time:end_time]); plt.xlim(0,1 + 100 * 18); plt.ylabel('output spike', fontsize=22)
    plt.subplot(3,1,3); plt.plot(soma_voltage_test[start_time:end_time]); plt.xlim(0,1 + 100 * 18); plt.ylabel('soma voltage [mV]', fontsize=22)
    plt.xlabel('time [ms]', fontsize=22)

#%% Fit an I&F model

# main parameters
# connections_per_axon = 5
#model_type = 'F&F'
model_type = 'I&F'

# neuron model parameters
num_axons = X_train_spikes[0].shape[0]
num_synapses = connections_per_axon * num_axons

v_reset     = -80
v_threshold = -55
current_to_voltage_mult_factor = 3
refreactory_time_constant = 15

# synapse non-learnable parameters
if model_type == 'F&F':
    tau_rise_range  = [1, 18]
    tau_decay_range = [8, 30]
elif model_type == 'I&F':
    tau_rise_range  = [ 1, 1]
    tau_decay_range = [30,30]

tau_rise_vec_IF  = np.random.uniform(low=tau_rise_range[0] , high=tau_rise_range[1] , size=(num_synapses, 1))
tau_decay_vec_IF = np.random.uniform(low=tau_decay_range[0], high=tau_decay_range[1], size=(num_synapses, 1))

# synapse learnable parameters
synaptic_weights_vec_IF = np.random.normal(size=(num_synapses, 1))

#%%

if use_interaction_terms:
    non_interaction_fraction_IF = 1.0 * num_axons / num_synapses
    non_interaction_fraction_IF = 0.4
interactions_map_IF = generate_dendritic_interactions_map(num_synapses, interactions_degree=interactions_degree, non_interaction_fraction=non_interaction_fraction_IF)

experiment_results_dict['script_main_params']['non_interaction_fraction_IF'] = non_interaction_fraction_IF
experiment_results_dict['interactions_map_IF'] = interactions_map_IF

# simulate cell with normlized currents
local_normlized_currents_IF, soma_voltage_IF, output_spike_times_in_ms_IF = simulate_filter_and_fire_cell_with_interactions_long(presynaptic_input_spikes, interactions_map_IF,
                                                                                                                                 synaptic_weights_vec_IF, tau_rise_vec_IF, tau_decay_vec_IF,
                                                                                                                                 refreactory_time_constant=refreactory_time_constant,
                                                                                                                                 v_reset=v_reset, v_threshold=v_threshold,
                                                                                                                                 current_to_voltage_mult_factor=current_to_voltage_mult_factor)

# fit linear model to local currents
integrate_and_fire_model = linear_model.LogisticRegression(C=100000, fit_intercept=False, penalty='l2')

# spike_safety_range_ms = 20
# negative_subsampling_fraction = 0.25

X, y = prepare_training_dataset(local_normlized_currents_IF, desired_output_spikes,
                                spike_safety_range_ms=spike_safety_range_ms,
                                negative_subsampling_fraction=negative_subsampling_fraction)

print('----------------------------')
print('number of data points = %d (%.2f%s positive class)' %(X.shape[0], 100 * y.mean(),'%'))
print('----------------------------')

# fit model
integrate_and_fire_model.fit(X, y)

# calculate train AUC
y_hat = integrate_and_fire_model.predict_proba(X)[:,1]
train_AUC = roc_auc_score(y, y_hat)

print('I&F train AUC = %.5f' %(train_AUC))

if show_plots:
    # display some training data predictions
    num_timepoints_to_show = 10000
    fitted_output_spike_prob = integrate_and_fire_model.predict_proba(local_normlized_currents_IF[:,:num_timepoints_to_show].T)[:,1]

    plt.figure(figsize=(30,10))
    plt.plot(1.05 * desired_output_spikes[:num_timepoints_to_show] - 0.025); plt.title('train AUC = %.5f' %(train_AUC), fontsize=22)
    plt.plot(fitted_output_spike_prob[:num_timepoints_to_show]); plt.xlabel('time [ms]'); plt.legend(['GT', 'prediction'], fontsize=22)

#%% display learned weights

# display learned weights
normlized_syn_filter_IF = construct_normlized_synaptic_filter(tau_rise_vec_IF, tau_decay_vec_IF)

# collect learned synaptic weights
IF_learned_synaptic_weights = np.fliplr(integrate_and_fire_model.coef_).T
weighted_syn_filter_IF = IF_learned_synaptic_weights * normlized_syn_filter_IF

axon_spatio_temporal_pattern_IF = np.zeros((num_axons, weighted_syn_filter_IF.shape[1]))
for k in range(num_axons):
    axon_spatio_temporal_pattern_IF[k] = weighted_syn_filter_IF[k::num_axons].sum(axis=0)

axon_spatio_temporal_pattern_short_IF = axon_spatio_temporal_pattern_IF[:,:X_train_spikes.shape[2]]

if show_plots:
    plt.figure(figsize=(24,10))
    plt.subplot(1,3,1); plt.imshow(logistic_reg_model.coef_.reshape([X_train_spikes.shape[1], X_train_spikes.shape[2]])); plt.title('logistic regression', fontsize=24)
    plt.subplot(1,3,2); plt.imshow(np.flip(axon_spatio_temporal_pattern_short)); plt.title('filter and fire neuron', fontsize=24)
    plt.subplot(1,3,3); plt.imshow(np.flip(axon_spatio_temporal_pattern_short_IF)); plt.title('integrate and fire neuron', fontsize=24)

experiment_results_dict['learned_weights_IF'] = np.flip(axon_spatio_temporal_pattern_short_IF)

#%% Display I&F accuracy

# IF_weight_mult_factors_list = [x for x in [10,50,100,1000,10000]]
IF_accuracy_list = []
IF_true_positive_list = []
IF_false_positive_list = []

for weight_mult_factor in IF_weight_mult_factors_list:

    # collect learned synaptic weights
    synaptic_weights_post_learning = weight_mult_factor * IF_learned_synaptic_weights

    _, soma_voltage_test, output_spike_times_in_ms_test = simulate_filter_and_fire_cell_with_interactions_long(presynaptic_input_spikes_test, interactions_map_IF,
                                                                                                               synaptic_weights_post_learning, tau_rise_vec_IF, tau_decay_vec_IF,
                                                                                                               refreactory_time_constant=refreactory_time_constant,
                                                                                                               v_reset=v_reset, v_threshold=v_threshold,
                                                                                                               current_to_voltage_mult_factor=current_to_voltage_mult_factor)


    output_spikes_test = np.zeros(soma_voltage_test.shape)
    try:
        output_spikes_test[np.array(output_spike_times_in_ms_test)] = 1.0
    except:
        print('no output spikes created')


    # calculate test accuracy
    compact_desired_output_test = Y_test_spikes[:num_test_patterns]

    compact_desired_output_test2 = np.zeros(compact_desired_output_test.shape, dtype=bool)
    compact_predicted_output_test = np.zeros(compact_desired_output_test.shape, dtype=bool)

    # go over all patterns and extract the prediction (depends if the spikes are in the desired window)
    for pattern_ind in range(num_test_patterns):
        start_ind = pattern_duration_ms * pattern_ind + output_spike_tolorance_window_offset
        end_ind = start_ind + pattern_duration_ms

        # extract prediction
        predicted_spike_train_for_pattern = output_spikes_test[start_ind:end_ind]
        desired_spike_train_for_pattern = desired_output_spikes_test[start_ind:end_ind]

        compact_desired_output_test2[pattern_ind]  = desired_spike_train_for_pattern.sum() > 0.1

        if Y_test_spikes[pattern_ind] == 1:
            # check if there is a spike in the desired window only
            compact_predicted_output_test[pattern_ind] = (desired_spike_train_for_pattern * predicted_spike_train_for_pattern).sum() > 0.1
        else:
            # check if there is any spike in the full pattern duration
            compact_predicted_output_test[pattern_ind] = predicted_spike_train_for_pattern.sum() > 0.1

    # small verificaiton
    assert((compact_desired_output_test == compact_desired_output_test2).sum() == num_test_patterns)

    # display accuracy
    percent_accuracy = 100 * (compact_desired_output_test == compact_predicted_output_test).mean()
    true_positive    = 100 * (np.logical_and(compact_desired_output_test == True , compact_predicted_output_test == True).sum() / (compact_desired_output_test == True).sum())
    false_positive   = 100 * (np.logical_and(compact_desired_output_test == False, compact_predicted_output_test == True).sum() / (compact_desired_output_test == False).sum())

    print('I&F: weights mult factor = %.1f: Accuracy = %.3f%s. (TP, FP) = (%.3f%s, %.3f%s)' %(weight_mult_factor, percent_accuracy,'%',true_positive,'%',false_positive,'%'))

    IF_accuracy_list.append(percent_accuracy)
    IF_true_positive_list.append(true_positive)
    IF_false_positive_list.append(false_positive)

if show_plots:
    plt.figure(figsize=(15,8))
    plt.subplot(1,2,1); plt.bar(x=range(len(IF_accuracy_list)),height=IF_accuracy_list)
    plt.xticks(range(len(IF_accuracy_list)), IF_weight_mult_factors_list); plt.title('max accuracy = %.2f%s' %(np.array(IF_accuracy_list).max(),'%'), fontsize=24)
    plt.ylim(87.8,100); plt.xlabel('weight mult factor ("gain")', fontsize=20); plt.ylabel('Accuracy (%)', fontsize=20)
    plt.plot([-1, len(IF_accuracy_list)], [zero_pred_baseline_accuracy, zero_pred_baseline_accuracy], color='r')

    plt.subplot(1,2,2); plt.plot(IF_false_positive_list, IF_true_positive_list)
    plt.ylabel('True Positive (%)', fontsize=20); plt.xlabel('False Positive (%)', fontsize=20)


experiment_results_dict['model_accuracy_IF'] = np.array(IF_accuracy_list).max()


if show_plots:
    plt.figure(figsize=(30,6))
    plt.plot(1.05 * desired_output_spikes_test[:25000] - 0.025)
    plt.plot(output_spikes_test[:25000])
    plt.legend(['desired outputs', 'actual outputs'])

    plt.figure(figsize=(30,6))
    plt.plot(1.05 * desired_output_spikes_test[2900:4600] - 0.025)
    plt.plot(output_spikes_test[2900:4600])
    plt.legend(['desired outputs', 'actual outputs'])

    plt.figure(figsize=(30,6))
    plt.plot(1.05 * desired_output_spikes_test[8500:9500] - 0.025)
    plt.plot(output_spikes_test[8500:9500])
    plt.legend(['desired outputs', 'actual outputs'])

#%% I&F model predictions

if show_plots:
    start_time = 100 * np.random.randint(int(axons_input_spikes_test.shape[1] / 100 - 20))
    end_time = start_time + 1 + 100 * 18

    plt.figure(figsize=(30,15))
    plt.subplot(3,1,1); plt.imshow(axons_input_spikes_test[:,start_time:end_time], cmap='gray'); plt.title('input axons raster (test set)', fontsize=22)
    plt.subplot(3,1,2); plt.plot(output_spikes_test[start_time:end_time]); plt.xlim(0,1 + 100 * 18); plt.ylabel('output spike', fontsize=22)
    plt.subplot(3,1,3); plt.plot(soma_voltage_test[start_time:end_time]); plt.xlim(0,1 + 100 * 18); plt.ylabel('soma voltage [mV]', fontsize=22)
    plt.xlabel('time [ms]', fontsize=22)

#%% save results

if use_interaction_terms:
    results_filename = 'MNIST__interactions_%d__digit_%d__N_axons_%d__T_%d__M_%d__p_%0.3d__N_pos_samples_%d__randseed_%d.pickle' %(interactions_degree, positive_digit,
                                                                                                                                  experiment_results_dict['script_main_params']['digit_sample_image_shape_expanded'][0],
                                                                                                                                  experiment_results_dict['script_main_params']['digit_sample_image_shape_expanded'][1],
                                                                                                                                  experiment_results_dict['script_main_params']['connections_per_axon'],
                                                                                                                                  100 * experiment_results_dict['script_main_params']['release_probability'],
                                                                                                                                  num_train_positive_patterns, random_seed)
else:
    results_filename = 'MNIST__digit_%d__N_axons_%d__T_%d__M_%d__p_%0.3d__N_pos_samples_%d__randseed_%d.pickle' %(positive_digit,
                                                                                                                 experiment_results_dict['script_main_params']['digit_sample_image_shape_expanded'][0],
                                                                                                                 experiment_results_dict['script_main_params']['digit_sample_image_shape_expanded'][1],
                                                                                                                 experiment_results_dict['script_main_params']['connections_per_axon'],
                                                                                                                 100 * experiment_results_dict['script_main_params']['release_probability'],
                                                                                                                 num_train_positive_patterns, random_seed)

if not os.path.exists(data_folder):
    os.makedirs(data_folder)

# pickle everythin
pickle.dump(experiment_results_dict, open(data_folder + results_filename, "wb"))

#%% Load the saved pickle just to check it's OK

loaded_script_results_dict = pickle.load(open(data_folder + results_filename, "rb" ))

print('-----------------------------------------------------------------------------------------------------------')
print('loaded_script_results_dict.keys():')
print('----------')
print(list(loaded_script_results_dict.keys()))
print('-----------------------------------------------------------------------------------------------------------')
print('loaded_script_results_dict["script_main_params"].keys():')
print('----------')
print(list(loaded_script_results_dict["script_main_params"].keys()))
print('-----------------------------------------------------------------------------------------------------------')
print('interactions_degree =', loaded_script_results_dict['script_main_params']['interactions_degree'])
print('positive_digit =', loaded_script_results_dict['script_main_params']['positive_digit'])
print('connections_per_axon =', loaded_script_results_dict['script_main_params']['connections_per_axon'])
print('digit_sample_image_shape_cropped =', loaded_script_results_dict['script_main_params']['digit_sample_image_shape_cropped'])
print('digit_sample_image_shape_expanded =', loaded_script_results_dict['script_main_params']['digit_sample_image_shape_expanded'])
print('num_train_positive_patterns =', loaded_script_results_dict['script_main_params']['num_train_positive_patterns'])
print('temporal_silence_ms =', loaded_script_results_dict['script_main_params']['temporal_silence_ms'])
print('release_probability =', loaded_script_results_dict['script_main_params']['release_probability'])
print('train_epochs =', loaded_script_results_dict['script_main_params']['train_epochs'])
print('test_epochs =', loaded_script_results_dict['script_main_params']['test_epochs'])
print('-----------------------------------------------------------------------------------------------------------')
print('model_accuracy_LR =', loaded_script_results_dict['model_accuracy_LR'])
print('model_accuracy_FF =', loaded_script_results_dict['model_accuracy_FF'])
print('model_accuracy_IF =', loaded_script_results_dict['model_accuracy_IF'])
print('model_accuracy_baseline =', loaded_script_results_dict['model_accuracy_baseline'])
print('-----------------------------------------------------------------------------------------------------------')

if show_plots:
    plt.figure(figsize=(24,10))
    plt.subplot(1,3,1); plt.imshow(loaded_script_results_dict['learned_weights_LR']); plt.title('logistic regression', fontsize=24)
    plt.subplot(1,3,2); plt.imshow(loaded_script_results_dict['learned_weights_FF']); plt.title('filter and fire neuron', fontsize=24)
    plt.subplot(1,3,3); plt.imshow(loaded_script_results_dict['learned_weights_IF']); plt.title('integrate and fire neuron', fontsize=24)

script_duration_min = (time.time() - start_time) / 60
print('-----------------------------------')
print('finished script! took %.1f minutes' %(script_duration_min))
print('-----------------------------------')

================================================================================
================================================================================
create_MNIST_figure_Fig3.py:
============================
import numpy as np
import pandas as pd
import pickle
import matplotlib.pyplot as plt
from scipy import signal
from sklearn import linear_model
from sklearn.metrics import roc_curve, roc_auc_score
from tensorflow import keras
import glob
import matplotlib
import matplotlib.gridspec as gridspec

matplotlib.rcParams['pdf.fonttype'] = 42
matplotlib.rcParams['svg.fonttype'] = 'none'

#%% script params

save_figures = True
save_figures = False
all_file_endings_to_use = ['.png', '.pdf', '.svg']

data_folder   = '/filter_and_fire_neuron/results_data_mnist/'
figure_folder = '/filter_and_fire_neuron/saved_figures/'

build_dataframe_from_scratch = False

#%% helper functions


def create_single_PSP_profile(tau_rise, tau_decay, temporal_filter_length=50):

    safety_factor = 1.5
    if tau_rise >= (tau_decay / safety_factor):
        tau_decay = safety_factor * tau_rise

    exp_r = signal.exponential(M=temporal_filter_length, center=0, tau=tau_rise , sym=False)
    exp_d = signal.exponential(M=temporal_filter_length, center=0, tau=tau_decay, sym=False)

    post_syn_potential = exp_d - exp_r
    post_syn_potential /= post_syn_potential.max()

    return post_syn_potential


def construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec):

    num_synapses = tau_rise_vec.shape[0]
    temporal_filter_length = int(4 * tau_decay_vec.max()) + 1

    syn_filter = np.zeros((num_synapses, temporal_filter_length))

    for k, (tau_r, tau_d) in enumerate(zip(tau_rise_vec, tau_decay_vec)):
        syn_filter[k,:] = create_single_PSP_profile(tau_r, tau_d, temporal_filter_length=temporal_filter_length)

    return syn_filter


def simulate_filter_and_fire_cell_training(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                           refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):

    temporal_filter_length = int(5 * refreactory_time_constant) + 1
    refreactory_filter = signal.exponential(M=temporal_filter_length,center=0,tau=refreactory_time_constant,sym=False)[np.newaxis,:]

    # padd input and get all synaptic filters
    normlized_syn_filter = np.flipud(construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec))
    padded_input = np.hstack((np.zeros(normlized_syn_filter.shape), presynaptic_input_spikes))

    # calc local currents
    local_normlized_currents = np.zeros(presynaptic_input_spikes.shape, dtype=np.float16)
    for k in range(normlized_syn_filter.shape[0]):
        local_normlized_currents[k] = signal.convolve(padded_input[k], normlized_syn_filter[k], mode='valid')[1:]

    # multiply by weights to get the somatic current
    soma_current = signal.convolve(local_normlized_currents, synaptic_weights, mode='valid')

    # simulate the cell
    soma_voltage = v_reset + current_to_voltage_mult_factor * soma_current.ravel()
    output_spike_times_in_ms = []
    # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
    for t in range(len(soma_voltage)):
        # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
        if (soma_voltage[t] > v_threshold) and ((t + 1) < len(soma_voltage)):
            t_start = t + 1
            t_end = min(len(soma_voltage), t_start + temporal_filter_length)
            soma_voltage[t_start:t_end] -= (soma_voltage[t + 1] - v_reset) * refreactory_filter.ravel()[:(t_end - t_start)]
            output_spike_times_in_ms.append(t)

    return local_normlized_currents, soma_voltage, output_spike_times_in_ms


def simulate_filter_and_fire_cell_training_long(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                                refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):


    total_duration_ms = presynaptic_input_spikes.shape[1]
    max_duration_per_call_ms = 50000
    overlap_time_ms = 500

    if max_duration_per_call_ms >= total_duration_ms:
        local_normlized_currents, soma_voltage, output_spike_times_in_ms = simulate_filter_and_fire_cell_training(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                                                                                                  refreactory_time_constant=refreactory_time_constant, v_reset=v_reset, v_threshold=v_threshold,
                                                                                                                  current_to_voltage_mult_factor=current_to_voltage_mult_factor)
        return local_normlized_currents, soma_voltage, output_spike_times_in_ms


    local_normlized_currents = np.zeros(presynaptic_input_spikes.shape, dtype=np.float16)
    soma_voltage = np.zeros((total_duration_ms,))
    output_spike_times_in_ms = []

    num_sub_calls = int(np.ceil(total_duration_ms / (max_duration_per_call_ms - overlap_time_ms)))
    end_ind = overlap_time_ms
    for k in range(num_sub_calls):
        start_ind = end_ind - overlap_time_ms
        end_ind = min(start_ind + max_duration_per_call_ms, total_duration_ms)

        curr_loc_norm_c, curr_soma_v, curr_out_sp_t = simulate_filter_and_fire_cell_training(presynaptic_input_spikes[:,start_ind:end_ind], synaptic_weights, tau_rise_vec, tau_decay_vec,
                                                                                             refreactory_time_constant=refreactory_time_constant, v_reset=v_reset, v_threshold=v_threshold,
                                                                                             current_to_voltage_mult_factor=current_to_voltage_mult_factor)

        # update fields
        if k == 0:
            local_normlized_currents[:,start_ind:end_ind] = curr_loc_norm_c
            soma_voltage[start_ind:end_ind] = curr_soma_v
            output_spike_times_in_ms += curr_out_sp_t
        else:
            local_normlized_currents[:,(start_ind+overlap_time_ms):end_ind] = curr_loc_norm_c[:,overlap_time_ms:end_ind]
            soma_voltage[(start_ind+overlap_time_ms):end_ind] = curr_soma_v[overlap_time_ms:]
            curr_out_sp_t = [x for x in curr_out_sp_t if x >= (overlap_time_ms-1)]
            output_spike_times_in_ms += [(start_ind + x) for x in curr_out_sp_t]

    return local_normlized_currents, soma_voltage, output_spike_times_in_ms


def simulate_filter_and_fire_cell_inference(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                            refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):

    temporal_filter_length = int(5 * refreactory_time_constant) + 1
    refreactory_filter = signal.exponential(M=temporal_filter_length,center=0,tau=refreactory_time_constant,sym=False)[np.newaxis,:]

    # padd input and get all synaptic filters
    normlized_syn_filter = construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec)
    padded_input = np.hstack((np.zeros(normlized_syn_filter.shape), presynaptic_input_spikes))

    # calc somatic current
    weighted_syn_filter  = synaptic_weights * normlized_syn_filter
    soma_current = signal.convolve(padded_input, weighted_syn_filter, mode='valid')[:,1:]

    # simulate the cell
    soma_voltage = v_reset + current_to_voltage_mult_factor * soma_current.ravel()
    output_spike_times_in_ms = []
    # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
    for t in range(len(soma_voltage)):

        # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
        if (soma_voltage[t] > v_threshold) and ((t + 1) < len(soma_voltage)):
            t_start = t + 1
            t_end = min(len(soma_voltage), t_start + temporal_filter_length)
            soma_voltage[t_start:t_end] -= (soma_voltage[t + 1] - v_reset) * refreactory_filter.ravel()[:(t_end - t_start)]
            output_spike_times_in_ms.append(t)

    return soma_voltage, output_spike_times_in_ms


def simulate_filter_and_fire_cell_inference_long(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                                refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):

    total_duration_ms = presynaptic_input_spikes.shape[1]
    max_duration_per_call_ms = 50000
    overlap_time_ms = 500

    if max_duration_per_call_ms >= total_duration_ms:
        soma_voltage, output_spike_times_in_ms = simulate_filter_and_fire_cell_inference(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                                                                         refreactory_time_constant=refreactory_time_constant, v_reset=v_reset, v_threshold=v_threshold,
                                                                                         current_to_voltage_mult_factor=current_to_voltage_mult_factor)
        return soma_voltage, output_spike_times_in_ms


    soma_voltage = np.zeros((total_duration_ms,))
    output_spike_times_in_ms = []

    num_sub_calls = int(np.ceil(total_duration_ms / (max_duration_per_call_ms - overlap_time_ms)))
    end_ind = overlap_time_ms
    for k in range(num_sub_calls):
        start_ind = end_ind - overlap_time_ms
        end_ind = min(start_ind + max_duration_per_call_ms, total_duration_ms)

        curr_soma_v, curr_out_sp_t = simulate_filter_and_fire_cell_inference(presynaptic_input_spikes[:,start_ind:end_ind], synaptic_weights, tau_rise_vec, tau_decay_vec,
                                                                             refreactory_time_constant=refreactory_time_constant, v_reset=v_reset, v_threshold=v_threshold,
                                                                             current_to_voltage_mult_factor=current_to_voltage_mult_factor)

        # update fields
        if k == 0:
            soma_voltage[start_ind:end_ind] = curr_soma_v
            output_spike_times_in_ms += curr_out_sp_t
        else:
            soma_voltage[(start_ind+overlap_time_ms):end_ind] = curr_soma_v[overlap_time_ms:]
            curr_out_sp_t = [x for x in curr_out_sp_t if x >= (overlap_time_ms-1)]
            output_spike_times_in_ms += [(start_ind + x) for x in curr_out_sp_t]

    return soma_voltage, output_spike_times_in_ms


# use local currents as "features" and fit a linear model to the data
def prepare_training_dataset(local_normlized_currents, desired_output_spikes, spike_safety_range_ms=10, negative_subsampling_fraction=0.1):

    # remove all "negative" time points that are too close to spikes
    desired_output_spikes_LPF = signal.convolve(desired_output_spikes, np.ones((spike_safety_range_ms,)), mode='same') > 0.1
    desired_timepoints = ~desired_output_spikes_LPF

    # massivly subsample the remaining timepoints
    desired_timepoints[np.random.rand(desired_timepoints.shape[0]) > negative_subsampling_fraction] = 0
    desired_timepoints[desired_output_spikes > 0.1] = 1

    X = local_normlized_currents.T[desired_timepoints,:]
    y = desired_output_spikes[desired_timepoints]

    return X, y


#%% load MNIST dataset and show the data

(x_train_original, y_train), (x_test_original, y_test) = keras.datasets.mnist.load_data()

num_rows = 5
num_cols = 7

plt.close('all')
plt.figure(figsize=(20,15))
for k in range(num_rows * num_cols):
    rand_sample_ind = np.random.randint(x_train_original.shape[0])
    plt.subplot(num_rows, num_cols, k + 1);
    plt.imshow(x_train_original[k]); plt.title('digit "%s"' %(y_train[k]))

#%% Crop the data and binarize it

h_crop_range = [4,24]
w_crop_range = [4,24]

positive_threshold = 150

x_train_original = x_train_original[:,h_crop_range[0]:h_crop_range[1],w_crop_range[0]:w_crop_range[1]] > positive_threshold
x_test_original  = x_test_original[: ,h_crop_range[0]:h_crop_range[1],w_crop_range[0]:w_crop_range[1]] > positive_threshold

num_rows = 5
num_cols = 7

plt.close('all')
plt.figure(figsize=(20,15))
for k in range(num_rows * num_cols):
    rand_sample_ind = np.random.randint(x_train_original.shape[0])
    plt.subplot(num_rows, num_cols, k + 1);
    plt.imshow(x_train_original[k]); plt.title('digit "%s"' %(y_train[k]))

#%% Transform Xs to spatio-temporal spike trains

spatial_extent_factor = 5
temporal_extent_factor_numerator = 2
temporal_extent_factor_denumerator = 1

num_const_firing_channels = 20
temporal_silence_ms = 70

# extend according to "temporal_extent_factor"
kernel = np.ones((1, spatial_extent_factor, temporal_extent_factor_numerator), dtype=bool)

x_train = x_train_original.copy()
x_test  = x_test_original.copy()

# reshape X according to what is needed
x_train = np.kron(x_train, kernel)
x_test = np.kron(x_test, kernel)

# subsample according to "temporal_extent_factor_denumerator"
x_train = x_train[:,:,::temporal_extent_factor_denumerator]
x_test = x_test[:,:,::temporal_extent_factor_denumerator]

# padd with ones on top (for "bias" learning)
top_pad_train = np.ones((1, num_const_firing_channels, x_train.shape[2]), dtype=bool)
top_pad_test  = np.ones((1, num_const_firing_channels, x_test.shape[2] ), dtype=bool)

# add a few zero rows for clear seperation for visualization purpuses
top_pad_train[:,-5:,:] = 0
top_pad_test[:,-5:,:] = 0

x_train = np.concatenate((np.tile(top_pad_train, [x_train.shape[0],1,1]), x_train), axis=1)
x_test  = np.concatenate((np.tile(top_pad_test , [x_test.shape[0],1,1] ), x_test ), axis=1)

# pad with "temporal_silence_ms" zeros in the begining of each pattern (for silence between patterns)
left_pad_train = np.zeros((1, x_train.shape[1], temporal_silence_ms), dtype=bool)
left_pad_test  = np.zeros((1, x_test.shape[1] , temporal_silence_ms), dtype=bool)

x_train = np.concatenate((np.tile(left_pad_train, [x_train.shape[0],1,1]), x_train), axis=2)
x_test  = np.concatenate((np.tile(left_pad_test , [x_test.shape[0],1,1] ), x_test ), axis=2)

# add background activity
desired_background_activity_firing_rate_Hz = 10
background_activity_fraction = desired_background_activity_firing_rate_Hz / 1000

x_train[np.random.rand(x_train.shape[0], x_train.shape[1], x_train.shape[2]) < background_activity_fraction] = 1
x_test[ np.random.rand(x_test.shape[0] , x_test.shape[1] , x_test.shape[2] ) < background_activity_fraction] = 1

# subsample the input spikes
desired_average_input_firing_rate_Hz = 20
actual_mean_firing_rate_Hz = 1000 * x_train.mean()

fraction_of_spikes_to_eliminate = desired_average_input_firing_rate_Hz / actual_mean_firing_rate_Hz

x_train = x_train * (np.random.rand(x_train.shape[0], x_train.shape[1], x_train.shape[2]) < fraction_of_spikes_to_eliminate)
x_test  = x_test  * (np.random.rand(x_test.shape[0], x_test.shape[1], x_test.shape[2]) < fraction_of_spikes_to_eliminate)

final_mean_firing_rate_Hz = 1000 * x_train.mean()

# display the patterns
num_rows = 5
num_cols = 7

plt.close('all')
plt.figure(figsize=(20,15))
for k in range(num_rows * num_cols):
    rand_sample_ind = np.random.randint(x_train.shape[0])
    plt.subplot(num_rows, num_cols, k + 1);
    plt.imshow(x_train[k], cmap='gray'); plt.title('digit "%s"' %(y_train[k]))

#%% Create "one-vs-all" dataset

positive_digit = 3
num_train_positive_patterns = 7000

release_probability = 1.0
apply_release_prob_during_train = False
apply_releash_prob_during_test = False

y_train_binary = y_train == positive_digit
y_test_binary = y_test == positive_digit

num_train_positive_patterns = min(int(y_train_binary.sum()), num_train_positive_patterns)

num_train_negative_patterns = int(2.0 * num_train_positive_patterns)

positive_inds = np.where(y_train_binary)[0]
negative_inds = np.where(~y_train_binary)[0]

selected_train_positives = np.random.choice(positive_inds, size=num_train_positive_patterns)
selected_train_negatives = np.random.choice(negative_inds, size=num_train_negative_patterns)

all_selected = np.random.permutation(np.concatenate((selected_train_positives, selected_train_negatives)))

X_train_spikes = x_train[all_selected]
Y_train_spikes = y_train_binary[all_selected]

X_test_spikes = x_test.copy()
Y_test_spikes = y_test_binary.copy()

zero_pred_baseline_accuracy = 100 * (1 - Y_test_spikes.mean())

#%% Create a regularized logistic regression baseline

logistic_reg_model = linear_model.LogisticRegression(C=0.1, fit_intercept=False, penalty='l2',verbose=False)

# fit model
logistic_reg_model.fit(X_train_spikes.reshape([X_train_spikes.shape[0],-1]), Y_train_spikes)

# predict and calculate AUC on train data
Y_train_spikes_hat = logistic_reg_model.predict_proba(X_train_spikes.reshape([X_train_spikes.shape[0],-1]))[:,1]
Y_test_spikes_hat = logistic_reg_model.predict_proba(X_test_spikes.reshape([X_test_spikes.shape[0],-1]))[:,1]

train_AUC = roc_auc_score(Y_train_spikes, Y_train_spikes_hat)
test_AUC = roc_auc_score(Y_test_spikes, Y_test_spikes_hat)

print('for (# pos = %d, # neg = %d): (train AUC, test AUC) = (%.5f, %.5f)' %(num_train_positive_patterns, num_train_negative_patterns, train_AUC, test_AUC))

plt.close('all')
plt.figure(figsize=(8,8))
plt.imshow(logistic_reg_model.coef_.reshape([X_train_spikes.shape[1], X_train_spikes.shape[2]]));
plt.title('Learned Weights \n (spatio-temporal ("image") logistic regression)');

#%% Calculate and Display LogReg Accuracy

LL_false_positive_list, LL_true_positive_list, LL_thresholds_list = roc_curve(Y_test_spikes, Y_test_spikes_hat)

num_pos_class = int((Y_test_spikes == True).sum())
num_neg_class = int((Y_test_spikes == False).sum())

tp = LL_true_positive_list * num_pos_class
tn = (1 - LL_false_positive_list) * num_neg_class
LL_accuracy_list = (tp + tn) / (num_pos_class + num_neg_class)

LL_false_positive_list = LL_false_positive_list[LL_false_positive_list < 0.05]
LL_true_positive_list = LL_true_positive_list[:len(LL_false_positive_list)]
LL_thresholds_list = LL_thresholds_list[:len(LL_false_positive_list)]
LL_accuracy_list = LL_accuracy_list[:len(LL_false_positive_list)]

LL_false_positive_list = 100 * LL_false_positive_list
LL_true_positive_list = 100 * LL_true_positive_list
LL_accuracy_list = 100 * LL_accuracy_list

LL_accuracy_max = LL_accuracy_list.max()

LL_accuracy_subsampled = LL_accuracy_list[30::30]
LL_thresholds_subsampled = LL_thresholds_list[30::30]

acc_bar_x_axis = range(LL_accuracy_subsampled.shape[0])

plt.close('all')
plt.figure(figsize=(15,8));
plt.subplot(1,2,1); plt.bar(x=acc_bar_x_axis,height=LL_accuracy_subsampled);
plt.xticks(acc_bar_x_axis, LL_thresholds_subsampled, rotation='vertical');
plt.title('max accuracy = %.2f%s' %(LL_accuracy_max,'%'), fontsize=24)
plt.ylim(87.8,100); plt.xlabel('threshold', fontsize=20); plt.ylabel('Accuracy (%)', fontsize=20);
plt.plot([acc_bar_x_axis[0]-1, acc_bar_x_axis[-1]+1], [zero_pred_baseline_accuracy, zero_pred_baseline_accuracy], color='r')
plt.subplot(1,2,2); plt.plot(LL_false_positive_list, LL_true_positive_list);
plt.ylabel('True Positive (%)', fontsize=20); plt.xlabel('False Positive (%)', fontsize=20);


#%% Fit a F&F model

# main parameters
connections_per_axon = 5
model_type = 'F&F'
#model_type = 'I&F'

# neuron model parameters
num_axons = X_train_spikes[0].shape[0]
num_synapses = connections_per_axon * num_axons

v_reset     = -80
v_threshold = -55
current_to_voltage_mult_factor = 3
refreactory_time_constant = 15

# synapse non-learnable parameters
if model_type == 'F&F':
    tau_rise_range  = [1,18]
    tau_decay_range = [8,27]
elif model_type == 'I&F':
    tau_rise_range  = [1,1]
    tau_decay_range = [27,27]

tau_rise_vec  = np.random.uniform(low=tau_rise_range[0] , high=tau_rise_range[1] , size=(num_synapses, 1))
tau_decay_vec = np.random.uniform(low=tau_decay_range[0], high=tau_decay_range[1], size=(num_synapses, 1))

# synapse learnable parameters
synaptic_weights_vec = np.random.normal(size=(num_synapses, 1))

# prepare input spikes
axons_input_spikes = np.concatenate([X_train_spikes[k] for k in range(X_train_spikes.shape[0])],axis=1)

# prepare output spikes
pattern_duration_ms = X_train_spikes[0].shape[1]
output_spike_offset = 1
output_kernel = np.zeros((pattern_duration_ms,))
output_kernel[-output_spike_offset] = 1

desired_output_spikes = np.kron(Y_train_spikes, output_kernel)

plt.close('all')
plt.figure(figsize=(30,15));
plt.imshow(axons_input_spikes[:,:1101], cmap='gray')
plt.title('input axons raster', fontsize=22)
plt.ylabel('axon index', fontsize=22);
plt.xlabel('time [ms]', fontsize=22);

plt.figure(figsize=(30,1));
plt.plot(desired_output_spikes[:1101]); plt.xlim(0,1101)
plt.ylabel('output spike', fontsize=22)
plt.xlabel('time [ms]', fontsize=22);

#%% simulate cell with normlized currents

presynaptic_input_spikes = np.kron(np.ones((connections_per_axon,1), dtype=bool), axons_input_spikes).astype(bool)

local_normlized_currents, soma_voltage, output_spike_times_in_ms = simulate_filter_and_fire_cell_training_long(presynaptic_input_spikes,
                                                                                                               synaptic_weights_vec, tau_rise_vec, tau_decay_vec,
                                                                                                               refreactory_time_constant=refreactory_time_constant,
                                                                                                               v_reset=v_reset, v_threshold=v_threshold,
                                                                                                               current_to_voltage_mult_factor=current_to_voltage_mult_factor)

#%% fit linear model to local currents

filter_and_fire_model = linear_model.LogisticRegression(C=10000, fit_intercept=False, penalty='l2')

spike_safety_range_ms = 20
negative_subsampling_fraction = 0.5

X, y = prepare_training_dataset(local_normlized_currents, desired_output_spikes,
                                spike_safety_range_ms=spike_safety_range_ms,
                                negative_subsampling_fraction=negative_subsampling_fraction)

# fit model
filter_and_fire_model.fit(X, y)

print('number of data points = %d (%.2f%s positive class)' %(X.shape[0], 100 * y.mean(),'%'))

# calculate train AUC
y_hat = filter_and_fire_model.predict_proba(X)[:,1]
train_AUC = roc_auc_score(y, y_hat)

# display some training data predictions
num_timepoints_to_show = 10000
fitted_output_spike_prob = filter_and_fire_model.predict_proba(local_normlized_currents[:,:num_timepoints_to_show].T)[:,1]

plt.close('all')
plt.figure(figsize=(30,10))
plt.plot(1.05 * desired_output_spikes[:num_timepoints_to_show] - 0.025); plt.title('train AUC = %.5f' %(train_AUC), fontsize=22)
plt.plot(fitted_output_spike_prob[:num_timepoints_to_show]); plt.xlabel('time [ms]'); plt.legend(['GT', 'prediction'], fontsize=22);

#%% display learned weights

normlized_syn_filter = construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec)

# collect learned synaptic weights
FF_learned_synaptic_weights = np.fliplr(filter_and_fire_model.coef_).T
weighted_syn_filter = FF_learned_synaptic_weights * normlized_syn_filter

axon_spatio_temporal_pattern = np.zeros((num_axons, weighted_syn_filter.shape[1]))
for k in range(num_axons):
    axon_spatio_temporal_pattern[k] = weighted_syn_filter[k::num_axons].sum(axis=0)

axon_spatio_temporal_pattern_short = axon_spatio_temporal_pattern[:,:X_train_spikes.shape[2]]

plt.close('all')
plt.figure(figsize=(18,8))
plt.subplot(1,2,1); plt.imshow(logistic_reg_model.coef_.reshape([X_train_spikes.shape[1], X_train_spikes.shape[2]])); plt.title('logistic regression', fontsize=20)
plt.subplot(1,2,2); plt.imshow(np.flip(axon_spatio_temporal_pattern_short)); plt.title('filter and fire neuron', fontsize=20);

#%% Make a prediction on the entire test trace

num_test_patterns = X_test_spikes.shape[0]

# prepare test outputs
output_spike_tolorance_window_duration = 20
output_spike_tolorance_window_offset   = 5
output_kernel_test = np.zeros((X_test_spikes[0].shape[1],))
output_kernel_test[-output_spike_tolorance_window_duration:] = 1

desired_output_spikes_test = np.kron(Y_test_spikes[:num_test_patterns], output_kernel_test)
desired_output_spikes_test = np.concatenate((np.zeros((output_spike_tolorance_window_offset,)), desired_output_spikes_test[:-output_spike_tolorance_window_offset]))

# prepare test inputs
axons_input_spikes_test = np.concatenate([X_test_spikes[k] for k in range(num_test_patterns)],axis=1)
presynaptic_input_spikes_test = np.kron(np.ones((connections_per_axon,1), dtype=bool), axons_input_spikes_test).astype(bool)

# add synaptic unrelability ("release probability" that is not 100%)
if apply_releash_prob_during_test:
    presynaptic_input_spikes_test = presynaptic_input_spikes_test * (np.random.rand(presynaptic_input_spikes_test.shape[0], presynaptic_input_spikes_test.shape[1]) < release_probability)

FF_weight_mult_factors_list = [x for x in [1,2,3,4,5,6,9,12,20,50,120,250]]
FF_accuracy_list = []
FF_true_positive_list = []
FF_false_positive_list = []
for weight_mult_factor in FF_weight_mult_factors_list:

    # collect learned synaptic weights
    synaptic_weights_post_learning = weight_mult_factor * FF_learned_synaptic_weights

    soma_voltage_test, output_spike_times_in_ms_test = simulate_filter_and_fire_cell_inference_long(presynaptic_input_spikes_test,
                                                                                                    synaptic_weights_post_learning, tau_rise_vec, tau_decay_vec,
                                                                                                    refreactory_time_constant=refreactory_time_constant,
                                                                                                    v_reset=v_reset, v_threshold=v_threshold,
                                                                                                    current_to_voltage_mult_factor=current_to_voltage_mult_factor)


    output_spikes_test = np.zeros(soma_voltage_test.shape)
    try:
        output_spikes_test[np.array(output_spike_times_in_ms_test)] = 1.0
    except:
        print('no output spikes created')


    # calculate test accuracy
    compact_desired_output_test = Y_test_spikes[:num_test_patterns]

    compact_desired_output_test2 = np.zeros(compact_desired_output_test.shape, dtype=bool)
    compact_predicted_output_test = np.zeros(compact_desired_output_test.shape, dtype=bool)

    # go over all patterns and extract the prediction (depends if the spikes are in the desired window)
    for pattern_ind in range(num_test_patterns):
        start_ind = pattern_duration_ms * pattern_ind + output_spike_tolorance_window_offset
        end_ind = start_ind + pattern_duration_ms

        # extract prediction
        predicted_spike_train_for_pattern = output_spikes_test[start_ind:end_ind]
        desired_spike_train_for_pattern = desired_output_spikes_test[start_ind:end_ind]

        compact_desired_output_test2[pattern_ind]  = desired_spike_train_for_pattern.sum() > 0.1

        if Y_test_spikes[pattern_ind] == 1:
            # check if there is a spike in the desired window only
            compact_predicted_output_test[pattern_ind] = (desired_spike_train_for_pattern * predicted_spike_train_for_pattern).sum() > 0.1
        else:
            # check if there is any spike in the full pattern duration
            compact_predicted_output_test[pattern_ind] = predicted_spike_train_for_pattern.sum() > 0.1

    # small verificaiton
    assert((compact_desired_output_test == compact_desired_output_test2).sum() == num_test_patterns)

    # display accuracy
    percent_accuracy = 100 * (compact_desired_output_test == compact_predicted_output_test).mean()
    true_positive    = 100 * (np.logical_and(compact_desired_output_test == True , compact_predicted_output_test == True).sum() / (compact_desired_output_test == True).sum())
    false_positive   = 100 * (np.logical_and(compact_desired_output_test == False, compact_predicted_output_test == True).sum() / (compact_desired_output_test == False).sum())

    print('weights mult factor = %.1f: Accuracy = %.3f%s. (TP, FP) = (%.3f%s, %.3f%s)' %(weight_mult_factor, percent_accuracy,'%',true_positive,'%',false_positive,'%'))

    FF_accuracy_list.append(percent_accuracy)
    FF_true_positive_list.append(true_positive)
    FF_false_positive_list.append(false_positive)

#%% "after learning" Build the nice looking figure of before and after learning

# get the max accuracy weight matrix
max_accuracy_weight_mult_factor = FF_weight_mult_factors_list[np.argsort(np.array(FF_accuracy_list))[-1]]

synaptic_weights_vec_after_learning = max_accuracy_weight_mult_factor * FF_learned_synaptic_weights

# simulate the max accuracy output after learning
soma_voltage_test, output_spike_times_in_ms_test = simulate_filter_and_fire_cell_inference_long(presynaptic_input_spikes_test,
                                                                                                synaptic_weights_vec_after_learning, tau_rise_vec, tau_decay_vec,
                                                                                                refreactory_time_constant=refreactory_time_constant,
                                                                                                v_reset=v_reset, v_threshold=v_threshold,
                                                                                                current_to_voltage_mult_factor=current_to_voltage_mult_factor)


output_spikes_test_after_learning_full = np.zeros(soma_voltage_test.shape)
try:
    output_spikes_test_after_learning_full[np.array(output_spike_times_in_ms_test)] = 1.0
except:
    print('no output spikes created')

#%% "before learning" Simulate response to test set before learning

synaptic_weights_vec_before_learning = 0.01 + 0.3 * np.random.normal(size=(num_synapses, 1))

# simulate response to test set before learning (randomly permuted learned weights vector)
soma_voltage_test, output_spike_times_in_ms_test = simulate_filter_and_fire_cell_inference_long(presynaptic_input_spikes_test,
                                                                                                synaptic_weights_vec_before_learning, tau_rise_vec, tau_decay_vec,
                                                                                                refreactory_time_constant=refreactory_time_constant,
                                                                                                v_reset=v_reset, v_threshold=v_threshold,
                                                                                                current_to_voltage_mult_factor=current_to_voltage_mult_factor)


output_spikes_test_before_learning_full = np.zeros(soma_voltage_test.shape)
try:
    output_spikes_test_before_learning_full[np.array(output_spike_times_in_ms_test)] = 1.0
except:
    print('no output spikes created')


#%% organize everything into a figure

# test digit images
extention_kernel = np.ones((1, spatial_extent_factor, temporal_extent_factor_numerator), dtype=bool)
x_test_original_extended = np.kron(x_test_original, extention_kernel)
left_pad_test  = np.zeros((1, x_test_original_extended.shape[1] , temporal_silence_ms), dtype=bool)
x_test_original_extended  = np.concatenate((np.tile(left_pad_test , [x_test_original_extended.shape[0],1,1] ), x_test_original_extended ), axis=2)
x_test_axons_input_spikes = np.concatenate([x_test_original_extended[k] for k in range(x_test_original_extended.shape[0])],axis=1)

test_set_full_duration_ms = x_test_axons_input_spikes.shape[1]

# select a subset of time to display
num_digits_to_display = 9
start_time = x_test_original_extended.shape[2] * np.random.randint(int(test_set_full_duration_ms / x_test_original_extended.shape[2] - temporal_silence_ms))
end_time = start_time + 1 + x_test_original_extended.shape[2] * num_digits_to_display + temporal_silence_ms

num_spikes_in_window = 0
while num_spikes_in_window != 3:
    start_time = x_test_original_extended.shape[2] * np.random.randint(int(test_set_full_duration_ms / x_test_original_extended.shape[2] - temporal_silence_ms))
    end_time = start_time + 1 + x_test_original_extended.shape[2] * num_digits_to_display + temporal_silence_ms

    output_spike_tolorance_window_duration = 20
    output_spike_tolorance_window_offset   = 5

    output_kernel_test = np.zeros((X_test_spikes[0].shape[1],))
    output_kernel_test[-1] = 1
    desired_output_spikes_test_full = np.kron(Y_test_spikes[:num_test_patterns], output_kernel_test)
    desired_output_spikes_test = desired_output_spikes_test_full[start_time:end_time]

    num_spikes_in_window = desired_output_spikes_test.sum()

# make sure we have something decent to show (randomise start and end times untill we do)
output_spikes_test_after_learning = output_spikes_test_after_learning_full[start_time:end_time]
while output_spikes_test_after_learning.sum() < 2:
    start_time = x_test_original_extended.shape[2] * np.random.randint(int(test_set_full_duration_ms / x_test_original_extended.shape[2] - temporal_silence_ms))
    end_time = start_time + 1 + x_test_original_extended.shape[2] * num_digits_to_display + temporal_silence_ms
    output_spikes_test_after_learning = output_spikes_test_after_learning_full[start_time:end_time]

min_time_ms = 0
max_time_ms = end_time - start_time

time_sec = np.arange(min_time_ms, max_time_ms) / 1000
min_time_sec = min_time_ms / 1000
max_time_sec = max_time_ms / 1000


before_color = '0.15'
after_color = 'blue'
target_color = 'red'

# input digits
x_test_input_digits = x_test_axons_input_spikes[:,start_time:end_time]

# axon input raster
syn_activation_time, syn_activation_index = np.nonzero(axons_input_spikes_test[-x_test_axons_input_spikes.shape[0]:,start_time:end_time].T)
syn_activation_time = syn_activation_time / 1000
syn_activation_index = x_test_axons_input_spikes.shape[0] - syn_activation_index

# output before learning
output_spikes_test_before_learning = output_spikes_test_before_learning_full[start_time:end_time]

# output after learning
output_spikes_test_after_learning = output_spikes_test_after_learning_full[start_time:end_time]

# desired output
output_spike_tolorance_window_duration = 20
output_spike_tolorance_window_offset   = 5
output_kernel_test = np.zeros((X_test_spikes[0].shape[1],))
output_kernel_test[-1] = 1
desired_output_spikes_test_full = np.kron(Y_test_spikes[:num_test_patterns], output_kernel_test)
desired_output_spikes_test = desired_output_spikes_test_full[start_time:end_time]


# build full figure
plt.close('all')
fig = plt.figure(figsize=(19,22))
gs_figure = gridspec.GridSpec(nrows=4,ncols=3)
gs_figure.update(left=0.03, right=0.97, bottom=0.57, top=0.985, wspace=0.1, hspace=0.11)

ax_digits            = plt.subplot(gs_figure[0,:])
ax_axons             = plt.subplot(gs_figure[1:3,:])
ax_learning_outcomes = plt.subplot(gs_figure[3,:])

ax_digits.imshow(x_test_input_digits, cmap='gray'); ax_digits.set_title('Input Digits', fontsize=18)
ax_digits.set_xticks([])
ax_digits.set_yticks([])
ax_digits.spines['top'].set_visible(False)
ax_digits.spines['bottom'].set_visible(False)
ax_digits.spines['left'].set_visible(False)
ax_digits.spines['right'].set_visible(False)

ax_axons.scatter(syn_activation_time, syn_activation_index, s=6, c='k');
ax_axons.set_ylabel('Input Axons Raster', fontsize=18)
ax_axons.set_xlim(min_time_sec, max_time_sec);
ax_axons.set_xticks([])
ax_axons.set_yticks([])
ax_axons.spines['top'].set_visible(False)
ax_axons.spines['bottom'].set_visible(False)
ax_axons.spines['left'].set_visible(False)
ax_axons.spines['right'].set_visible(False)


ax_learning_outcomes.plot(time_sec, 2.2 + output_spikes_test_before_learning, c=before_color, lw=2.5);
ax_learning_outcomes.plot(time_sec, 1.1 + output_spikes_test_after_learning, c=after_color, lw=2.5);
ax_learning_outcomes.plot(time_sec, 0.0 + desired_output_spikes_test, c=target_color, lw=2.5);

ax_learning_outcomes.set_xlim(min_time_sec, max_time_sec);
ax_learning_outcomes.set_xticks([])
ax_learning_outcomes.set_yticks([])
ax_learning_outcomes.spines['top'].set_visible(False)
ax_learning_outcomes.spines['bottom'].set_visible(False)
ax_learning_outcomes.spines['left'].set_visible(False)
ax_learning_outcomes.spines['right'].set_visible(False)

ax_learning_outcomes.text(0.025,2.5, 'Before Learning', color=before_color, fontsize=20)
ax_learning_outcomes.text(0.025,1.4, 'After Learning', color=after_color, fontsize=20)
ax_learning_outcomes.text(0.025,0.3, 'Desired Output', color=target_color, fontsize=20)

# load data into dataframe

list_of_files = glob.glob(data_folder + 'MNIST_*.pickle')
print(len(list_of_files))

if build_dataframe_from_scratch:

    # Load one saved pickle
    filename_to_load = list_of_files[np.random.randint(len(list_of_files))]
    loaded_script_results_dict = pickle.load(open(filename_to_load, "rb" ))

    # display basic fields in the saved pickle file
    print('-----------------------------------------------------------------------------------------------------------')
    print('loaded_script_results_dict.keys():')
    print('----------')
    print(list(loaded_script_results_dict.keys()))
    print('-----------------------------------------------------------------------------------------------------------')
    print('loaded_script_results_dict["script_main_params"].keys():')
    print('----------')
    print(list(loaded_script_results_dict["script_main_params"].keys()))
    print('-----------------------------------------------------------------------------------------------------------')
    print('positive_digit =', loaded_script_results_dict['script_main_params']['positive_digit'])
    print('connections_per_axon =', loaded_script_results_dict['script_main_params']['connections_per_axon'])
    print('digit_sample_image_shape_cropped =', loaded_script_results_dict['script_main_params']['digit_sample_image_shape_cropped'])
    print('digit_sample_image_shape_expanded =', loaded_script_results_dict['script_main_params']['digit_sample_image_shape_expanded'])
    print('num_train_positive_patterns =', loaded_script_results_dict['script_main_params']['num_train_positive_patterns'])
    print('temporal_silence_ms =', loaded_script_results_dict['script_main_params']['temporal_silence_ms'])
    print('-----------------------------------------------------------------------------------------------------------')
    print('model_accuracy_LR =', loaded_script_results_dict['model_accuracy_LR'])
    print('model_accuracy_FF =', loaded_script_results_dict['model_accuracy_FF'])
    print('model_accuracy_IF =', loaded_script_results_dict['model_accuracy_IF'])
    print('model_accuracy_baseline =', loaded_script_results_dict['model_accuracy_baseline'])
    print('-----------------------------------------------------------------------------------------------------------')

    try:
        print('-----------------------------------------------------------------------------------------------------------')
        print('positive_digit =', loaded_script_results_dict['script_main_params']['positive_digit'])
        print('num_train_positive_patterns =', loaded_script_results_dict['script_main_params']['num_train_positive_patterns'])
        print('release_probability =', loaded_script_results_dict['script_main_params']['release_probability'])
        print('train_epochs =', loaded_script_results_dict['script_main_params']['train_epochs'])
        print('test_epochs =', loaded_script_results_dict['script_main_params']['test_epochs'])
        print('create_output_burst =', loaded_script_results_dict['script_main_params']['create_output_burst'])
        print('-----------------------------------------------------------------------------------------------------------')
    except:
        print('no prob release fields')

    # display the learned weights
    # plt.close('all')
    # plt.figure(figsize=(24,10))
    # plt.subplot(1,3,1); plt.imshow(loaded_script_results_dict['learned_weights_LR']); plt.title('logistic regression', fontsize=24)
    # plt.subplot(1,3,2); plt.imshow(loaded_script_results_dict['learned_weights_FF']); plt.title('filter and fire neuron', fontsize=24)
    # plt.subplot(1,3,3); plt.imshow(loaded_script_results_dict['learned_weights_IF']); plt.title('integrate and fire neuron', fontsize=24)

    # go over all files and insert into a large dataframe
    columns_to_use = ['digit','M_connections','N_axons', 'T', 'N_positive_samples',
                      'Accuracy LR', 'Accuracy FF', 'Accuracy IF', 'Accuracy baseline',
                      'release probability', 'train_epochs', 'test_epochs']
    results_df = pd.DataFrame(index=range(len(list_of_files)), columns=columns_to_use)

    for k, filename_to_load in enumerate(list_of_files):
        loaded_script_results_dict = pickle.load(open(filename_to_load, "rb" ))

        results_df.loc[k, 'digit']              = loaded_script_results_dict['script_main_params']['positive_digit']
        results_df.loc[k, 'M_connections']      = loaded_script_results_dict['script_main_params']['connections_per_axon']
        results_df.loc[k, 'N_axons']            = loaded_script_results_dict['script_main_params']['digit_sample_image_shape_expanded'][0]
        results_df.loc[k, 'T']                  = loaded_script_results_dict['script_main_params']['digit_sample_image_shape_expanded'][1]
        results_df.loc[k, 'N_positive_samples'] = loaded_script_results_dict['script_main_params']['num_train_positive_patterns']
        results_df.loc[k, 'Accuracy LR']        = loaded_script_results_dict['model_accuracy_LR']
        results_df.loc[k, 'Accuracy FF']        = loaded_script_results_dict['model_accuracy_FF']
        results_df.loc[k, 'Accuracy IF']        = loaded_script_results_dict['model_accuracy_IF']
        results_df.loc[k, 'Accuracy baseline']  = loaded_script_results_dict['model_accuracy_baseline']

        try:
            results_df.loc[k, 'release probability'] = loaded_script_results_dict['script_main_params']['release_probability']
            results_df.loc[k, 'train_epochs']        = loaded_script_results_dict['script_main_params']['train_epochs']
            results_df.loc[k, 'test_epochs']         = loaded_script_results_dict['script_main_params']['test_epochs']
        except:
            results_df.loc[k, 'release probability'] = 1.0
            results_df.loc[k, 'train_epochs']        = 1
            results_df.loc[k, 'test_epochs']         = 1

    print(results_df.shape)

    # save the dataframe
    filename = 'MNIST_classification_LR_FF_IF_%d_rows_%d_cols.csv' %(results_df.shape[0], results_df.shape[1])
    results_df.to_csv(data_folder + filename, index=False)
else:
    filename = 'MNIST_classification_LR_FF_IF_5162_rows_12_cols.csv'

# open previously saved file
results_df = pd.read_csv(data_folder + filename)
print(results_df.shape)

# display accuracy per digit at specific condition for (I&F, F&F, LR)
selected_T = 40
selected_M = 5
selected_N_axons = 100
selected_N_samples = 4000
selected_release_prob = 1.0

condition_rows = results_df.loc[:, 'T'] == selected_T
condition_rows = np.logical_and(condition_rows, results_df.loc[:, 'M_connections']       == selected_M)
condition_rows = np.logical_and(condition_rows, results_df.loc[:, 'N_axons']             == selected_N_axons)
condition_rows = np.logical_and(condition_rows, results_df.loc[:, 'N_positive_samples']  >= selected_N_samples)
condition_rows = np.logical_and(condition_rows, results_df.loc[:, 'release probability'] == selected_release_prob)

average_for_condition_per_digit_df = results_df.loc[condition_rows,:].groupby('digit').mean()
stddev_for_condition_per_digit_df  = results_df.loc[condition_rows,:].groupby('digit').std()
counts_for_condition_per_digit_df  = results_df.loc[condition_rows,:].groupby('digit').count().iloc[:,0]

digits_list = average_for_condition_per_digit_df.index.tolist()
LR_accuracy       = average_for_condition_per_digit_df['Accuracy LR'].tolist()
FF_accuracy       = average_for_condition_per_digit_df['Accuracy FF'].tolist()
IF_accuracy       = average_for_condition_per_digit_df['Accuracy IF'].tolist()
baseline_accuracy = average_for_condition_per_digit_df['Accuracy baseline'].tolist()

LR_stderr = stddev_for_condition_per_digit_df['Accuracy LR'].tolist()
FF_stderr = stddev_for_condition_per_digit_df['Accuracy FF'].tolist()
IF_stderr = stddev_for_condition_per_digit_df['Accuracy IF'].tolist()

# display plot
bar_plot_x_axis = 1.0 * np.arange(len(digits_list))
bar_widths = 0.7 / 3
x_tick_names = ['"%s"' %(str(x)) for x in digits_list]

gs_accuracy_per_digit = gridspec.GridSpec(nrows=2,ncols=1)
gs_accuracy_per_digit.update(left=0.045, right=0.675, bottom=0.32, top=0.52, wspace=0.1, hspace=0.1)
ax_accuracy_per_digit = plt.subplot(gs_accuracy_per_digit[:,:])

ax_accuracy_per_digit.bar(bar_plot_x_axis + 0 * bar_widths, IF_accuracy, bar_widths, yerr=IF_stderr, color='0.05'  , label='I&F')
ax_accuracy_per_digit.bar(bar_plot_x_axis + 1 * bar_widths, FF_accuracy, bar_widths, yerr=FF_stderr, color='orange', alpha=0.95, label='F&F')
ax_accuracy_per_digit.bar(bar_plot_x_axis + 2 * bar_widths, LR_accuracy, bar_widths, yerr=LR_stderr, color='0.45'  , label='Spatio\nTemporal LR')

for k in range(len(digits_list)):
    if k == 0:
        ax_accuracy_per_digit.plot([bar_plot_x_axis[k] - 0.7 * bar_widths, bar_plot_x_axis[k] + 2.7 * bar_widths], [baseline_accuracy[k], baseline_accuracy[k]], color='r', label='Baseline')
    else:
        ax_accuracy_per_digit.plot([bar_plot_x_axis[k] - 0.7 * bar_widths, bar_plot_x_axis[k] + 2.7 * bar_widths], [baseline_accuracy[k], baseline_accuracy[k]], color='r')

ax_accuracy_per_digit.set_title('Accuracy Comparison per digit (M = %d, T = %d (ms))' %(selected_M, selected_T), fontsize=22)
ax_accuracy_per_digit.set_yticks([90,92,94,96,98]);
ax_accuracy_per_digit.set_yticklabels([90,92,94,96,98], fontsize=16);
ax_accuracy_per_digit.set_ylim(87.9,98.9);
ax_accuracy_per_digit.set_xticks(bar_plot_x_axis + bar_widths);
ax_accuracy_per_digit.set_xticklabels(x_tick_names, rotation=0, fontsize=26);
ax_accuracy_per_digit.set_ylabel('Test accuracy (%)', fontsize=18)
ax_accuracy_per_digit.legend(fontsize=18, ncol=1);
ax_accuracy_per_digit.set_xlim(-0.5,10);
ax_accuracy_per_digit.spines['top'].set_visible(False)
ax_accuracy_per_digit.spines['right'].set_visible(False)

# display accuracy across all digits as function of pattern presentation duration for (I&F, F&F, LR)
selected_M = 5
selected_N_axons = 100
selected_N_samples = 4000
selected_release_prob = 1.0

condition_rows = results_df.loc[:, 'M_connections'] == selected_M
condition_rows = np.logical_and(condition_rows, results_df.loc[:, 'N_axons']             == selected_N_axons)
condition_rows = np.logical_and(condition_rows, results_df.loc[:, 'N_positive_samples']  >= selected_N_samples)
condition_rows = np.logical_and(condition_rows, results_df.loc[:, 'release probability'] == selected_release_prob)

all_T_values = sorted(results_df['T'].unique().tolist())
digits_list  = sorted(results_df['digit'].unique().tolist())
num_digits   = len(digits_list)

LR_acc_mean = []
FF_acc_mean = []
IF_acc_mean = []
baseline_acc_mean = []

LR_acc_std = []
FF_acc_std = []
IF_acc_std = []
baseline_acc_std = []

for selected_T in all_T_values:
    curr_rows = np.logical_and(condition_rows, results_df.loc[:, 'T'] == selected_T)

    average_acc_per_T_per_digit_df = results_df.loc[curr_rows,:].groupby('digit').mean()
    stddev_acc_per_T_per_digit_df  = results_df.loc[curr_rows,:].groupby('digit').std()
    # print(selected_T, ':\n', results_df.loc[curr_rows,:].groupby('digit').count().iloc[:,0])

    LR_acc_mean.append(average_acc_per_T_per_digit_df['Accuracy LR'].mean())
    FF_acc_mean.append(average_acc_per_T_per_digit_df['Accuracy FF'].mean())
    IF_acc_mean.append(average_acc_per_T_per_digit_df['Accuracy IF'].mean())
    baseline_acc_mean.append(average_acc_per_T_per_digit_df['Accuracy baseline'].mean())

    LR_acc_std.append(average_acc_per_T_per_digit_df['Accuracy LR'].std() / np.sqrt(num_digits))
    FF_acc_std.append(average_acc_per_T_per_digit_df['Accuracy FF'].std() / np.sqrt(num_digits))
    IF_acc_std.append(average_acc_per_T_per_digit_df['Accuracy IF'].std() / np.sqrt(num_digits))
    baseline_acc_std.append(average_acc_per_T_per_digit_df['Accuracy baseline'].std() / np.sqrt(num_digits))

gs_accuracy_vs_T = gridspec.GridSpec(nrows=2,ncols=1)
gs_accuracy_vs_T.update(left=0.045, right=0.3375, bottom=0.04, top=0.28, wspace=0.1, hspace=0.1)
ax_accuracy_vs_T = plt.subplot(gs_accuracy_vs_T[:,:])

ax_accuracy_vs_T.errorbar(all_T_values, LR_acc_mean, yerr=LR_acc_std, linewidth=4, color='0.45')
ax_accuracy_vs_T.errorbar(all_T_values, FF_acc_mean, yerr=FF_acc_std, linewidth=4, color='orange')
ax_accuracy_vs_T.errorbar(all_T_values, IF_acc_mean, yerr=IF_acc_std, linewidth=4, color='0.05')
ax_accuracy_vs_T.errorbar(all_T_values, baseline_acc_mean, yerr=baseline_acc_std, linewidth=1, color='red')
ax_accuracy_vs_T.set_xlabel('Pattern presentation duration - T (ms)', fontsize=18)
ax_accuracy_vs_T.set_ylabel('Test Accuracy (%)', fontsize=18)
ax_accuracy_vs_T.set_ylim(89.5,96.5)
ax_accuracy_vs_T.set_xticks(all_T_values)
ax_accuracy_vs_T.set_xticklabels(all_T_values, fontsize=15)
ax_accuracy_vs_T.set_yticks([90,92,94,96]);
ax_accuracy_vs_T.set_yticklabels([90,92,94,96], fontsize=15);
ax_accuracy_vs_T.spines['top'].set_visible(False)
ax_accuracy_vs_T.spines['right'].set_visible(False)


# display accuracy as function of M, for a specific digit and multiple release probabilities
selected_digit = 2
selected_N_axons = 100
selected_N_samples = 2048

condition_rows = results_df.loc[:, 'digit'] == selected_digit
condition_rows = np.logical_and(condition_rows, results_df.loc[:, 'N_axons']             == selected_N_axons)
condition_rows = np.logical_and(condition_rows, results_df.loc[:, 'N_positive_samples']  == selected_N_samples)

all_M_values = sorted(results_df.loc[condition_rows, 'M_connections'].unique().tolist())
all_P_values = sorted(results_df.loc[condition_rows, 'release probability'].unique().tolist())

all_M_values = [1,2,3,5,8]

results_dict_Acc_vs_M = {}
for selected_release_P in all_P_values:
    results_dict_Acc_vs_M[selected_release_P] = {}
    results_dict_Acc_vs_M[selected_release_P]['LR_acc_mean'] = []
    results_dict_Acc_vs_M[selected_release_P]['FF_acc_mean'] = []
    results_dict_Acc_vs_M[selected_release_P]['IF_acc_mean'] = []
    results_dict_Acc_vs_M[selected_release_P]['baseline_acc_mean'] = []

    results_dict_Acc_vs_M[selected_release_P]['LR_acc_std'] = []
    results_dict_Acc_vs_M[selected_release_P]['FF_acc_std'] = []
    results_dict_Acc_vs_M[selected_release_P]['IF_acc_std'] = []
    results_dict_Acc_vs_M[selected_release_P]['baseline_acc_std'] = []


for selected_M in all_M_values:
    curr_condition_rows = np.logical_and(condition_rows, results_df.loc[:, 'M_connections'] == selected_M)

    for selected_release_P in all_P_values:
        curr_rows = np.logical_and(curr_condition_rows, results_df.loc[:, 'release probability'] == selected_release_P)

        num_rows = curr_rows.sum()
        num_rows = 1
        results_dict_Acc_vs_M[selected_release_P]['LR_acc_mean'].append(results_df.loc[curr_rows,'Accuracy LR'].mean())
        results_dict_Acc_vs_M[selected_release_P]['FF_acc_mean'].append(results_df.loc[curr_rows,'Accuracy FF'].mean())
        results_dict_Acc_vs_M[selected_release_P]['IF_acc_mean'].append(results_df.loc[curr_rows,'Accuracy IF'].mean())
        results_dict_Acc_vs_M[selected_release_P]['baseline_acc_mean'].append(results_df.loc[curr_rows,'Accuracy baseline'].mean())

        results_dict_Acc_vs_M[selected_release_P]['LR_acc_std'].append(results_df.loc[curr_rows,'Accuracy LR'].std() / np.sqrt(num_rows))
        results_dict_Acc_vs_M[selected_release_P]['FF_acc_std'].append(results_df.loc[curr_rows,'Accuracy FF'].std() / np.sqrt(num_rows))
        results_dict_Acc_vs_M[selected_release_P]['IF_acc_std'].append(results_df.loc[curr_rows,'Accuracy IF'].std() / np.sqrt(num_rows))
        results_dict_Acc_vs_M[selected_release_P]['baseline_acc_std'].append(results_df.loc[curr_rows,'Accuracy baseline'].std() / np.sqrt(num_rows))

gs_accuracy_vs_M = gridspec.GridSpec(nrows=2,ncols=1)
gs_accuracy_vs_M.update(left=0.37, right=0.66, bottom=0.04, top=0.28, wspace=0.1, hspace=0.1)
ax_accuracy_vs_M = plt.subplot(gs_accuracy_vs_M[:,:])

selected_release_P = 1.0
ax_accuracy_vs_M.errorbar(all_M_values, results_dict_Acc_vs_M[selected_release_P]['LR_acc_mean'], yerr=results_dict_Acc_vs_M[selected_release_P]['LR_acc_std'], lw=4, color='0.45')
ax_accuracy_vs_M.errorbar(all_M_values, results_dict_Acc_vs_M[selected_release_P]['FF_acc_mean'], yerr=results_dict_Acc_vs_M[selected_release_P]['FF_acc_std'], lw=4, color='orange')
ax_accuracy_vs_M.errorbar(all_M_values, results_dict_Acc_vs_M[selected_release_P]['IF_acc_mean'], yerr=results_dict_Acc_vs_M[selected_release_P]['IF_acc_std'], lw=4, color='0.05')

selected_release_P = 0.5
ax_accuracy_vs_M.errorbar(all_M_values, results_dict_Acc_vs_M[selected_release_P]['LR_acc_mean'], yerr=results_dict_Acc_vs_M[selected_release_P]['LR_acc_std'], lw=4, ls=':', color='0.45')
ax_accuracy_vs_M.errorbar(all_M_values, results_dict_Acc_vs_M[selected_release_P]['FF_acc_mean'], yerr=results_dict_Acc_vs_M[selected_release_P]['FF_acc_std'], lw=4, ls=':', color='orange')
ax_accuracy_vs_M.errorbar(all_M_values, results_dict_Acc_vs_M[selected_release_P]['IF_acc_mean'], yerr=results_dict_Acc_vs_M[selected_release_P]['IF_acc_std'], lw=4, ls=':', color='0.05')

legend_list = ['release Prob = 1.0', 'release Prob = 1.0', 'release Prob = 1.0',
               'P = 0.5', 'P = 0.5', 'P = 0.5']
ax_accuracy_vs_M.legend(legend_list, ncol=2, mode='expand', fontsize=18)

ax_accuracy_vs_M.set_xlabel('Number of Multiple Contacts - M', fontsize=18)
ax_accuracy_vs_M.set_xticks(all_M_values)
ax_accuracy_vs_M.set_xticklabels(all_M_values, fontsize=15)
ax_accuracy_vs_M.set_yticks([91,93,95,97]);
ax_accuracy_vs_M.set_yticklabels([91,93,95,97], fontsize=15);
ax_accuracy_vs_M.set_ylim(90.5,98.2)
ax_accuracy_vs_M.spines['top'].set_visible(False)
ax_accuracy_vs_M.spines['right'].set_visible(False)
ax_accuracy_vs_M.set_xlim(0.8,8.2)


# display accuracy as function of number of positive training samples
selected_digit = 7
selected_T = 30
selected_M = 5
selected_N_axons = 100
max_N_samples = 1100
num_train_epochs = 15

condition_rows = results_df.loc[:, 'digit'] == selected_digit
condition_rows = np.logical_and(condition_rows, results_df.loc[:, 'T']                  == selected_T)
condition_rows = np.logical_and(condition_rows, results_df.loc[:, 'M_connections']      == selected_M)
condition_rows = np.logical_and(condition_rows, results_df.loc[:, 'N_axons']            == selected_N_axons)
condition_rows = np.logical_and(condition_rows, results_df.loc[:, 'N_positive_samples'] <= max_N_samples)

good_epoch_rows = np.logical_or(results_df.loc[:, 'release probability'] == 1.0, results_df.loc[:, 'train_epochs'] == num_train_epochs)
condition_rows  = np.logical_and(condition_rows, good_epoch_rows)

all_N_samples_values = sorted(results_df.loc[condition_rows, 'N_positive_samples'].unique().tolist())
all_P_values         = sorted(results_df.loc[condition_rows, 'release probability'].unique().tolist())

all_N_samples_values = [16, 32, 64, 128, 256, 512, 1024]

results_dict_Acc_vs_N_samples = {}
for selected_release_P in all_P_values:
    results_dict_Acc_vs_N_samples[selected_release_P] = {}
    results_dict_Acc_vs_N_samples[selected_release_P]['LR_acc_mean'] = []
    results_dict_Acc_vs_N_samples[selected_release_P]['FF_acc_mean'] = []
    results_dict_Acc_vs_N_samples[selected_release_P]['IF_acc_mean'] = []
    results_dict_Acc_vs_N_samples[selected_release_P]['baseline_acc_mean'] = []

    results_dict_Acc_vs_N_samples[selected_release_P]['LR_acc_std'] = []
    results_dict_Acc_vs_N_samples[selected_release_P]['FF_acc_std'] = []
    results_dict_Acc_vs_N_samples[selected_release_P]['IF_acc_std'] = []
    results_dict_Acc_vs_N_samples[selected_release_P]['baseline_acc_std'] = []


for selected_N_samples in all_N_samples_values:
    curr_condition_rows = np.logical_and(condition_rows, results_df.loc[:, 'N_positive_samples'] == selected_N_samples)

    for selected_release_P in all_P_values:
        curr_rows = np.logical_and(curr_condition_rows, results_df.loc[:, 'release probability'] == selected_release_P)
        num_rows = curr_rows.sum()

        results_dict_Acc_vs_N_samples[selected_release_P]['LR_acc_mean'].append(results_df.loc[curr_rows,'Accuracy LR'].mean())
        results_dict_Acc_vs_N_samples[selected_release_P]['FF_acc_mean'].append(results_df.loc[curr_rows,'Accuracy FF'].mean())
        results_dict_Acc_vs_N_samples[selected_release_P]['IF_acc_mean'].append(results_df.loc[curr_rows,'Accuracy IF'].mean())
        results_dict_Acc_vs_N_samples[selected_release_P]['baseline_acc_mean'].append(results_df.loc[curr_rows,'Accuracy baseline'].mean())

        results_dict_Acc_vs_N_samples[selected_release_P]['LR_acc_std'].append(results_df.loc[curr_rows,'Accuracy LR'].std() / np.sqrt(num_rows))
        results_dict_Acc_vs_N_samples[selected_release_P]['FF_acc_std'].append(results_df.loc[curr_rows,'Accuracy FF'].std() / np.sqrt(num_rows))
        results_dict_Acc_vs_N_samples[selected_release_P]['IF_acc_std'].append(results_df.loc[curr_rows,'Accuracy IF'].std() / np.sqrt(num_rows))
        results_dict_Acc_vs_N_samples[selected_release_P]['baseline_acc_std'].append(results_df.loc[curr_rows,'Accuracy baseline'].std() / np.sqrt(num_rows))


gs_accuracy_vs_N_samples = gridspec.GridSpec(nrows=2,ncols=1)
gs_accuracy_vs_N_samples.update(left=0.6925, right=0.97, bottom=0.04, top=0.28, wspace=0.1, hspace=0.1)
ax_accuracy_vs_N_samples = plt.subplot(gs_accuracy_vs_N_samples[:,:])

selected_release_P = 1.0
ax_accuracy_vs_N_samples.errorbar(all_N_samples_values, results_dict_Acc_vs_N_samples[selected_release_P]['LR_acc_mean'], yerr=results_dict_Acc_vs_N_samples[selected_release_P]['LR_acc_std'], lw=4, color='0.45')
ax_accuracy_vs_N_samples.errorbar(all_N_samples_values, results_dict_Acc_vs_N_samples[selected_release_P]['FF_acc_mean'], yerr=results_dict_Acc_vs_N_samples[selected_release_P]['FF_acc_std'], lw=4, color='orange')
ax_accuracy_vs_N_samples.errorbar(all_N_samples_values, results_dict_Acc_vs_N_samples[selected_release_P]['IF_acc_mean'], yerr=results_dict_Acc_vs_N_samples[selected_release_P]['IF_acc_std'], lw=4, color='0.05')

selected_release_P = 0.5
ax_accuracy_vs_N_samples.errorbar(all_N_samples_values, results_dict_Acc_vs_N_samples[selected_release_P]['LR_acc_mean'], yerr=results_dict_Acc_vs_N_samples[selected_release_P]['LR_acc_std'], lw=4, ls=':', color='0.45')
ax_accuracy_vs_N_samples.errorbar(all_N_samples_values, results_dict_Acc_vs_N_samples[selected_release_P]['FF_acc_mean'], yerr=results_dict_Acc_vs_N_samples[selected_release_P]['FF_acc_std'], lw=4, ls=':', color='orange')
ax_accuracy_vs_N_samples.errorbar(all_N_samples_values, results_dict_Acc_vs_N_samples[selected_release_P]['IF_acc_mean'], yerr=results_dict_Acc_vs_N_samples[selected_release_P]['IF_acc_std'], lw=4, ls=':', color='0.05')

ax_accuracy_vs_N_samples.set_xlabel('Number of Positive Training Patterns - N', fontsize=18)
ax_accuracy_vs_N_samples.set_xticks(all_N_samples_values)
ax_accuracy_vs_N_samples.set_xticklabels(all_N_samples_values, fontsize=15)
ax_accuracy_vs_N_samples.set_ylim(89.5,97.2)
ax_accuracy_vs_N_samples.set_yticks([90,92,94,96]);
ax_accuracy_vs_N_samples.set_yticklabels([90,92,94,96], fontsize=15);
ax_accuracy_vs_N_samples.spines['top'].set_visible(False)
ax_accuracy_vs_N_samples.spines['right'].set_visible(False)
ax_accuracy_vs_N_samples.set_xticks([16,128,256,512,1024]);
ax_accuracy_vs_N_samples.set_xticklabels([16,128,256,512,1024], fontsize=15);


# open digit 3 and display the learned weights of the 3 models for it
digit = 3
T = 50
N_axons = 100
M_connections = 5
temporal_silence_ms = 70

min_pos_samples = 5000

all_LR_weights = []
all_FF_weights = []
all_IF_weights = []

for k, filename_to_load in enumerate(list_of_files):
    loaded_script_results_dict = pickle.load(open(filename_to_load, "rb" ))

    digit_OK   = loaded_script_results_dict['script_main_params']['positive_digit'] == digit
    M_OK       = loaded_script_results_dict['script_main_params']['connections_per_axon'] == M_connections
    N_axons_OK = loaded_script_results_dict['script_main_params']['digit_sample_image_shape_expanded'][0] == N_axons
    T_OK       = loaded_script_results_dict['script_main_params']['digit_sample_image_shape_expanded'][1] == T
    N_pos_OK   = loaded_script_results_dict['script_main_params']['num_train_positive_patterns'] >= min_pos_samples
    ISS_OK     = loaded_script_results_dict['script_main_params']['temporal_silence_ms'] == temporal_silence_ms

    if digit_OK and M_OK and N_axons_OK and T_OK and N_pos_OK and ISS_OK:
        all_LR_weights.append(loaded_script_results_dict['learned_weights_LR'])
        all_FF_weights.append(loaded_script_results_dict['learned_weights_FF'])
        all_IF_weights.append(loaded_script_results_dict['learned_weights_IF'])

all_LR_weights = np.array(all_LR_weights)
all_FF_weights = np.array(all_FF_weights)
all_IF_weights = np.array(all_IF_weights)

# the first "num_const_firing_channels" axons are a "bias" term, so don't show them
h_start = loaded_script_results_dict['script_main_params']['num_const_firing_channels']
# the first "temporal_silence_ms" time points are silence, so they are boring
w_start = 39

rand_index = np.random.randint(all_LR_weights.shape[0])

single_trial_weights_LR = all_LR_weights[rand_index][h_start:,w_start:]
single_trial_weights_FF = all_FF_weights[rand_index][h_start:,w_start:]
single_trial_weights_IF = all_IF_weights[rand_index][h_start:,w_start:]

mean_weights_LR = all_LR_weights.mean(axis=0)[h_start:,w_start:]
mean_weights_FF = all_FF_weights.mean(axis=0)[h_start:,w_start:]
mean_weights_IF = all_IF_weights.mean(axis=0)[h_start:,w_start:]


def get_weight_symmetric_range(weights_matrix):
    top_value = np.percentile(weights_matrix, 99)
    bottom_value = np.percentile(weights_matrix, 1)
    symmetric_range = np.array([-1,1]) * max(np.abs(top_value), np.abs(bottom_value))

    return symmetric_range


symmetric_weight_range = get_weight_symmetric_range(single_trial_weights_LR)
symmetric_weight_range = get_weight_symmetric_range(mean_weights_LR)
colormap = 'viridis'
vmin = symmetric_weight_range[0]
vmax = symmetric_weight_range[1]

gs_learned_weights = gridspec.GridSpec(nrows=2,ncols=3)
gs_learned_weights.update(left=0.6925, right=0.97, bottom=0.32, top=0.529, wspace=0.06, hspace=0.06)

ax_learned_weights_00 = plt.subplot(gs_learned_weights[0,0])
ax_learned_weights_01 = plt.subplot(gs_learned_weights[0,1])
ax_learned_weights_02 = plt.subplot(gs_learned_weights[0,2])
ax_learned_weights_10 = plt.subplot(gs_learned_weights[1,0])
ax_learned_weights_11 = plt.subplot(gs_learned_weights[1,1])
ax_learned_weights_12 = plt.subplot(gs_learned_weights[1,2])

ax_learned_weights_00.set_title('Spatio-Temporal\n Logistic Regression', fontsize=13);
ax_learned_weights_00.set_ylabel('single trial', fontsize=14)
ax_learned_weights_00.imshow(single_trial_weights_LR, vmin=vmin, vmax=vmax, cmap=colormap);
ax_learned_weights_01.imshow(single_trial_weights_FF, vmin=vmin, vmax=vmax, cmap=colormap);
ax_learned_weights_01.set_title('Filter & Fire\n neuron', fontsize=13)
ax_learned_weights_02.imshow(single_trial_weights_IF, vmin=vmin, vmax=vmax, cmap=colormap);
ax_learned_weights_02.set_title('Integrate & Fire\n neuron', fontsize=13)

ax_learned_weights_10.set_ylabel('mean of %d trials' %(all_LR_weights.shape[0]), fontsize=14)
ax_learned_weights_10.imshow(mean_weights_LR, vmin=vmin, vmax=vmax, cmap=colormap);
ax_learned_weights_11.imshow(mean_weights_FF, vmin=vmin, vmax=vmax, cmap=colormap);
ax_learned_weights_12.imshow(mean_weights_IF, vmin=vmin, vmax=vmax, cmap=colormap);

def set_xy_ticks_ticklabels_to_None(ax_input):
    ax_input.set_xticks([])
    ax_input.set_xticklabels([])
    ax_input.set_yticks([])
    ax_input.set_yticklabels([])

set_xy_ticks_ticklabels_to_None(ax_learned_weights_00)
set_xy_ticks_ticklabels_to_None(ax_learned_weights_01)
set_xy_ticks_ticklabels_to_None(ax_learned_weights_02)
set_xy_ticks_ticklabels_to_None(ax_learned_weights_10)
set_xy_ticks_ticklabels_to_None(ax_learned_weights_11)
set_xy_ticks_ticklabels_to_None(ax_learned_weights_12)


# save figure
if save_figures:
    figure_name = 'F&F_MNIST_Figure_3_%d' %(np.random.randint(200))
    for file_ending in all_file_endings_to_use:
        if file_ending == '.png':
            fig.savefig(figure_folder + figure_name + file_ending, bbox_inches='tight')
        else:
            fig.savefig(figure_folder + figure_name + file_ending, bbox_inches='tight')

#%%


================================================================================
================================================================================
saved_figures/F&F_Axon_Reduction_Figure_5_4.png:
================================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a technical illustration likely related to neural networks or spiking neural networks.  It shows a layered representation of data processing.  The top section displays a large array of tiny dots in three colors (black, orange, and purple), possibly representing different neuron populations or input features. These dots are irregularly distributed, suggesting a stochastic or noisy input.

Below this, three horizontal lines depict time series data. The top line (gold) represents a "F&F" (likely Fire & Forget) signal, showing a series of regularly spaced pulses. The middle line (black) indicates an "I&F" (Integrate & Fire) signal, which is derived from the original axons plus a two-time step delay.  The bottom line (red) shows the "Desired Output," a pattern of pulses that the system aims to produce, which is also a series of regularly spaced pulses.  The number of spikes in the desired output is specified as 40.

The middle section of the image contains a sequence of handwritten digits (6, 5, 3, 0, 7, 2, 7, 4, 6) displayed in three different ways: as a clean, black, high-contrast image; as a set of noisy, scattered dots in purple; and finally, as a set of noisy, scattered dots in orange. This suggests that the digits are being processed or represented in different ways within the system.

The bottom section mirrors the structure of the middle section's timing diagrams, again showing the F&F, I&F, and Desired Output signals.  However, here, the signals are different, representing the processing of a shorter portion of the digit sequence.

Overall, the image appears to illustrate a neural network's input (the dots), its intermediate processing (the I&F signal), and its target output (the Desired Output) for a pattern recognition task involving handwritten digits. The different representations of the digits highlight potential processing stages within the network.

================================================================================
================================================================================
saved_figures/F&F_Introduction_Figure_1_11.png:
===============================================
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
This image is a diagram illustrating the process of synaptic integration in a neuron. It's broken down into three main sections:

**1. Input Axons:** This section on the left shows three input axons firing action potentials at different times.  Each axon is represented by a different color (purple, teal, magenta) indicating distinct input signals. The vertical lines represent the action potentials (spikes) arriving at the synapse.

**2. Synaptic Filters and Synaptic Contact Voltage Contribution:** The central part of the diagram displays two subsections.  The "Synaptic Filters" section shows the postsynaptic response (synaptic potential) of each synapse individually. Each color corresponds to an axon from the previous section. The curves show how the initial spike is filtered and transformed into a more prolonged depolarization or hyperpolarization (depending on the synapse type). The "Synaptic Contact Voltage Contribution" section shows the combined effect of all three inputs, illustrating how the individual synaptic potentials summate at the dendritic branches. The dashed lines likely represent the individual contributions of each synapse, while the solid lines represent their summation.

**3. Somatic Voltage:** This section on the right shows the final result of the synaptic integration process, which is the voltage change at the soma (cell body) of the neuron. The blue and black lines might represent different aspects of the somatic voltage, possibly showing the contribution of different compartments or the effect of different types of ion channels. The significant voltage change indicates the summation of excitatory postsynaptic potentials (EPSPs) causing the neuron to potentially fire its own action potential.

In essence, the diagram visually demonstrates how individual synaptic inputs are filtered, summed, and eventually influence the neuron's firing behavior. It showcases the temporal and spatial aspects of synaptic integration, emphasizing the process by which a neuron integrates multiple signals to produce a unified output.

================================================================================
================================================================================
create_hardware_saving_figure_Fig5.py:
======================================
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from scipy import signal
from sklearn import linear_model
from sklearn.metrics import roc_curve, roc_auc_score, auc
from tensorflow import keras
import matplotlib.gridspec as gridspec

matplotlib.rcParams['pdf.fonttype'] = 42
matplotlib.rcParams['svg.fonttype'] = 'none'

#%% script params

# input parameters
num_axons_FF_cap = 100
time_delays_list_IF_cap = [250, 500]
num_time_delays_IF = len(time_delays_list_IF_cap) + 1
num_axons_IF = num_time_delays_IF * num_axons_FF_cap

stimulus_duration_ms = 10000
requested_number_of_output_spikes = 40
min_time_between_spikes_ms = 135

# neuron model parameters
v_reset     = -80
v_threshold = -55
current_to_voltage_mult_factor = 3
refreactory_time_constant = 15

# F&F neuron model parameters
connections_per_axon_FF = 5
num_synapses_FF = connections_per_axon_FF * num_axons_FF_cap

# synapse non-learnable parameters
tau_rise_range_FF  = [1,16]
tau_decay_range_FF = [8,24]

tau_rise_vec_FF  = np.random.uniform(low=tau_rise_range_FF[0] , high=tau_rise_range_FF[1] , size=(num_synapses_FF, 1))
tau_decay_vec_FF = np.random.uniform(low=tau_decay_range_FF[0], high=tau_decay_range_FF[1], size=(num_synapses_FF, 1))

# synapse learnable parameters
synaptic_weights_vec_FF = np.random.normal(size=(num_synapses_FF, 1))

# I&F neuron model parameters
connections_per_axon_IF = 1
num_synapses_IF = connections_per_axon_IF * num_axons_IF

# synapse non-learnable parameters
tau_rise_range_IF  = [1,1]
tau_decay_range_IF = [24,24]

tau_rise_vec_IF  = np.random.uniform(low=tau_rise_range_IF[0] , high=tau_rise_range_IF[1] , size=(num_synapses_IF, 1))
tau_decay_vec_IF = np.random.uniform(low=tau_decay_range_IF[0], high=tau_decay_range_IF[1], size=(num_synapses_IF, 1))

# synapse learnable parameters
synaptic_weights_vec_IF = np.random.normal(size=(num_synapses_IF, 1))

# book-keeping
save_figures = True
save_figures = False
all_file_endings_to_use = ['.png', '.pdf', '.svg']

figure_folder = '/filter_and_fire_neuron/saved_figures/'

#%% helper functions

def create_single_PSP_profile(tau_rise, tau_decay, temporal_filter_length=50):

    safety_factor = 1.5
    if tau_rise >= (tau_decay / safety_factor):
        tau_decay = safety_factor * tau_rise

    exp_r = signal.exponential(M=temporal_filter_length, center=0, tau=tau_rise , sym=False)
    exp_d = signal.exponential(M=temporal_filter_length, center=0, tau=tau_decay, sym=False)

    post_syn_potential = exp_d - exp_r
    post_syn_potential /= post_syn_potential.max()

    return post_syn_potential


def construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec):

    num_synapses = tau_rise_vec.shape[0]
    temporal_filter_length = int(4 * tau_decay_vec.max()) + 1

    syn_filter = np.zeros((num_synapses, temporal_filter_length))

    for k, (tau_r, tau_d) in enumerate(zip(tau_rise_vec, tau_decay_vec)):
        syn_filter[k,:] = create_single_PSP_profile(tau_r, tau_d, temporal_filter_length=temporal_filter_length)

    return syn_filter


def simulate_filter_and_fire_cell_training(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                           refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):

    temporal_filter_length = int(5 * refreactory_time_constant) + 1
    refreactory_filter = signal.exponential(M=temporal_filter_length,center=0,tau=refreactory_time_constant,sym=False)[np.newaxis,:]

    # padd input and get all synaptic filters
    normlized_syn_filter = np.flipud(construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec))
    padded_input = np.hstack((np.zeros(normlized_syn_filter.shape), presynaptic_input_spikes))

    # calc local currents
    local_normlized_currents = np.zeros(presynaptic_input_spikes.shape)
    for k in range(normlized_syn_filter.shape[0]):
        local_normlized_currents[k] = signal.convolve(padded_input[k], normlized_syn_filter[k], mode='valid')[1:]

    # multiply by weights to get the somatic current
    soma_current = signal.convolve(local_normlized_currents, synaptic_weights, mode='valid')

    # simulate the cell
    soma_voltage = v_reset + current_to_voltage_mult_factor * soma_current.ravel()
    output_spike_times_in_ms = []
    # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
    for t in range(len(soma_voltage)):
        # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
        if (soma_voltage[t] > v_threshold) and ((t + 1) < len(soma_voltage)):
            t_start = t + 1
            t_end = min(len(soma_voltage), t_start + temporal_filter_length)
            soma_voltage[t_start:t_end] -= (soma_voltage[t + 1] - v_reset) * refreactory_filter.ravel()[:(t_end - t_start)]
            output_spike_times_in_ms.append(t)

    return local_normlized_currents, soma_voltage, output_spike_times_in_ms


def simulate_filter_and_fire_cell_training_long(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                                refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):


    total_duration_ms = presynaptic_input_spikes.shape[1]
    max_duration_per_call_ms = 50000
    overlap_time_ms = 500

    if max_duration_per_call_ms >= total_duration_ms:
        local_normlized_currents, soma_voltage, output_spike_times_in_ms = simulate_filter_and_fire_cell_training(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                                                                                                  refreactory_time_constant=refreactory_time_constant, v_reset=v_reset, v_threshold=v_threshold,
                                                                                                                  current_to_voltage_mult_factor=current_to_voltage_mult_factor)
        return local_normlized_currents, soma_voltage, output_spike_times_in_ms


    local_normlized_currents = np.zeros(presynaptic_input_spikes.shape, dtype=np.float16)
    soma_voltage = np.zeros((total_duration_ms,))
    output_spike_times_in_ms = []

    num_sub_calls = int(np.ceil(total_duration_ms / (max_duration_per_call_ms - overlap_time_ms)))
    end_ind = overlap_time_ms
    for k in range(num_sub_calls):
        start_ind = end_ind - overlap_time_ms
        end_ind = min(start_ind + max_duration_per_call_ms, total_duration_ms)

        curr_loc_norm_c, curr_soma_v, curr_out_sp_t = simulate_filter_and_fire_cell_training(presynaptic_input_spikes[:,start_ind:end_ind], synaptic_weights, tau_rise_vec, tau_decay_vec,
                                                                                             refreactory_time_constant=refreactory_time_constant, v_reset=v_reset, v_threshold=v_threshold,
                                                                                             current_to_voltage_mult_factor=current_to_voltage_mult_factor)

        # update fields
        if k == 0:
            local_normlized_currents[:,start_ind:end_ind] = curr_loc_norm_c
            soma_voltage[start_ind:end_ind] = curr_soma_v
            output_spike_times_in_ms += curr_out_sp_t
        else:
            local_normlized_currents[:,(start_ind+overlap_time_ms):end_ind] = curr_loc_norm_c[:,overlap_time_ms:end_ind]
            soma_voltage[(start_ind+overlap_time_ms):end_ind] = curr_soma_v[overlap_time_ms:]
            curr_out_sp_t = [x for x in curr_out_sp_t if x >= (overlap_time_ms-1)]
            output_spike_times_in_ms += [(start_ind + x) for x in curr_out_sp_t]

    return local_normlized_currents, soma_voltage, output_spike_times_in_ms


def simulate_filter_and_fire_cell_inference(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                            refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):

    temporal_filter_length = int(5 * refreactory_time_constant) + 1
    refreactory_filter = signal.exponential(M=temporal_filter_length,center=0,tau=refreactory_time_constant,sym=False)[np.newaxis,:]

    # padd input and get all synaptic filters
    normlized_syn_filter = construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec)
    padded_input = np.hstack((np.zeros(normlized_syn_filter.shape), presynaptic_input_spikes))

    # calc somatic current
    weighted_syn_filter  = synaptic_weights * normlized_syn_filter
    soma_current = signal.convolve(padded_input, weighted_syn_filter, mode='valid')[:,1:]

    # simulate the cell
    soma_voltage = v_reset + current_to_voltage_mult_factor * soma_current.ravel()
    output_spike_times_in_ms = []
    # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
    for t in range(len(soma_voltage)):

        # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
        if (soma_voltage[t] > v_threshold) and ((t + 1) < len(soma_voltage)):
            t_start = t + 1
            t_end = min(len(soma_voltage), t_start + temporal_filter_length)
            soma_voltage[t_start:t_end] -= (soma_voltage[t + 1] - v_reset) * refreactory_filter.ravel()[:(t_end - t_start)]
            output_spike_times_in_ms.append(t)

    return soma_voltage, output_spike_times_in_ms


def simulate_filter_and_fire_cell_inference_long(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                                refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):

    total_duration_ms = presynaptic_input_spikes.shape[1]
    max_duration_per_call_ms = 50000
    overlap_time_ms = 500

    if max_duration_per_call_ms >= total_duration_ms:
        soma_voltage, output_spike_times_in_ms = simulate_filter_and_fire_cell_inference(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                                                                         refreactory_time_constant=refreactory_time_constant, v_reset=v_reset, v_threshold=v_threshold,
                                                                                         current_to_voltage_mult_factor=current_to_voltage_mult_factor)
        return soma_voltage, output_spike_times_in_ms


    soma_voltage = np.zeros((total_duration_ms,))
    output_spike_times_in_ms = []

    num_sub_calls = int(np.ceil(total_duration_ms / (max_duration_per_call_ms - overlap_time_ms)))
    end_ind = overlap_time_ms
    for k in range(num_sub_calls):
        start_ind = end_ind - overlap_time_ms
        end_ind = min(start_ind + max_duration_per_call_ms, total_duration_ms)

        curr_soma_v, curr_out_sp_t = simulate_filter_and_fire_cell_inference(presynaptic_input_spikes[:,start_ind:end_ind], synaptic_weights, tau_rise_vec, tau_decay_vec,
                                                                             refreactory_time_constant=refreactory_time_constant, v_reset=v_reset, v_threshold=v_threshold,
                                                                             current_to_voltage_mult_factor=current_to_voltage_mult_factor)

        # update fields
        if k == 0:
            soma_voltage[start_ind:end_ind] = curr_soma_v
            output_spike_times_in_ms += curr_out_sp_t
        else:
            soma_voltage[(start_ind+overlap_time_ms):end_ind] = curr_soma_v[overlap_time_ms:]
            curr_out_sp_t = [x for x in curr_out_sp_t if x >= (overlap_time_ms-1)]
            output_spike_times_in_ms += [(start_ind + x) for x in curr_out_sp_t]

    return soma_voltage, output_spike_times_in_ms


def prepare_training_dataset(local_normlized_currents, desired_output_spikes, spike_safety_range_ms=10, negative_subsampling_fraction=0.1):

    # remove all "negative" time points that are too close to spikes
    desired_output_spikes_LPF = signal.convolve(desired_output_spikes, np.ones((spike_safety_range_ms,)), mode='same') > 0.1
    desired_timepoints = ~desired_output_spikes_LPF

    # massivly subsample the remaining timepoints
    desired_timepoints[np.random.rand(desired_timepoints.shape[0]) > negative_subsampling_fraction] = 0
    desired_timepoints[desired_output_spikes > 0.1] = 1

    X = local_normlized_currents.T[desired_timepoints,:]
    y = desired_output_spikes[desired_timepoints]

    return X, y


#%% define random input for capacity plot

# generate sample input
axons_input_spikes_capacity = np.random.rand(num_axons_FF_cap, stimulus_duration_ms) < 0.0016

# F&F
presynaptic_input_spikes_FF = np.kron(np.ones((connections_per_axon_FF,1)), axons_input_spikes_capacity)
assert presynaptic_input_spikes_FF.shape[0] == num_synapses_FF, 'number of synapses doesnt match the number of presynaptic inputs'

# I&F
presynaptic_input_spikes_IF = axons_input_spikes_capacity.copy()
for delay_ms in time_delays_list_IF_cap:
    curr_delay_axons_input_spikes_IF = np.zeros(axons_input_spikes_capacity.shape)
    curr_delay_axons_input_spikes_IF[:,delay_ms:] = axons_input_spikes_capacity[:,:-delay_ms]
    presynaptic_input_spikes_IF = np.vstack((presynaptic_input_spikes_IF, curr_delay_axons_input_spikes_IF))
assert presynaptic_input_spikes_IF.shape[0] == num_synapses_IF, 'number of synapses doesnt match the number of presynaptic inputs'


# generate desired pattern of output spikes
desired_output_spike_times = min_time_between_spikes_ms * np.random.randint(int(stimulus_duration_ms / min_time_between_spikes_ms), size=requested_number_of_output_spikes)
desired_output_spike_times = np.sort(np.unique(desired_output_spike_times))

desired_output_spikes = np.zeros((stimulus_duration_ms,))
desired_output_spikes[desired_output_spike_times] = 1.0

print('number of requested output spikes = %d' %(requested_number_of_output_spikes))


#%% fit F&F model to the input

# simulate cell with normlized currents
local_normlized_currents_FF, _, _ = simulate_filter_and_fire_cell_training(presynaptic_input_spikes_FF,
                                                                           synaptic_weights_vec_FF, tau_rise_vec_FF, tau_decay_vec_FF,
                                                                           refreactory_time_constant=refreactory_time_constant,
                                                                           v_reset=v_reset, v_threshold=v_threshold,
                                                                           current_to_voltage_mult_factor=current_to_voltage_mult_factor)


# fit linear model to local currents
filter_and_fire_model = linear_model.LogisticRegression(C=30000, fit_intercept=True, penalty='l2', max_iter=3000)

spike_safety_range_ms = 1
negative_subsampling_fraction = 0.99

X, y = prepare_training_dataset(local_normlized_currents_FF, desired_output_spikes,
                                spike_safety_range_ms=spike_safety_range_ms,
                                negative_subsampling_fraction=negative_subsampling_fraction)

# fit model
filter_and_fire_model.fit(X,y)

print('number of data points = %d (%.2f%s positive class)' %(X.shape[0], 100 * y.mean(),'%'))

y_hat = filter_and_fire_model.predict_proba(X)[:,1]

# calculate AUC
train_AUC = roc_auc_score(y, y_hat)

fitted_output_spike_prob_FF = filter_and_fire_model.predict_proba(local_normlized_currents_FF.T)[:,1]
full_AUC = roc_auc_score(desired_output_spikes, fitted_output_spike_prob_FF)

# get desired FP threshold
desired_false_positive_rate = 0.004

fpr, tpr, thresholds = roc_curve(desired_output_spikes, fitted_output_spike_prob_FF)

desired_fp_ind = np.argmin(abs(fpr-desired_false_positive_rate))
if desired_fp_ind == 0:
    desired_fp_ind = 1

actual_false_positive_rate = fpr[desired_fp_ind]
true_positive_rate         = tpr[desired_fp_ind]
desired_fp_threshold       = thresholds[desired_fp_ind]

AUC_score = auc(fpr, tpr)

if AUC_score > 0.9995:
    desired_fp_threshold = 0.15

print('F&F fitting AUC = %.4f' %(AUC_score))
print('at %.4f FP rate, TP = %.4f' %(actual_false_positive_rate, true_positive_rate))

output_spikes_after_learning_FF = fitted_output_spike_prob_FF > desired_fp_threshold

#%% fit I&F to the input with 2 delayed versions of the same input

# simulate cell with normlized currents
local_normlized_currents_IF, _, _ = simulate_filter_and_fire_cell_training(presynaptic_input_spikes_IF,
                                                                           synaptic_weights_vec_IF, tau_rise_vec_IF, tau_decay_vec_IF,
                                                                           refreactory_time_constant=refreactory_time_constant,
                                                                           v_reset=v_reset, v_threshold=v_threshold,
                                                                           current_to_voltage_mult_factor=current_to_voltage_mult_factor)


# fit linear model to local currents
integrate_and_fire_model = linear_model.LogisticRegression(C=30000, fit_intercept=True, penalty='l2', max_iter=3000)

spike_safety_range_ms = 1
negative_subsampling_fraction = 0.99

X, y = prepare_training_dataset(local_normlized_currents_IF, desired_output_spikes,
                                spike_safety_range_ms=spike_safety_range_ms,
                                negative_subsampling_fraction=negative_subsampling_fraction)

# fit model
integrate_and_fire_model.fit(X,y)

print('number of data points = %d (%.2f%s positive class)' %(X.shape[0], 100 * y.mean(),'%'))

y_hat = integrate_and_fire_model.predict_proba(X)[:,1]

# calculate AUC
train_AUC = roc_auc_score(y, y_hat)

fitted_output_spike_prob_IF = integrate_and_fire_model.predict_proba(local_normlized_currents_IF.T)[:,1]
full_AUC = roc_auc_score(desired_output_spikes, fitted_output_spike_prob_IF)

# get desired FP threshold
desired_false_positive_rate = 0.004

fpr, tpr, thresholds = roc_curve(desired_output_spikes, fitted_output_spike_prob_IF)

desired_fp_ind = np.argmin(abs(fpr - desired_false_positive_rate))
if desired_fp_ind == 0:
    desired_fp_ind = 1

actual_false_positive_rate = fpr[desired_fp_ind]
true_positive_rate         = tpr[desired_fp_ind]
desired_fp_threshold       = thresholds[desired_fp_ind]

AUC_score = auc(fpr, tpr)

if AUC_score > 0.9995:
    desired_fp_threshold = 0.15

print('I&F fitting AUC = %.4f' %(AUC_score))
print('at %.4f FP rate, TP = %.4f' %(actual_false_positive_rate, true_positive_rate))

output_spikes_after_learning_IF = fitted_output_spike_prob_IF > desired_fp_threshold

#%% MNIST params

spatial_extent_factor = 5
temporal_extent_factor_numerator = 2
temporal_extent_factor_denumerator = 1

num_const_firing_channels = 20
temporal_silence_ms = 70

positive_digit = 7
num_train_positive_patterns = 2000

release_probability = 1.0
apply_release_prob_during_train = False
apply_releash_prob_during_test = False

output_spike_tolorance_window_duration = 20
output_spike_tolorance_window_offset   = 5

time_delays_list_IF = [12,24]
num_axons_FF = spatial_extent_factor * 20 + num_const_firing_channels
num_time_delays_IF = len(time_delays_list_IF) + 1
num_axons_IF = num_time_delays_IF * num_axons_FF

num_synapses_FF = connections_per_axon_FF * num_axons_FF
connections_per_axon_IF = 1
num_synapses_IF = connections_per_axon_IF * num_axons_IF

# synapse non-learnable parameters
tau_rise_range_FF  = [1,16]
tau_decay_range_FF = [8,24]

tau_rise_vec_FF  = np.random.uniform(low=tau_rise_range_FF[0] , high=tau_rise_range_FF[1] , size=(num_synapses_FF, 1))
tau_decay_vec_FF = np.random.uniform(low=tau_decay_range_FF[0], high=tau_decay_range_FF[1], size=(num_synapses_FF, 1))

# synapse learnable parameters
synaptic_weights_vec_FF = np.random.normal(size=(num_synapses_FF, 1))

# I&F neuron model parameters
connections_per_axon_IF = 1
num_synapses_IF = connections_per_axon_IF * num_axons_IF

# synapse non-learnable parameters
tau_rise_range_IF  = [1,1]
tau_decay_range_IF = [24,24]

tau_rise_vec_IF  = np.random.uniform(low=tau_rise_range_IF[0] , high=tau_rise_range_IF[1] , size=(num_synapses_IF, 1))
tau_decay_vec_IF = np.random.uniform(low=tau_decay_range_IF[0], high=tau_decay_range_IF[1], size=(num_synapses_IF, 1))

# synapse learnable parameters
synaptic_weights_vec_IF = np.random.normal(size=(num_synapses_IF, 1))

#%% load MNIST dataset and and transform into spikes

(x_train_original, y_train), (x_test_original, y_test) = keras.datasets.mnist.load_data()

# crop the data and binarize it
h_crop_range = [4,24]
w_crop_range = [4,24]

positive_threshold = 150

x_train_original = x_train_original[:,h_crop_range[0]:h_crop_range[1],w_crop_range[0]:w_crop_range[1]] > positive_threshold
x_test_original  = x_test_original[: ,h_crop_range[0]:h_crop_range[1],w_crop_range[0]:w_crop_range[1]] > positive_threshold

#%% Transform Xs to spatio-temporal spike trains

# extend according to "temporal_extent_factor"
kernel = np.ones((1, spatial_extent_factor, temporal_extent_factor_numerator), dtype=bool)

x_train = x_train_original.copy()
x_test  = x_test_original.copy()

# reshape X according to what is needed
x_train = np.kron(x_train, kernel)
x_test = np.kron(x_test, kernel)

# subsample according to "temporal_extent_factor_denumerator"
x_train = x_train[:,:,::temporal_extent_factor_denumerator]
x_test = x_test[:,:,::temporal_extent_factor_denumerator]

# padd with ones on top (for "bias" learning)
top_pad_train = np.ones((1, num_const_firing_channels, x_train.shape[2]), dtype=bool)
top_pad_test  = np.ones((1, num_const_firing_channels, x_test.shape[2] ), dtype=bool)

# add a few zero rows for clear seperation for visualization purpuses
top_pad_train[:,-5:,:] = 0
top_pad_test[:,-5:,:] = 0

x_train = np.concatenate((np.tile(top_pad_train, [x_train.shape[0],1,1]), x_train), axis=1)
x_test  = np.concatenate((np.tile(top_pad_test , [x_test.shape[0],1,1] ), x_test ), axis=1)

# pad with "temporal_silence_ms" zeros in the begining of each pattern (for silence between patterns)
left_pad_train = np.zeros((1, x_train.shape[1], temporal_silence_ms), dtype=bool)
left_pad_test  = np.zeros((1, x_test.shape[1] , temporal_silence_ms), dtype=bool)

x_train = np.concatenate((np.tile(left_pad_train, [x_train.shape[0],1,1]), x_train), axis=2)
x_test  = np.concatenate((np.tile(left_pad_test , [x_test.shape[0],1,1] ), x_test ), axis=2)

# add background activity
desired_background_activity_firing_rate_Hz = 10
background_activity_fraction = desired_background_activity_firing_rate_Hz / 1000

x_train[np.random.rand(x_train.shape[0], x_train.shape[1], x_train.shape[2]) < background_activity_fraction] = 1
x_test[ np.random.rand(x_test.shape[0] , x_test.shape[1] , x_test.shape[2] ) < background_activity_fraction] = 1

# subsample the input spikes
desired_average_input_firing_rate_Hz = 20
actual_mean_firing_rate_Hz = 1000 * x_train.mean()

fraction_of_spikes_to_eliminate = desired_average_input_firing_rate_Hz / actual_mean_firing_rate_Hz

x_train = x_train * (np.random.rand(x_train.shape[0], x_train.shape[1], x_train.shape[2]) < fraction_of_spikes_to_eliminate)
x_test  = x_test  * (np.random.rand(x_test.shape[0], x_test.shape[1], x_test.shape[2]) < fraction_of_spikes_to_eliminate)

final_mean_firing_rate_Hz = 1000 * x_train.mean()

#%% Create "one-vs-all" dataset

y_train_binary = y_train == positive_digit
y_test_binary  = y_test  == positive_digit

num_train_positive_patterns = min(int(y_train_binary.sum()), num_train_positive_patterns)

num_train_negative_patterns = int(2.0 * num_train_positive_patterns)

positive_inds = np.where(y_train_binary)[0]
negative_inds = np.where(~y_train_binary)[0]

selected_train_positives = np.random.choice(positive_inds, size=num_train_positive_patterns)
selected_train_negatives = np.random.choice(negative_inds, size=num_train_negative_patterns)

all_selected = np.random.permutation(np.concatenate((selected_train_positives, selected_train_negatives)))

X_train_spikes = x_train[all_selected]
Y_train_spikes = y_train_binary[all_selected]

X_test_spikes = x_test.copy()
Y_test_spikes = y_test_binary.copy()

zero_pred_baseline_accuracy = 100 * (1 - Y_test_spikes.mean())


#%% prepare input spikes for F&F and I&F training

axons_input_spikes = np.concatenate([X_train_spikes[k] for k in range(X_train_spikes.shape[0])],axis=1)

# prepare output spikes
pattern_duration_ms = X_train_spikes[0].shape[1]
output_spike_offset = 1
output_kernel = np.zeros((pattern_duration_ms,))
output_kernel[-output_spike_offset] = 1

desired_output_spikes_mnist = np.kron(Y_train_spikes, output_kernel)

# F&F
presynaptic_input_spikes_FF = np.kron(np.ones((connections_per_axon_FF,1)), axons_input_spikes)
assert presynaptic_input_spikes_FF.shape[0] == num_synapses_FF, 'number of synapses doesnt match the number of presynaptic inputs'

# I&F
presynaptic_input_spikes_IF = axons_input_spikes.copy()
for delay_ms in time_delays_list_IF:
    curr_delay_axons_input_spikes_IF = np.zeros(axons_input_spikes.shape)
    curr_delay_axons_input_spikes_IF[:,delay_ms:] = axons_input_spikes[:,:-delay_ms]
    presynaptic_input_spikes_IF = np.vstack((presynaptic_input_spikes_IF, curr_delay_axons_input_spikes_IF))
assert presynaptic_input_spikes_IF.shape[0] == num_synapses_IF, 'number of synapses doesnt match the number of presynaptic inputs'


#%% prepare input spikes for F&F and I&F testing

num_test_patterns = X_test_spikes.shape[0]

# prepare test outputs
output_kernel_test = np.zeros((X_test_spikes[0].shape[1],))
output_kernel_test[-output_spike_tolorance_window_duration:] = 1

desired_output_spikes_test = np.kron(Y_test_spikes[:num_test_patterns], output_kernel_test)
desired_output_spikes_test = np.concatenate((np.zeros((output_spike_tolorance_window_offset,)), desired_output_spikes_test[:-output_spike_tolorance_window_offset]))

# prepare test inputs
axons_input_spikes_test = np.concatenate([X_test_spikes[k] for k in range(num_test_patterns)],axis=1)

# F&F
presynaptic_input_spikes_test_FF = np.kron(np.ones((connections_per_axon_FF,1)), axons_input_spikes_test)
assert presynaptic_input_spikes_test_FF.shape[0] == num_synapses_FF, 'number of synapses doesnt match the number of presynaptic inputs'

# I&F
presynaptic_input_spikes_test_IF = axons_input_spikes_test.copy()
for delay_ms in time_delays_list_IF:
    curr_delay_axons_input_spikes_IF = np.zeros(axons_input_spikes_test.shape)
    curr_delay_axons_input_spikes_IF[:,delay_ms:] = axons_input_spikes_test[:,:-delay_ms]
    presynaptic_input_spikes_test_IF = np.vstack((presynaptic_input_spikes_test_IF, curr_delay_axons_input_spikes_IF))
assert presynaptic_input_spikes_test_IF.shape[0] == num_synapses_IF, 'number of synapses doesnt match the number of presynaptic inputs'


#%% simulate F&F cell with normlized currents on train

local_normlized_currents_FF, _, _ = simulate_filter_and_fire_cell_training_long(presynaptic_input_spikes_FF,
                                                                                synaptic_weights_vec_FF, tau_rise_vec_FF, tau_decay_vec_FF,
                                                                                refreactory_time_constant=refreactory_time_constant,
                                                                                v_reset=v_reset, v_threshold=v_threshold,
                                                                                current_to_voltage_mult_factor=current_to_voltage_mult_factor)

#%% fit linear model to local currents

filter_and_fire_model = linear_model.LogisticRegression(C=10000, fit_intercept=False, penalty='l2')

spike_safety_range_ms = 20
negative_subsampling_fraction = 0.5

X, y = prepare_training_dataset(local_normlized_currents_FF, desired_output_spikes_mnist,
                                spike_safety_range_ms=spike_safety_range_ms,
                                negative_subsampling_fraction=negative_subsampling_fraction)

# fit model
filter_and_fire_model.fit(X, y)

print('number of data points = %d (%.2f%s positive class)' %(X.shape[0], 100 * y.mean(),'%'))

# calculate train AUC
y_hat = filter_and_fire_model.predict_proba(X)[:,1]
train_AUC = roc_auc_score(y, y_hat)

FF_learned_synaptic_weights = np.fliplr(filter_and_fire_model.coef_).T

#%% find the best multiplicative factor for test prediction F&F

# FF_weight_mult_factors_list = [1,2,3,4,5,6,9,12,20,50,120,250]
FF_weight_mult_factors_list = [1,4,7,11,15]
FF_accuracy_list = []
FF_true_positive_list = []
FF_false_positive_list = []
for weight_mult_factor in FF_weight_mult_factors_list:

    # collect learned synaptic weights
    synaptic_weights_post_learning = weight_mult_factor * FF_learned_synaptic_weights

    soma_voltage_test, output_spike_times_in_ms_test = simulate_filter_and_fire_cell_inference_long(presynaptic_input_spikes_test_FF,
                                                                                                    synaptic_weights_post_learning, tau_rise_vec_FF, tau_decay_vec_FF,
                                                                                                    refreactory_time_constant=refreactory_time_constant,
                                                                                                    v_reset=v_reset, v_threshold=v_threshold,
                                                                                                    current_to_voltage_mult_factor=current_to_voltage_mult_factor)


    output_spikes_test = np.zeros(soma_voltage_test.shape)
    try:
        output_spikes_test[np.array(output_spike_times_in_ms_test)] = 1.0
    except:
        print('no output spikes created')


    # calculate test accuracy
    compact_desired_output_test = Y_test_spikes[:num_test_patterns]

    compact_desired_output_test2 = np.zeros(compact_desired_output_test.shape, dtype=bool)
    compact_predicted_output_test = np.zeros(compact_desired_output_test.shape, dtype=bool)

    # go over all patterns and extract the prediction (depends if the spikes are in the desired window)
    for pattern_ind in range(num_test_patterns):
        start_ind = pattern_duration_ms * pattern_ind + output_spike_tolorance_window_offset
        end_ind = start_ind + pattern_duration_ms

        # extract prediction
        predicted_spike_train_for_pattern = output_spikes_test[start_ind:end_ind]
        desired_spike_train_for_pattern = desired_output_spikes_test[start_ind:end_ind]

        compact_desired_output_test2[pattern_ind]  = desired_spike_train_for_pattern.sum() > 0.1

        if Y_test_spikes[pattern_ind] == 1:
            # check if there is a spike in the desired window only
            compact_predicted_output_test[pattern_ind] = (desired_spike_train_for_pattern * predicted_spike_train_for_pattern).sum() > 0.1
        else:
            # check if there is any spike in the full pattern duration
            compact_predicted_output_test[pattern_ind] = predicted_spike_train_for_pattern.sum() > 0.1

    # small verificaiton
    assert((compact_desired_output_test == compact_desired_output_test2).sum() == num_test_patterns)

    # display accuracy
    percent_accuracy = 100 * (compact_desired_output_test == compact_predicted_output_test).mean()
    true_positive    = 100 * (np.logical_and(compact_desired_output_test == True , compact_predicted_output_test == True).sum() / (compact_desired_output_test == True).sum())
    false_positive   = 100 * (np.logical_and(compact_desired_output_test == False, compact_predicted_output_test == True).sum() / (compact_desired_output_test == False).sum())

    print('weights mult factor = %.1f: Accuracy = %.3f%s. (TP, FP) = (%.3f%s, %.3f%s)' %(weight_mult_factor, percent_accuracy,'%',true_positive,'%',false_positive,'%'))

    FF_accuracy_list.append(percent_accuracy)
    FF_true_positive_list.append(true_positive)
    FF_false_positive_list.append(false_positive)

#%% make a final prediction on the test set F&F

# get the max accuracy weight matrix
max_accuracy_weight_mult_factor = FF_weight_mult_factors_list[np.argsort(np.array(FF_accuracy_list))[-1]]

synaptic_weights_vec_after_learning_FF = max_accuracy_weight_mult_factor * FF_learned_synaptic_weights

# simulate the max accuracy output after learning
soma_voltage_test, output_spike_times_in_ms_test_FF = simulate_filter_and_fire_cell_inference_long(presynaptic_input_spikes_test_FF,
                                                                                                   synaptic_weights_vec_after_learning_FF, tau_rise_vec_FF, tau_decay_vec_FF,
                                                                                                   refreactory_time_constant=refreactory_time_constant,
                                                                                                   v_reset=v_reset, v_threshold=v_threshold,
                                                                                                   current_to_voltage_mult_factor=current_to_voltage_mult_factor)


output_spikes_test_after_learning_full_FF = np.zeros(soma_voltage_test.shape)
try:
    output_spikes_test_after_learning_full_FF[np.array(output_spike_times_in_ms_test)] = 1.0
except:
    print('no output spikes created')


#%% simulate I&F cell with normlized currents on train

local_normlized_currents_IF, _, _ = simulate_filter_and_fire_cell_training_long(presynaptic_input_spikes_IF,
                                                                                synaptic_weights_vec_IF, tau_rise_vec_IF, tau_decay_vec_IF,
                                                                                refreactory_time_constant=refreactory_time_constant,
                                                                                v_reset=v_reset, v_threshold=v_threshold,
                                                                                current_to_voltage_mult_factor=current_to_voltage_mult_factor)

#%% fit linear model to local currents

integrate_and_fire_model = linear_model.LogisticRegression(C=10000, fit_intercept=False, penalty='l2')

spike_safety_range_ms = 20
negative_subsampling_fraction = 0.5

X, y = prepare_training_dataset(local_normlized_currents_IF, desired_output_spikes_mnist,
                                spike_safety_range_ms=spike_safety_range_ms,
                                negative_subsampling_fraction=negative_subsampling_fraction)

# fit model
integrate_and_fire_model.fit(X, y)

print('number of data points = %d (%.2f%s positive class)' %(X.shape[0], 100 * y.mean(),'%'))

# calculate train AUC
y_hat = integrate_and_fire_model.predict_proba(X)[:,1]
train_AUC = roc_auc_score(y, y_hat)

IF_learned_synaptic_weights = np.fliplr(integrate_and_fire_model.coef_).T

#%% find the best multiplicative factor for test prediction F&F

# FF_weight_mult_factors_list = [1,2,3,4,5,6,9,12,20,50,120,250]
IF_weight_mult_factors_list = [1,4,7,11,15]
IF_accuracy_list = []
IF_true_positive_list = []
IF_false_positive_list = []
for weight_mult_factor in IF_weight_mult_factors_list:

    # collect learned synaptic weights
    synaptic_weights_post_learning = weight_mult_factor * IF_learned_synaptic_weights

    soma_voltage_test, output_spike_times_in_ms_test = simulate_filter_and_fire_cell_inference_long(presynaptic_input_spikes_test_IF,
                                                                                                    synaptic_weights_post_learning, tau_rise_vec_IF, tau_decay_vec_IF,
                                                                                                    refreactory_time_constant=refreactory_time_constant,
                                                                                                    v_reset=v_reset, v_threshold=v_threshold,
                                                                                                    current_to_voltage_mult_factor=current_to_voltage_mult_factor)


    output_spikes_test = np.zeros(soma_voltage_test.shape)
    try:
        output_spikes_test[np.array(output_spike_times_in_ms_test)] = 1.0
    except:
        print('no output spikes created')


    # calculate test accuracy
    compact_desired_output_test = Y_test_spikes[:num_test_patterns]

    compact_desired_output_test2 = np.zeros(compact_desired_output_test.shape, dtype=bool)
    compact_predicted_output_test = np.zeros(compact_desired_output_test.shape, dtype=bool)

    # go over all patterns and extract the prediction (depends if the spikes are in the desired window)
    for pattern_ind in range(num_test_patterns):
        start_ind = pattern_duration_ms * pattern_ind + output_spike_tolorance_window_offset
        end_ind = start_ind + pattern_duration_ms

        # extract prediction
        predicted_spike_train_for_pattern = output_spikes_test[start_ind:end_ind]
        desired_spike_train_for_pattern = desired_output_spikes_test[start_ind:end_ind]

        compact_desired_output_test2[pattern_ind]  = desired_spike_train_for_pattern.sum() > 0.1

        if Y_test_spikes[pattern_ind] == 1:
            # check if there is a spike in the desired window only
            compact_predicted_output_test[pattern_ind] = (desired_spike_train_for_pattern * predicted_spike_train_for_pattern).sum() > 0.1
        else:
            # check if there is any spike in the full pattern duration
            compact_predicted_output_test[pattern_ind] = predicted_spike_train_for_pattern.sum() > 0.1

    # small verificaiton
    assert((compact_desired_output_test == compact_desired_output_test2).sum() == num_test_patterns)

    # display accuracy
    percent_accuracy = 100 * (compact_desired_output_test == compact_predicted_output_test).mean()
    true_positive    = 100 * (np.logical_and(compact_desired_output_test == True , compact_predicted_output_test == True).sum() / (compact_desired_output_test == True).sum())
    false_positive   = 100 * (np.logical_and(compact_desired_output_test == False, compact_predicted_output_test == True).sum() / (compact_desired_output_test == False).sum())

    print('weights mult factor = %.1f: Accuracy = %.3f%s. (TP, FP) = (%.3f%s, %.3f%s)' %(weight_mult_factor, percent_accuracy,'%',true_positive,'%',false_positive,'%'))

    IF_accuracy_list.append(percent_accuracy)
    IF_true_positive_list.append(true_positive)
    IF_false_positive_list.append(false_positive)

#%% make a final prediction on the test set F&F

# get the max accuracy weight matrix
max_accuracy_weight_mult_factor = IF_weight_mult_factors_list[np.argsort(np.array(IF_accuracy_list))[-1]]

synaptic_weights_vec_after_learning_IF = max_accuracy_weight_mult_factor * IF_learned_synaptic_weights

# simulate the max accuracy output after learning
soma_voltage_test, output_spike_times_in_ms_test_IF = simulate_filter_and_fire_cell_inference_long(presynaptic_input_spikes_test_IF,
                                                                                                   synaptic_weights_vec_after_learning_IF, tau_rise_vec_IF, tau_decay_vec_IF,
                                                                                                   refreactory_time_constant=refreactory_time_constant,
                                                                                                   v_reset=v_reset, v_threshold=v_threshold,
                                                                                                   current_to_voltage_mult_factor=current_to_voltage_mult_factor)


output_spikes_test_after_learning_full_IF = np.zeros(soma_voltage_test.shape)
try:
    output_spikes_test_after_learning_full_IF[np.array(output_spike_times_in_ms_test)] = 1.0
except:
    print('no output spikes created')


#%% Build the figure


plt.close('all')
fig = plt.figure(figsize=(12,22))
gs_figure = gridspec.GridSpec(nrows=17,ncols=1)
gs_figure.update(left=0.04, right=0.95, bottom=0.02, top=0.98, wspace=0.45, hspace=0.4)

ax_axons          = plt.subplot(gs_figure[:5,:])
ax_FF_spikes      = plt.subplot(gs_figure[5,:])
ax_IF_spikes      = plt.subplot(gs_figure[6,:])
ax_desired_output = plt.subplot(gs_figure[7,:])

ax_digits            = plt.subplot(gs_figure[9:11,:])
ax_axons_mnist       = plt.subplot(gs_figure[11:15,:])
ax_learning_outcomes = plt.subplot(gs_figure[15:,:])

FF_color = 'orange'
IF_color = '0.05'
target_color = 'red'

syn_activation_time, syn_activation_index = np.nonzero(presynaptic_input_spikes_IF.T)

syn_activation_time_1_cap, syn_activation_index_1_cap = np.nonzero(axons_input_spikes_capacity.T)
syn_activation_time_1_cap = syn_activation_time_1_cap / 1000

syn_activation_time_2_cap = syn_activation_time_1_cap + (time_delays_list_IF_cap[0] / 1000)
syn_activation_time_3_cap = syn_activation_time_1_cap + (time_delays_list_IF_cap[1] / 1000)
syn_activation_index_2_cap = syn_activation_index_1_cap + num_axons_FF_cap
syn_activation_index_3_cap = syn_activation_index_2_cap + num_axons_FF_cap

min_time_sec = -0.1
max_time_sec = stimulus_duration_ms / 1000
time_sec = np.linspace(0, max_time_sec, output_spikes_after_learning_FF.shape[0])

ax_axons.scatter(syn_activation_time_1_cap, syn_activation_index_1_cap, s=8, c='black');
ax_axons.scatter(syn_activation_time_2_cap, syn_activation_index_2_cap, s=8, c='chocolate');
ax_axons.scatter(syn_activation_time_3_cap, syn_activation_index_3_cap, s=8, c='purple');
ax_axons.set_xlim(min_time_sec, max_time_sec);
ax_axons.set_xticks([])
ax_axons.set_yticks([])
ax_axons.spines['top'].set_visible(False)
ax_axons.spines['bottom'].set_visible(False)
ax_axons.spines['left'].set_visible(False)
ax_axons.spines['right'].set_visible(False)

ax_FF_spikes.plot(time_sec, output_spikes_after_learning_FF, c=FF_color, lw=2.5);
ax_FF_spikes.set_title('F&F (M = 5)', fontsize=17, color=FF_color)
ax_FF_spikes.set_xlim(min_time_sec, max_time_sec);
ax_FF_spikes.set_xticks([])
ax_FF_spikes.set_yticks([])
ax_FF_spikes.spines['top'].set_visible(False)
ax_FF_spikes.spines['bottom'].set_visible(False)
ax_FF_spikes.spines['left'].set_visible(False)
ax_FF_spikes.spines['right'].set_visible(False)

ax_IF_spikes.plot(time_sec, output_spikes_after_learning_IF, c=IF_color, lw=2.5);
ax_IF_spikes.set_title('I&F (Orig Axons + 2 delayed)', fontsize=17, color=IF_color)
ax_IF_spikes.set_xlim(min_time_sec, max_time_sec);
ax_IF_spikes.set_xticks([])
ax_IF_spikes.set_yticks([])
ax_IF_spikes.spines['top'].set_visible(False)
ax_IF_spikes.spines['bottom'].set_visible(False)
ax_IF_spikes.spines['left'].set_visible(False)
ax_IF_spikes.spines['right'].set_visible(False)

ax_desired_output.plot(time_sec, desired_output_spikes, c=target_color, lw=2.5);
ax_desired_output.set_title('Desired Output (num spikes = %d)' %(requested_number_of_output_spikes), fontsize=17, color=target_color);
ax_desired_output.set_xlim(min_time_sec, max_time_sec);
ax_desired_output.set_xticks([]);
ax_desired_output.set_yticks([]);
ax_desired_output.spines['top'].set_visible(False)
ax_desired_output.spines['bottom'].set_visible(False)
ax_desired_output.spines['left'].set_visible(False)
ax_desired_output.spines['right'].set_visible(False)





##%% organize MNIST part into a figure

# test digit images
extention_kernel = np.ones((1, spatial_extent_factor, temporal_extent_factor_numerator), dtype=bool)
x_test_original_extended = np.kron(x_test_original, extention_kernel)
left_pad_test  = np.zeros((1, x_test_original_extended.shape[1] , temporal_silence_ms), dtype=bool)
x_test_original_extended  = np.concatenate((np.tile(left_pad_test , [x_test_original_extended.shape[0],1,1] ), x_test_original_extended ), axis=2)
x_test_axons_input_spikes = np.concatenate([x_test_original_extended[k] for k in range(x_test_original_extended.shape[0])],axis=1)

test_set_full_duration_ms = x_test_axons_input_spikes.shape[1]

# select a subset of time to display
num_digits_to_display = 9
start_time = x_test_original_extended.shape[2] * np.random.randint(int(test_set_full_duration_ms / x_test_original_extended.shape[2] - temporal_silence_ms))
end_time = start_time + 1 + x_test_original_extended.shape[2] * num_digits_to_display + temporal_silence_ms

# make sure we have something decent to show (randomise start and end times untill we do)
output_spikes_test_after_learning = output_spikes_test_after_learning_full_FF[start_time:end_time]
while output_spikes_test_after_learning.sum() < 2:
    start_time = x_test_original_extended.shape[2] * np.random.randint(int(test_set_full_duration_ms / x_test_original_extended.shape[2] - temporal_silence_ms))
    end_time = start_time + 1 + x_test_original_extended.shape[2] * num_digits_to_display + temporal_silence_ms
    output_spikes_test_after_learning = output_spikes_test_after_learning_full_FF[start_time:end_time]

min_time_ms = 0
max_time_ms = end_time - start_time

time_sec_mnist = np.arange(min_time_ms, max_time_ms) / 1000
min_time_sec_mnist = min_time_ms / 1000
max_time_sec_mnist = max_time_ms / 1000

# input digits
x_test_input_digits = x_test_axons_input_spikes[:,start_time:end_time]

syn_activation_time_1, syn_activation_index_1 = np.nonzero(axons_input_spikes_test[-x_test_axons_input_spikes.shape[0]:,start_time:end_time].T)
syn_activation_time_1 = syn_activation_time_1 / 1000
syn_activation_index_1 = x_test_axons_input_spikes.shape[0] - syn_activation_index_1

syn_activation_time_2 = syn_activation_time_1 + (time_delays_list_IF[0] / 1000)
syn_activation_time_3 = syn_activation_time_1 + (time_delays_list_IF[1] / 1000)
syn_activation_index_2 = syn_activation_index_1 + num_axons_FF
syn_activation_index_3 = syn_activation_index_2 + num_axons_FF


# output after learning
output_spikes_test_after_learning_FF = output_spikes_test_after_learning_full_FF[start_time:end_time]
output_spikes_test_after_learning_IF = output_spikes_test_after_learning_full_IF[start_time:end_time]

# desired output
output_kernel_test = np.zeros((X_test_spikes[0].shape[1],))
output_kernel_test[-1] = 1
desired_output_spikes_test_full = np.kron(Y_test_spikes[:num_test_patterns], output_kernel_test)
desired_output_spikes_test_mnist = desired_output_spikes_test_full[start_time:end_time]


# build figure
ax_digits.imshow(x_test_input_digits, cmap='gray');
ax_digits.set_xticks([])
ax_digits.set_yticks([])
ax_digits.spines['top'].set_visible(False)
ax_digits.spines['bottom'].set_visible(False)
ax_digits.spines['left'].set_visible(False)
ax_digits.spines['right'].set_visible(False)

ax_axons_mnist.scatter(syn_activation_time_1, syn_activation_index_1, s=8, c='black');
ax_axons_mnist.scatter(syn_activation_time_2, syn_activation_index_2, s=8, c='chocolate');
ax_axons_mnist.scatter(syn_activation_time_3, syn_activation_index_3, s=8, c='purple');
ax_axons_mnist.set_xlim(min_time_sec_mnist, max_time_sec_mnist);
ax_axons_mnist.set_xticks([])
ax_axons_mnist.set_yticks([])
ax_axons_mnist.spines['top'].set_visible(False)
ax_axons_mnist.spines['bottom'].set_visible(False)
ax_axons_mnist.spines['left'].set_visible(False)
ax_axons_mnist.spines['right'].set_visible(False)


ax_learning_outcomes.plot(time_sec_mnist, 2.2 + output_spikes_test_after_learning_FF, c=FF_color, lw=2.5);
ax_learning_outcomes.plot(time_sec_mnist, 1.1 + output_spikes_test_after_learning_IF, c=IF_color, lw=2.5);
ax_learning_outcomes.plot(time_sec_mnist, 0.0 + desired_output_spikes_test_mnist, c=target_color, lw=2.5);

ax_learning_outcomes.set_xlim(min_time_sec_mnist, max_time_sec_mnist);
ax_learning_outcomes.set_xticks([])
ax_learning_outcomes.set_yticks([])
ax_learning_outcomes.spines['top'].set_visible(False)
ax_learning_outcomes.spines['bottom'].set_visible(False)
ax_learning_outcomes.spines['left'].set_visible(False)
ax_learning_outcomes.spines['right'].set_visible(False)

ax_learning_outcomes.text(0.02,2.5, 'F&F (M=5)', color=FF_color, fontsize=20)
ax_learning_outcomes.text(0.02,1.4, 'I&F (Orig Axons + 2 delayed)', color=IF_color, fontsize=20)
ax_learning_outcomes.text(0.02,0.3, 'Desired Output', color=target_color, fontsize=20)


# save figure
if save_figures:
    figure_name = 'F&F_hardware_saving_Figure_5_%d' %(np.random.randint(200))
    for file_ending in all_file_endings_to_use:
        if file_ending == '.png':
            fig.savefig(figure_folder + figure_name + file_ending, bbox_inches='tight')
        else:
            fig.savefig(figure_folder + figure_name + file_ending, bbox_inches='tight')


================================================================================
================================================================================
create_intro_figure_Fig1.py:
============================
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
import matplotlib.gridspec as gridspec
from scipy import signal

matplotlib.rcParams['pdf.fonttype'] = 42
matplotlib.rcParams['svg.fonttype'] = 'none'

#%% script params

num_axons = 3
num_dendrites = 9

axon_colors = ['magenta', 'teal', 'purple']

num_spikes_per_axon = 3
experiment_time_ms  = 300

tau_rise_range  = [1,9]
tau_decay_range = [5,30]

v_reset     = -80
v_threshold = -55
current_to_voltage_mult_factor = 4
refreactory_time_constant = 25

save_figures = False
all_file_endings_to_use = ['.png', '.pdf', '.svg']

figure_folder = '/filter_and_fire_neuron/saved_figures/'

#%% helper functions


def create_single_PSP_profile(tau_rise, tau_decay, temporal_filter_length=50):

    if tau_rise >= tau_decay:
        tau_decay = tau_rise + 5

    exp_r = signal.exponential(M=temporal_filter_length, center=0, tau=tau_rise , sym=False)
    exp_d = signal.exponential(M=temporal_filter_length, center=0, tau=tau_decay, sym=False)

    post_syn_potential = exp_d - exp_r
    post_syn_potential /= post_syn_potential.max()

    return post_syn_potential


def construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec):

    num_synapses = tau_rise_vec.shape[0]
    temporal_filter_length = int(7 * tau_decay_vec.max()) + 1

    syn_filter = np.zeros((num_synapses, temporal_filter_length))

    for k, (tau_r, tau_d) in enumerate(zip(tau_rise_vec, tau_decay_vec)):
        syn_filter[k,:] = create_single_PSP_profile(tau_r, tau_d, temporal_filter_length=temporal_filter_length)

    return syn_filter


def simulate_filter_and_fire_cell_training(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,
                                           refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):

    temporal_filter_length = int(5 * refreactory_time_constant) + 1
    refreactory_filter = signal.exponential(M=temporal_filter_length,center=0,tau=refreactory_time_constant,sym=False)[np.newaxis,:]

    # padd input and get all synaptic filters
    normlized_syn_filter = np.flipud(construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec))
    padded_input = np.hstack((np.zeros(normlized_syn_filter.shape), presynaptic_input_spikes))

    # calc local currents
    local_normlized_currents = np.zeros(presynaptic_input_spikes.shape, dtype=np.float16)
    for k in range(normlized_syn_filter.shape[0]):
        local_normlized_currents[k] = signal.convolve(padded_input[k], normlized_syn_filter[k], mode='valid')[1:]

    # multiply by weights to get the somatic current
    soma_current = signal.convolve(local_normlized_currents, synaptic_weights, mode='valid')

    # simulate the cell
    soma_voltage = v_reset + current_to_voltage_mult_factor * soma_current.ravel()
    output_spike_times_in_ms = []
    # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
    for t in range(len(soma_voltage)):
        # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)
        if (soma_voltage[t] > v_threshold) and ((t + 1) < len(soma_voltage)):
            t_start = t + 1
            t_end = min(len(soma_voltage), t_start + temporal_filter_length)
            soma_voltage[t_start:t_end] -= (soma_voltage[t + 1] - v_reset) * refreactory_filter.ravel()[:(t_end - t_start)]
            output_spike_times_in_ms.append(t)

    return local_normlized_currents, soma_voltage, output_spike_times_in_ms


def add_offset_for_plotting(traces_matrix, offset_size=1.1):

    traces_matrix_with_offset = offset_size * np.kron(np.arange(traces_matrix.shape[0])[:,np.newaxis], np.ones((1,traces_matrix.shape[1])))
    traces_matrix_with_offset = traces_matrix_with_offset + traces_matrix

    return traces_matrix_with_offset


#%% simulate the cell

connections_per_axon = int(num_dendrites / num_axons)
num_synapses = num_dendrites

tau_rise_vec  = np.linspace(tau_rise_range[0] , tau_rise_range[1] , num_synapses)[:,np.newaxis]
tau_decay_vec = np.linspace(tau_decay_range[0], tau_decay_range[1], num_synapses)[:,np.newaxis]

# synapse learnable parameters
synaptic_weights_vec = 1.0 + 0.1 * np.random.uniform(size=(num_synapses, 1))

synaptic_filters = construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec)

#%% run once

# generate sample input
stimulus_duration_ms = experiment_time_ms

axon_input_spike_train = np.zeros((num_axons, stimulus_duration_ms))
for k in range(num_axons):
    curr_axon_spike_times = 20 + np.random.randint(stimulus_duration_ms -80, size=num_spikes_per_axon)
    axon_input_spike_train[k,curr_axon_spike_times] = 1.0

presynaptic_input_spikes = np.kron(np.ones((connections_per_axon,1), dtype=bool), axon_input_spike_train).astype(bool)

# simulate F&F cell with normlized currents
local_normlized_currents, soma_voltage, output_spike_times_in_ms = simulate_filter_and_fire_cell_training(presynaptic_input_spikes,
                                                                                                          synaptic_weights_vec, tau_rise_vec, tau_decay_vec,
                                                                                                          refreactory_time_constant=refreactory_time_constant,
                                                                                                          v_reset=v_reset, v_threshold=v_threshold,
                                                                                                          current_to_voltage_mult_factor=current_to_voltage_mult_factor)

local_normlized_currents = np.flipud(local_normlized_currents)

soma_voltage_with_spikes = soma_voltage
soma_voltage_with_spikes[output_spike_times_in_ms] = -25


max_local_added_voltage = add_offset_for_plotting(local_normlized_currents).T.max()

print('running once')


#%% run again until we have at least 1 spike

while len(output_spike_times_in_ms) != 1 or max_local_added_voltage > 10.15:
    axon_input_spike_train = np.zeros((num_axons, stimulus_duration_ms))
    for k in range(num_axons):
        curr_axon_spike_times = 30 + np.random.randint(stimulus_duration_ms -60, size=num_spikes_per_axon)
        axon_input_spike_train[k,curr_axon_spike_times] = 1.0

    presynaptic_input_spikes = np.kron(np.ones((connections_per_axon,1), dtype=bool), axon_input_spike_train).astype(bool)

    # simulate F&F cell with normlized currents
    local_normlized_currents, soma_voltage, output_spike_times_in_ms = simulate_filter_and_fire_cell_training(presynaptic_input_spikes,
                                                                                                              synaptic_weights_vec, tau_rise_vec, tau_decay_vec,
                                                                                                              refreactory_time_constant=refreactory_time_constant,
                                                                                                              v_reset=v_reset, v_threshold=v_threshold,
                                                                                                              current_to_voltage_mult_factor=current_to_voltage_mult_factor)

    local_normlized_currents = np.flipud(local_normlized_currents)

    soma_voltage_with_spikes = soma_voltage
    soma_voltage_with_spikes[output_spike_times_in_ms] = -15

    max_local_added_voltage = add_offset_for_plotting(local_normlized_currents).T.max()

print('there is at least 1 spike')

#%% run untill we have a changed enough spike location

min_spike_time_diff = 35
max_local_added_voltage = 0

while True:

    output_spike_times_in_ms =[]

    # generate input axons with a single spike
    while len(output_spike_times_in_ms) != 1 or max_local_added_voltage > 10.15:
        axon_input_spike_train = np.zeros((num_axons, stimulus_duration_ms))
        for k in range(num_axons):
            curr_axon_spike_times = 30 + np.random.randint(stimulus_duration_ms -60, size=num_spikes_per_axon)
            axon_input_spike_train[k,curr_axon_spike_times] = 1.0

        presynaptic_input_spikes = np.kron(np.ones((connections_per_axon,1), dtype=bool), axon_input_spike_train).astype(bool)

        # simulate F&F cell with normlized currents
        local_normlized_currents, soma_voltage, output_spike_times_in_ms = simulate_filter_and_fire_cell_training(presynaptic_input_spikes,
                                                                                                                  synaptic_weights_vec, tau_rise_vec, tau_decay_vec,
                                                                                                                  refreactory_time_constant=refreactory_time_constant,
                                                                                                                  v_reset=v_reset, v_threshold=v_threshold,
                                                                                                                  current_to_voltage_mult_factor=current_to_voltage_mult_factor)

        local_normlized_currents = np.flipud(local_normlized_currents)

        soma_voltage_with_spikes = soma_voltage
        soma_voltage_with_spikes[output_spike_times_in_ms] = -15

        max_local_added_voltage = add_offset_for_plotting(local_normlized_currents).T.max()

        if len(output_spike_times_in_ms) == 1 and (output_spike_times_in_ms[0] > 200 or output_spike_times_in_ms[0] < 120):
            output_spike_times_in_ms = []

    print('there is at least 1 spike, spike times: ', output_spike_times_in_ms)

    # generate a weights change that will move this spikes by a minimum amount
    mult_vector = np.random.permutation([0.25,0.5,0.5,0.75,1.25,1.5,1.75,1.75,2.0])[:,np.newaxis]
    synaptic_weights_vec_2 = mult_vector * synaptic_weights_vec

    # make sure only the a the middle axon weights are changed (for non cluttered visualization)
    synaptic_weights_vec_2[0::3] = synaptic_weights_vec[0::3]
    synaptic_weights_vec_2[2::3] = synaptic_weights_vec[2::3]
    mult_vector[0::3] = 1
    mult_vector[2::3] = 1

    # simulate F&F cell with normlized currents
    local_normlized_currents_2, soma_voltage_2, output_spike_times_in_ms_2 = simulate_filter_and_fire_cell_training(presynaptic_input_spikes,
                                                                                                                    synaptic_weights_vec_2, tau_rise_vec, tau_decay_vec,
                                                                                                                    refreactory_time_constant=refreactory_time_constant,
                                                                                                                    v_reset=v_reset, v_threshold=v_threshold,
                                                                                                                    current_to_voltage_mult_factor=current_to_voltage_mult_factor)

    local_normlized_currents_2 = np.flipud(local_normlized_currents_2)

    soma_voltage_with_spikes_2 = soma_voltage_2
    soma_voltage_with_spikes_2[output_spike_times_in_ms_2] = -15

    max_local_added_voltage_2 = add_offset_for_plotting(local_normlized_currents_2).T.max()

    local_normlized_currents_2 = mult_vector * local_normlized_currents_2

    if len(output_spike_times_in_ms_2) == 1 and ((output_spike_times_in_ms[0] - output_spike_times_in_ms_2[0]) <= -min_spike_time_diff):
        break

print('changed weights such that the spike changed location for more than %d ms' %(min_spike_time_diff))


#%% Display Input Axons, Synaptic Filters and Local Voltage Traces "Slopily"

plt.close('all')
fig = plt.figure(figsize=(20,8))
plt.subplots_adjust(left=0.03, right=0.97, top=0.95, bottom=0.05, wspace=0.25, hspace=0.35)

plt.subplot(1,3,1);
for k, axon_color in enumerate(axon_colors):
    plt.plot(add_offset_for_plotting(np.flipud(axon_input_spike_train)).T[:,k], color=axon_color); plt.title('Input Axons', fontsize=24);

plt.subplot(1,3,2);
for k, axon_color in enumerate(axon_colors):
    plt.plot(add_offset_for_plotting(synaptic_filters).T[:100,k::num_axons], color=axon_color); plt.title('Synaptic Filters', fontsize=24);

plt.subplot(1,3,3);
for k, axon_color in enumerate(axon_colors):
    plt.plot(add_offset_for_plotting(local_normlized_currents).T[:,k::num_axons], color=axon_color); plt.title('Synaptic Contact Voltage Contribution', fontsize=24);
    plt.plot(add_offset_for_plotting(local_normlized_currents_2).T[:,k::num_axons], color=axon_color, linestyle='dashed');

#%% plot one below each other

plt.close('all')
plt.figure(figsize=(10,25))
gs_figure = gridspec.GridSpec(nrows=11,ncols=1)
gs_figure.update(left=0.04, right=0.95, bottom=0.05, top=0.95, wspace=0.1, hspace=0.8)

ax_axons          = plt.subplot(gs_figure[:3,:])
ax_local_voltages = plt.subplot(gs_figure[3:9,:])
ax_soma_voltage   = plt.subplot(gs_figure[9:,:])

ax_axons.plot(add_offset_for_plotting(axon_input_spike_train).T); ax_axons.set_title('Input Axons', fontsize=24);
ax_local_voltages.plot(add_offset_for_plotting(local_normlized_currents).T); ax_local_voltages.set_title('Synaptic Contact Voltage Contribution', fontsize=24);
ax_local_voltages.plot(add_offset_for_plotting(local_normlized_currents_2).T, linestyle='dashed');
ax_soma_voltage.plot(soma_voltage_with_spikes); ax_soma_voltage.set_title('Somatic Voltage', fontsize=24);
ax_soma_voltage.plot(soma_voltage_with_spikes_2, linestyle='dashed');

#%% Display The Full Figure

plt.close('all')
fig = plt.figure(figsize=(25,18))
gs_figure = gridspec.GridSpec(nrows=11,ncols=30)
gs_figure.update(left=0.04, right=0.95, bottom=0.05, top=0.92, wspace=0.2, hspace=0.7)

ax_axons         = plt.subplot(gs_figure[2:6,:9])
ax_syn_filters   = plt.subplot(gs_figure[:8,13:17])
ax_local_voltges = plt.subplot(gs_figure[:8,21:])
ax_soma_voltage  = plt.subplot(gs_figure[8:,21:])


for k, axon_color in enumerate(axon_colors):
    ax_axons.plot(add_offset_for_plotting(np.flipud(axon_input_spike_train)).T[:,k], color=axon_color, lw=3);
ax_axons.set_yticks([])
ax_axons.set_xticks([])
ax_axons.set_title('Input Axons', fontsize=20);
ax_axons.spines['top'].set_visible(False)
ax_axons.spines['bottom'].set_visible(False)
ax_axons.spines['left'].set_visible(False)
ax_axons.spines['right'].set_visible(False)

for k, axon_color in enumerate(axon_colors):
    ax_syn_filters.plot(add_offset_for_plotting(synaptic_filters).T[:100,k::num_axons], color=axon_color, lw=3);
ax_syn_filters.set_yticks([])
ax_syn_filters.set_xticks([])
ax_syn_filters.set_title('Synaptic Filters', fontsize=20);
ax_syn_filters.spines['top'].set_visible(False)
ax_syn_filters.spines['bottom'].set_visible(False)
ax_syn_filters.spines['left'].set_visible(False)
ax_syn_filters.spines['right'].set_visible(False)

for k, axon_color in enumerate(axon_colors):
    ax_local_voltges.plot(add_offset_for_plotting(local_normlized_currents).T[:,k::num_axons], color=axon_color, lw=3);
    ax_local_voltges.plot(add_offset_for_plotting(local_normlized_currents_2).T[:,k::num_axons], color=axon_color, ls='dashed', lw=2.5);
ax_local_voltges.set_xticks([])
ax_local_voltges.set_yticks([])
ax_local_voltges.set_title('Synaptic Contact Voltage Contribution', fontsize=20);
ax_local_voltges.spines['top'].set_visible(False)
ax_local_voltges.spines['bottom'].set_visible(False)
ax_local_voltges.spines['left'].set_visible(False)
ax_local_voltges.spines['right'].set_visible(False)

ax_soma_voltage.plot(soma_voltage_with_spikes, color='0.1', lw=3, alpha=0.85);
ax_soma_voltage.plot(soma_voltage_with_spikes_2, color='dodgerblue', lw=3);
ax_soma_voltage.set_yticks([])
ax_soma_voltage.set_xticks([])
ax_soma_voltage.set_title('Somatic Voltage', fontsize=20);
ax_soma_voltage.spines['top'].set_visible(False)
ax_soma_voltage.spines['bottom'].set_visible(False)
ax_soma_voltage.spines['left'].set_visible(False)
ax_soma_voltage.spines['right'].set_visible(False)

# save figure
if save_figures:
    figure_name = 'F&F_A_Introduction_Figure_1_%d' %(np.random.randint(2000))
    for file_ending in all_file_endings_to_use:
        if file_ending == '.png':
            fig.savefig(figure_folder + figure_name + file_ending, bbox_inches='tight')
        else:
            fig.savefig(figure_folder + figure_name + file_ending, bbox_inches='tight')


#%%




================================================================================
================================================================================
results_data_mnist/MNIST_classification_LR_FF_IF_5162_rows_12_cols.csv:
=======================================================================
digit,M_connections,N_axons,T,N_positive_samples,Accuracy LR,Accuracy FF,Accuracy IF,Accuracy baseline,release probability,train_epochs,test_epochs
7,5,100,60,7000,96.81,94.44,89.72,89.72,1.0,1,1
6,5,100,60,7000,96.85000000000001,94.71000000000001,90.42,90.42,1.0,1,1
3,5,100,30,16,90.24666666666667,89.93666666666667,89.99666666666667,89.9,0.5,15,3
7,5,100,60,7000,96.73,94.61,89.74,89.72,1.0,1,1
4,5,100,10,5000,95.35,92.55,91.36999999999999,90.18,1.0,15,3
2,5,100,10,5000,96.16,94.22,92.02,89.68,1.0,15,3
5,5,100,10,5000,94.0,91.21000000000001,91.09,91.08000000000001,1.0,15,3
2,5,100,20,32,91.47333333333333,91.56333333333333,90.93,89.68,0.5,15,3
8,5,100,20,7000,92.85,90.62,90.25999999999999,90.25999999999999,1.0,1,1
4,1,100,10,2048,95.25,91.14,91.4,90.18,1.0,15,3
3,8,100,20,2048,94.44666666666667,92.47,90.45333333333333,89.9,0.5,15,3
3,1,100,10,1024,94.29333333333332,90.12,90.64,89.9,0.5,15,3
2,5,100,60,7000,95.41,94.02000000000001,89.68,89.68,1.0,1,1
1,5,100,60,7000,97.86,94.0,88.64999999999999,88.64999999999999,1.0,1,1
3,5,100,40,5000,95.14,93.63,89.95,89.9,1.0,1,1
3,1,100,20,5000,94.98,90.59,90.67,89.9,1.0,1,1
8,5,100,100,5000,92.92,90.57,90.25999999999999,90.25999999999999,1.0,1,1
0,10,100,50,16,92.9,90.20333333333333,90.2,90.2,0.75,15,3
7,5,100,80,5000,96.91,94.04,89.72,89.72,1.0,15,3
5,5,100,10,7000,93.5,91.18,91.09,91.08000000000001,1.0,1,1
4,1,100,30,4096,95.07,90.77,90.49000000000001,90.18,1.0,15,3
2,5,100,20,16,89.8,90.38666666666667,89.88666666666667,89.68,0.5,15,3
9,5,100,100,5000,92.78,91.71000000000001,89.91,89.91,1.0,15,3
2,5,100,50,5000,95.17999999999999,94.28999999999999,89.78,89.68,1.0,1,1
1,5,100,50,7000,97.78,94.39999999999999,88.64999999999999,88.64999999999999,1.0,1,1
3,8,100,20,2048,94.51333333333334,92.42,90.63333333333333,89.9,0.5,15,3
7,2,100,20,2048,96.36,93.0,90.48,89.72,0.5,15,3
2,5,100,40,1024,95.27,94.95,90.69,89.68,1.0,15,3
0,5,100,80,5000,97.87,92.15,90.2,90.2,1.0,1,1
3,2,100,50,2048,95.12,92.11,89.92999999999999,89.9,1.0,15,3
5,5,100,10,5000,93.71000000000001,91.21000000000001,91.08000000000001,91.08000000000001,1.0,15,3
4,5,100,10,512,94.96,91.72,91.09,90.18,1.0,15,3
7,5,100,60,5000,96.84,94.92,89.72,89.72,1.0,15,3
8,5,100,20,7000,93.06,90.92,90.25999999999999,90.25999999999999,1.0,1,1
6,5,100,20,5000,96.97,94.48,90.42,90.42,1.0,15,3
6,5,100,10,7000,96.84,92.51,90.42,90.42,1.0,1,1
4,2,100,30,2048,94.89,91.86999999999999,90.56,90.18,1.0,15,3
3,2,100,20,2048,94.64666666666666,91.57,90.72666666666666,89.9,0.5,15,3
7,5,100,40,16,89.82,89.79666666666667,89.72666666666666,89.72,0.25,15,3
2,5,100,30,1024,95.30999999999999,94.57,91.23,89.68,1.0,15,3
6,2,100,40,1000,96.35000000000001,92.86,90.42,90.42,1.0,1,1
8,5,100,80,32,90.27,90.25999999999999,90.25999999999999,90.25999999999999,0.25,15,3
3,2,100,10,4096,95.67,91.67999999999999,91.02,89.9,1.0,15,3
3,5,100,30,5000,95.23,93.47999999999999,90.58,89.9,1.0,15,3
2,1,100,10,2048,94.83,91.44666666666666,91.01333333333334,89.68,0.5,15,3
3,2,100,30,2048,95.09,92.41,90.61,89.9,1.0,15,3
4,5,100,30,2048,95.37,92.81,90.53,90.18,1.0,15,3
0,5,100,100,7000,98.1,91.23,90.2,90.2,1.0,1,1
2,10,100,20,2048,95.63000000000001,94.64,92.11,89.68,1.0,15,3
3,2,100,30,2048,95.17,92.47,90.51,89.9,1.0,15,3
1,1,100,40,5000,97.66,88.64999999999999,88.64999999999999,88.64999999999999,1.0,1,1
3,3,100,20,4096,95.57,92.74,91.16,89.9,1.0,15,3
9,5,100,40,5000,93.0,92.52,89.91,89.91,1.0,15,3
3,2,100,10,4096,95.43,91.85,90.72,89.9,1.0,15,3
4,3,100,30,2048,95.11,92.78999999999999,90.59,90.18,1.0,15,3
6,5,100,10,5000,97.07000000000001,92.95,90.42,90.42,1.0,15,3
2,5,100,30,5958,95.1,94.12,90.58333333333334,89.68,0.5,3,3
2,5,100,30,512,94.26333333333334,93.63,90.33333333333333,89.68,0.5,15,3
7,5,100,40,5000,96.81,95.24000000000001,89.96,89.72,1.0,15,3
4,8,100,30,1024,94.15333333333334,91.91,90.36333333333333,90.18,0.5,15,3
1,5,100,80,16,92.60000000000001,88.64999999999999,88.64999999999999,88.64999999999999,0.5,15,3
7,5,100,20,64,93.66666666666667,91.51,90.38666666666667,89.72,0.5,15,3
2,3,100,20,1024,95.32000000000001,94.3,91.81,89.68,1.0,15,3
2,3,100,30,2048,95.16333333333333,93.91000000000001,90.55,89.68,0.5,15,3
8,5,100,50,7000,92.58999999999999,90.97,90.25999999999999,90.25999999999999,1.0,1,1
5,5,100,100,7000,93.25,91.96,91.08000000000001,91.08000000000001,1.0,1,1
1,2,100,60,1024,97.59333333333333,88.64999999999999,88.64999999999999,88.64999999999999,0.5,15,3
1,5,100,50,5000,98.19,94.76,88.64999999999999,88.64999999999999,1.0,15,3
4,5,100,20,128,93.83,91.06,90.85,90.18,1.0,15,3
9,5,100,30,5000,93.42,91.41,89.91,89.91,1.0,15,3
8,3,100,40,1024,92.44333333333333,90.44666666666666,90.25999999999999,90.25999999999999,0.75,15,3
0,1,100,30,1024,97.63,90.2,90.24,90.2,1.0,15,3
2,5,100,10,32,91.77,90.85666666666667,90.41666666666667,89.68,0.5,15,3
0,5,100,40,7000,97.68,93.39,90.31,90.2,1.0,1,1
4,5,100,60,5000,95.35,92.22,90.18,90.18,1.0,15,3
7,8,100,10,2048,97.25,93.57,91.0,89.72,1.0,15,3
0,2,100,20,64,96.11,90.36999999999999,90.2,90.2,0.75,15,3
8,5,100,100,7000,93.23,90.45,90.25999999999999,90.25999999999999,1.0,1,1
2,5,100,30,1024,94.63000000000001,94.05,90.48333333333333,89.68,0.5,15,3
7,5,100,80,7000,97.09,93.86,89.73,89.72,1.0,1,1
5,5,100,40,5000,94.17999999999999,92.74,91.09,91.08000000000001,1.0,15,3
3,5,100,10,256,95.07,90.49000000000001,90.8,89.9,1.0,15,3
7,5,100,50,5000,96.86,95.43,89.73,89.72,1.0,15,3
1,5,100,40,5000,97.94,92.60000000000001,88.64999999999999,88.64999999999999,1.0,15,3
2,5,100,30,4096,95.47,95.24000000000001,90.91,89.68,1.0,15,3
3,10,100,10,4096,95.47,92.36999999999999,91.23,89.9,1.0,15,3
7,5,100,20,32,92.47333333333333,90.45333333333333,90.31666666666666,89.72,0.5,15,3
2,5,100,20,256,93.74,93.24,91.18333333333334,89.68,0.5,15,3
2,1,100,30,2048,95.02666666666667,91.33,90.52666666666667,89.68,0.5,15,3
6,5,100,100,7000,97.0,92.57,90.42,90.42,1.0,1,1
3,2,100,20,1024,94.48333333333333,91.28,90.46,89.9,0.5,15,3
6,5,100,20,5000,96.89999999999999,94.39,90.42,90.42,1.0,15,3
7,5,100,50,7000,96.78,94.93,89.73,89.72,1.0,1,1
7,5,100,30,7000,96.78,94.55,90.05,89.72,1.0,1,1
0,5,100,10,7000,97.82,91.35,90.21000000000001,90.2,1.0,1,1
0,5,100,50,5000,97.78999999999999,92.74,90.2,90.2,1.0,1,1
3,3,100,20,2048,95.49,92.53,90.88000000000001,89.9,1.0,15,3
1,5,100,40,5000,97.7,91.29,88.64999999999999,88.64999999999999,1.0,1,1
8,5,100,10,5000,93.08,90.32,90.25999999999999,90.25999999999999,1.0,1,1
4,1,100,10,5000,94.71000000000001,91.24,91.42,90.18,1.0,1,1
2,5,100,10,32,92.93,91.49000000000001,90.48,89.68,1.0,15,3
3,5,100,20,256,94.34,91.84,90.75999999999999,89.9,1.0,15,3
2,1,100,30,2048,95.7,91.95,90.95,89.68,1.0,15,3
9,3,100,80,128,90.15333333333334,90.02666666666667,89.91,89.91,0.25,15,3
3,5,100,20,64,92.23,90.49666666666667,90.06333333333333,89.9,0.5,15,3
7,5,100,30,5000,96.95,95.27,90.14999999999999,89.72,1.0,15,3
4,5,100,30,1024,94.08,91.8,90.42666666666666,90.18,0.5,15,3
2,3,100,20,2048,95.74000000000001,94.55,91.55,89.68,1.0,15,3
0,5,100,60,5000,97.95,92.17999999999999,90.21000000000001,90.2,1.0,15,3
3,1,100,10,2048,95.67999999999999,90.25999999999999,90.94,89.9,1.0,15,3
7,5,100,80,5000,96.98,94.57,89.72,89.72,1.0,15,3
7,5,100,10,7000,96.83,93.13,90.88000000000001,89.72,1.0,1,1
7,5,100,30,64,95.22,90.4,89.8,89.72,1.0,15,3
2,5,100,20,64,93.31,90.60000000000001,90.86999999999999,89.68,1.0,15,3
0,5,100,30,5000,97.64,91.81,90.29,90.2,1.0,1,1
3,1,100,20,2048,95.28999999999999,91.19,90.88000000000001,89.9,1.0,15,3
2,8,100,10,2048,95.8,93.83,91.75,89.68,1.0,15,3
8,5,100,40,5000,93.46,91.18,90.25999999999999,90.25999999999999,1.0,15,3
3,5,100,20,32,92.86999999999999,90.36999999999999,89.9,89.9,1.0,15,3
2,8,100,20,2048,95.89,94.95,91.82000000000001,89.68,1.0,15,3
2,1,100,30,5958,95.41333333333334,91.01666666666667,90.68,89.68,0.5,3,3
9,5,100,80,7000,92.38,91.99000000000001,89.91,89.91,1.0,1,1
2,5,100,30,32,91.86999999999999,89.8,89.68,89.68,1.0,15,3
6,3,100,100,2048,95.91666666666666,91.19,90.42,90.42,0.25,15,3
5,5,100,50,7000,93.25,92.65,91.08000000000001,91.08000000000001,1.0,1,1
3,2,100,30,4096,95.46,92.06,90.58,89.9,1.0,15,3
2,5,100,20,128,93.08666666666666,92.38333333333333,91.14666666666666,89.68,0.5,15,3
8,2,100,100,64,91.2,90.25999999999999,90.25999999999999,90.25999999999999,1.0,15,3
4,1,100,20,1024,94.07,90.71000000000001,90.57,90.18,0.5,15,3
7,5,100,10,128,96.34,90.59,90.88000000000001,89.72,1.0,15,3
9,5,100,100,7000,92.44,91.05,89.91,89.91,1.0,1,1
2,5,100,20,5000,95.96000000000001,95.0,91.89,89.68,1.0,15,3
7,5,100,10,16,91.24,90.32333333333334,89.85666666666667,89.72,0.5,15,3
1,5,100,20,5000,98.44000000000001,90.28,89.25,88.64999999999999,1.0,15,3
3,5,100,40,256,93.68666666666667,92.88333333333333,90.10000000000001,89.9,0.5,15,3
3,1,100,10,1024,95.48,90.64,90.79,89.9,1.0,15,3
5,5,100,50,7000,93.17,92.21000000000001,91.08000000000001,91.08000000000001,1.0,1,1
3,3,100,30,2048,95.38,92.58999999999999,90.48,89.9,1.0,15,3
2,2,100,30,2048,95.05,93.20333333333333,90.66666666666666,89.68,0.5,15,3
2,2,100,30,5958,95.93,94.19999999999999,90.91,89.68,1.0,3,3
0,5,100,50,5000,98.16,92.86999999999999,90.28,90.2,1.0,15,3
5,5,100,60,5000,93.49,92.47999999999999,91.08000000000001,91.08000000000001,1.0,1,1
9,5,100,40,7000,92.57,92.33,89.92,89.91,1.0,1,1
7,5,100,50,7000,96.63000000000001,94.95,89.77000000000001,89.72,1.0,1,1
7,5,100,20,5000,96.92,94.67999999999999,90.81,89.72,1.0,15,3
2,1,100,10,1024,94.67,91.25666666666666,90.91333333333334,89.68,0.5,15,3
1,5,100,20,5000,97.87,88.79,89.12,88.64999999999999,1.0,1,1
8,5,100,40,5000,93.08,91.31,90.25999999999999,90.25999999999999,1.0,15,3
7,5,100,20,32,94.15,90.12,89.72,89.72,1.0,15,3
5,5,100,40,1024,93.43,92.47999999999999,91.08000000000001,91.08000000000001,1.0,15,3
7,2,100,10,1024,96.92,93.54,91.11,89.72,1.0,15,3
2,10,100,20,2048,95.8,94.66,91.73,89.68,1.0,15,3
3,10,100,10,2048,95.78999999999999,92.21000000000001,91.14999999999999,89.9,1.0,15,3
1,5,100,50,5000,98.29,94.46,88.64999999999999,88.64999999999999,1.0,15,3
7,5,100,20,256,95.36666666666666,93.07,90.42666666666666,89.72,0.5,15,3
2,5,100,10,1024,95.00666666666666,92.93333333333334,91.49000000000001,89.68,0.5,15,3
4,3,100,30,2048,94.67999999999999,92.17333333333333,90.34666666666666,90.18,0.5,15,3
5,5,100,20,7000,93.44,91.22,91.08000000000001,91.08000000000001,1.0,1,1
7,5,100,20,32,92.82666666666667,91.23333333333333,89.93333333333334,89.72,0.5,15,3
1,5,100,30,7000,97.98,90.71000000000001,88.64999999999999,88.64999999999999,1.0,1,1
5,5,100,50,7000,93.35,92.60000000000001,91.08000000000001,91.08000000000001,1.0,1,1
4,2,100,20,2048,94.18666666666667,91.53666666666666,90.54333333333334,90.18,0.5,15,3
2,2,100,10,1024,95.65,93.61,91.67,89.68,1.0,15,3
4,2,100,20,4096,94.38,91.67333333333333,90.84333333333333,90.18,0.5,15,3
4,5,100,20,32,91.00333333333333,90.51666666666667,90.44666666666666,90.18,0.5,15,3
2,5,100,10,7000,95.61,93.46,91.91,89.68,1.0,1,1
3,5,100,10,256,94.27,90.94333333333333,90.68666666666667,89.9,0.5,15,3
2,1,100,20,2048,95.76,92.88,92.12,89.68,1.0,15,3
2,8,100,20,2048,95.03333333333333,94.01666666666667,91.24333333333333,89.68,0.5,15,3
9,5,100,100,7000,92.4,91.52,89.91,89.91,1.0,1,1
2,10,100,10,1024,94.67333333333333,92.83666666666667,91.05666666666666,89.68,0.5,15,3
2,5,100,20,32,91.00333333333333,91.36666666666666,90.38333333333334,89.68,0.5,15,3
6,5,100,80,5000,96.47,93.51,90.42,90.42,1.0,1,1
4,10,100,10,1024,95.17999999999999,92.30000000000001,91.23,90.18,1.0,15,3
5,5,100,10,5000,93.27,91.08000000000001,91.09,91.08000000000001,1.0,1,1
2,5,100,20,512,94.72333333333334,93.63,91.12333333333333,89.68,0.5,15,3
7,5,100,10,64,95.99,90.66,90.41,89.72,1.0,15,3
2,2,100,20,2048,95.81,94.39999999999999,91.86,89.68,1.0,15,3
4,5,100,20,5000,95.48,93.17,91.23,90.18,1.0,15,3
2,5,100,30,7000,95.32000000000001,94.45,91.08000000000001,89.68,1.0,1,1
2,5,100,20,32,92.11,91.82000000000001,90.0,89.68,1.0,15,3
2,5,100,10,64,93.93,90.86,91.06,89.68,1.0,15,3
3,5,100,40,7000,95.21,93.69,89.97,89.9,1.0,1,1
0,5,100,10,5000,98.07000000000001,91.97,90.2,90.2,1.0,15,3
2,5,100,40,5000,95.5,95.36,90.39,89.68,1.0,15,3
3,5,100,20,512,94.28333333333333,91.73666666666666,90.54333333333334,89.9,0.5,15,3
6,5,100,30,5000,97.13000000000001,94.69999999999999,90.42,90.42,1.0,15,3
3,5,100,30,2048,94.85,92.73333333333333,90.16333333333333,89.9,0.5,15,3
3,5,100,20,128,93.04,91.15666666666667,90.55333333333333,89.9,0.5,15,3
7,5,100,40,5000,96.96000000000001,95.32000000000001,89.98,89.72,1.0,15,3
3,10,100,10,4096,95.56,92.24,90.86,89.9,1.0,15,3
7,5,100,30,32,94.19,89.81,89.77000000000001,89.72,1.0,15,3
4,3,100,30,4096,95.30999999999999,93.16,90.67,90.18,1.0,15,3
1,1,100,80,1024,97.57333333333334,88.64999999999999,88.64999999999999,88.64999999999999,0.5,15,3
6,5,100,60,7000,96.98,94.36,90.42,90.42,1.0,1,1
0,5,100,30,7000,97.81,92.74,90.23,90.2,1.0,1,1
7,5,100,50,7000,96.56,94.8,89.74,89.72,1.0,1,1
7,5,100,20,7000,96.75,94.35,90.97,89.72,1.0,1,1
3,2,100,30,2048,94.86666666666666,91.36999999999999,90.22666666666666,89.9,0.5,15,3
4,8,100,20,2048,95.33,93.06,91.09,90.18,1.0,15,3
5,5,100,40,5000,93.67,93.07,91.08000000000001,91.08000000000001,1.0,15,3
3,2,100,30,4096,94.64333333333333,91.23666666666666,90.24333333333333,89.9,0.5,15,3
0,3,100,20,512,97.77,91.18,90.25999999999999,90.2,1.0,15,3
4,3,100,10,4096,95.55,92.64,91.38,90.18,1.0,15,3
3,5,100,40,1024,94.91000000000001,93.28999999999999,90.29,89.9,1.0,15,3
3,5,100,20,16,90.50333333333333,90.09333333333333,90.03333333333333,89.9,0.5,15,3
7,5,100,20,5000,97.17,94.89999999999999,90.78,89.72,1.0,15,3
3,1,100,20,4096,94.46,90.57666666666667,90.66666666666666,89.9,0.5,15,3
4,5,100,30,5000,94.76,91.91,90.42,90.18,1.0,1,1
2,1,100,10,2048,95.78,92.75999999999999,91.95,89.68,1.0,15,3
2,2,100,30,5000,95.14,93.27333333333333,90.65333333333334,89.68,0.5,10,3
3,3,100,30,2048,95.30999999999999,92.94,90.63,89.9,1.0,15,3
7,5,100,40,1024,96.93,95.08,89.86,89.72,1.0,15,3
9,10,100,30,1024,92.44,91.25,89.91,89.91,1.0,15,3
2,5,100,30,32,91.67999999999999,90.02,89.72,89.68,1.0,15,3
2,5,100,80,7000,95.14,92.57,89.68,89.68,1.0,1,1
2,5,100,10,512,95.45,93.10000000000001,91.73,89.68,1.0,15,3
7,5,100,50,5000,96.54,94.53,89.79,89.72,1.0,1,1
3,5,100,20,64,92.48333333333333,90.97,90.36999999999999,89.9,0.5,15,3
2,10,100,30,2048,95.48,95.16,91.03,89.68,1.0,15,3
7,5,100,100,7000,96.87,93.28999999999999,89.72,89.72,1.0,1,1
6,5,100,10,7000,96.57,92.14,90.46,90.42,1.0,1,1
2,3,100,10,2048,94.74333333333334,92.74,91.24333333333333,89.68,0.5,15,3
1,1,100,60,512,97.12333333333333,88.64999999999999,88.64999999999999,88.64999999999999,0.5,15,3
2,5,100,20,2048,95.74000000000001,94.64,91.88,89.68,1.0,15,3
0,1,100,80,32,93.89333333333333,90.2,90.2,90.2,0.5,15,3
2,5,100,10,1024,94.85666666666667,92.86999999999999,91.09333333333333,89.68,0.5,15,3
6,5,100,40,7000,96.61999999999999,94.67,90.42,90.42,1.0,1,1
6,5,100,30,7000,96.81,94.14,90.42,90.42,1.0,1,1
4,5,100,10,64,92.34333333333333,90.54333333333334,90.67333333333333,90.18,0.5,15,3
5,5,100,10,5000,93.38,91.09,91.08000000000001,91.08000000000001,1.0,1,1
6,5,100,30,7000,96.85000000000001,94.43,90.42,90.42,1.0,1,1
3,5,100,20,64,93.67999999999999,89.96,90.5,89.9,1.0,15,3
2,5,100,30,5958,95.58,95.33,91.07,89.68,1.0,3,3
3,5,100,10,1024,95.46,92.05,90.95,89.9,1.0,15,3
3,2,100,10,4096,95.58,91.64999999999999,90.96,89.9,1.0,15,3
2,5,100,10,128,94.88,91.4,91.63,89.68,1.0,15,3
9,5,100,30,7000,92.99,91.28,89.91,89.91,1.0,1,1
3,5,100,20,64,92.53,90.40333333333334,90.36666666666666,89.9,0.5,15,3
6,5,100,40,5000,96.63000000000001,94.45,90.42,90.42,1.0,1,1
3,2,100,30,2048,94.71000000000001,91.25333333333333,90.21666666666667,89.9,0.5,15,3
2,3,100,10,4096,94.54333333333334,92.82000000000001,91.07666666666667,89.68,0.5,15,3
8,1,100,40,512,91.91,90.25999999999999,90.25999999999999,90.25999999999999,0.5,15,3
2,3,100,20,1024,94.91000000000001,93.48666666666666,91.06,89.68,0.5,15,3
9,1,100,60,32,89.92666666666666,89.91,89.91,89.91,0.25,15,3
4,3,100,20,2048,94.36333333333333,91.58666666666667,90.81333333333333,90.18,0.5,15,3
7,5,100,30,5000,96.91,95.0,90.27,89.72,1.0,15,3
3,1,100,30,4096,95.35,90.52,90.44,89.9,1.0,15,3
6,5,100,40,5000,96.89999999999999,95.54,90.42,90.42,1.0,15,3
5,5,100,20,5000,93.89999999999999,92.2,91.11,91.08000000000001,1.0,15,3
3,2,100,10,2048,95.58,91.44,90.99000000000001,89.9,1.0,15,3
6,5,100,50,5000,96.56,94.67999999999999,90.42,90.42,1.0,1,1
4,5,100,80,7000,94.84,90.96,90.18,90.18,1.0,1,1
5,1,100,10,5000,93.2,91.08000000000001,91.08000000000001,91.08000000000001,1.0,1,1
6,5,100,40,5000,96.85000000000001,95.17999999999999,90.42,90.42,1.0,15,3
7,5,100,20,5000,97.13000000000001,94.63000000000001,90.64999999999999,89.72,1.0,15,3
7,3,100,20,2048,96.86,94.41000000000001,90.73,89.72,1.0,15,3
3,1,100,30,4096,95.50999999999999,90.78,90.42999999999999,89.9,1.0,15,3
3,5,100,60,7000,95.19999999999999,93.5,89.9,89.9,1.0,1,1
3,1,100,30,6131,94.97333333333333,90.14333333333333,90.31,89.9,0.5,3,3
8,5,100,100,7000,92.82000000000001,90.42999999999999,90.25999999999999,90.25999999999999,1.0,1,1
7,1,100,30,2048,96.77,90.95,90.2,89.72,1.0,15,3
6,5,100,30,7000,96.83,94.47,90.42,90.42,1.0,1,1
2,5,100,10,64,94.05,91.66,90.96,89.68,1.0,15,3
2,5,100,20,1024,95.5,94.42,91.77,89.68,1.0,15,3
2,5,100,30,32,91.60000000000001,89.95,89.75999999999999,89.68,1.0,15,3
3,5,100,10,2048,95.49,92.07,90.96,89.9,1.0,15,3
2,5,100,20,32,92.77,91.35,89.69,89.68,1.0,15,3
4,10,100,20,5000,95.54,93.15,91.11,90.18,1.0,15,3
3,5,100,10,7000,95.21,91.83,90.99000000000001,89.9,1.0,1,1
7,2,100,20,1024,96.32333333333334,93.13666666666667,90.55666666666666,89.72,0.5,15,3
7,5,100,20,512,95.82000000000001,93.58333333333333,90.44666666666666,89.72,0.5,15,3
9,5,100,10,7000,93.03,90.42,89.97,89.91,1.0,1,1
3,5,100,10,5000,95.57,92.19000000000001,90.81,89.9,1.0,15,3
1,5,100,10,7000,97.91,88.7,89.64,88.64999999999999,1.0,1,1
8,5,100,40,1024,92.80000000000001,90.77,90.25999999999999,90.25999999999999,1.0,15,3
3,10,100,10,4096,95.38,92.09,90.69,89.9,1.0,15,3
3,2,100,30,2048,94.5,91.45333333333333,90.21000000000001,89.9,0.5,15,3
5,5,100,10,5000,93.84,91.27,91.08000000000001,91.08000000000001,1.0,15,3
1,5,100,30,5000,98.15,90.60000000000001,88.7,88.64999999999999,1.0,15,3
3,5,100,100,7000,95.02000000000001,92.33,89.9,89.9,1.0,1,1
3,3,100,30,4096,95.45,93.30000000000001,90.49000000000001,89.9,1.0,15,3
2,5,100,20,5000,95.89999999999999,94.88,91.96,89.68,1.0,15,3
5,2,100,60,16,91.08000000000001,91.09,91.08000000000001,91.08000000000001,1.0,15,3
5,5,100,60,5000,94.13,93.24,91.08000000000001,91.08000000000001,1.0,15,3
5,5,100,100,7000,93.41000000000001,92.05,91.08000000000001,91.08000000000001,1.0,1,1
8,3,100,40,256,91.38333333333334,90.34,90.25999999999999,90.25999999999999,0.5,15,3
8,5,100,60,5000,93.23,91.46,90.25999999999999,90.25999999999999,1.0,15,3
0,5,100,50,5000,98.00999999999999,92.32000000000001,90.27,90.2,1.0,15,3
2,5,100,30,256,93.86666666666666,93.35,90.57,89.68,0.5,15,3
7,5,100,20,1024,96.32666666666667,93.24666666666667,90.26333333333334,89.72,0.5,15,3
3,5,100,20,1024,94.34,91.77333333333333,90.53333333333333,89.9,0.5,15,3
1,1,100,30,5000,97.74000000000001,88.64999999999999,88.64999999999999,88.64999999999999,1.0,1,1
0,5,100,40,1024,97.76,92.63,90.25,90.2,1.0,15,3
3,8,100,30,1024,95.11,92.85,90.49000000000001,89.9,1.0,15,3
3,10,100,20,1024,94.64666666666666,92.39333333333335,90.55,89.9,0.5,15,3
3,5,100,30,64,93.5,90.08,90.11,89.9,1.0,15,3
4,5,100,30,512,93.90666666666667,91.68666666666667,90.29,90.18,0.5,15,3
5,5,100,100,7000,93.51,91.84,91.08000000000001,91.08000000000001,1.0,1,1
2,5,100,30,1024,94.57333333333334,93.98666666666666,90.70333333333333,89.68,0.5,15,3
7,2,100,40,256,95.92,92.58999999999999,89.94,89.72,1.0,15,3
1,5,100,40,5000,98.31,91.81,88.64999999999999,88.64999999999999,1.0,15,3
6,10,100,20,4096,97.07000000000001,94.34,90.42,90.42,1.0,15,3
6,1,100,30,1024,96.53333333333333,90.50666666666667,90.42,90.42,0.5,15,3
4,5,100,100,7000,95.37,90.62,90.18,90.18,1.0,1,1
2,10,100,20,2048,95.45,94.81,92.15,89.68,1.0,15,3
4,1,100,10,2048,95.43,91.57,91.36,90.18,1.0,15,3
5,5,100,20,5000,94.07,91.9,91.08000000000001,91.08000000000001,1.0,15,3
9,5,100,60,1024,91.58666666666667,91.36666666666666,89.91,89.91,0.5,15,3
4,5,100,50,5000,95.34,92.51,90.18,90.18,1.0,15,3
2,3,100,10,4096,95.66,94.22,92.14,89.68,1.0,15,3
6,5,100,40,1024,96.84,95.02000000000001,90.42,90.42,1.0,15,3
0,5,100,30,7000,97.76,92.96,90.27,90.2,1.0,1,1
0,5,100,60,5000,97.92999999999999,93.0,90.25,90.2,1.0,15,3
2,5,100,40,1024,95.05,94.59,90.32,89.68,1.0,15,3
2,8,100,30,4096,95.76,95.19999999999999,91.17,89.68,1.0,15,3
3,5,100,20,256,95.32000000000001,91.36999999999999,90.42,89.9,1.0,15,3
0,5,100,50,5000,98.09,93.21000000000001,90.3,90.2,1.0,15,3
3,5,100,80,5000,95.22,93.24,89.91,89.9,1.0,15,3
3,5,100,50,7000,94.92,93.71000000000001,89.9,89.9,1.0,1,1
2,5,100,80,5000,95.67999999999999,93.43,89.69,89.68,1.0,15,3
0,5,100,20,5000,97.85000000000001,92.56,90.24,90.2,1.0,15,3
6,5,100,60,7000,96.6,94.37,90.42,90.42,1.0,1,1
4,3,100,30,2048,95.00999999999999,92.74,90.52,90.18,1.0,15,3
3,5,100,30,6131,95.41,93.58,90.55,89.9,1.0,3,3
4,5,100,30,1024,94.77,92.71000000000001,90.59,90.18,1.0,15,3
1,5,100,30,5000,98.26,90.35,88.66000000000001,88.64999999999999,1.0,15,3
2,2,100,10,4096,94.52000000000001,93.01,91.0,89.68,0.5,15,3
1,5,100,80,7000,97.92999999999999,93.96,88.64999999999999,88.64999999999999,1.0,1,1
3,5,100,20,7000,95.23,92.86999999999999,91.29,89.9,1.0,1,1
7,1,100,60,5000,96.7,89.73,89.72,89.72,1.0,1,1
2,3,100,10,4096,95.93,93.82000000000001,92.17,89.68,1.0,15,3
9,5,100,10,7000,92.66,90.10000000000001,89.91,89.91,1.0,1,1
0,5,100,60,7000,97.82,92.02,90.2,90.2,1.0,1,1
7,5,100,10,7000,96.91,93.60000000000001,91.10000000000001,89.72,1.0,1,1
7,5,100,20,7000,96.87,93.97,91.10000000000001,89.72,1.0,1,1
7,5,100,20,1024,96.17666666666666,93.73,90.53666666666666,89.72,0.5,15,3
4,5,100,40,5000,95.39,93.34,90.24,90.18,1.0,15,3
3,5,100,40,7000,95.17,93.43,89.95,89.9,1.0,1,1
8,5,100,100,7000,92.99,90.49000000000001,90.25999999999999,90.25999999999999,1.0,1,1
6,1,100,20,5000,96.58,90.42999999999999,90.42,90.42,1.0,1,1
3,10,100,10,4096,95.69,92.62,90.64999999999999,89.9,1.0,15,3
3,5,100,20,7000,95.39,92.69,91.06,89.9,1.0,1,1
6,5,100,80,7000,96.96000000000001,93.42,90.42,90.42,1.0,1,1
9,1,100,80,32,90.10000000000001,89.91,89.91,89.91,1.0,15,3
3,2,100,10,2048,94.39,90.69666666666667,90.46666666666667,89.9,0.5,15,3
8,5,100,40,1024,93.04,91.17,90.25999999999999,90.25999999999999,1.0,15,3
7,5,100,30,5000,96.76,95.08,90.33,89.72,1.0,15,3
5,5,100,30,7000,93.41000000000001,91.86,91.09,91.08000000000001,1.0,1,1
4,5,100,50,5000,94.89,92.22,90.18,90.18,1.0,1,1
2,2,100,20,1024,95.7,94.64,91.75999999999999,89.68,1.0,15,3
7,5,100,100,5000,97.15,93.54,89.72,89.72,1.0,15,3
6,5,100,20,5000,97.0,94.46,90.42999999999999,90.42,1.0,15,3
7,5,100,20,5000,97.25,95.03,90.63,89.72,1.0,15,3
2,8,100,10,2048,94.72,93.00333333333334,90.91666666666667,89.68,0.5,15,3
3,10,100,30,6131,95.19,93.43,90.49000000000001,89.9,1.0,3,3
4,2,100,30,1024,94.15666666666667,91.43333333333334,90.4,90.18,0.5,15,3
4,5,100,20,256,93.43,91.47999999999999,90.63333333333333,90.18,0.5,15,3
4,5,100,10,1024,95.47,92.14,91.36,90.18,1.0,15,3
4,3,100,10,1024,95.26,92.66,91.32000000000001,90.18,1.0,15,3
8,5,100,20,7000,93.19,90.83,90.25999999999999,90.25999999999999,1.0,1,1
4,5,100,50,7000,94.98,91.96,90.18,90.18,1.0,1,1
0,5,100,60,7000,97.89999999999999,92.55,90.21000000000001,90.2,1.0,1,1
4,3,100,10,4096,95.6,92.56,91.31,90.18,1.0,15,3
2,2,100,10,4096,95.85000000000001,94.06,92.22,89.68,1.0,15,3
4,5,100,80,5000,94.92,91.03,90.18,90.18,1.0,1,1
0,5,100,20,5000,97.68,91.3,90.27,90.2,1.0,1,1
2,5,100,30,4096,95.67999999999999,95.23,91.08000000000001,89.68,1.0,15,3
9,5,100,30,5000,93.35,91.73,89.97,89.91,1.0,15,3
7,5,100,30,7000,96.65,94.26,90.14,89.72,1.0,1,1
2,5,100,20,5000,95.97,94.83,91.94,89.68,1.0,15,3
2,2,100,10,2048,94.35666666666667,92.57333333333332,91.0,89.68,0.5,15,3
4,5,100,10,16,92.53,90.47,91.21000000000001,90.18,1.0,15,3
4,5,100,30,7000,94.8,92.52,90.39,90.18,1.0,1,1
3,5,100,20,1024,94.56666666666666,91.95333333333333,90.59,89.9,0.5,15,3
0,5,100,80,5000,98.02,91.79,90.2,90.2,1.0,15,3
3,5,100,10,512,94.44333333333333,91.36999999999999,90.60666666666667,89.9,0.75,15,3
3,5,100,20,1024,95.12,92.75999999999999,90.79,89.9,1.0,15,3
2,5,100,10,1024,94.66666666666667,92.86666666666666,90.98333333333333,89.68,0.5,15,3
6,1,100,100,5000,96.74000000000001,90.42,90.42,90.42,1.0,1,1
0,2,100,100,5000,98.28,90.93,90.2,90.2,1.0,15,3
6,5,100,40,5000,97.03,95.55,90.42,90.42,1.0,15,3
2,5,100,10,32,92.05333333333333,90.55666666666666,90.59,89.68,0.5,15,3
3,5,100,50,7000,94.95,93.5,89.9,89.9,1.0,1,1
2,5,100,20,16,91.74,91.49000000000001,90.41,89.68,1.0,15,3
3,5,100,10,7000,95.39999999999999,91.81,91.16,89.9,1.0,1,1
7,5,100,30,1024,96.75,94.67999999999999,90.2,89.72,1.0,15,3
3,5,100,20,128,92.84,90.74,90.31666666666666,89.9,0.5,15,3
4,1,100,20,2048,95.43,91.43,91.07,90.18,1.0,15,3
7,5,100,30,16,90.80666666666667,90.29666666666667,89.82333333333334,89.72,0.5,15,3
4,3,100,60,2048,94.47,91.04333333333334,90.18,90.18,0.5,15,3
2,8,100,20,2048,95.02000000000001,93.77333333333333,91.18,89.68,0.5,15,3
7,1,100,10,2048,97.1,92.66,90.98,89.72,1.0,15,3
0,5,100,10,5000,97.88,91.61,90.2,90.2,1.0,15,3
6,5,100,80,5000,97.03,93.69,90.42,90.42,1.0,15,3
2,5,100,10,64,93.86,90.94,90.57,89.68,1.0,15,3
2,5,100,30,7000,95.28999999999999,94.52000000000001,91.12,89.68,1.0,1,1
0,1,100,40,5000,97.72,90.2,90.22,90.2,1.0,1,1
2,3,100,10,4096,95.88,94.01,91.91,89.68,1.0,15,3
3,1,100,10,4096,93.96333333333334,90.08,90.57666666666667,89.9,0.5,15,3
3,5,100,10,7000,95.55,91.64,90.85,89.9,1.0,1,1
4,5,100,30,2048,94.29666666666667,91.83,90.41,90.18,0.5,15,3
9,5,100,40,5000,93.10000000000001,92.5,89.92,89.91,1.0,15,3
4,5,100,30,256,93.95,91.47,90.56,90.18,1.0,15,3
2,10,100,30,4096,95.56,95.55,90.94,89.68,1.0,15,3
3,1,100,20,16,90.57333333333332,90.01,89.95666666666666,89.9,0.5,15,3
9,5,100,40,5000,92.78999999999999,92.13,89.91,89.91,1.0,15,3
6,3,100,20,512,96.13000000000001,92.70666666666668,90.42333333333333,90.42,0.75,15,3
7,5,100,80,7000,96.63000000000001,93.42,89.73,89.72,1.0,1,1
7,5,100,10,256,95.81666666666668,92.32333333333334,90.58666666666667,89.72,0.5,15,3
3,5,100,30,5000,95.3,93.32000000000001,90.49000000000001,89.9,1.0,15,3
7,5,100,30,32,94.47,90.56,89.77000000000001,89.72,1.0,15,3
7,5,100,20,128,96.15,91.44,90.66,89.72,1.0,15,3
2,5,100,40,5000,95.6,95.39,90.60000000000001,89.68,1.0,15,3
2,1,100,10,2048,95.55,92.15,91.86,89.68,1.0,15,3
0,5,100,20,7000,98.1,91.97,90.21000000000001,90.2,1.0,1,1
4,3,100,20,2048,95.09,92.36,90.98,90.18,1.0,15,3
4,5,100,20,2048,94.13333333333334,91.69333333333334,90.64,90.18,0.5,15,3
4,5,100,40,5000,95.35,93.13,90.35,90.18,1.0,15,3
3,5,100,20,512,94.38,91.70666666666666,90.45333333333333,89.9,0.5,15,3
3,2,100,10,2048,94.28,90.99333333333334,90.42,89.9,0.5,15,3
3,10,100,30,4096,95.30999999999999,93.30000000000001,90.45,89.9,1.0,15,3
7,5,100,10,7000,96.64,93.54,91.29,89.72,1.0,1,1
3,2,100,30,2048,95.48,92.13,90.44,89.9,1.0,15,3
2,5,100,50,5000,94.95,94.51,89.81,89.68,1.0,1,1
7,10,100,20,32,94.8,90.11,89.72,89.72,1.0,15,3
3,2,100,10,2048,94.26333333333334,90.69,90.41333333333334,89.9,0.5,15,3
2,1,100,10,1024,95.57,92.38,92.01,89.68,1.0,15,3
3,5,100,40,7000,95.17,93.41000000000001,89.95,89.9,1.0,1,1
2,5,100,40,1024,95.04,94.49,90.38000000000001,89.68,1.0,15,3
7,5,100,10,128,95.08333333333333,91.34,90.56,89.72,0.5,15,3
7,5,100,30,512,95.86333333333333,93.53,90.14666666666666,89.72,0.5,15,3
2,5,100,20,16,90.03,90.72666666666666,90.66666666666666,89.68,0.5,15,3
2,2,100,10,2048,94.54,92.68666666666667,91.00666666666667,89.68,0.5,15,3
4,5,100,10,1024,95.24000000000001,91.84,91.25999999999999,90.18,1.0,15,3
0,5,100,40,5000,97.96000000000001,92.86,90.25,90.2,1.0,15,3
4,5,100,50,5000,95.41,92.75,90.18,90.18,1.0,15,3
3,5,100,30,32,91.62666666666667,90.83666666666666,90.14666666666666,89.9,0.5,15,3
4,1,100,10,2048,94.01,90.53999999999999,90.77,90.18,0.5,15,3
6,5,100,40,4096,96.97,95.43,90.42,90.42,1.0,15,3
3,5,100,40,5000,95.07,93.99,90.12,89.9,1.0,15,3
3,5,100,10,32,92.93,89.9,89.91,89.9,1.0,15,3
0,1,100,20,64,96.97,90.2,90.2,90.2,1.0,15,3
2,5,100,10,128,93.75666666666666,92.00666666666667,90.95,89.68,0.5,15,3
1,5,100,40,5000,98.2,92.96,88.64999999999999,88.64999999999999,1.0,15,3
7,5,100,100,5000,97.03,93.02,89.72,89.72,1.0,15,3
4,5,100,20,5000,94.63000000000001,92.17999999999999,91.19,90.18,1.0,1,1
7,3,100,10,2048,95.93,92.45333333333333,90.60333333333334,89.72,0.5,15,3
2,5,100,30,1024,95.26,94.85,90.78,89.68,1.0,15,3
2,5,100,10,512,94.59666666666666,92.85,91.16,89.68,0.5,15,3
2,1,100,10,4096,95.86,92.83,91.78,89.68,1.0,15,3
0,5,100,40,5000,97.98,93.64,90.23,90.2,1.0,15,3
7,5,100,30,128,94.66,92.74666666666667,89.96666666666667,89.72,0.5,15,3
9,5,100,30,5000,93.13,91.64,89.94,89.91,1.0,15,3
7,2,100,20,2048,96.89,94.15,90.73,89.72,1.0,15,3
3,3,100,40,1024,95.00999999999999,93.35,90.11,89.9,1.0,15,3
2,10,100,10,4096,95.89999999999999,94.32000000000001,91.92,89.68,1.0,15,3
7,5,100,60,5000,96.78,95.1,89.72,89.72,1.0,15,3
0,2,100,40,4096,97.61999999999999,90.35333333333332,90.21666666666667,90.2,0.5,15,3
7,3,100,20,2048,96.73,94.56,90.77,89.72,1.0,15,3
5,5,100,60,7000,93.37,92.73,91.08000000000001,91.08000000000001,1.0,1,1
2,5,100,30,5000,95.7,95.07,91.06,89.68,1.0,10,3
3,5,100,30,32,92.69,89.95,89.9,89.9,1.0,15,3
9,5,100,10,5000,93.31,90.5,89.96,89.91,1.0,15,3
2,2,100,30,4096,95.08,93.07666666666667,90.57666666666667,89.68,0.5,15,3
7,5,100,10,16,93.8,90.7,90.07,89.72,1.0,15,3
0,5,100,100,7000,97.89999999999999,91.16,90.2,90.2,1.0,1,1
9,10,100,20,256,91.9,90.35,89.96,89.91,1.0,15,3
4,3,100,20,2048,95.43,93.21000000000001,91.03,90.18,1.0,15,3
5,3,100,10,1024,93.52000000000001,91.08000000000001,91.08000000000001,91.08000000000001,1.0,15,3
2,5,100,20,16,91.0,90.66,89.82,89.68,1.0,15,3
3,5,100,30,7000,95.19999999999999,93.06,90.32,89.9,1.0,1,1
7,5,100,30,128,95.89,91.53999999999999,89.86,89.72,1.0,15,3
4,5,100,30,64,92.91,90.77,90.81,90.18,1.0,15,3
3,5,100,40,5000,95.3,93.97,90.13,89.9,1.0,15,3
2,5,100,20,32,91.08333333333334,91.06333333333333,90.97666666666666,89.68,0.5,15,3
2,2,100,60,128,91.56666666666666,90.53333333333333,89.69,89.68,0.25,15,3
2,2,100,10,1024,94.78333333333333,92.81333333333333,91.21000000000001,89.68,0.5,15,3
0,1,100,20,64,93.84,90.20333333333333,90.20333333333333,90.2,0.25,15,3
4,1,100,20,1024,94.19,90.81333333333333,90.67333333333333,90.18,0.5,15,3
8,5,100,30,7000,93.01,91.2,90.27,90.25999999999999,1.0,1,1
2,2,100,30,4096,95.11666666666667,93.12333333333333,90.49000000000001,89.68,0.5,15,3
3,1,100,30,6131,95.36,90.57,90.46,89.9,1.0,3,3
2,5,100,10,64,94.13,90.53999999999999,90.91,89.68,1.0,15,3
6,5,100,20,5000,97.09,94.32000000000001,90.42,90.42,1.0,15,3
2,5,100,10,16,90.71666666666667,90.18333333333334,90.09666666666666,89.68,0.5,15,3
0,5,100,20,5000,97.99,92.30000000000001,90.23,90.2,1.0,15,3
7,5,100,40,1024,96.65,95.03,89.92,89.72,1.0,15,3
2,2,100,10,2048,95.72,93.56,92.07,89.68,1.0,15,3
3,1,100,30,1024,94.35,90.33333333333333,90.32,89.9,0.5,15,3
4,5,100,10,128,93.12,90.77666666666667,90.75333333333333,90.18,0.5,15,3
7,5,100,40,7000,96.6,95.03,89.81,89.72,1.0,1,1
3,5,100,30,1024,94.57666666666667,92.62666666666667,90.17666666666668,89.9,0.5,15,3
2,5,100,30,32,91.23,90.75666666666666,90.27666666666667,89.68,0.5,15,3
2,3,100,30,4096,95.08333333333333,93.72333333333333,90.46,89.68,0.5,15,3
9,5,100,40,7000,92.57,91.69,89.91,89.91,1.0,1,1
9,5,100,40,7000,92.77,92.30000000000001,89.91,89.91,1.0,1,1
4,5,100,20,1024,93.91000000000001,91.75999999999999,90.68,90.18,0.5,15,3
3,1,100,30,1024,95.17,90.45,90.39,89.9,1.0,15,3
3,1,100,50,1024,94.51333333333334,89.90333333333334,89.91,89.9,0.5,15,3
2,3,100,30,1024,95.35,94.5,91.03,89.68,1.0,15,3
4,5,100,10,128,93.19333333333333,90.75999999999999,90.48,90.18,0.5,15,3
8,1,100,50,5000,92.57,90.25999999999999,90.25999999999999,90.25999999999999,1.0,1,1
4,5,100,20,1024,95.62,92.93,91.05,90.18,1.0,15,3
9,5,100,30,5000,93.37,92.07,89.95,89.91,1.0,15,3
2,5,100,20,128,94.42,91.39,91.16,89.68,1.0,15,3
5,5,100,10,7000,93.44,91.12,91.08000000000001,91.08000000000001,1.0,1,1
3,3,100,20,1024,94.33333333333334,91.78333333333333,90.63666666666667,89.9,0.5,15,3
4,1,100,10,4096,93.74666666666667,90.82333333333334,90.82666666666667,90.18,0.5,15,3
0,5,100,30,5000,98.02,92.63,90.22,90.2,1.0,15,3
8,5,100,80,5000,93.12,91.02,90.25999999999999,90.25999999999999,1.0,15,3
3,5,100,20,16,91.29,90.69,90.51,89.9,1.0,15,3
3,5,100,50,7000,94.97,93.7,89.9,89.9,1.0,1,1
4,5,100,30,32,91.94,90.25,90.22,90.18,1.0,15,3
4,5,100,30,16,90.23666666666666,90.18333333333334,90.22333333333333,90.18,0.5,15,3
4,5,100,30,16,90.29333333333334,90.28,90.19,90.18,0.5,15,3
3,5,100,30,7000,94.91000000000001,93.14,90.69,89.9,1.0,1,1
6,5,100,20,5000,97.02,94.07,90.42,90.42,1.0,15,3
4,3,100,30,1024,94.69999999999999,92.25,90.47,90.18,1.0,15,3
3,5,100,30,1024,95.11,93.33,90.47,89.9,1.0,15,3
6,3,100,50,512,95.78999999999999,93.17999999999999,90.42,90.42,0.5,15,3
4,5,100,20,16,90.47333333333333,90.30666666666667,90.38333333333334,90.18,0.5,15,3
1,5,100,80,5000,98.17,94.89,88.64999999999999,88.64999999999999,1.0,15,3
4,2,100,30,2048,94.26,91.17,90.33333333333333,90.18,0.5,15,3
5,5,100,20,5000,94.13,91.74,91.08000000000001,91.08000000000001,1.0,15,3
2,8,100,10,4096,96.13000000000001,94.15,91.99000000000001,89.68,1.0,15,3
4,3,100,80,2048,95.33,90.96,90.18,90.18,1.0,15,3
4,5,100,40,5000,94.69999999999999,92.47999999999999,90.18,90.18,1.0,1,1
7,2,100,20,2048,96.94,93.92,90.68,89.72,1.0,15,3
2,3,100,30,4096,95.20666666666666,93.74666666666667,90.66,89.68,0.5,15,3
3,5,100,80,5000,95.54,92.7,89.9,89.9,1.0,15,3
5,5,100,60,512,92.02,91.52,91.08000000000001,91.08000000000001,0.25,15,3
3,10,100,60,128,93.78333333333333,92.55333333333333,89.90333333333334,89.9,0.75,15,3
4,5,100,30,4096,95.49,93.19,90.64999999999999,90.18,1.0,15,3
7,5,100,30,16,90.75,89.82,89.75999999999999,89.72,1.0,15,3
6,5,100,30,7000,96.74000000000001,94.19999999999999,90.44,90.42,1.0,1,1
9,5,100,20,5000,93.67999999999999,90.99000000000001,89.91,89.91,1.0,15,3
2,5,100,10,5000,95.84,94.04,92.0,89.68,1.0,15,3
0,5,100,80,5000,98.07000000000001,91.84,90.2,90.2,1.0,15,3
4,1,100,40,5000,94.96,90.42,90.18,90.18,1.0,1,1
4,1,100,10,5000,94.56,90.85,91.06,90.18,1.0,1,1
5,5,100,60,5000,93.58999999999999,92.81,91.08000000000001,91.08000000000001,1.0,15,3
0,5,100,80,7000,97.87,91.57,90.2,90.2,1.0,1,1
2,5,100,40,1024,95.13000000000001,94.39999999999999,90.42999999999999,89.68,1.0,15,3
2,5,100,20,64,92.22333333333333,91.64,91.32000000000001,89.68,0.5,15,3
4,5,100,20,1024,95.22,92.82000000000001,90.98,90.18,1.0,15,3
2,2,100,10,4096,94.62,92.68333333333332,91.07666666666667,89.68,0.5,15,3
2,1,100,50,5000,95.11,89.89,89.73,89.68,1.0,1,1
2,8,100,20,2048,95.62,94.85,91.82000000000001,89.68,1.0,15,3
2,5,100,20,2048,95.63000000000001,94.72,91.89,89.68,1.0,15,3
2,2,100,30,2048,95.16,93.16666666666666,90.65333333333334,89.68,0.5,15,3
3,5,100,30,32,90.71333333333334,90.69,89.98333333333333,89.9,0.5,15,3
5,5,100,40,1024,93.19,92.63,91.08000000000001,91.08000000000001,1.0,15,3
7,5,100,10,2048,97.04,94.06,91.03999999999999,89.72,1.0,15,3
3,1,100,40,5000,94.81,89.99000000000001,89.91,89.9,1.0,1,1
3,5,100,50,7000,95.22,93.67,89.9,89.9,1.0,1,1
6,5,100,60,7000,96.67999999999999,94.24,90.42,90.42,1.0,1,1
2,5,100,30,32,91.83,90.06,90.05,89.68,1.0,15,3
4,10,100,20,1024,95.03,92.78999999999999,90.98,90.18,1.0,15,3
6,5,100,60,7000,97.17,94.15,90.42,90.42,1.0,1,1
3,5,100,10,512,95.08,91.25999999999999,90.69,89.9,1.0,15,3
0,5,100,50,5000,97.77,93.47999999999999,90.21000000000001,90.2,1.0,1,1
0,1,100,100,5000,97.95,90.22,90.2,90.2,1.0,1,1
8,5,100,10,5000,93.43,90.62,90.25999999999999,90.25999999999999,1.0,15,3
4,1,100,10,2048,95.28999999999999,91.47,91.02,90.18,1.0,15,3
8,5,100,50,64,91.01,90.25999999999999,90.25999999999999,90.25999999999999,1.0,15,3
5,5,100,60,7000,93.26,92.65,91.08000000000001,91.08000000000001,1.0,1,1
3,5,100,50,7000,95.00999999999999,93.76,89.9,89.9,1.0,1,1
4,2,100,10,2048,95.44,92.25999999999999,91.63,90.18,1.0,15,3
3,8,100,20,2048,95.38,92.86,90.83,89.9,1.0,15,3
3,5,100,30,16,91.43,89.92999999999999,89.92999999999999,89.9,1.0,15,3
2,5,100,10,512,94.63666666666667,92.71333333333334,91.01333333333334,89.68,0.5,15,3
4,5,100,10,5000,95.64,92.95,91.3,90.18,1.0,15,3
2,5,100,10,7000,95.50999999999999,93.11,92.17999999999999,89.68,1.0,1,1
3,5,100,20,16,90.36,89.92999999999999,89.9,89.9,0.5,15,3
4,8,100,20,1024,94.02333333333334,91.56,90.72666666666666,90.18,0.5,15,3
5,5,100,50,7000,93.72,92.56,91.08000000000001,91.08000000000001,1.0,1,1
0,5,100,30,5000,98.14,93.07,90.25,90.2,1.0,15,3
3,2,100,30,2048,95.28999999999999,92.53,90.57,89.9,1.0,15,3
2,5,100,60,5000,95.30999999999999,93.97,89.68,89.68,1.0,1,1
9,5,100,10,5000,93.17,90.67,89.96,89.91,1.0,15,3
9,2,100,20,128,91.60000000000001,89.91,89.91,89.91,1.0,15,3
8,5,100,50,7000,92.72,90.97,90.25999999999999,90.25999999999999,1.0,1,1
4,8,100,10,2048,93.89666666666666,91.59,90.75,90.18,0.5,15,3
9,5,100,20,5000,93.38,90.99000000000001,89.97,89.91,1.0,15,3
3,8,100,10,1024,94.45666666666666,91.43,90.41,89.9,0.5,15,3
2,2,100,20,2048,94.86666666666666,93.21333333333334,91.24666666666667,89.68,0.5,15,3
2,1,100,60,5000,95.33,89.77000000000001,89.69,89.68,1.0,1,1
9,5,100,40,1024,92.73,92.07,89.91,89.91,1.0,15,3
2,3,100,10,256,92.9,91.14,90.05,89.68,0.25,15,3
0,5,100,30,5000,97.65,92.58999999999999,90.25999999999999,90.2,1.0,1,1
2,5,100,20,16,90.18,90.77,90.60666666666667,89.68,0.5,15,3
7,5,100,20,32,93.10666666666667,91.28333333333333,90.13,89.72,0.5,15,3
7,10,100,50,128,95.45,91.03999999999999,89.84,89.72,1.0,15,3
5,5,100,100,7000,93.28,91.82000000000001,91.08000000000001,91.08000000000001,1.0,1,1
1,5,100,100,5000,98.31,92.39,88.64999999999999,88.64999999999999,1.0,15,3
4,2,100,20,2048,95.23,92.35,91.10000000000001,90.18,1.0,15,3
7,2,100,30,2048,96.86,94.11,90.16,89.72,1.0,15,3
3,5,100,20,16,91.16666666666666,90.00666666666667,90.10000000000001,89.9,0.5,15,3
4,5,100,60,7000,94.93,91.28,90.18,90.18,1.0,1,1
3,5,100,20,32,92.97,89.9,89.9,89.9,1.0,15,3
6,3,100,10,256,96.08333333333333,91.49000000000001,90.42,90.42,0.75,15,3
2,10,100,30,5958,95.89999999999999,94.99,91.07,89.68,1.0,3,3
5,5,100,30,7000,93.62,92.0,91.08000000000001,91.08000000000001,1.0,1,1
2,2,100,10,4096,96.12,94.06,92.08,89.68,1.0,15,3
4,5,100,40,7000,94.89,92.86,90.2,90.18,1.0,1,1
0,5,100,40,5000,97.98,92.77,90.3,90.2,1.0,15,3
3,5,100,50,7000,94.96,93.34,89.9,89.9,1.0,1,1
2,5,100,30,256,93.79333333333332,93.26,90.72,89.68,0.5,15,3
0,1,100,50,128,96.87,90.22,90.2,90.2,1.0,15,3
6,5,100,30,7000,96.82,94.38,90.42,90.42,1.0,1,1
3,2,100,10,2048,94.18666666666667,90.77333333333333,90.51,89.9,0.5,15,3
2,2,100,30,1024,94.72666666666667,93.01333333333334,90.66666666666666,89.68,0.5,15,3
2,2,100,10,2048,95.64,94.03,92.12,89.68,1.0,15,3
2,5,100,60,16,89.68,89.78,89.68,89.68,0.25,15,3
2,3,100,20,2048,94.98333333333333,93.50666666666667,91.20666666666666,89.68,0.5,15,3
5,5,100,40,5000,93.66,93.06,91.08000000000001,91.08000000000001,1.0,15,3
7,5,100,30,256,95.27333333333333,93.13666666666667,90.05666666666666,89.72,0.5,15,3
9,5,100,40,1024,92.19000000000001,92.06,89.92,89.91,1.0,15,3
0,5,100,40,5000,97.99,92.73,90.24,90.2,1.0,15,3
2,2,100,20,2048,95.66,94.66,91.89,89.68,1.0,15,3
3,5,100,40,7000,95.00999999999999,93.74,90.01,89.9,1.0,1,1
2,3,100,30,2048,95.6,94.67999999999999,91.01,89.68,1.0,15,3
3,3,100,10,2048,95.52000000000001,91.66,90.97,89.9,1.0,15,3
3,5,100,20,16,91.56,89.92999999999999,89.92,89.9,1.0,15,3
7,5,100,30,1024,96.17,93.91000000000001,90.16,89.72,0.5,15,3
7,5,100,10,5000,96.96000000000001,94.13,90.89,89.72,1.0,15,3
0,10,100,10,1024,97.37,91.09,90.2,90.2,0.5,15,3
7,5,100,10,256,96.67999999999999,91.28,91.10000000000001,89.72,1.0,15,3
2,5,100,30,5958,95.25333333333333,94.21000000000001,90.64333333333333,89.68,0.5,3,3
4,3,100,20,2048,94.22,91.92,90.75999999999999,90.18,0.5,15,3
1,5,100,50,5000,98.03,93.78999999999999,88.64999999999999,88.64999999999999,1.0,15,3
7,2,100,80,128,94.45333333333333,90.86,89.72,89.72,0.5,15,3
3,2,100,20,4096,94.35333333333334,91.18666666666667,90.48,89.9,0.5,15,3
3,10,100,100,64,92.69333333333334,90.41,89.9,89.9,0.75,15,3
6,1,100,100,128,94.55666666666667,90.42,90.42,90.42,0.5,15,3
2,5,100,20,256,94.25,92.90333333333334,91.07,89.68,0.5,15,3
4,5,100,60,7000,94.93,91.60000000000001,90.19,90.18,1.0,1,1
2,5,100,20,5000,96.03,94.74000000000001,91.86,89.68,1.0,15,3
2,1,100,30,4096,95.72,91.61,91.05,89.68,1.0,15,3
3,5,100,10,7000,95.43,92.17999999999999,91.22,89.9,1.0,1,1
8,1,100,30,5000,92.67,90.25999999999999,90.25999999999999,90.25999999999999,1.0,1,1
7,5,100,80,5000,97.08,93.42,89.72,89.72,1.0,15,3
4,5,100,20,1024,95.03,92.67,91.10000000000001,90.18,1.0,15,3
2,5,100,30,16,89.98,89.95,89.68,89.68,1.0,15,3
2,2,100,30,1024,95.19999999999999,94.0,91.14,89.68,1.0,15,3
7,5,100,100,7000,96.78999999999999,93.01,89.72,89.72,1.0,1,1
7,5,100,40,1024,96.45,94.87,89.92999999999999,89.72,1.0,15,3
5,5,100,40,5000,93.12,92.15,91.08000000000001,91.08000000000001,1.0,1,1
4,1,100,30,2048,95.22,90.84,90.73,90.18,1.0,15,3
4,1,100,30,1024,95.04,91.02,90.71000000000001,90.18,1.0,15,3
3,8,100,20,2048,94.62,92.39333333333335,90.64666666666666,89.9,0.5,15,3
8,5,100,40,5000,93.03,91.49000000000001,90.25999999999999,90.25999999999999,1.0,15,3
1,5,100,60,7000,97.89,94.39,88.64999999999999,88.64999999999999,1.0,1,1
7,5,100,40,7000,96.77,95.32000000000001,89.85,89.72,1.0,1,1
3,10,100,10,2048,95.39999999999999,92.35,91.13,89.9,1.0,15,3
3,5,100,20,7000,95.12,92.53,91.11,89.9,1.0,1,1
7,10,100,40,2048,96.89999999999999,95.19,89.95,89.72,1.0,15,3
1,5,100,40,1024,97.94,92.80000000000001,88.64999999999999,88.64999999999999,1.0,15,3
0,1,100,20,5000,97.88,90.23,90.29,90.2,1.0,1,1
2,8,100,30,2048,95.61,94.97,91.14,89.68,1.0,15,3
3,8,100,30,2048,95.5,93.28,90.61,89.9,1.0,15,3
6,5,100,60,7000,96.74000000000001,94.24,90.42,90.42,1.0,1,1
3,3,100,10,1024,95.28,91.95,91.11,89.9,1.0,15,3
3,3,100,30,1024,95.12,92.69,90.58,89.9,1.0,15,3
0,5,100,80,5000,98.0,91.52,90.21000000000001,90.2,1.0,15,3
5,2,100,40,5000,94.08999999999999,91.84,91.08000000000001,91.08000000000001,1.0,15,3
0,5,100,40,5000,97.75,93.05,90.2,90.2,1.0,1,1
0,5,100,50,5000,98.14,92.96,90.22,90.2,1.0,15,3
9,5,100,80,64,90.16666666666666,90.13333333333333,89.91,89.91,0.5,15,3
3,5,100,20,512,93.97666666666666,92.04,90.41333333333334,89.9,0.5,15,3
6,5,100,50,5000,96.89,95.28,90.42,90.42,1.0,15,3
3,5,100,10,1024,95.16,91.78,90.69,89.9,1.0,15,3
5,5,100,10,5000,94.05,91.25999999999999,91.08000000000001,91.08000000000001,1.0,15,3
0,1,100,30,5000,97.61999999999999,90.22,90.36,90.2,1.0,1,1
4,5,100,40,5000,95.34,93.19,90.24,90.18,1.0,15,3
2,1,100,10,1024,94.70333333333333,91.25,91.12,89.68,0.5,15,3
7,5,100,10,7000,96.67,93.17,91.09,89.72,1.0,1,1
2,5,100,20,16,89.87666666666667,90.25333333333333,90.53333333333333,89.68,0.5,15,3
4,3,100,30,2048,94.33,91.64,90.39,90.18,0.5,15,3
4,8,100,10,2048,95.23,92.42,91.23,90.18,1.0,15,3
5,5,100,30,7000,93.46,91.81,91.08000000000001,91.08000000000001,1.0,1,1
2,3,100,20,2048,95.19333333333333,93.56666666666666,91.27666666666666,89.68,0.5,15,3
4,5,100,30,16,90.82000000000001,90.18,90.18,90.18,1.0,15,3
4,5,100,10,512,94.01333333333334,91.28333333333333,90.85,90.18,0.5,15,3
2,5,100,10,256,94.16,92.27,90.93666666666667,89.68,0.5,15,3
7,5,100,10,16,94.56,91.99000000000001,90.16999999999999,89.72,1.0,15,3
7,1,100,10,5000,96.67999999999999,91.31,91.07,89.72,1.0,1,1
0,5,100,40,5000,98.03,93.28,90.24,90.2,1.0,15,3
1,5,100,60,5000,98.22999999999999,94.55,88.64999999999999,88.64999999999999,1.0,15,3
3,5,100,30,32,92.04,89.94,89.9,89.9,1.0,15,3
5,5,100,40,1024,93.22,92.63,91.08000000000001,91.08000000000001,1.0,15,3
3,2,100,30,1024,95.1,92.54,90.55,89.9,1.0,15,3
9,5,100,50,5000,93.16,92.85,89.94,89.91,1.0,15,3
3,5,100,20,16,91.47999999999999,91.07,90.28,89.9,1.0,15,3
6,5,100,50,7000,96.61999999999999,95.08,90.42,90.42,1.0,1,1
2,3,100,20,4096,95.19666666666666,93.53,91.18,89.68,0.5,15,3
4,5,100,20,1024,93.97333333333333,91.66666666666666,90.69333333333334,90.18,0.5,15,3
6,5,100,80,5000,96.92,93.87,90.42,90.42,1.0,15,3
4,5,100,10,1024,93.81666666666668,91.33666666666667,90.73666666666666,90.18,0.5,15,3
3,5,100,50,7000,94.78,93.60000000000001,89.9,89.9,1.0,1,1
7,1,100,30,2048,96.38333333333333,90.13333333333333,90.08666666666667,89.72,0.5,15,3
7,5,100,80,5000,96.72,93.51,89.72,89.72,1.0,1,1
4,2,100,10,2048,93.75666666666666,91.19333333333334,90.71666666666667,90.18,0.5,15,3
1,5,100,10,7000,97.8,88.96,89.59,88.64999999999999,1.0,1,1
3,1,100,20,5000,94.98,90.61,91.12,89.9,1.0,1,1
8,5,100,40,64,91.19,90.25999999999999,90.25999999999999,90.25999999999999,1.0,15,3
6,5,100,80,7000,96.7,93.17,90.42,90.42,1.0,1,1
4,5,100,10,7000,95.07,91.91,91.59,90.18,1.0,1,1
6,5,100,50,7000,96.75,94.82000000000001,90.42,90.42,1.0,1,1
7,3,100,10,2048,97.16,93.71000000000001,91.02,89.72,1.0,15,3
8,5,100,50,7000,92.49000000000001,91.06,90.25999999999999,90.25999999999999,1.0,1,1
6,1,100,100,5000,96.78999999999999,90.42,90.42,90.42,1.0,1,1
2,5,100,10,2048,94.53333333333333,93.07333333333332,90.96,89.68,0.5,15,3
2,3,100,30,4096,95.07,93.97999999999999,90.53999999999999,89.68,0.5,15,3
3,5,100,80,5000,94.89,92.99,89.9,89.9,1.0,1,1
2,1,100,30,4096,95.48,91.47999999999999,91.06,89.68,1.0,15,3
4,5,100,10,16,91.32333333333334,90.19,90.43333333333334,90.18,0.5,15,3
8,5,100,50,5000,92.89,91.53,90.25999999999999,90.25999999999999,1.0,15,3
4,5,100,40,7000,94.96,92.74,90.18,90.18,1.0,1,1
8,5,100,60,7000,92.74,90.9,90.25999999999999,90.25999999999999,1.0,1,1
8,5,100,20,256,92.62,90.29,90.25999999999999,90.25999999999999,1.0,15,3
6,2,100,10,512,96.39,91.75,90.42999999999999,90.42,1.0,15,3
2,3,100,30,2048,95.92,94.69,91.25999999999999,89.68,1.0,15,3
2,5,100,10,512,95.5,92.77,91.89,89.68,1.0,15,3
2,1,100,10,1024,94.74333333333334,91.11,91.22,89.68,0.5,15,3
3,5,100,40,7000,94.82000000000001,93.61,89.91,89.9,1.0,1,1
3,5,100,20,16,91.29,90.9,90.66,89.9,1.0,15,3
3,1,100,40,5000,94.92,89.91,90.0,89.9,1.0,1,1
2,5,100,10,1024,94.69,92.84666666666666,90.93666666666667,89.68,0.5,15,3
3,5,100,30,16,90.32,90.17333333333333,89.93333333333334,89.9,0.5,15,3
7,5,100,30,64,94.65,90.38000000000001,89.72,89.72,1.0,15,3
4,5,100,30,1024,94.94,92.45,90.53999999999999,90.18,1.0,15,3
9,10,100,20,16,89.91333333333333,89.91,89.91,89.91,0.25,15,3
3,5,100,10,1024,95.39999999999999,91.86,91.01,89.9,1.0,15,3
2,5,100,50,7000,95.28999999999999,94.87,89.74,89.68,1.0,1,1
3,10,100,40,16,90.8,89.9,89.89,89.9,1.0,15,3
2,5,100,20,16,89.97,90.02,89.73,89.68,0.5,15,3
2,10,100,20,2048,95.53,94.66,92.11,89.68,1.0,15,3
4,5,100,20,32,91.93,90.61,90.18,90.18,1.0,15,3
3,5,100,30,2048,94.66,92.49000000000001,90.29,89.9,0.5,15,3
2,8,100,10,1024,95.55,93.96,91.71000000000001,89.68,1.0,15,3
3,1,100,10,1024,95.16,90.52,90.83,89.9,1.0,15,3
2,5,100,10,256,94.41666666666667,92.57666666666667,91.04333333333334,89.68,0.5,15,3
7,5,100,30,16,90.61666666666667,89.82,89.72333333333333,89.72,0.5,15,3
3,3,100,60,128,92.45666666666666,91.66333333333333,89.9,89.9,0.5,15,3
2,1,100,30,1024,94.75666666666666,90.87333333333333,90.52333333333334,89.68,0.5,15,3
2,3,100,20,1024,94.74000000000001,93.36333333333333,91.34333333333333,89.68,0.5,15,3
2,1,100,10,4096,94.53666666666666,91.11333333333333,91.25333333333333,89.68,0.5,15,3
5,1,100,20,5000,93.39,91.08000000000001,91.10000000000001,91.08000000000001,1.0,1,1
3,5,100,30,16,90.62666666666667,90.16999999999999,89.94,89.9,0.5,15,3
7,5,100,20,16,93.05,90.99000000000001,90.01,89.72,1.0,15,3
2,5,100,30,512,94.3,93.73333333333333,90.61333333333333,89.68,0.5,15,3
4,5,100,20,64,92.72,91.28,90.66,90.18,1.0,15,3
3,5,100,80,7000,95.19,93.08,89.9,89.9,1.0,1,1
3,2,100,30,2048,94.96,91.90666666666667,90.40333333333334,89.9,0.5,15,3
2,3,100,30,4096,95.59,94.78,91.02,89.68,1.0,15,3
4,2,100,20,4096,95.48,93.02,91.33,90.18,1.0,15,3
9,5,100,50,7000,92.84,92.31,89.91,89.91,1.0,1,1
4,5,100,20,7000,95.16,92.89,91.4,90.18,1.0,1,1
9,5,100,50,5000,92.82000000000001,92.47999999999999,89.91,89.91,1.0,15,3
9,5,100,30,7000,92.92,91.69,89.96,89.91,1.0,1,1
5,5,100,100,5000,93.86,92.17999999999999,91.08000000000001,91.08000000000001,1.0,15,3
3,5,100,20,128,94.38,90.23,90.7,89.9,1.0,15,3
2,5,100,10,4096,96.00999999999999,94.27,91.97999999999999,89.68,1.0,15,3
2,5,100,20,64,93.23,90.39,90.23,89.68,1.0,15,3
2,5,100,50,7000,95.13000000000001,94.25,89.84,89.68,1.0,1,1
4,5,100,20,2048,94.11666666666667,91.95,90.67333333333333,90.18,0.5,15,3
4,2,100,10,4096,93.93,91.31666666666666,90.78666666666668,90.18,0.5,15,3
6,3,100,20,2048,96.81,93.57,90.42,90.42,1.0,15,3
7,5,100,20,512,95.86333333333333,93.34,90.53,89.72,0.5,15,3
8,5,100,20,5000,93.52000000000001,90.75999999999999,90.25999999999999,90.25999999999999,1.0,15,3
3,5,100,20,1024,94.49333333333333,91.87666666666667,90.48333333333333,89.9,0.5,15,3
4,5,100,10,16,92.24,91.0,90.41,90.18,1.0,15,3
3,5,100,30,7000,95.07,93.2,90.69,89.9,1.0,1,1
2,5,100,10,128,94.64,91.72,91.03,89.68,1.0,15,3
0,5,100,40,5000,97.66,93.08,90.25999999999999,90.2,1.0,1,1
4,5,100,20,1024,95.09,92.45,90.78,90.18,1.0,15,3
3,3,100,20,2048,94.64666666666666,91.66,90.63,89.9,0.5,15,3
2,5,100,20,256,94.07,93.17666666666666,91.12666666666667,89.68,0.5,15,3
5,2,100,10,32,91.19333333333334,91.08000000000001,91.08000000000001,91.08000000000001,0.5,15,3
8,5,100,60,7000,92.85,91.29,90.25999999999999,90.25999999999999,1.0,1,1
3,5,100,50,64,90.73666666666666,91.26333333333334,89.9,89.9,0.25,15,3
0,5,100,30,5000,97.98,93.12,90.23,90.2,1.0,15,3
7,1,100,20,5000,96.81,91.27,91.06,89.72,1.0,1,1
2,5,100,20,16,91.53999999999999,91.47,90.44,89.68,1.0,15,3
4,5,100,60,5000,95.41,92.21000000000001,90.2,90.18,1.0,15,3
3,5,100,20,32,91.7,89.91,89.91,89.9,1.0,15,3
7,3,100,20,2048,96.93,94.32000000000001,90.62,89.72,1.0,15,3
0,5,100,60,5000,98.1,92.78,90.2,90.2,1.0,15,3
3,10,100,20,2048,95.06,93.34,90.8,89.9,1.0,15,3
4,3,100,30,2048,94.36333333333333,91.59333333333333,90.55,90.18,0.5,15,3
3,3,100,30,2048,94.76,92.41666666666667,90.19,89.9,0.5,15,3
3,5,100,30,7000,95.13000000000001,92.74,90.75999999999999,89.9,1.0,1,1
1,5,100,80,7000,98.09,94.0,88.64999999999999,88.64999999999999,1.0,1,1
4,3,100,30,4096,95.17999999999999,92.66,90.71000000000001,90.18,1.0,15,3
1,5,100,40,5000,98.07000000000001,93.67999999999999,88.64999999999999,88.64999999999999,1.0,15,3
4,3,100,30,1024,94.16,91.7,90.36999999999999,90.18,0.5,15,3
4,5,100,40,64,91.52666666666667,90.63333333333333,90.19666666666667,90.18,0.5,15,3
3,5,100,30,32,91.14,90.55333333333333,89.97,89.9,0.5,15,3
1,5,100,80,7000,97.95,94.42,88.64999999999999,88.64999999999999,1.0,1,1
3,1,100,20,5000,94.85,90.60000000000001,90.92,89.9,1.0,1,1
2,5,100,10,64,93.05666666666667,91.44333333333333,90.72333333333333,89.68,0.5,15,3
2,5,100,20,1024,94.65333333333334,93.44333333333333,91.25666666666666,89.68,0.5,15,3
2,1,100,20,1024,95.37,92.81,92.11,89.68,1.0,15,3
3,5,100,30,512,93.85,92.24666666666667,90.28,89.9,0.5,15,3
3,5,100,20,16,91.08000000000001,89.9,90.27,89.9,1.0,15,3
2,5,100,20,512,94.98,93.86,91.46,89.68,1.0,15,3
3,5,100,40,7000,95.02000000000001,93.56,89.92,89.9,1.0,1,1
7,1,100,60,5000,96.8,89.94,89.72,89.72,1.0,1,1
3,10,100,30,1024,95.23,92.9,90.53,89.9,1.0,15,3
1,1,100,10,5000,97.76,88.66000000000001,89.46,88.64999999999999,1.0,1,1
6,5,100,60,7000,96.64,94.47,90.42,90.42,1.0,1,1
4,1,100,50,5000,94.64,90.36999999999999,90.18,90.18,1.0,1,1
6,5,100,20,5000,97.18,94.34,90.42,90.42,1.0,15,3
3,1,100,10,1024,94.58666666666666,90.15333333333334,90.51333333333334,89.9,0.5,15,3
0,5,100,40,5000,97.87,93.47999999999999,90.21000000000001,90.2,1.0,15,3
3,2,100,20,2048,94.92,91.85,90.79,89.9,1.0,15,3
3,5,100,80,5000,95.07,93.2,89.9,89.9,1.0,15,3
9,5,100,20,5000,93.32000000000001,90.93,89.96,89.91,1.0,15,3
4,5,100,40,5000,94.97,92.97,90.22,90.18,1.0,15,3
4,5,100,30,16,90.34666666666666,90.18,90.23666666666666,90.18,0.5,15,3
7,1,100,20,5000,96.61,91.77,90.69,89.72,1.0,1,1
4,1,100,60,2048,94.47333333333333,90.18,90.18,90.18,0.5,15,3
9,5,100,40,1024,92.67,91.78,89.92,89.91,1.0,15,3
2,5,100,20,16,90.56,90.89,90.61666666666667,89.68,0.5,15,3
7,5,100,10,5000,97.06,94.14,91.14,89.72,1.0,15,3
6,5,100,50,7000,96.78999999999999,94.54,90.42,90.42,1.0,1,1
3,8,100,10,2048,94.45,91.41,90.41,89.9,0.5,15,3
2,1,100,20,4096,95.12333333333333,92.17333333333333,91.21333333333334,89.68,0.5,15,3
3,5,100,20,16,90.85333333333332,90.18333333333334,90.14666666666666,89.9,0.5,15,3
8,5,100,40,7000,93.06,91.02,90.25999999999999,90.25999999999999,1.0,1,1
6,5,100,30,7000,96.3,94.46,90.42,90.42,1.0,1,1
9,5,100,40,1024,92.33,92.09,89.91,89.91,1.0,15,3
3,2,100,10,2048,94.44,90.76666666666667,90.38333333333334,89.9,0.5,15,3
7,1,100,20,5000,96.72,91.24,91.07,89.72,1.0,1,1
3,10,100,10,4096,95.25,92.19000000000001,90.86999999999999,89.9,1.0,15,3
4,1,100,20,4096,95.5,91.61,90.86999999999999,90.18,1.0,15,3
8,10,100,20,256,92.35,90.31,90.25999999999999,90.25999999999999,1.0,15,3
2,1,100,30,2048,95.34,91.5,90.94,89.68,1.0,15,3
4,5,100,20,2048,95.39,93.27,91.24,90.18,1.0,15,3
2,2,100,30,4096,94.95,93.23,90.53999999999999,89.68,0.5,15,3
5,5,100,80,7000,93.49,92.29,91.08000000000001,91.08000000000001,1.0,1,1
2,5,100,20,16,90.12,90.26666666666667,90.5,89.68,0.5,15,3
3,5,100,40,7000,94.95,93.35,89.92999999999999,89.9,1.0,1,1
2,5,100,10,512,94.46333333333334,92.58666666666666,91.19666666666667,89.68,0.5,15,3
0,5,100,40,1024,97.71,93.04,90.22,90.2,1.0,15,3
6,5,100,60,5000,97.02,94.42,90.42,90.42,1.0,15,3
2,5,100,10,2048,94.75666666666666,93.00333333333334,91.13,89.68,0.5,15,3
2,10,100,30,5958,95.7,95.28999999999999,91.14999999999999,89.68,1.0,3,3
3,5,100,30,16,91.34,89.9,89.9,89.9,1.0,15,3
7,5,100,20,5000,96.85000000000001,94.28,91.17,89.72,1.0,1,1
4,1,100,30,2048,95.24000000000001,90.75,90.74,90.18,1.0,15,3
4,8,100,30,2048,95.19,92.72,90.64999999999999,90.18,1.0,15,3
6,2,100,100,4096,97.07000000000001,92.17999999999999,90.42,90.42,1.0,15,3
3,5,100,50,7000,95.1,93.72,89.9,89.9,1.0,1,1
3,1,100,100,5000,95.3,89.9,89.9,89.9,1.0,1,1
2,5,100,20,32,93.23,91.95,90.75,89.68,1.0,15,3
1,5,100,20,7000,97.96000000000001,89.51,89.42,88.64999999999999,1.0,1,1
3,5,100,40,7000,95.15,93.72,89.91,89.9,1.0,1,1
2,8,100,10,2048,94.63666666666667,92.90666666666667,90.98666666666666,89.68,0.5,15,3
3,5,100,30,512,95.0,92.5,90.4,89.9,1.0,15,3
4,5,100,40,7000,95.08,92.97,90.18,90.18,1.0,1,1
3,5,100,20,64,93.63,90.06,90.08,89.9,1.0,15,3
2,5,100,10,32,92.34666666666666,91.04666666666667,90.57666666666667,89.68,0.5,15,3
5,5,100,40,2048,93.17,92.11,91.08000000000001,91.08000000000001,0.5,15,3
9,5,100,20,5000,93.25,90.95,89.92999999999999,89.91,1.0,15,3
2,5,100,40,5000,95.55,95.24000000000001,90.4,89.68,1.0,15,3
8,5,100,20,5000,93.4,90.79,90.25999999999999,90.25999999999999,1.0,15,3
2,5,100,10,16,92.75999999999999,90.57,91.03,89.68,1.0,15,3
9,2,100,40,512,90.83666666666666,89.95,89.91,89.91,0.25,15,3
2,3,100,10,4096,94.61,92.86999999999999,91.01666666666667,89.68,0.5,15,3
3,3,100,10,1024,94.25333333333333,90.95333333333333,90.42999999999999,89.9,0.5,15,3
4,5,100,10,7000,94.94,91.81,91.25,90.18,1.0,1,1
8,5,100,50,5000,93.26,91.19,90.25999999999999,90.25999999999999,1.0,15,3
6,5,100,100,7000,97.00999999999999,92.43,90.42,90.42,1.0,1,1
3,5,100,50,7000,94.92,93.51,89.92,89.9,1.0,1,1
2,5,100,20,256,94.07333333333334,93.05333333333333,91.18,89.68,0.5,15,3
4,10,100,80,128,93.87,90.48,90.18,90.18,1.0,15,3
1,5,100,40,7000,97.83,92.39,88.64999999999999,88.64999999999999,1.0,1,1
3,5,100,60,5000,95.28999999999999,93.84,89.9,89.9,1.0,15,3
5,5,100,20,7000,93.74,91.53,91.08000000000001,91.08000000000001,1.0,1,1
3,3,100,60,512,94.15333333333334,92.23666666666666,89.9,89.9,0.5,15,3
7,2,100,30,2048,96.41,92.85666666666667,90.08666666666667,89.72,0.5,15,3
1,5,100,30,7000,97.91,90.22,88.64999999999999,88.64999999999999,1.0,1,1
3,5,100,20,5000,95.65,93.37,91.11,89.9,1.0,15,3
3,5,100,40,7000,94.83,93.78999999999999,89.96,89.9,1.0,1,1
0,1,100,10,4096,98.24000000000001,90.25,90.2,90.2,1.0,15,3
1,5,100,20,5000,98.45,89.46,89.05,88.64999999999999,1.0,15,3
3,5,100,20,1024,94.98,92.33,90.79,89.9,1.0,15,3
7,5,100,20,1024,96.88,94.08,90.7,89.72,1.0,15,3
2,5,100,40,7000,95.36,94.87,90.13,89.68,1.0,1,1
7,5,100,30,64,95.19,90.62,89.72,89.72,1.0,15,3
4,3,100,20,1024,94.00333333333334,91.52666666666667,90.67666666666668,90.18,0.5,15,3
6,5,100,40,1024,96.61,94.71000000000001,90.42,90.42,1.0,15,3
4,5,100,10,16,91.47,90.41,90.36999999999999,90.18,1.0,15,3
2,1,100,30,2048,95.03,90.81333333333333,90.43333333333334,89.68,0.5,15,3
4,1,100,100,5000,95.23,90.18,90.18,90.18,1.0,1,1
9,5,100,20,7000,92.82000000000001,91.09,89.95,89.91,1.0,1,1
0,5,100,60,7000,97.81,92.72,90.2,90.2,1.0,1,1
0,5,100,80,7000,97.88,91.61,90.2,90.2,1.0,1,1
4,5,100,60,7000,94.98,91.62,90.18,90.18,1.0,1,1
2,5,100,10,256,94.39333333333333,92.31666666666666,91.05333333333333,89.68,0.5,15,3
5,5,100,50,5000,93.63,92.73,91.08000000000001,91.08000000000001,1.0,15,3
2,5,100,30,1024,95.02000000000001,94.52000000000001,91.07,89.68,1.0,15,3
2,8,100,20,2048,95.77,94.49,92.09,89.68,1.0,15,3
2,2,100,20,4096,95.78,94.3,91.99000000000001,89.68,1.0,15,3
5,5,100,30,7000,93.77,91.97999999999999,91.08000000000001,91.08000000000001,1.0,1,1
7,1,100,30,128,95.46,90.23,90.22,89.72,1.0,15,3
7,5,100,20,64,94.13,91.75666666666666,90.3,89.72,0.5,15,3
2,8,100,30,2048,95.59,94.89,91.14,89.68,1.0,15,3
3,5,100,20,128,93.12666666666667,91.43666666666667,90.5,89.9,0.5,15,3
3,5,100,20,32,91.12666666666667,89.97,89.91,89.9,0.5,15,3
2,1,100,20,2048,95.69,92.24,91.88,89.68,1.0,15,3
3,5,100,20,5000,95.7,93.26,90.86999999999999,89.9,1.0,15,3
0,5,100,20,5000,98.0,92.11,90.24,90.2,1.0,15,3
3,5,100,60,5000,94.94,93.83,89.9,89.9,1.0,1,1
4,5,100,80,5000,95.46,91.74,90.18,90.18,1.0,15,3
3,10,100,20,2048,95.1,93.4,90.64,89.9,1.0,15,3
0,2,100,50,2048,97.5,90.25,90.21333333333334,90.2,0.5,15,3
0,5,100,20,5000,98.08,91.89,90.25,90.2,1.0,15,3
8,5,100,80,7000,92.89,90.77,90.25999999999999,90.25999999999999,1.0,1,1
4,3,100,10,2048,93.79333333333332,91.35333333333332,90.69333333333334,90.18,0.5,15,3
2,1,100,10,64,94.56,91.02,91.12,89.68,1.0,15,3
3,5,100,20,1024,94.54333333333334,92.34333333333333,90.45333333333333,89.9,0.5,15,3
3,1,100,10,4096,95.61,90.75999999999999,90.88000000000001,89.9,1.0,15,3
9,5,100,10,7000,92.86999999999999,90.14,89.92,89.91,1.0,1,1
2,5,100,10,32,92.62,90.57,90.01,89.68,1.0,15,3
8,5,100,10,5000,93.28999999999999,90.44,90.25999999999999,90.25999999999999,1.0,15,3
6,10,100,30,32,92.60333333333334,91.59333333333333,90.42,90.42,0.5,15,3
3,2,100,10,2048,95.56,91.56,91.18,89.9,1.0,15,3
1,10,100,20,32,96.06,88.64999999999999,88.66000000000001,88.64999999999999,1.0,15,3
7,5,100,20,32,94.69,91.63,89.72,89.72,1.0,15,3
6,5,100,40,5000,96.78,95.05,90.42,90.42,1.0,15,3
5,5,100,60,5000,93.25,92.44,91.08000000000001,91.08000000000001,1.0,1,1
4,5,100,20,16,90.27333333333334,90.27333333333334,90.45333333333333,90.18,0.5,15,3
3,5,100,40,7000,95.12,93.66,89.9,89.9,1.0,1,1
7,5,100,10,32,93.89,90.42333333333333,90.05,89.72,0.5,15,3
0,5,100,40,16,94.47,90.2,90.19,90.2,1.0,15,3
1,5,100,50,5000,97.74000000000001,92.88,88.64999999999999,88.64999999999999,1.0,1,1
4,5,100,10,64,93.4,91.25,90.33,90.18,1.0,15,3
4,5,100,60,5000,95.27,92.11,90.18,90.18,1.0,15,3
5,5,100,10,5000,93.91000000000001,91.25,91.08000000000001,91.08000000000001,1.0,15,3
6,5,100,50,7000,96.54,94.73,90.42,90.42,1.0,1,1
6,5,100,10,7000,96.7,92.23,90.42,90.42,1.0,1,1
4,5,100,30,128,93.63,90.84,90.45,90.18,1.0,15,3
6,5,100,40,7000,96.61999999999999,94.6,90.42,90.42,1.0,1,1
2,1,100,20,1024,95.28,92.91,91.62,89.68,1.0,15,3
5,5,100,100,5000,93.43,92.0,91.08000000000001,91.08000000000001,1.0,15,3
3,5,100,30,128,93.06,91.96333333333332,90.4,89.9,0.5,15,3
3,5,100,80,5000,94.91000000000001,93.16,89.9,89.9,1.0,1,1
6,5,100,40,7000,96.74000000000001,94.83,90.42,90.42,1.0,1,1
7,5,100,30,16,90.83666666666666,90.26666666666667,89.79333333333334,89.72,0.5,15,3
5,5,100,50,5000,93.30000000000001,92.67999999999999,91.08000000000001,91.08000000000001,1.0,1,1
2,5,100,10,32,91.36999999999999,90.19666666666667,90.73333333333333,89.68,0.5,15,3
7,5,100,60,5000,96.81,94.98,89.72,89.72,1.0,15,3
9,5,100,60,5000,92.54,92.39,89.91,89.91,1.0,15,3
2,5,100,10,32,93.0,91.52,89.81,89.68,1.0,15,3
0,5,100,60,128,96.30666666666666,90.90333333333334,90.2,90.2,0.5,15,3
1,5,100,40,1024,97.85000000000001,91.47,88.64999999999999,88.64999999999999,1.0,15,3
2,5,100,20,16,91.94,90.89,90.42,89.68,1.0,15,3
1,1,100,40,128,96.09666666666666,88.64999999999999,88.64999999999999,88.64999999999999,0.5,15,3
3,5,100,60,7000,95.27,93.36,89.9,89.9,1.0,1,1
2,8,100,10,1024,95.67999999999999,93.27,91.96,89.68,1.0,15,3
4,5,100,20,32,92.09,90.74,90.16,90.18,1.0,15,3
7,1,100,30,1024,96.97,91.36,90.09,89.72,1.0,15,3
3,1,100,20,2048,94.80666666666666,90.21333333333334,90.60000000000001,89.9,0.5,15,3
3,5,100,40,7000,95.1,93.5,89.91,89.9,1.0,1,1
6,5,100,40,2048,96.43333333333334,94.19333333333333,90.42,90.42,0.5,15,3
3,5,100,20,64,93.64,89.91,90.21000000000001,89.9,1.0,15,3
0,5,100,50,7000,97.86,93.39,90.23,90.2,1.0,1,1
2,1,100,30,2048,95.03333333333333,90.97333333333333,90.57333333333332,89.68,0.5,15,3
0,5,100,30,5000,98.22999999999999,92.64,90.24,90.2,1.0,15,3
2,5,100,30,32,92.09,90.18,89.68,89.68,1.0,15,3
2,1,100,30,5000,95.17,91.21666666666667,90.65666666666667,89.68,0.5,10,3
3,3,100,30,2048,95.13000000000001,93.19,90.48,89.9,1.0,15,3
4,1,100,20,1024,94.39999999999999,90.89,90.71000000000001,90.18,0.5,15,3
7,5,100,100,5000,97.02,93.4,89.72,89.72,1.0,15,3
6,1,100,60,5000,96.55,90.45,90.42,90.42,1.0,1,1
4,1,100,40,5000,95.04,90.25999999999999,90.18,90.18,1.0,1,1
4,5,100,20,256,94.15,91.27,91.05,90.18,1.0,15,3
7,5,100,40,7000,96.75,95.22,89.72,89.72,1.0,1,1
2,8,100,20,1024,95.08,94.6,91.78,89.68,1.0,15,3
3,5,100,40,5000,95.13000000000001,94.17,90.25999999999999,89.9,1.0,15,3
3,1,100,30,1024,94.95,90.18,90.5,89.9,1.0,15,3
2,5,100,80,7000,95.11,92.80000000000001,89.68,89.68,1.0,1,1
3,5,100,80,5000,95.34,93.67,89.9,89.9,1.0,15,3
4,5,100,100,7000,94.92,90.59,90.18,90.18,1.0,1,1
5,3,100,40,1024,92.13666666666667,91.33333333333333,91.08000000000001,91.08000000000001,0.25,15,3
1,5,100,20,7000,98.05,89.53999999999999,89.78,88.64999999999999,1.0,1,1
2,2,100,50,512,94.11666666666667,92.22666666666667,89.83333333333333,89.68,0.5,15,3
7,5,100,40,1024,96.57,95.04,89.83,89.72,1.0,15,3
4,5,100,10,256,93.53333333333333,91.12333333333333,90.61666666666667,90.18,0.5,15,3
3,5,100,20,5000,95.62,92.85,91.19,89.9,1.0,15,3
3,5,100,30,1024,94.89999999999999,93.43,90.68,89.9,1.0,15,3
3,5,100,20,64,93.8,90.25,90.25,89.9,1.0,15,3
2,1,100,10,128,94.89999999999999,91.75,91.66,89.68,1.0,15,3
4,1,100,10,1024,94.15666666666667,90.45,90.78333333333333,90.18,0.5,15,3
4,5,100,20,512,94.84,92.41,90.96,90.18,1.0,15,3
2,3,100,10,2048,94.58666666666666,92.70666666666668,91.09,89.68,0.5,15,3
4,5,100,20,16,90.46,90.28333333333333,90.33,90.18,0.5,15,3
2,5,100,10,128,94.84,91.39,92.01,89.68,1.0,15,3
2,3,100,20,1024,95.28999999999999,94.47,92.06,89.68,1.0,15,3
2,5,100,20,7000,95.54,94.06,92.13,89.68,1.0,1,1
7,5,100,10,128,96.45,90.98,91.19,89.72,1.0,15,3
4,3,100,10,4096,95.61,93.02,91.39,90.18,1.0,15,3
2,2,100,30,2048,94.87333333333333,92.74333333333334,90.57333333333332,89.68,0.5,15,3
9,5,100,60,7000,92.93,92.30000000000001,89.91,89.91,1.0,1,1
6,5,100,60,7000,96.73,94.64,90.42,90.42,1.0,1,1
6,5,100,50,7000,96.91,94.46,90.42,90.42,1.0,1,1
3,8,100,30,4096,95.0,93.78,90.49000000000001,89.9,1.0,15,3
3,5,100,10,16,91.42,89.9,89.96,89.9,1.0,15,3
2,5,100,10,128,93.78333333333333,92.04,90.88666666666667,89.68,0.5,15,3
0,5,100,100,5000,98.06,91.12,90.2,90.2,1.0,15,3
3,5,100,10,7000,95.24000000000001,91.64,91.09,89.9,1.0,1,1
3,5,100,40,1024,95.16,93.30000000000001,90.02,89.9,1.0,15,3
2,5,100,10,5000,95.91,94.17999999999999,92.02,89.68,1.0,15,3
2,8,100,20,2048,95.15333333333334,93.76666666666667,91.19666666666667,89.68,0.5,15,3
7,5,100,30,1024,96.84,94.89,90.32,89.72,1.0,15,3
4,1,100,20,512,93.78,90.92666666666666,90.68,90.18,0.5,15,3
4,1,100,10,2048,93.73666666666666,90.62,90.65666666666667,90.18,0.5,15,3
6,5,100,100,5000,97.21,92.89,90.42,90.42,1.0,15,3
7,5,100,40,5000,96.52,94.89,89.8,89.72,1.0,1,1
3,5,100,20,5000,95.34,92.58999999999999,90.98,89.9,1.0,1,1
3,5,100,30,1024,95.00999999999999,92.81,90.39,89.9,1.0,15,3
3,5,100,10,1024,94.31333333333333,91.24333333333333,90.37666666666667,89.9,0.5,15,3
3,1,100,10,1024,94.13,90.09333333333333,90.50333333333333,89.9,0.5,15,3
3,5,100,60,128,94.12,91.23,89.9,89.9,1.0,15,3
8,5,100,30,5000,93.12,91.10000000000001,90.25999999999999,90.25999999999999,1.0,15,3
8,1,100,50,5000,92.43,90.25999999999999,90.25999999999999,90.25999999999999,1.0,1,1
2,1,100,30,5000,95.11666666666667,90.73,90.5,89.68,0.5,10,3
2,3,100,20,1024,95.58,94.17999999999999,91.78,89.68,1.0,15,3
2,5,100,30,7000,95.52000000000001,94.59,91.23,89.68,1.0,1,1
5,3,100,20,4096,94.34,91.89,91.08000000000001,91.08000000000001,1.0,15,3
6,1,100,20,5000,96.48,90.82000000000001,90.42,90.42,1.0,1,1
7,5,100,20,2048,96.98,94.5,90.78,89.72,1.0,15,3
2,8,100,30,1024,94.73666666666666,94.06,90.64,89.68,0.5,15,3
3,5,100,40,7000,94.73,93.43,89.92,89.9,1.0,1,1
9,1,100,30,256,91.07333333333332,89.93333333333334,89.92,89.91,0.5,15,3
2,3,100,30,2048,95.55,94.73,90.89,89.68,1.0,15,3
4,5,100,40,5000,95.03,92.93,90.25999999999999,90.18,1.0,15,3
3,5,100,40,7000,94.83,93.43,89.92,89.9,1.0,1,1
0,10,100,20,128,97.06,90.73666666666666,90.21666666666667,90.2,0.75,15,3
4,5,100,20,64,93.39,90.35,90.72,90.18,1.0,15,3
0,1,100,30,64,95.73333333333333,90.2,90.2,90.2,0.5,15,3
7,1,100,30,16,91.26333333333334,89.81666666666666,89.79,89.72,0.5,15,3
2,5,100,100,5000,95.67999999999999,92.14,89.68,89.68,1.0,15,3
2,1,100,30,1024,95.72,91.94,90.91,89.68,1.0,15,3
2,8,100,20,2048,94.99333333333333,93.86666666666666,91.19333333333334,89.68,0.5,15,3
2,1,100,20,2048,95.12,91.88,91.32000000000001,89.68,0.5,15,3
3,5,100,20,1024,95.38,92.57,91.13,89.9,1.0,15,3
0,2,100,60,128,95.02000000000001,90.20666666666666,90.2,90.2,0.25,15,3
2,5,100,50,7000,95.25,94.67999999999999,89.83,89.68,1.0,1,1
3,5,100,60,7000,94.76,93.32000000000001,89.9,89.9,1.0,1,1
7,5,100,80,5000,96.74000000000001,93.89999999999999,89.72,89.72,1.0,15,3
3,5,100,20,5000,95.5,93.28,90.83,89.9,1.0,15,3
3,2,100,30,2048,94.68333333333334,91.55666666666666,90.25,89.9,0.5,15,3
3,3,100,10,4096,95.6,92.06,90.9,89.9,1.0,15,3
7,5,100,20,16,91.06666666666666,91.16,89.79333333333334,89.72,0.5,15,3
2,5,100,20,16,90.64,91.39,90.51,89.68,1.0,15,3
2,2,100,10,1024,95.7,93.25,92.19000000000001,89.68,1.0,15,3
7,5,100,40,7000,96.86,94.97,89.8,89.72,1.0,1,1
3,5,100,20,128,94.44,90.27,90.45,89.9,1.0,15,3
4,2,100,10,4096,95.57,92.69,91.34,90.18,1.0,15,3
3,1,100,30,6131,94.91000000000001,90.2,90.13666666666667,89.9,0.5,3,3
4,3,100,20,1024,95.28,92.82000000000001,91.22,90.18,1.0,15,3
7,5,100,30,256,95.23666666666666,93.43,90.18,89.72,0.5,15,3
0,5,100,30,5000,98.08,92.71000000000001,90.22,90.2,1.0,15,3
2,10,100,40,2048,95.43,94.85,90.47,89.68,1.0,15,3
3,10,100,20,1024,94.64,92.39666666666668,90.50333333333333,89.9,0.5,15,3
3,5,100,20,128,93.37,91.06666666666666,90.38000000000001,89.9,0.5,15,3
7,5,100,40,7000,97.05,95.14,89.73,89.72,1.0,1,1
2,1,100,30,2048,95.93,91.97,91.2,89.68,1.0,15,3
8,5,100,20,5000,92.85,90.39,90.25999999999999,90.25999999999999,1.0,1,1
3,5,100,50,5000,95.11,93.86,89.9,89.9,1.0,1,1
7,5,100,30,32,92.37333333333333,91.80333333333334,89.98666666666666,89.72,0.5,15,3
4,3,100,10,2048,95.23,92.57,91.38,90.18,1.0,15,3
7,5,100,10,1024,97.08,93.30000000000001,91.13,89.72,1.0,15,3
7,5,100,60,7000,96.6,94.44,89.72,89.72,1.0,1,1
7,5,100,30,2048,96.67,94.72,90.24,89.72,1.0,15,3
7,1,100,10,2048,95.73333333333333,90.96666666666667,90.54333333333334,89.72,0.5,15,3
2,2,100,10,2048,94.61666666666667,92.59666666666668,90.99666666666667,89.68,0.5,15,3
7,5,100,10,2048,96.97,93.61,91.16,89.72,1.0,15,3
3,5,100,40,7000,94.86,93.47,89.91,89.9,1.0,1,1
2,2,100,20,1024,95.39,94.24,91.97999999999999,89.68,1.0,15,3
2,5,100,20,64,93.5,90.24,90.86,89.68,1.0,15,3
5,5,100,20,5000,93.56,91.4,91.08000000000001,91.08000000000001,1.0,1,1
3,5,100,10,512,94.23333333333333,90.82000000000001,90.36666666666666,89.9,0.5,15,3
4,8,100,30,2048,95.34,93.07,90.85,90.18,1.0,15,3
0,5,100,100,7000,98.00999999999999,91.43,90.2,90.2,1.0,1,1
3,5,100,40,7000,95.21,93.61,89.95,89.9,1.0,1,1
3,5,100,50,7000,95.28999999999999,93.36,89.9,89.9,1.0,1,1
2,8,100,30,2048,95.54,95.21,91.03999999999999,89.68,1.0,15,3
8,5,100,10,128,92.17999999999999,90.25999999999999,90.25999999999999,90.25999999999999,1.0,15,3
7,5,100,20,16,91.83,90.27333333333334,90.19,89.72,0.5,15,3
1,5,100,20,5000,98.26,90.74,89.53999999999999,88.64999999999999,1.0,15,3
3,10,100,30,128,91.74333333333334,90.93333333333334,90.03666666666666,89.9,0.25,15,3
7,2,100,30,2048,96.41999999999999,93.22,90.22333333333333,89.72,0.5,15,3
2,5,100,10,512,94.40666666666667,93.10000000000001,91.03999999999999,89.68,0.5,15,3
4,10,100,30,1024,93.92333333333333,92.05333333333333,90.32333333333334,90.18,0.5,15,3
2,1,100,20,4096,96.0,92.62,91.92,89.68,1.0,15,3
0,5,100,50,7000,97.95,92.88,90.22,90.2,1.0,1,1
4,1,100,20,2048,95.52000000000001,91.95,91.19,90.18,1.0,15,3
3,5,100,20,5000,95.23,92.43,90.85,89.9,1.0,1,1
1,1,100,60,5000,97.87,88.64999999999999,88.64999999999999,88.64999999999999,1.0,1,1
2,5,100,10,16,90.47333333333333,90.47666666666667,90.20333333333333,89.68,0.5,15,3
2,5,100,10,128,93.42666666666666,91.79666666666667,91.13333333333333,89.68,0.5,15,3
0,5,100,80,7000,97.87,91.86999999999999,90.2,90.2,1.0,1,1
4,10,100,30,2048,94.85,93.08999999999999,90.60000000000001,90.18,1.0,15,3
0,1,100,20,2048,97.41,90.2,90.20333333333333,90.2,0.5,15,3
7,5,100,30,1024,96.61999999999999,94.78999999999999,90.22,89.72,1.0,15,3
8,5,100,50,7000,92.60000000000001,90.91,90.25999999999999,90.25999999999999,1.0,1,1
6,5,100,100,7000,96.89,92.17,90.42,90.42,1.0,1,1
7,5,100,10,512,97.11,92.67999999999999,91.3,89.72,1.0,15,3
7,5,100,30,32,93.4,90.23,89.72,89.72,1.0,15,3
8,5,100,50,5000,93.33,91.21000000000001,90.25999999999999,90.25999999999999,1.0,15,3
6,5,100,30,7000,96.48,94.28,90.42,90.42,1.0,1,1
2,5,100,30,5000,95.81,95.15,90.86999999999999,89.68,1.0,15,3
9,10,100,40,512,92.09,91.75999999999999,89.92,89.91,1.0,15,3
4,5,100,20,256,93.13666666666667,91.39666666666668,90.79,90.18,0.5,15,3
3,5,100,50,5000,95.06,93.94,90.03999999999999,89.9,1.0,15,3
4,3,100,30,2048,94.28666666666666,91.76333333333334,90.41,90.18,0.5,15,3
2,5,100,20,256,94.20333333333333,93.22333333333333,91.21666666666667,89.68,0.5,15,3
3,5,100,20,7000,95.30999999999999,92.62,91.25999999999999,89.9,1.0,1,1
0,5,100,100,7000,98.14,91.33,90.2,90.2,1.0,1,1
1,2,100,20,5000,98.2,89.24,89.23,88.64999999999999,1.0,15,3
2,5,100,30,512,94.18666666666667,93.78999999999999,90.56666666666666,89.68,0.5,15,3
3,8,100,10,2048,95.59,91.81,90.75999999999999,89.9,1.0,15,3
7,1,100,30,2048,96.25666666666667,90.36,90.18333333333334,89.72,0.5,15,3
3,5,100,30,4096,95.49,93.7,90.64999999999999,89.9,1.0,15,3
7,5,100,10,64,95.57,91.03999999999999,90.44,89.72,1.0,15,3
0,5,100,10,7000,97.67,90.81,90.2,90.2,1.0,1,1
4,2,100,10,2048,95.27,92.5,91.34,90.18,1.0,15,3
3,10,100,20,1024,94.25,92.17333333333333,90.56333333333333,89.9,0.5,15,3
2,5,100,10,1024,94.51666666666667,92.66,91.05333333333333,89.68,0.5,15,3
1,5,100,40,5000,98.17,92.65,88.64999999999999,88.64999999999999,1.0,15,3
3,5,100,30,4096,95.52000000000001,93.89,90.44,89.9,1.0,15,3
3,5,100,20,16,90.65333333333334,90.03666666666666,89.94333333333333,89.9,0.5,15,3
0,5,100,10,7000,97.83,91.28,90.2,90.2,1.0,1,1
4,5,100,20,7000,95.15,92.58999999999999,91.31,90.18,1.0,1,1
2,8,100,20,4096,95.92,94.81,91.99000000000001,89.68,1.0,15,3
9,5,100,30,7000,92.30000000000001,91.2,89.92999999999999,89.91,1.0,1,1
4,3,100,20,4096,95.63000000000001,92.64,91.36,90.18,1.0,15,3
0,5,100,60,7000,97.92,92.12,90.2,90.2,1.0,1,1
7,1,100,10,5000,96.67,91.33,91.11,89.72,1.0,1,1
3,2,100,10,4096,95.47,91.75999999999999,90.82000000000001,89.9,1.0,15,3
4,3,100,10,2048,95.21,92.58,91.34,90.18,1.0,15,3
9,2,100,10,4096,93.47,90.55,90.03,89.91,1.0,15,3
2,1,100,10,2048,95.73,92.74,92.06,89.68,1.0,15,3
7,5,100,20,16,92.86,90.53,90.23,89.72,1.0,15,3
2,5,100,10,16,90.62,91.25666666666666,90.62333333333333,89.68,0.5,15,3
5,5,100,30,7000,93.57,92.12,91.08000000000001,91.08000000000001,1.0,1,1
7,2,100,10,1024,96.14666666666668,92.60000000000001,90.78666666666668,89.72,0.5,15,3
3,5,100,10,128,93.78999999999999,90.35666666666667,90.36666666666666,89.9,0.5,15,3
3,5,100,20,256,94.72,91.19,90.7,89.9,1.0,15,3
3,5,100,60,7000,94.98,93.13,89.9,89.9,1.0,1,1
3,5,100,30,1024,94.74000000000001,93.10000000000001,90.61,89.9,1.0,15,3
8,5,100,20,5000,93.41000000000001,91.09,90.25999999999999,90.25999999999999,1.0,15,3
4,3,100,20,2048,95.39,93.04,91.3,90.18,1.0,15,3
7,5,100,50,5000,97.11,95.57,89.74,89.72,1.0,15,3
7,5,100,30,5000,96.75,94.19999999999999,90.14999999999999,89.72,1.0,1,1
2,1,100,20,1024,94.75,92.02,91.10000000000001,89.68,0.5,15,3
5,5,100,100,7000,93.25,91.61,91.08000000000001,91.08000000000001,1.0,1,1
0,1,100,30,5000,97.46000000000001,90.21000000000001,90.27,90.2,1.0,1,1
2,5,100,20,256,95.00999999999999,92.83,92.05,89.68,1.0,15,3
3,5,100,10,64,94.26,90.14999999999999,90.38000000000001,89.9,1.0,15,3
2,5,100,20,16,89.81666666666666,90.07333333333334,90.07,89.68,0.5,15,3
3,2,100,10,1024,95.25,91.46,91.03,89.9,1.0,15,3
4,10,100,20,2048,95.49,93.01,91.39,90.18,1.0,15,3
3,5,100,30,512,94.15333333333334,92.36666666666666,90.31333333333333,89.9,0.5,15,3
2,5,100,10,16,90.5,90.81333333333333,90.25666666666666,89.68,0.5,15,3
4,1,100,20,2048,95.24000000000001,91.3,91.06,90.18,1.0,15,3
3,1,100,20,1024,94.83,91.0,90.81,89.9,1.0,15,3
2,1,100,20,5000,95.64,92.09,92.04,89.68,1.0,1,1
0,5,100,30,5000,98.16,93.28,90.25,90.2,1.0,15,3
4,5,100,80,7000,95.28,91.01,90.18,90.18,1.0,1,1
2,2,100,30,5000,95.58,94.03,90.94,89.68,1.0,10,3
3,3,100,20,2048,95.57,92.71000000000001,91.10000000000001,89.9,1.0,15,3
3,5,100,40,7000,95.03,93.21000000000001,89.9,89.9,1.0,1,1
2,3,100,10,4096,94.72,93.05333333333333,91.19666666666667,89.68,0.5,15,3
1,2,100,60,64,95.43,88.64999999999999,88.64999999999999,88.64999999999999,0.5,15,3
3,1,100,80,5000,95.07,89.9,89.9,89.9,1.0,1,1
7,10,100,100,128,95.83,91.17,89.72,89.72,1.0,15,3
2,5,100,40,1024,95.38,94.98,90.41,89.68,1.0,15,3
9,10,100,100,512,92.29,91.43,89.91,89.91,1.0,15,3
3,5,100,40,5000,95.04,93.46,89.94,89.9,1.0,1,1
4,5,100,50,5000,94.75,91.83,90.18,90.18,1.0,1,1
7,5,100,10,128,95.17,91.67666666666666,90.63666666666667,89.72,0.5,15,3
3,10,100,30,256,92.34666666666666,91.35333333333332,90.03999999999999,89.9,0.25,15,3
7,5,100,40,128,95.37666666666667,93.42666666666666,89.92999999999999,89.72,0.75,15,3
0,5,100,100,7000,98.07000000000001,91.21000000000001,90.2,90.2,1.0,1,1
7,1,100,50,128,94.34333333333333,89.72,89.72,89.72,0.5,15,3
3,5,100,10,64,92.57666666666667,90.07,90.07,89.9,0.5,15,3
0,5,100,80,5000,98.07000000000001,91.67,90.2,90.2,1.0,15,3
3,10,100,20,4096,95.64,93.47,90.96,89.9,1.0,15,3
2,2,100,40,64,93.17,90.59,89.95,89.68,1.0,15,3
7,5,100,20,16,93.53,90.31,90.3,89.72,1.0,15,3
7,1,100,80,5000,96.47,89.74,89.72,89.72,1.0,1,1
3,5,100,20,256,93.64,91.49000000000001,90.29666666666667,89.9,0.5,15,3
3,5,100,50,7000,94.83,93.66,89.9,89.9,1.0,1,1
7,5,100,20,5000,97.28,95.23,90.73,89.72,1.0,15,3
0,5,100,40,5000,97.92,93.46,90.27,90.2,1.0,15,3
2,3,100,30,4096,95.14333333333333,93.77333333333333,90.64333333333333,89.68,0.5,15,3
8,5,100,60,5000,93.07,91.27,90.25999999999999,90.25999999999999,1.0,15,3
2,5,100,30,16,91.14,90.24,89.92,89.68,1.0,15,3
8,1,100,80,32,90.27333333333334,90.25999999999999,90.25999999999999,90.25999999999999,0.5,15,3
5,5,100,20,5000,94.08,91.95,91.08000000000001,91.08000000000001,1.0,15,3
3,5,100,30,1024,94.28666666666666,92.49666666666667,90.26333333333334,89.9,0.5,15,3
3,5,100,30,16,90.28333333333333,90.18,89.90333333333334,89.9,0.5,15,3
0,5,100,100,7000,98.0,91.21000000000001,90.2,90.2,1.0,1,1
5,2,100,20,2048,92.44333333333333,91.08000000000001,91.08333333333334,91.08000000000001,0.25,15,3
1,5,100,10,7000,98.00999999999999,88.8,89.64999999999999,88.64999999999999,1.0,1,1
4,10,100,10,4096,95.14,92.72,91.16,90.18,1.0,15,3
6,5,100,40,5000,96.36,94.57,90.42,90.42,1.0,1,1
5,5,100,50,7000,93.30000000000001,92.41,91.08000000000001,91.08000000000001,1.0,1,1
2,5,100,30,32,91.14333333333333,91.21666666666667,90.14,89.68,0.5,15,3
8,10,100,30,64,90.36,90.28666666666668,90.25999999999999,90.25999999999999,0.25,15,3
7,5,100,20,1024,96.85000000000001,94.19,90.75999999999999,89.72,1.0,15,3
6,5,100,80,5000,97.06,94.15,90.42,90.42,1.0,15,3
8,5,100,40,5000,93.01,91.44,90.25999999999999,90.25999999999999,1.0,15,3
7,5,100,30,128,96.12,91.97999999999999,90.51,89.72,1.0,15,3
2,10,100,30,1024,94.73,94.26333333333334,90.49666666666667,89.68,0.5,15,3
4,8,100,20,2048,94.98,92.84,90.99000000000001,90.18,1.0,15,3
2,5,100,20,16,90.52666666666667,90.3,90.23666666666666,89.68,0.5,15,3
7,5,100,10,7000,96.91,93.30000000000001,91.27,89.72,1.0,1,1
4,5,100,30,5000,95.15,92.67,90.66,90.18,1.0,15,3
6,5,100,40,7000,96.47,94.67,90.42,90.42,1.0,1,1
2,5,100,30,1024,94.86,94.09666666666666,90.53666666666666,89.68,0.5,15,3
0,5,100,40,1024,97.83,93.28999999999999,90.21000000000001,90.2,1.0,15,3
2,3,100,20,4096,94.97,93.31666666666666,91.14333333333333,89.68,0.5,15,3
2,2,100,20,2048,95.82000000000001,94.21000000000001,92.05,89.68,1.0,15,3
3,3,100,80,512,94.6,92.25999999999999,89.9,89.9,0.75,15,3
8,5,100,50,7000,92.69,91.02,90.25999999999999,90.25999999999999,1.0,1,1
2,5,100,20,5000,95.94,94.82000000000001,92.0,89.68,1.0,15,3
3,5,100,20,128,94.15,90.24,90.36,89.9,1.0,15,3
4,5,100,40,5000,95.39,93.33,90.34,90.18,1.0,15,3
2,5,100,10,32,91.19,90.68666666666667,90.70333333333333,89.68,0.5,15,3
4,5,100,100,7000,95.61,90.57,90.18,90.18,1.0,1,1
0,5,100,40,7000,97.89,93.17999999999999,90.28,90.2,1.0,1,1
2,1,100,20,4096,96.11,92.67,92.01,89.68,1.0,15,3
2,3,100,10,1024,94.54666666666667,92.78333333333333,91.01666666666667,89.68,0.5,15,3
0,5,100,40,5000,98.19,93.62,90.28,90.2,1.0,15,3
7,5,100,40,5000,97.17,95.89999999999999,89.98,89.72,1.0,15,3
4,5,100,30,1024,94.74000000000001,93.05,90.85,90.18,1.0,15,3
3,5,100,10,4096,95.72,92.01,91.03999999999999,89.9,1.0,15,3
7,5,100,20,256,95.67333333333333,92.67666666666666,90.61333333333333,89.72,0.5,15,3
6,5,100,20,5000,97.04,94.22,90.42,90.42,1.0,15,3
2,5,100,30,512,94.88,93.75,90.8,89.68,1.0,15,3
7,5,100,80,5000,96.92,94.33,89.72,89.72,1.0,15,3
4,10,100,20,1024,95.03,92.80000000000001,90.96,90.18,1.0,15,3
6,5,100,40,5000,96.93,95.37,90.42,90.42,1.0,15,3
8,1,100,40,5000,91.69666666666667,90.25999999999999,90.25999999999999,90.25999999999999,0.25,15,3
3,5,100,30,32,92.46,89.96,89.9,89.9,1.0,15,3
2,5,100,10,16,91.64,90.85,90.14999999999999,89.68,1.0,15,3
5,5,100,40,1024,93.10000000000001,92.58,91.08000000000001,91.08000000000001,1.0,15,3
2,1,100,10,1024,95.78999999999999,92.62,91.86,89.68,1.0,15,3
7,5,100,20,512,95.84666666666666,93.15333333333334,90.53333333333333,89.72,0.5,15,3
3,5,100,40,1024,94.85,93.51,90.19,89.9,1.0,15,3
2,5,100,60,7000,95.1,93.86,89.68,89.68,1.0,1,1
3,1,100,20,1024,94.93,90.95,91.18,89.9,1.0,15,3
7,3,100,30,2048,96.43333333333334,93.66,90.19,89.72,0.5,15,3
2,5,100,100,7000,95.42,91.75,89.68,89.68,1.0,1,1
5,5,100,20,5000,94.45,91.86,91.09,91.08000000000001,1.0,15,3
4,5,100,30,1024,93.93,91.69666666666667,90.38666666666667,90.18,0.5,15,3
2,3,100,30,4096,95.22,93.86,90.59333333333333,89.68,0.5,15,3
0,5,100,10,5000,98.11999999999999,91.7,90.2,90.2,1.0,15,3
7,5,100,40,5000,96.39,94.87,89.75999999999999,89.72,1.0,1,1
2,5,100,10,64,94.03,91.03999999999999,90.57,89.68,1.0,15,3
3,5,100,20,64,92.46666666666667,90.46333333333332,90.45,89.9,0.5,15,3
7,5,100,10,1024,96.89999999999999,93.44,91.11,89.72,1.0,15,3
3,5,100,50,7000,95.15,93.58999999999999,89.9,89.9,1.0,1,1
4,3,100,20,4096,95.41,93.05,90.94,90.18,1.0,15,3
4,3,100,10,2048,93.84666666666666,91.22,90.74333333333333,90.18,0.5,15,3
3,5,100,20,128,93.22,90.93666666666667,90.39333333333333,89.9,0.5,15,3
7,5,100,10,16,92.13666666666667,89.94,89.8,89.72,0.5,15,3
4,5,100,20,5000,95.57,93.19,91.39,90.18,1.0,15,3
2,5,100,60,5000,95.42,94.69,89.81,89.68,1.0,15,3
2,5,100,20,32,93.10000000000001,91.32000000000001,90.14999999999999,89.68,1.0,15,3
8,5,100,40,5000,93.38,91.17,90.25999999999999,90.25999999999999,1.0,15,3
4,2,100,20,4096,94.08333333333333,91.40333333333334,90.57333333333332,90.18,0.5,15,3
2,5,100,30,7000,95.23,94.44,91.09,89.68,1.0,1,1
8,5,100,10,7000,93.01,90.32,90.25999999999999,90.25999999999999,1.0,1,1
2,1,100,20,4096,95.06,91.66,91.22666666666667,89.68,0.5,15,3
2,5,100,30,5000,95.67,94.96,90.92,89.68,1.0,15,3
2,2,100,20,5000,95.06666666666666,93.03,91.12333333333333,89.68,0.5,15,3
4,5,100,10,32,91.59333333333333,90.38666666666667,90.43333333333334,90.18,0.5,15,3
7,5,100,30,32,93.97,90.47,89.72,89.72,1.0,15,3
2,5,100,20,64,92.34,92.25666666666666,90.58,89.68,0.5,15,3
3,5,100,10,2048,95.77,92.39,91.11,89.9,1.0,15,3
2,5,100,40,5000,94.89999999999999,94.14,90.23,89.68,1.0,1,1
3,5,100,30,128,93.89999999999999,90.27,90.14999999999999,89.9,1.0,15,3
3,5,100,50,7000,95.08,93.54,89.9,89.9,1.0,1,1
4,8,100,20,2048,94.23333333333333,91.92333333333333,90.88333333333334,90.18,0.5,15,3
3,5,100,20,512,94.27,91.66333333333333,90.46,89.9,0.5,15,3
3,10,100,10,2048,95.54,92.17999999999999,90.82000000000001,89.9,1.0,15,3
3,5,100,20,16,91.79,90.07,89.9,89.9,1.0,15,3
5,5,100,30,7000,93.99,91.97999999999999,91.08000000000001,91.08000000000001,1.0,1,1
2,5,100,30,1024,95.50999999999999,94.71000000000001,91.14,89.68,1.0,15,3
7,5,100,40,1024,96.55,94.92,89.92,89.72,1.0,15,3
7,3,100,30,2048,96.56,93.71666666666667,90.28666666666668,89.72,0.5,15,3
2,5,100,10,16,92.77,90.85,89.84,89.68,1.0,15,3
0,5,100,20,5000,97.94,92.03,90.2,90.2,1.0,15,3
8,5,100,40,1024,93.03,90.84,90.25999999999999,90.25999999999999,1.0,15,3
9,3,100,100,5000,92.94,91.14999999999999,89.91,89.91,1.0,15,3
4,2,100,30,4096,95.35,92.42,90.55,90.18,1.0,15,3
6,5,100,80,5000,96.91,93.7,90.42,90.42,1.0,15,3
1,5,100,50,7000,98.0,94.3,88.64999999999999,88.64999999999999,1.0,1,1
2,10,100,20,1024,95.03666666666668,93.81666666666668,91.24333333333333,89.68,0.5,15,3
2,5,100,30,128,92.87666666666667,92.72666666666667,90.34666666666666,89.68,0.5,15,3
9,5,100,40,5000,93.19,92.47999999999999,89.91,89.91,1.0,15,3
2,5,100,20,256,94.94,92.57,91.57,89.68,1.0,15,3
2,5,100,10,2048,95.82000000000001,93.83,91.99000000000001,89.68,1.0,15,3
8,1,100,60,5000,92.65,90.25999999999999,90.25999999999999,90.25999999999999,1.0,1,1
3,5,100,10,256,94.41000000000001,90.36999999999999,90.86999999999999,89.9,1.0,15,3
3,5,100,30,32,91.85,89.99000000000001,89.92,89.9,1.0,15,3
4,3,100,30,1024,95.11,92.91,90.64999999999999,90.18,1.0,15,3
6,1,100,60,256,96.05,90.48,90.42,90.42,1.0,15,3
8,5,100,30,7000,92.99,90.79,90.25999999999999,90.25999999999999,1.0,1,1
2,5,100,20,5000,95.99,95.0,91.91,89.68,1.0,15,3
2,5,100,10,128,93.56666666666666,91.75333333333333,90.62666666666667,89.68,0.5,15,3
2,5,100,40,7000,95.50999999999999,94.66,90.13,89.68,1.0,1,1
7,5,100,30,512,96.15,93.77666666666667,90.22,89.72,0.5,15,3
1,5,100,40,7000,97.92999999999999,92.51,88.64999999999999,88.64999999999999,1.0,1,1
4,1,100,20,2048,94.15,90.61666666666667,90.64333333333333,90.18,0.5,15,3
0,5,100,10,5000,98.22,91.5,90.2,90.2,1.0,15,3
3,8,100,20,2048,95.19999999999999,93.06,90.94,89.9,1.0,15,3
3,8,100,10,2048,95.49,92.28,91.0,89.9,1.0,15,3
9,5,100,100,5000,93.25,91.71000000000001,89.91,89.91,1.0,15,3
4,2,100,20,2048,94.48,91.54333333333334,90.83666666666666,90.18,0.5,15,3
3,5,100,20,5000,95.50999999999999,93.26,90.95,89.9,1.0,15,3
2,5,100,20,128,93.51,92.85,90.98,89.68,0.5,15,3
3,3,100,10,2048,95.67999999999999,92.07,90.99000000000001,89.9,1.0,15,3
1,5,100,20,7000,97.87,89.18,89.62,88.64999999999999,1.0,1,1
5,5,100,50,5000,93.42,92.33,91.08000000000001,91.08000000000001,1.0,1,1
2,5,100,20,256,95.00999999999999,92.54,91.74,89.68,1.0,15,3
2,5,100,20,256,94.02333333333334,92.85333333333334,91.29333333333334,89.68,0.5,15,3
1,5,100,10,7000,98.00999999999999,88.81,89.68,88.64999999999999,1.0,1,1
5,5,100,100,7000,93.39,91.81,91.08000000000001,91.08000000000001,1.0,1,1
5,5,100,10,7000,93.67,91.10000000000001,91.08000000000001,91.08000000000001,1.0,1,1
3,1,100,40,1024,94.54333333333334,89.96,90.16,89.9,0.5,15,3
0,5,100,10,7000,97.81,91.29,90.2,90.2,1.0,1,1
1,5,100,20,7000,97.94,89.84,89.46,88.64999999999999,1.0,1,1
4,5,100,10,2048,95.17999999999999,92.78999999999999,91.57,90.18,1.0,15,3
3,5,100,20,7000,95.27,92.36,91.11,89.9,1.0,1,1
3,5,100,20,16,90.83,90.25999999999999,89.97,89.9,1.0,15,3
0,5,100,30,7000,97.89,92.72,90.24,90.2,1.0,1,1
2,5,100,10,256,95.39999999999999,91.89,91.83,89.68,1.0,15,3
2,3,100,20,1024,94.82000000000001,93.26333333333334,91.06333333333333,89.68,0.5,15,3
2,5,100,30,256,93.75333333333333,93.21333333333334,90.66333333333333,89.68,0.5,15,3
4,5,100,20,16,91.18,90.25,90.31,90.18,1.0,15,3
3,5,100,20,5000,95.84,93.17999999999999,90.96,89.9,1.0,15,3
7,5,100,30,512,96.61999999999999,94.04,90.27,89.72,1.0,15,3
2,5,100,10,2048,95.83,93.96,91.93,89.68,1.0,15,3
2,1,100,30,1024,95.41,91.56,90.99000000000001,89.68,1.0,15,3
5,5,100,40,1024,92.96,92.49000000000001,91.08000000000001,91.08000000000001,1.0,15,3
3,3,100,20,4096,94.75,91.74333333333334,90.56333333333333,89.9,0.5,15,3
2,5,100,20,128,93.12666666666667,92.54666666666667,90.91,89.68,0.5,15,3
2,2,100,30,5000,95.59,94.07,91.09,89.68,1.0,10,3
2,5,100,10,16,90.86,91.04666666666667,90.27666666666667,89.68,0.5,15,3
2,8,100,10,2048,95.74000000000001,94.05,92.14,89.68,1.0,15,3
1,5,100,30,7000,97.94,90.12,88.64999999999999,88.64999999999999,1.0,1,1
2,3,100,80,256,94.33,91.11,89.68,89.68,1.0,15,3
3,3,100,30,2048,95.19,92.72,90.47,89.9,1.0,15,3
2,5,100,40,1024,95.32000000000001,94.92,90.51,89.68,1.0,15,3
4,5,100,20,2048,95.36,92.88,91.06,90.18,1.0,15,3
8,5,100,40,5000,93.03,91.13,90.25999999999999,90.25999999999999,1.0,15,3
2,5,100,20,512,94.55333333333333,93.48666666666666,91.2,89.68,0.5,15,3
3,8,100,30,2048,94.89999999999999,93.25,90.36,89.9,1.0,15,3
1,5,100,50,7000,97.83,94.46,88.64999999999999,88.64999999999999,1.0,1,1
6,5,100,100,7000,96.86,92.4,90.42,90.42,1.0,1,1
2,5,100,20,256,93.95333333333333,93.22,90.92666666666666,89.68,0.5,15,3
3,10,100,20,2048,95.27,93.4,90.86999999999999,89.9,1.0,15,3
2,2,100,30,5000,95.17,92.74,90.67333333333333,89.68,0.5,10,3
8,5,100,10,7000,92.94,90.28,90.25999999999999,90.25999999999999,1.0,1,1
2,5,100,80,5000,95.57,93.63,89.68,89.68,1.0,15,3
2,5,100,30,1024,94.84,94.14666666666666,90.75666666666666,89.68,0.5,15,3
9,2,100,40,64,90.01,89.92333333333333,89.91,89.91,0.25,15,3
4,5,100,30,16,90.51,90.28,90.27,90.18,0.5,15,3
4,5,100,60,7000,94.99,91.44,90.18,90.18,1.0,1,1
7,5,100,20,4096,97.00999999999999,94.75,90.86,89.72,1.0,15,3
8,5,100,40,7000,93.0,90.96,90.25999999999999,90.25999999999999,1.0,1,1
2,2,100,30,5958,95.34,93.32333333333334,90.58333333333334,89.68,0.5,3,3
2,5,100,30,32,90.88666666666667,91.96,90.54666666666667,89.68,0.5,15,3
7,1,100,20,2048,97.03,92.19000000000001,90.68,89.72,1.0,15,3
2,5,100,20,512,94.54666666666667,93.51333333333334,91.14,89.68,0.5,15,3
2,5,100,80,7000,95.41,92.94,89.68,89.68,1.0,1,1
7,5,100,30,512,95.86,93.50333333333334,90.21000000000001,89.72,0.5,15,3
9,5,100,30,7000,92.36,90.99000000000001,89.97,89.91,1.0,1,1
3,2,100,20,128,94.19,89.92999999999999,90.49000000000001,89.9,1.0,15,3
2,5,100,30,16,90.75,89.77000000000001,89.68,89.68,1.0,15,3
1,5,100,80,5000,98.25,94.13,88.64999999999999,88.64999999999999,1.0,15,3
5,5,100,50,7000,93.24,92.34,91.08000000000001,91.08000000000001,1.0,1,1
2,5,100,30,512,95.09,94.32000000000001,91.19,89.68,1.0,15,3
3,5,100,20,64,93.49,90.16999999999999,90.09,89.9,1.0,15,3
5,5,100,50,32,91.08000000000001,91.09666666666666,91.08000000000001,91.08000000000001,0.5,15,3
3,5,100,10,7000,95.15,91.46,90.73,89.9,1.0,1,1
0,5,100,80,7000,98.08,91.71000000000001,90.2,90.2,1.0,1,1
1,5,100,10,7000,97.74000000000001,88.78,89.44,88.64999999999999,1.0,1,1
6,5,100,100,7000,96.82,92.78999999999999,90.42,90.42,1.0,1,1
2,5,100,20,32,91.27666666666666,91.55,90.85666666666667,89.68,0.5,15,3
2,5,100,30,16,89.77666666666667,90.28666666666668,89.84,89.68,0.5,15,3
4,5,100,10,128,94.41000000000001,91.03999999999999,90.95,90.18,1.0,15,3
2,5,100,10,128,94.51,90.83,91.53999999999999,89.68,1.0,15,3
4,5,100,30,4096,95.47,93.06,90.73,90.18,1.0,15,3
3,5,100,20,7000,95.05,92.12,90.98,89.9,1.0,1,1
6,5,100,80,5000,97.02,93.89999999999999,90.42,90.42,1.0,15,3
8,5,100,40,5000,93.28,91.18,90.25999999999999,90.25999999999999,1.0,15,3
7,5,100,40,7000,96.6,95.06,89.79,89.72,1.0,1,1
0,1,100,20,5000,97.77,90.2,90.22,90.2,1.0,1,1
2,5,100,10,2048,95.86,93.97,92.01,89.68,1.0,15,3
5,5,100,60,7000,92.91,92.61,91.08000000000001,91.08000000000001,1.0,1,1
2,10,100,40,256,94.69,93.53,90.28,89.68,1.0,15,3
7,5,100,80,7000,96.78999999999999,93.67999999999999,89.72,89.72,1.0,1,1
7,5,100,20,64,95.96000000000001,91.86,90.48,89.72,1.0,15,3
5,5,100,30,5000,93.75,92.55,91.08000000000001,91.08000000000001,1.0,15,3
3,3,100,10,1024,94.45333333333333,90.97666666666666,90.66,89.9,0.5,15,3
7,5,100,20,16,91.56666666666666,90.94,89.91666666666667,89.72,0.5,15,3
2,1,100,30,5000,95.83,91.79,90.91,89.68,1.0,10,3
4,5,100,100,5000,95.54,91.10000000000001,90.18,90.18,1.0,15,3
2,5,100,30,5000,95.56,95.16,91.08000000000001,89.68,1.0,15,3
9,10,100,20,64,90.36333333333333,90.01,89.91666666666667,89.91,0.5,15,3
9,10,100,40,1024,91.81333333333333,91.47333333333333,89.91,89.91,0.5,15,3
6,5,100,40,5000,96.86,95.39,90.42,90.42,1.0,15,3
4,5,100,20,5000,95.42,92.97,90.96,90.18,1.0,15,3
7,5,100,30,5000,97.03,95.05,90.27,89.72,1.0,15,3
2,5,100,50,5000,95.73,94.69999999999999,90.16,89.68,1.0,15,3
2,5,100,20,16,90.14,90.03666666666666,89.8,89.68,0.5,15,3
2,5,100,30,1024,95.08,94.74000000000001,90.98,89.68,1.0,15,3
3,5,100,20,128,93.16,91.17,90.47,89.9,0.5,15,3
2,5,100,80,7000,95.43,92.56,89.68,89.68,1.0,1,1
2,5,100,20,5000,95.83,94.83,91.85,89.68,1.0,15,3
3,5,100,40,1024,95.3,93.24,90.2,89.9,1.0,15,3
4,5,100,20,5000,95.39999999999999,93.17999999999999,91.22,90.18,1.0,15,3
3,5,100,30,6131,95.05666666666667,92.81,90.27,89.9,0.5,3,3
7,5,100,20,7000,96.72,94.26,91.01,89.72,1.0,1,1
5,1,100,100,5000,93.08999999999999,91.08000000000001,91.08000000000001,91.08000000000001,1.0,1,1
4,5,100,60,64,92.51,90.28333333333333,90.18333333333334,90.18,0.75,15,3
3,1,100,30,1024,95.04,90.35,90.62,89.9,1.0,15,3
2,5,100,20,256,95.06,92.74,92.16,89.68,1.0,15,3
1,5,100,20,5000,98.24000000000001,89.63,89.62,88.64999999999999,1.0,15,3
2,5,100,50,7000,95.44,94.67999999999999,89.88000000000001,89.68,1.0,1,1
4,1,100,50,5000,94.62,90.19,90.18,90.18,1.0,1,1
3,5,100,20,512,95.05,91.78,90.93,89.9,1.0,15,3
6,1,100,40,16,90.55333333333333,90.42,90.42,90.42,0.25,15,3
9,5,100,30,7000,92.72,91.39,89.94,89.91,1.0,1,1
4,5,100,40,5000,95.38,92.73,90.24,90.18,1.0,15,3
3,5,100,10,64,93.75,89.99000000000001,90.14999999999999,89.9,1.0,15,3
8,5,100,20,5000,93.47,90.86999999999999,90.25999999999999,90.25999999999999,1.0,15,3
4,5,100,20,1024,94.07333333333334,91.87666666666667,90.69666666666667,90.18,0.5,15,3
4,1,100,30,2048,94.17333333333333,90.29333333333334,90.28,90.18,0.5,15,3
4,5,100,20,16,91.18,90.42999999999999,90.25,90.18,1.0,15,3
4,5,100,10,16,92.05,90.64,90.63,90.18,1.0,15,3
1,5,100,40,7000,98.0,91.92,88.64999999999999,88.64999999999999,1.0,1,1
2,2,100,30,2048,95.54,94.32000000000001,90.94,89.68,1.0,15,3
3,1,100,30,2048,94.68333333333334,90.35333333333332,90.29,89.9,0.5,15,3
1,1,100,100,512,97.83,88.64999999999999,88.64999999999999,88.64999999999999,1.0,15,3
3,5,100,30,4096,95.22,93.22,90.42999999999999,89.9,1.0,15,3
6,5,100,30,5000,96.75,93.88,90.42,90.42,1.0,1,1
3,5,100,40,7000,95.1,93.58,90.03,89.9,1.0,1,1
2,5,100,40,7000,95.30999999999999,95.09,90.25,89.68,1.0,1,1
3,5,100,50,7000,94.99,93.67999999999999,89.9,89.9,1.0,1,1
2,5,100,20,64,93.77,91.36,91.03999999999999,89.68,1.0,15,3
2,1,100,30,2048,94.85666666666667,91.28,90.53,89.68,0.5,15,3
6,1,100,40,5000,96.85000000000001,90.58,90.42,90.42,1.0,1,1
5,5,100,40,1024,93.28999999999999,92.66,91.08000000000001,91.08000000000001,1.0,15,3
7,5,100,40,5000,96.89,95.12,89.83,89.72,1.0,15,3
3,10,100,30,2048,95.45,93.52000000000001,90.52,89.9,1.0,15,3
9,5,100,100,7000,92.5,91.44,89.91,89.91,1.0,1,1
3,1,100,10,1024,94.34666666666666,90.27,90.43333333333334,89.9,0.5,15,3
5,5,100,50,7000,93.22,92.43,91.08000000000001,91.08000000000001,1.0,1,1
7,1,100,40,256,95.26,89.99000000000001,89.77333333333334,89.72,0.5,15,3
1,5,100,40,1024,97.86,92.34,88.64999999999999,88.64999999999999,1.0,15,3
4,3,100,30,4096,94.29666666666667,91.60666666666667,90.35,90.18,0.5,15,3
7,5,100,40,1024,96.87,95.34,89.95,89.72,1.0,15,3
0,5,100,40,7000,97.64,93.60000000000001,90.25999999999999,90.2,1.0,1,1
7,5,100,20,5000,96.74000000000001,94.28999999999999,90.97,89.72,1.0,1,1
2,3,100,30,2048,95.18666666666667,93.75,90.58,89.68,0.5,15,3
0,1,100,20,512,97.50999999999999,90.2,90.2,90.2,0.75,15,3
2,5,100,20,16,92.19000000000001,92.12,90.74,89.68,1.0,15,3
3,5,100,40,7000,94.59,93.47999999999999,89.96,89.9,1.0,1,1
0,5,100,40,5000,97.95,93.5,90.31,90.2,1.0,15,3
4,3,100,20,1024,95.23,92.67999999999999,91.38,90.18,1.0,15,3
4,2,100,10,1024,95.04,92.30000000000001,91.45,90.18,1.0,15,3
2,3,100,10,4096,95.88,94.28999999999999,91.96,89.68,1.0,15,3
0,5,100,20,5000,97.91,91.78,90.22,90.2,1.0,15,3
9,5,100,50,7000,92.67999999999999,92.15,89.91,89.91,1.0,1,1
1,5,100,50,7000,97.92,94.84,88.64999999999999,88.64999999999999,1.0,1,1
9,5,100,50,4096,92.97999999999999,93.01,89.92,89.91,1.0,15,3
1,5,100,10,7000,97.78,88.85,89.47,88.64999999999999,1.0,1,1
1,5,100,80,7000,98.11999999999999,93.58,88.64999999999999,88.64999999999999,1.0,1,1
7,5,100,80,256,93.76333333333334,91.71000000000001,89.72,89.72,0.25,15,3
3,5,100,50,7000,95.00999999999999,93.76,89.9,89.9,1.0,1,1
4,5,100,30,16,90.25999999999999,90.20333333333333,90.18333333333334,90.18,0.5,15,3
4,5,100,20,7000,95.19999999999999,92.77,91.59,90.18,1.0,1,1
1,10,100,50,32,92.43666666666667,90.0,88.64999999999999,88.64999999999999,0.25,15,3
3,5,100,20,512,94.08666666666666,91.81,90.46666666666667,89.9,0.5,15,3
4,5,100,30,4096,95.5,92.71000000000001,90.66,90.18,1.0,15,3
6,5,100,40,1024,96.58,94.58,90.42,90.42,1.0,15,3
7,5,100,30,1024,96.05,93.84666666666666,90.10333333333334,89.72,0.5,15,3
3,5,100,20,128,92.82000000000001,90.73,90.30333333333334,89.9,0.5,15,3
0,1,100,40,5000,97.56,90.2,90.24,90.2,1.0,1,1
2,5,100,20,1024,94.69666666666666,93.56,91.32666666666667,89.68,0.5,15,3
7,1,100,20,2048,96.17,90.72,90.41333333333334,89.72,0.5,15,3
8,5,100,40,128,90.86333333333333,90.30666666666667,90.25999999999999,90.25999999999999,0.5,15,3
3,5,100,20,128,94.17,90.21000000000001,90.16,89.9,1.0,15,3
9,1,100,100,512,92.09,89.91,89.91,89.91,1.0,15,3
1,5,100,50,7000,97.89999999999999,93.74,88.64999999999999,88.64999999999999,1.0,1,1
3,5,100,20,5000,95.66,93.33,91.09,89.9,1.0,15,3
4,5,100,20,7000,95.0,92.21000000000001,90.88000000000001,90.18,1.0,1,1
3,5,100,50,7000,95.02000000000001,93.71000000000001,89.9,89.9,1.0,1,1
4,5,100,20,64,91.66666666666666,90.55333333333333,90.39333333333333,90.18,0.5,15,3
2,5,100,40,5000,95.71,95.12,90.42,89.68,1.0,15,3
2,5,100,30,16,90.49000000000001,89.73,89.68,89.68,1.0,15,3
4,8,100,10,1024,93.88666666666666,91.19,90.74,90.18,0.5,15,3
4,5,100,10,2048,95.28999999999999,92.54,91.43,90.18,1.0,15,3
4,5,100,20,64,93.03,90.7,90.8,90.18,1.0,15,3
7,5,100,50,7000,96.6,94.63000000000001,89.72,89.72,1.0,1,1
4,10,100,20,1024,94.13333333333334,91.64999999999999,90.61666666666667,90.18,0.5,15,3
3,5,100,40,1024,95.04,93.56,90.22,89.9,1.0,15,3
2,5,100,10,16,92.93,91.03,89.92,89.68,1.0,15,3
2,1,100,80,4096,95.33666666666667,89.68,89.68,89.68,0.5,15,3
4,5,100,10,7000,94.93,91.86,91.3,90.18,1.0,1,1
9,5,100,50,5000,92.96,92.10000000000001,89.92,89.91,1.0,15,3
8,10,100,20,64,90.36333333333333,90.26666666666667,90.25999999999999,90.25999999999999,0.25,15,3
2,10,100,20,1024,95.14333333333333,93.71000000000001,91.15333333333334,89.68,0.5,15,3
2,5,100,20,512,95.3,93.78999999999999,91.84,89.68,1.0,15,3
3,1,100,30,4096,95.28999999999999,90.46,90.46,89.9,1.0,15,3
2,5,100,50,7000,95.3,94.54,89.8,89.68,1.0,1,1
9,5,100,40,1024,92.66,92.01,89.92,89.91,1.0,15,3
4,2,100,30,2048,94.29666666666667,91.49000000000001,90.39333333333333,90.18,0.5,15,3
4,5,100,30,1024,94.95,92.47,90.61,90.18,1.0,15,3
2,2,100,10,2048,95.75,93.83,91.86,89.68,1.0,15,3
2,5,100,10,16,90.33333333333333,90.21333333333334,89.95666666666666,89.68,0.5,15,3
4,5,100,20,2048,94.19333333333333,91.64333333333333,90.73666666666666,90.18,0.5,15,3
2,1,100,20,5000,95.53,91.97999999999999,91.9,89.68,1.0,1,1
8,1,100,30,1024,92.02666666666667,90.25999999999999,90.25999999999999,90.25999999999999,0.5,15,3
2,5,100,80,7000,95.13000000000001,92.44,89.68,89.68,1.0,1,1
9,5,100,50,7000,92.56,91.91,89.91,89.91,1.0,1,1
5,1,100,80,5000,93.21000000000001,91.09,91.08000000000001,91.08000000000001,1.0,1,1
9,5,100,20,5000,93.06,90.64999999999999,89.91,89.91,1.0,15,3
7,5,100,100,7000,96.78999999999999,92.71000000000001,89.72,89.72,1.0,1,1
2,5,100,20,1024,94.87666666666667,93.58666666666666,91.09333333333333,89.68,0.5,15,3
2,8,100,10,1024,94.66333333333333,92.94,90.91333333333334,89.68,0.5,15,3
2,5,100,20,7000,95.64,94.39999999999999,92.13,89.68,1.0,1,1
4,5,100,20,5000,95.46,93.41000000000001,91.31,90.18,1.0,15,3
2,5,100,30,128,93.09666666666668,93.16333333333333,90.36,89.68,0.5,15,3
6,5,100,40,5000,96.5,94.44,90.42,90.42,1.0,1,1
3,1,100,10,2048,95.5,90.64,91.08000000000001,89.9,1.0,15,3
6,5,100,80,7000,96.91,93.87,90.42,90.42,1.0,1,1
5,1,100,20,32,91.13666666666667,91.08000000000001,91.08333333333334,91.08000000000001,0.5,15,3
3,3,100,30,2048,94.61,92.30666666666667,90.24333333333333,89.9,0.5,15,3
9,5,100,40,7000,92.81,92.27,89.91,89.91,1.0,1,1
8,5,100,40,5000,93.30000000000001,91.36,90.25999999999999,90.25999999999999,1.0,15,3
0,5,100,40,7000,97.88,93.17999999999999,90.36,90.2,1.0,1,1
3,2,100,80,1024,94.44,90.65666666666667,89.9,89.9,0.5,15,3
7,5,100,30,16,92.52,89.84,89.72,89.72,1.0,15,3
2,3,100,20,1024,94.92,93.46,91.05666666666666,89.68,0.5,15,3
8,3,100,100,2048,93.19,90.67,90.25999999999999,90.25999999999999,1.0,15,3
2,5,100,10,16,92.29,91.19,90.25999999999999,89.68,1.0,15,3
2,5,100,30,16,90.52,90.25999999999999,90.0,89.68,1.0,15,3
2,3,100,10,1024,94.66333333333333,92.92333333333333,90.98333333333333,89.68,0.5,15,3
4,10,100,30,1024,94.8,92.54,90.72,90.18,1.0,15,3
4,5,100,30,64,92.67999999999999,91.03999999999999,90.56,90.18,1.0,15,3
8,5,100,40,5000,93.23,91.13,90.25999999999999,90.25999999999999,1.0,15,3
9,10,100,100,5000,92.77,92.44,89.91,89.91,1.0,15,3
4,5,100,10,16,92.58,91.03,90.53,90.18,1.0,15,3
4,5,100,80,7000,94.84,90.85,90.18,90.18,1.0,1,1
7,5,100,30,1024,96.65,94.24,90.18,89.72,1.0,15,3
7,5,100,40,5000,96.94,95.3,89.99000000000001,89.72,1.0,15,3
4,3,100,20,2048,95.24000000000001,93.0,91.14999999999999,90.18,1.0,15,3
6,5,100,20,7000,96.77,93.78,90.42,90.42,1.0,1,1
2,5,100,40,1024,95.15,94.24,90.28,89.68,1.0,15,3
3,5,100,40,5000,95.13000000000001,93.89999999999999,90.18,89.9,1.0,15,3
3,5,100,20,7000,95.26,92.73,91.07,89.9,1.0,1,1
2,5,100,40,5000,95.7,94.93,90.38000000000001,89.68,1.0,15,3
1,5,100,30,7000,97.98,89.99000000000001,88.64999999999999,88.64999999999999,1.0,1,1
3,5,100,20,64,93.77,89.94,90.01,89.9,1.0,15,3
6,5,100,80,5000,97.24000000000001,93.89,90.42,90.42,1.0,15,3
3,5,100,60,7000,94.83,93.53,89.9,89.9,1.0,1,1
4,5,100,20,512,94.63000000000001,92.43,91.02,90.18,1.0,15,3
9,5,100,100,7000,92.73,91.12,89.91,89.91,1.0,1,1
8,5,100,50,5000,93.17999999999999,91.10000000000001,90.25999999999999,90.25999999999999,1.0,15,3
3,5,100,40,7000,95.06,93.69,89.94,89.9,1.0,1,1
7,5,100,30,128,94.44,92.75999999999999,90.14,89.72,0.5,15,3
8,5,100,10,5000,92.84,90.27,90.25999999999999,90.25999999999999,1.0,1,1
3,5,100,50,7000,95.16,93.43,89.9,89.9,1.0,1,1
2,1,100,10,2048,94.77666666666667,91.22,91.16,89.68,0.5,15,3
5,5,100,80,5000,93.66,92.58999999999999,91.08000000000001,91.08000000000001,1.0,15,3
3,8,100,20,2048,95.28999999999999,92.92,90.98,89.9,1.0,15,3
2,2,100,20,2048,95.82000000000001,94.49,91.97,89.68,1.0,15,3
2,10,100,30,1024,95.38,94.92,91.02,89.68,1.0,15,3
2,5,100,10,128,93.48333333333333,91.57,90.96333333333332,89.68,0.5,15,3
3,5,100,10,5000,94.91000000000001,91.63,90.83,89.9,1.0,1,1
1,5,100,80,5000,98.14,94.84,88.64999999999999,88.64999999999999,1.0,15,3
2,2,100,10,1024,95.61,93.54,91.86999999999999,89.68,1.0,15,3
2,5,100,10,5000,95.44,93.13,91.94,89.68,1.0,1,1
5,5,100,40,5000,93.34,92.19000000000001,91.08000000000001,91.08000000000001,1.0,1,1
7,1,100,20,2048,96.89,91.9,90.57,89.72,1.0,15,3
2,5,100,40,5000,94.91000000000001,94.36,90.25999999999999,89.68,1.0,1,1
3,10,100,100,64,91.75999999999999,90.45333333333333,89.9,89.9,0.5,15,3
2,5,100,30,5000,95.78,95.39999999999999,91.34,89.68,1.0,15,3
2,5,100,30,1024,94.74333333333334,94.01,90.55333333333333,89.68,0.5,15,3
1,5,100,40,1024,97.99,91.88,88.64999999999999,88.64999999999999,1.0,15,3
7,2,100,20,4096,96.39,92.92333333333333,90.42999999999999,89.72,0.5,15,3
8,5,100,100,5000,92.65,90.71000000000001,90.25999999999999,90.25999999999999,1.0,1,1
6,5,100,80,7000,96.87,93.38,90.42,90.42,1.0,1,1
3,5,100,40,7000,94.98,93.64,89.9,89.9,1.0,1,1
3,5,100,20,256,93.8,91.62333333333333,90.58666666666667,89.9,0.5,15,3
3,1,100,20,4096,95.50999999999999,91.03,90.86999999999999,89.9,1.0,15,3
5,5,100,40,1024,92.65,92.45,91.08000000000001,91.08000000000001,1.0,15,3
1,5,100,60,5000,98.16,94.47,88.64999999999999,88.64999999999999,1.0,15,3
6,10,100,10,128,96.06,90.42,90.44,90.42,1.0,15,3
9,3,100,50,32,89.96,89.92666666666666,89.91,89.91,0.25,15,3
4,5,100,60,5000,95.35,92.24,90.18,90.18,1.0,15,3
6,5,100,10,7000,96.87,92.45,90.42,90.42,1.0,1,1
2,2,100,30,2048,95.64,94.08,90.91,89.68,1.0,15,3
6,10,100,60,512,95.67,94.05666666666667,90.42,90.42,0.5,15,3
0,5,100,30,5000,98.22,93.05,90.28,90.2,1.0,15,3
2,3,100,10,2048,95.81,93.63,91.83,89.68,1.0,15,3
3,5,100,100,7000,94.82000000000001,92.60000000000001,89.9,89.9,1.0,1,1
3,5,100,40,7000,94.97,93.45,89.91,89.9,1.0,1,1
4,5,100,20,32,92.64,90.77,90.18,90.18,1.0,15,3
4,8,100,20,1024,94.96,92.35,91.06,90.18,1.0,15,3
3,1,100,30,1024,94.29666666666667,90.24,90.20333333333333,89.9,0.5,15,3
2,5,100,20,1024,95.81,94.37,91.97,89.68,1.0,15,3
7,5,100,10,32,93.37333333333333,90.58,90.33333333333333,89.72,0.5,15,3
4,8,100,10,2048,93.92,91.41333333333334,90.73,90.18,0.5,15,3
4,5,100,30,2048,94.94,92.97,90.61,90.18,1.0,15,3
7,8,100,30,2048,96.98,95.07,90.44,89.72,1.0,15,3
3,5,100,40,7000,94.89999999999999,93.65,89.92,89.9,1.0,1,1
2,5,100,20,128,93.25666666666666,92.24333333333334,90.77333333333333,89.68,0.5,15,3
2,8,100,20,1024,94.89333333333333,93.64,91.20333333333333,89.68,0.5,15,3
3,5,100,30,1024,95.23,92.91,90.52,89.9,1.0,15,3
4,2,100,10,1024,93.97333333333333,91.23333333333333,90.62333333333333,90.18,0.5,15,3
4,5,100,30,64,93.03,90.53999999999999,90.23,90.18,1.0,15,3
2,5,100,10,4096,96.04,94.42,92.17999999999999,89.68,1.0,15,3
2,1,100,10,4096,94.58,91.14666666666666,91.05666666666666,89.68,0.5,15,3
4,1,100,30,2048,95.0,90.66,90.63,90.18,1.0,15,3
3,5,100,30,6131,95.58,93.58999999999999,90.67,89.9,1.0,3,3
2,1,100,30,4096,95.77,91.49000000000001,90.83,89.68,1.0,15,3
5,5,100,40,7000,93.47999999999999,92.24,91.08000000000001,91.08000000000001,1.0,1,1
9,5,100,100,5000,93.01,91.53,89.91,89.91,1.0,15,3
6,10,100,20,256,95.16333333333333,92.47666666666666,90.42,90.42,0.5,15,3
6,5,100,40,5000,97.00999999999999,95.55,90.42,90.42,1.0,15,3
4,5,100,10,7000,95.17,92.28,91.14999999999999,90.18,1.0,1,1
4,5,100,20,16,91.14999999999999,90.34,90.24,90.18,1.0,15,3
3,5,100,10,2048,94.26666666666667,91.22666666666667,90.56,89.9,0.5,15,3
4,5,100,40,1024,94.67999999999999,92.53,90.24,90.18,1.0,15,3
5,5,100,100,5000,93.13,91.69,91.08000000000001,91.08000000000001,1.0,1,1
0,5,100,50,5000,98.04,93.60000000000001,90.22,90.2,1.0,15,3
3,5,100,10,32,93.30000000000001,89.92,89.91,89.9,1.0,15,3
4,8,100,10,4096,95.43,92.83,91.32000000000001,90.18,1.0,15,3
3,3,100,10,4096,95.83,91.95,90.75999999999999,89.9,1.0,15,3
7,5,100,20,5000,96.83,93.73,90.86999999999999,89.72,1.0,1,1
2,2,100,20,1024,94.86,93.17333333333333,91.21000000000001,89.68,0.5,15,3
6,5,100,30,7000,96.71,94.0,90.42,90.42,1.0,1,1
2,2,100,30,2048,94.90666666666667,93.10000000000001,90.68333333333334,89.68,0.5,15,3
8,5,100,100,7000,92.86999999999999,90.51,90.25999999999999,90.25999999999999,1.0,1,1
6,10,100,100,128,94.62333333333333,92.23333333333333,90.42,90.42,0.5,15,3
6,5,100,40,7000,96.52,94.76,90.42,90.42,1.0,1,1
3,1,100,20,1024,94.38666666666666,90.54333333333334,90.53666666666666,89.9,0.5,15,3
2,1,100,30,2048,94.92,91.26666666666667,90.72,89.68,0.5,15,3
2,8,100,20,1024,95.30999999999999,94.49,91.89,89.68,1.0,15,3
7,1,100,40,5000,96.39,90.07,89.8,89.72,1.0,1,1
9,5,100,10,7000,92.97999999999999,90.25,89.96,89.91,1.0,1,1
7,5,100,80,7000,96.86,93.87,89.72,89.72,1.0,1,1
3,3,100,20,2048,94.84333333333333,91.81,90.63,89.9,0.5,15,3
3,5,100,20,1024,95.41,92.75999999999999,91.0,89.9,1.0,15,3
3,1,100,50,5000,94.72,89.95,89.9,89.9,1.0,1,1
9,5,100,60,7000,92.81,92.66,89.91,89.91,1.0,1,1
3,5,100,20,32,92.11,90.32,89.94,89.9,1.0,15,3
8,5,100,20,5000,93.11,90.92,90.25999999999999,90.25999999999999,1.0,15,3
1,5,100,80,7000,98.18,93.47,88.64999999999999,88.64999999999999,1.0,1,1
4,3,100,10,1024,94.03,91.41333333333334,90.63,90.18,0.5,15,3
3,5,100,30,256,94.01,91.34,90.33,89.9,1.0,15,3
5,5,100,30,7000,93.35,91.83,91.08000000000001,91.08000000000001,1.0,1,1
4,3,100,20,4096,94.13,91.49000000000001,90.63,90.18,0.5,15,3
3,2,100,30,1024,95.09,92.11,90.58,89.9,1.0,15,3
9,5,100,40,1024,92.31,91.59,89.91,89.91,1.0,15,3
0,5,100,20,5000,97.98,91.9,90.21000000000001,90.2,1.0,15,3
0,5,100,20,7000,97.75,91.75999999999999,90.23,90.2,1.0,1,1
4,5,100,100,5000,95.52000000000001,90.98,90.18,90.18,1.0,15,3
2,3,100,30,2048,95.09,93.72666666666667,90.62,89.68,0.5,15,3
3,1,100,20,1024,95.17,90.78,91.07,89.9,1.0,15,3
3,5,100,20,5000,95.56,93.12,90.83,89.9,1.0,15,3
2,2,100,10,4096,94.47,92.84,91.12,89.68,0.5,15,3
4,5,100,30,256,92.99333333333333,91.02,90.24,90.18,0.5,15,3
4,5,100,20,128,93.75,90.42999999999999,91.10000000000001,90.18,1.0,15,3
3,8,100,20,2048,95.46,93.39,90.81,89.9,1.0,15,3
2,5,100,20,1024,95.33,94.6,91.96,89.68,1.0,15,3
2,10,100,20,2048,95.62,94.82000000000001,92.17,89.68,1.0,15,3
7,5,100,30,128,94.53,92.86,90.02,89.72,0.5,15,3
3,5,100,20,7000,95.27,92.61,91.07,89.9,1.0,1,1
2,5,100,100,7000,95.47,91.8,89.68,89.68,1.0,1,1
5,5,100,30,5000,93.36,91.53999999999999,91.08000000000001,91.08000000000001,1.0,1,1
2,5,100,40,5000,95.63000000000001,95.49,90.42,89.68,1.0,15,3
0,1,100,80,5000,98.00999999999999,90.23,90.2,90.2,1.0,1,1
2,1,100,100,5000,95.3,89.68,89.68,89.68,1.0,1,1
0,1,100,10,5000,97.72999999999999,90.2,90.2,90.2,1.0,1,1
2,10,100,40,1024,94.64333333333333,94.45,90.10666666666667,89.68,0.5,15,3
7,5,100,30,128,94.69999999999999,92.33333333333333,89.86333333333333,89.72,0.5,15,3
2,2,100,30,5958,95.29666666666667,93.28999999999999,90.56666666666666,89.68,0.5,3,3
3,1,100,100,5000,94.86,89.9,89.9,89.9,1.0,1,1
4,5,100,50,5000,95.24000000000001,92.88,90.2,90.18,1.0,15,3
9,5,100,40,1024,92.47,92.02,89.94,89.91,1.0,15,3
4,1,100,10,4096,93.67333333333333,90.59666666666666,90.73,90.18,0.5,15,3
3,8,100,30,1024,94.33666666666667,92.67333333333333,90.23,89.9,0.5,15,3
6,5,100,40,1024,96.67,94.44,90.42,90.42,1.0,15,3
3,8,100,20,2048,95.04,92.94,91.03,89.9,1.0,15,3
4,5,100,60,5000,95.48,92.45,90.18,90.18,1.0,15,3
8,5,100,100,7000,92.95,90.58,90.25999999999999,90.25999999999999,1.0,1,1
2,8,100,20,2048,95.07333333333334,93.75666666666666,91.25666666666666,89.68,0.5,15,3
2,5,100,80,5000,95.39,93.23,89.69,89.68,1.0,1,1
6,5,100,40,5000,96.57,94.34,90.42,90.42,1.0,1,1
4,5,100,10,5000,95.46,92.81,91.42,90.18,1.0,15,3
4,5,100,20,5000,95.34,92.89,90.97,90.18,1.0,15,3
4,5,100,100,5000,95.03,90.64,90.18,90.18,1.0,1,1
4,2,100,20,2048,94.28333333333333,91.7,90.66666666666666,90.18,0.5,15,3
8,5,100,20,5000,93.28999999999999,91.01,90.25999999999999,90.25999999999999,1.0,15,3
3,5,100,40,7000,95.49,93.45,89.9,89.9,1.0,1,1
7,5,100,30,64,93.60333333333334,92.66333333333333,90.03,89.72,0.5,15,3
3,1,100,30,2048,95.27,90.31,90.5,89.9,1.0,15,3
2,1,100,10,1024,95.47,92.51,91.88,89.68,1.0,15,3
3,5,100,30,5000,95.43,93.53,90.77,89.9,1.0,15,3
4,1,100,10,4096,95.33,91.47999999999999,91.3,90.18,1.0,15,3
6,5,100,40,7000,96.58,95.04,90.42,90.42,1.0,1,1
4,5,100,30,7000,94.51,92.17999999999999,90.36999999999999,90.18,1.0,1,1
2,1,100,20,2048,95.89999999999999,93.08999999999999,92.14,89.68,1.0,15,3
2,5,100,30,128,93.17,93.31,90.60666666666667,89.68,0.5,15,3
4,3,100,30,2048,95.26,92.80000000000001,90.81,90.18,1.0,15,3
5,5,100,40,7000,93.28999999999999,92.34,91.08000000000001,91.08000000000001,1.0,1,1
2,5,100,40,5000,95.72,95.34,90.39,89.68,1.0,15,3
2,5,100,20,128,94.16,90.85,91.7,89.68,1.0,15,3
2,5,100,30,2048,95.53,95.0,90.97,89.68,1.0,15,3
6,5,100,100,7000,97.18,92.24,90.42,90.42,1.0,1,1
3,5,100,10,2048,94.26666666666667,91.4,90.53,89.9,0.5,15,3
5,5,100,50,5000,93.52000000000001,92.77,91.09,91.08000000000001,1.0,15,3
3,5,100,20,1024,94.40333333333332,91.75666666666666,90.59333333333333,89.9,0.5,15,3
4,5,100,30,1024,93.93,91.43,90.36666666666666,90.18,0.5,15,3
2,10,100,30,1024,94.58333333333333,93.90333333333334,90.63666666666667,89.68,0.5,15,3
3,5,100,40,7000,95.28999999999999,93.75,89.91,89.9,1.0,1,1
9,5,100,20,7000,92.89,90.60000000000001,89.94,89.91,1.0,1,1
7,2,100,100,32,93.57,89.72,89.72,89.72,1.0,15,3
9,5,100,50,5000,92.81,92.5,89.91,89.91,1.0,15,3
2,2,100,20,2048,94.85,93.21333333333334,91.13,89.68,0.5,15,3
7,5,100,20,16,93.47999999999999,91.59,89.72,89.72,1.0,15,3
4,5,100,10,1024,93.80666666666667,91.32333333333334,90.76333333333334,90.18,0.5,15,3
3,5,100,20,5000,95.57,92.99,90.98,89.9,1.0,15,3
4,3,100,20,2048,95.21,92.77,91.23,90.18,1.0,15,3
2,5,100,60,5000,95.48,94.62,89.81,89.68,1.0,15,3
6,5,100,80,5000,96.95,93.8,90.42,90.42,1.0,15,3
7,8,100,20,2048,96.76,94.76,90.58,89.72,1.0,15,3
7,5,100,20,7000,96.85000000000001,94.26,90.98,89.72,1.0,1,1
2,5,100,30,7000,95.38,94.48,91.27,89.68,1.0,1,1
0,5,100,20,7000,97.87,91.51,90.25999999999999,90.2,1.0,1,1
7,2,100,10,2048,97.13000000000001,93.88,90.94,89.72,1.0,15,3
3,5,100,20,512,94.39999999999999,91.84333333333333,90.52666666666667,89.9,0.5,15,3
4,1,100,20,5000,94.81,91.17,91.05,90.18,1.0,1,1
4,10,100,20,2048,95.45,93.19,91.29,90.18,1.0,15,3
2,5,100,10,1024,94.53333333333333,92.99333333333333,91.19333333333334,89.68,0.5,15,3
5,5,100,80,5000,93.75,92.65,91.08000000000001,91.08000000000001,1.0,15,3
2,1,100,20,5000,95.53,91.97,92.07,89.68,1.0,1,1
2,5,100,60,5000,95.53,94.3,89.74,89.68,1.0,15,3
4,5,100,30,128,93.49,90.85,90.60000000000001,90.18,1.0,15,3
7,5,100,60,5000,96.95,94.35,89.72,89.72,1.0,15,3
2,8,100,20,2048,95.06333333333333,94.16,91.19,89.68,0.5,15,3
9,10,100,30,256,90.81,90.77,89.91,89.91,0.5,15,3
3,5,100,30,256,94.56,91.77,90.53999999999999,89.9,1.0,15,3
9,5,100,50,5000,92.64,92.81,89.92999999999999,89.91,1.0,15,3
3,5,100,50,7000,95.17,93.96,89.9,89.9,1.0,1,1
3,5,100,20,256,93.45666666666666,91.78666666666668,90.55333333333333,89.9,0.5,15,3
4,5,100,10,32,93.21000000000001,91.24,90.18,90.18,1.0,15,3
3,5,100,40,7000,94.97,93.4,89.91,89.9,1.0,1,1
7,5,100,20,32,94.02000000000001,90.45,89.81,89.72,1.0,15,3
4,5,100,30,16,91.60000000000001,90.19,90.33,90.18,1.0,15,3
7,5,100,40,1024,96.78999999999999,94.73,89.96,89.72,1.0,15,3
2,10,100,30,1024,95.33,94.72,91.0,89.68,1.0,15,3
8,2,100,80,4096,92.59666666666668,90.38000000000001,90.25999999999999,90.25999999999999,0.75,15,3
6,1,100,30,1024,96.21666666666667,90.64999999999999,90.42,90.42,0.75,15,3
8,1,100,20,5000,92.97,90.25999999999999,90.25999999999999,90.25999999999999,1.0,1,1
3,3,100,80,1024,94.90333333333332,92.41333333333334,89.9,89.9,0.75,15,3
2,5,100,20,1024,94.91333333333334,93.83333333333333,91.10000000000001,89.68,0.5,15,3
4,2,100,30,2048,94.27666666666667,91.3,90.39333333333333,90.18,0.5,15,3
8,3,100,80,256,91.96,90.44,90.25999999999999,90.25999999999999,0.75,15,3
3,2,100,20,2048,95.48,91.9,91.06,89.9,1.0,15,3
3,5,100,20,512,94.39,91.80333333333334,90.56666666666666,89.9,0.5,15,3
5,5,100,30,5000,93.66,92.55,91.08000000000001,91.08000000000001,1.0,15,3
7,5,100,40,5000,96.89999999999999,95.15,89.96,89.72,1.0,15,3
7,1,100,30,2048,96.39666666666666,90.55,90.05666666666666,89.72,0.5,15,3
2,5,100,10,5000,96.03,94.65,91.93,89.68,1.0,15,3
8,3,100,50,16,90.26666666666667,90.25999999999999,90.25999999999999,90.25999999999999,0.25,15,3
8,5,100,20,5000,93.38,91.02,90.25999999999999,90.25999999999999,1.0,15,3
3,5,100,20,16,90.48666666666666,89.96333333333332,90.03666666666666,89.9,0.5,15,3
2,5,100,20,32,92.63,91.47,90.13,89.68,1.0,15,3
4,5,100,10,512,95.02000000000001,91.47999999999999,91.0,90.18,1.0,15,3
2,5,100,20,7000,95.86,94.37,92.19000000000001,89.68,1.0,1,1
7,1,100,40,1024,96.87,90.48,90.09,89.72,1.0,15,3
3,5,100,40,5000,95.09,93.74,90.10000000000001,89.9,1.0,15,3
8,5,100,10,7000,92.99,90.25999999999999,90.25999999999999,90.25999999999999,1.0,1,1
3,5,100,30,64,92.21333333333334,90.81,90.08,89.9,0.5,15,3
7,2,100,10,2048,96.21333333333332,92.60666666666667,90.8,89.72,0.5,15,3
7,2,100,10,4096,97.05,93.82000000000001,91.2,89.72,1.0,15,3
7,5,100,10,7000,96.72,93.30000000000001,91.14,89.72,1.0,1,1
7,5,100,10,512,95.78666666666666,92.23333333333333,90.54333333333334,89.72,0.5,15,3
7,3,100,10,2048,95.84,92.34333333333333,90.47666666666667,89.72,0.5,15,3
3,5,100,100,7000,94.62,92.86,89.9,89.9,1.0,1,1
7,5,100,60,5000,97.08,95.05,89.72,89.72,1.0,15,3
5,5,100,50,5000,93.67999999999999,92.99,91.08000000000001,91.08000000000001,1.0,15,3
2,3,100,10,1024,94.86,93.03,90.97666666666666,89.68,0.5,15,3
4,10,100,20,4096,95.45,93.24,91.03999999999999,90.18,1.0,15,3
4,10,100,20,4096,95.56,93.16,91.2,90.18,1.0,15,3
6,5,100,60,5000,96.88,95.04,90.42,90.42,1.0,15,3
1,1,100,100,5000,97.97,88.64999999999999,88.64999999999999,88.64999999999999,1.0,1,1
4,10,100,20,1024,95.06,92.54,90.88000000000001,90.18,1.0,15,3
3,5,100,30,128,94.23,90.35,90.29,89.9,1.0,15,3
3,3,100,20,4096,94.94,92.55,90.77666666666667,89.9,0.75,15,3
3,10,100,40,4096,95.49,94.08,90.16999999999999,89.9,1.0,15,3
2,1,100,30,5000,95.57,91.38,91.06,89.68,1.0,10,3
3,2,100,20,2048,95.61,92.22,91.01,89.9,1.0,15,3
7,5,100,10,256,96.54,91.7,91.10000000000001,89.72,1.0,15,3
7,5,100,30,7000,96.81,94.34,90.06,89.72,1.0,1,1
4,1,100,20,2048,94.06666666666666,90.64,90.59333333333333,90.18,0.5,15,3
6,5,100,100,5000,96.61,92.78,90.42,90.42,1.0,1,1
2,5,100,10,1024,95.58,93.55,92.24,89.68,1.0,15,3
3,2,100,30,2048,94.70333333333333,91.56333333333333,90.31,89.9,0.5,15,3
2,2,100,10,1024,95.71,93.71000000000001,92.29,89.68,1.0,15,3
0,5,100,40,7000,97.67,92.83,90.22,90.2,1.0,1,1
3,3,100,10,2048,94.45333333333333,91.16333333333333,90.54333333333334,89.9,0.5,15,3
3,5,100,40,5000,95.15,93.81,90.16999999999999,89.9,1.0,15,3
0,1,100,80,5000,97.89,90.21000000000001,90.2,90.2,1.0,1,1
2,3,100,30,2048,95.64,94.74000000000001,90.74,89.68,1.0,15,3
5,5,100,60,5000,93.97999999999999,93.31,91.08000000000001,91.08000000000001,1.0,15,3
3,5,100,40,7000,94.88,93.77,89.9,89.9,1.0,1,1
6,1,100,50,5000,96.43,90.42,90.42,90.42,1.0,1,1
3,5,100,20,128,94.41000000000001,90.12,90.36,89.9,1.0,15,3
1,1,100,20,5000,97.8,88.64999999999999,89.12,88.64999999999999,1.0,1,1
4,2,100,30,4096,94.42333333333333,91.49666666666667,90.27,90.18,0.5,15,3
7,3,100,20,2048,96.43333333333334,93.44666666666667,90.56,89.72,0.5,15,3
5,10,100,30,256,92.42,91.38,91.09,91.08000000000001,1.0,15,3
8,5,100,100,7000,92.58,90.41,90.25999999999999,90.25999999999999,1.0,1,1
4,5,100,10,64,92.71333333333334,90.66666666666666,90.56333333333333,90.18,0.5,15,3
4,5,100,10,16,90.58666666666667,90.18,90.34,90.18,0.5,15,3
3,5,100,40,7000,94.97,93.47,89.9,89.9,1.0,1,1
4,8,100,30,2048,94.96,92.83,90.41,90.18,1.0,15,3
2,5,100,20,256,94.09333333333333,93.02333333333334,90.85666666666667,89.68,0.5,15,3
2,5,100,10,4096,95.96000000000001,93.78999999999999,92.02,89.68,1.0,15,3
7,5,100,40,1024,96.86,95.03,90.01,89.72,1.0,15,3
9,5,100,50,7000,92.88,92.39,89.91,89.91,1.0,1,1
7,8,100,30,2048,96.92,94.84,90.16,89.72,1.0,15,3
1,5,100,50,5000,98.13,94.41000000000001,88.64999999999999,88.64999999999999,1.0,15,3
7,5,100,20,32,94.01,90.28,90.19,89.72,1.0,15,3
8,5,100,40,5000,92.99,90.94,90.25999999999999,90.25999999999999,1.0,15,3
0,5,100,10,7000,98.0,91.36999999999999,90.2,90.2,1.0,1,1
4,3,100,20,1024,94.10333333333334,91.74666666666667,90.65333333333334,90.18,0.5,15,3
4,1,100,20,512,93.75666666666666,90.84,90.66333333333333,90.18,0.5,15,3
4,8,100,10,4096,95.45,92.47,91.45,90.18,1.0,15,3
3,5,100,50,7000,94.82000000000001,93.7,89.9,89.9,1.0,1,1
7,5,100,20,16,93.67,90.95,90.08,89.72,1.0,15,3
2,1,100,60,5000,95.35,90.0,89.69,89.68,1.0,1,1
5,5,100,60,7000,93.60000000000001,92.65,91.08000000000001,91.08000000000001,1.0,1,1
2,5,100,100,7000,95.43,91.25999999999999,89.68,89.68,1.0,1,1
0,5,100,40,512,95.98,91.27333333333333,90.20333333333333,90.2,0.25,15,3
6,5,100,100,5000,97.05,93.42,90.42,90.42,1.0,15,3
1,5,100,10,5000,98.0,88.98,89.52,88.64999999999999,1.0,15,3
3,5,100,50,7000,94.83,93.52000000000001,89.91,89.9,1.0,1,1
5,5,100,20,7000,93.97999999999999,91.64,91.08000000000001,91.08000000000001,1.0,1,1
2,2,100,30,1024,94.71000000000001,92.59666666666668,90.62,89.68,0.5,15,3
3,1,100,30,2048,95.39,90.28,90.57,89.9,1.0,15,3
5,5,100,10,5000,94.3,91.25999999999999,91.09,91.08000000000001,1.0,15,3
5,1,100,50,5000,93.31,91.09,91.08000000000001,91.08000000000001,1.0,1,1
0,5,100,10,5000,98.16,91.5,90.2,90.2,1.0,15,3
8,3,100,80,5000,93.28999999999999,90.71000000000001,90.25999999999999,90.25999999999999,1.0,15,3
4,2,100,30,4096,95.17999999999999,92.33,90.71000000000001,90.18,1.0,15,3
4,1,100,20,4096,94.23333333333333,90.67333333333333,90.56666666666666,90.18,0.5,15,3
2,5,100,10,512,94.56333333333333,92.79333333333334,91.04666666666667,89.68,0.5,15,3
3,5,100,40,5000,95.58,94.0,90.27,89.9,1.0,15,3
4,3,100,20,2048,95.35,92.91,91.16,90.18,1.0,15,3
2,5,100,30,1024,95.30999999999999,94.36,91.03,89.68,1.0,15,3
2,5,100,30,1024,95.02000000000001,94.48,91.13,89.68,1.0,15,3
2,5,100,10,1024,95.52000000000001,93.76,91.93,89.68,1.0,15,3
1,5,100,80,5000,97.92999999999999,94.33,88.64999999999999,88.64999999999999,1.0,1,1
6,5,100,40,7000,96.63000000000001,95.00999999999999,90.42,90.42,1.0,1,1
4,5,100,60,7000,94.77,91.57,90.18,90.18,1.0,1,1
4,2,100,40,1000,94.03,91.17,90.18,90.18,1.0,1,1
6,2,100,20,1000,96.1,92.16,90.44,90.42,1.0,1,1
3,8,100,20,2048,94.81666666666668,92.38,90.56666666666666,89.9,0.5,15,3
8,1,100,40,512,92.64,90.25999999999999,90.25999999999999,90.25999999999999,1.0,15,3
9,5,100,40,7000,92.49000000000001,91.88,89.91,89.91,1.0,1,1
4,3,100,30,4096,95.12,92.73,90.61,90.18,1.0,15,3
2,2,100,30,5958,95.75,94.24,91.06,89.68,1.0,3,3
2,5,100,100,16,89.87333333333333,89.88000000000001,89.68,89.68,0.5,15,3
2,5,100,20,2048,95.06666666666666,93.44333333333333,91.25333333333333,89.68,0.5,15,3
3,3,100,10,1024,95.56,91.84,90.86999999999999,89.9,1.0,15,3
2,1,100,10,1024,95.85000000000001,92.75,91.97,89.68,1.0,15,3
7,5,100,40,1024,96.75,95.07,89.97,89.72,1.0,15,3
3,3,100,10,4096,95.55,92.25,91.02,89.9,1.0,15,3
2,5,100,10,64,93.71000000000001,90.83,91.19,89.68,1.0,15,3
3,5,100,30,32,92.58,89.97,89.91,89.9,1.0,15,3
4,5,100,10,4096,95.43,92.83,91.4,90.18,1.0,15,3
8,5,100,30,5000,93.13,91.10000000000001,90.25999999999999,90.25999999999999,1.0,15,3
6,5,100,60,5000,96.95,94.62,90.42,90.42,1.0,1,1
3,5,100,30,512,95.0,93.28999999999999,90.53999999999999,89.9,1.0,15,3
4,1,100,20,1024,94.02666666666667,90.92666666666666,90.57666666666667,90.18,0.5,15,3
2,5,100,80,5000,95.50999999999999,93.36,89.68,89.68,1.0,15,3
2,5,100,40,1024,95.37,94.53,90.36,89.68,1.0,15,3
4,10,100,20,32,91.23,90.73333333333333,90.55,90.18,0.5,15,3
7,5,100,30,128,95.77,91.53,90.02,89.72,1.0,15,3
3,5,100,20,128,94.38,90.02,90.25,89.9,1.0,15,3
4,1,100,10,4096,93.73333333333333,90.41333333333334,90.75999999999999,90.18,0.5,15,3
3,5,100,20,128,92.97666666666666,91.25333333333333,90.52333333333334,89.9,0.5,15,3
2,5,100,10,16,92.62,91.72,90.22,89.68,1.0,15,3
9,5,100,100,7000,92.46,91.61,89.91,89.91,1.0,1,1
7,5,100,20,128,95.87,92.34,90.34,89.72,1.0,15,3
4,1,100,30,2048,94.42666666666668,90.28,90.29,90.18,0.5,15,3
7,5,100,20,1024,96.25333333333333,93.49666666666666,90.45666666666666,89.72,0.5,15,3
2,5,100,20,1024,95.07666666666667,93.68666666666667,91.25333333333333,89.68,0.5,15,3
6,5,100,10,7000,96.93,92.25,90.42,90.42,1.0,1,1
3,5,100,10,16,92.61,90.58,90.4,89.9,1.0,15,3
3,5,100,20,5000,95.88,93.57,91.14999999999999,89.9,1.0,15,3
7,3,100,30,1024,96.03,93.36333333333333,90.04333333333334,89.72,0.5,15,3
6,5,100,80,5000,97.07000000000001,94.28999999999999,90.42,90.42,1.0,15,3
3,1,100,10,512,93.19,89.92333333333333,90.05,89.9,0.25,15,3
8,5,100,80,7000,92.97,90.58,90.25999999999999,90.25999999999999,1.0,1,1
6,1,100,40,5000,96.59,90.55,90.42,90.42,1.0,1,1
7,5,100,20,64,94.04333333333334,92.06333333333333,90.31,89.72,0.5,15,3
9,5,100,40,1024,92.54,91.96,89.92999999999999,89.91,1.0,15,3
2,5,100,50,5000,95.37,94.87,90.11,89.68,1.0,15,3
3,5,100,20,1024,94.70333333333333,91.87666666666667,90.63,89.9,0.5,15,3
7,5,100,30,256,95.28666666666666,93.30000000000001,90.11,89.72,0.5,15,3
5,3,100,100,256,92.07,91.47333333333333,91.08000000000001,91.08000000000001,0.75,15,3
8,5,100,20,7000,93.05,90.75,90.25999999999999,90.25999999999999,1.0,1,1
4,5,100,20,1024,94.98,92.7,91.22,90.18,1.0,15,3
4,5,100,10,32,93.04,90.8,90.41,90.18,1.0,15,3
7,10,100,20,2048,97.06,94.66,90.72,89.72,1.0,15,3
3,10,100,30,2048,95.5,93.93,90.58,89.9,1.0,15,3
4,5,100,20,16,92.46,91.4,90.28,90.18,1.0,15,3
2,2,100,10,1024,94.89666666666666,92.51666666666667,91.2,89.68,0.5,15,3
7,5,100,60,5000,96.96000000000001,95.15,89.72,89.72,1.0,15,3
3,5,100,10,1024,94.33666666666667,91.06,90.40666666666667,89.9,0.5,15,3
8,5,100,50,7000,92.82000000000001,91.27,90.25999999999999,90.25999999999999,1.0,1,1
3,5,100,30,16,91.47,89.91,89.84,89.9,1.0,15,3
2,5,100,20,128,94.28,91.60000000000001,91.02,89.68,1.0,15,3
4,5,100,80,7000,94.83,91.08000000000001,90.18,90.18,1.0,1,1
4,1,100,30,2048,95.19999999999999,90.75,90.73,90.18,1.0,15,3
2,3,100,20,2048,95.85000000000001,94.8,92.08,89.68,1.0,15,3
4,2,100,10,2048,95.36,92.41,91.64,90.18,1.0,15,3
2,5,100,10,32,93.17999999999999,90.99000000000001,89.72,89.68,1.0,15,3
9,1,100,40,5000,92.76666666666667,89.92,89.92666666666666,89.91,0.75,15,3
2,2,100,30,4096,95.69,94.59,91.13,89.68,1.0,15,3
3,1,100,30,4096,94.75,90.21333333333334,90.33666666666666,89.9,0.5,15,3
5,5,100,20,5000,94.38,91.97,91.09,91.08000000000001,1.0,15,3
8,5,100,80,64,90.62,90.31,90.25999999999999,90.25999999999999,0.75,15,3
3,5,100,60,5000,95.49,93.87,89.9,89.9,1.0,15,3
7,1,100,30,2048,96.85000000000001,91.14,90.36,89.72,1.0,15,3
0,1,100,50,5000,97.72,90.2,90.2,90.2,1.0,1,1
2,5,100,20,5000,95.97,95.03,91.89,89.68,1.0,15,3
7,5,100,30,128,93.97,92.84,89.95333333333333,89.72,0.5,15,3
4,5,100,100,7000,95.07,90.69,90.18,90.18,1.0,1,1
3,5,100,10,32,93.31,89.92999999999999,89.9,89.9,1.0,15,3
4,8,100,10,2048,93.88333333333333,91.39333333333333,90.77666666666667,90.18,0.5,15,3
2,5,100,20,5000,95.92,94.99,92.25999999999999,89.68,1.0,15,3
8,5,100,100,7000,92.86,90.75,90.25999999999999,90.25999999999999,1.0,1,1
2,5,100,40,5000,95.39,94.31,90.16999999999999,89.68,1.0,1,1
4,5,100,20,32,91.66666666666666,90.78333333333333,90.71666666666667,90.18,0.5,15,3
5,5,100,40,7000,93.33,92.43,91.08000000000001,91.08000000000001,1.0,1,1
7,1,100,30,2048,96.78,91.06,90.13,89.72,1.0,15,3
3,1,100,20,4096,95.71,91.12,91.14,89.9,1.0,15,3
4,5,100,30,7000,94.82000000000001,92.60000000000001,90.45,90.18,1.0,1,1
3,5,100,30,5000,95.33,93.31,90.44,89.9,1.0,15,3
0,5,100,10,5000,98.14,91.92,90.2,90.2,1.0,15,3
8,5,100,10,7000,93.25,90.4,90.25999999999999,90.25999999999999,1.0,1,1
4,5,100,20,2048,95.00999999999999,93.02,91.13,90.18,1.0,15,3
4,8,100,10,2048,95.66,92.36999999999999,91.25999999999999,90.18,1.0,15,3
7,5,100,20,5000,97.25,94.76,90.83,89.72,1.0,15,3
7,5,100,20,64,95.15,91.71000000000001,90.34,89.72,1.0,15,3
7,5,100,10,16,94.48,91.03999999999999,90.39,89.72,1.0,15,3
3,5,100,20,128,94.12,90.09,90.21000000000001,89.9,1.0,15,3
7,5,100,10,16,91.73333333333333,89.98666666666666,89.99000000000001,89.72,0.5,15,3
2,5,100,100,7000,95.66,91.67,89.68,89.68,1.0,1,1
2,2,100,30,4096,95.01333333333334,93.34,90.64999999999999,89.68,0.5,15,3
5,5,100,10,7000,93.47,91.18,91.08000000000001,91.08000000000001,1.0,1,1
2,3,100,20,2048,95.36,94.38,91.86,89.68,1.0,15,3
3,5,100,20,32,92.93,90.10000000000001,89.92999999999999,89.9,1.0,15,3
2,1,100,10,4096,95.91,92.67999999999999,92.0,89.68,1.0,15,3
2,5,100,20,2048,95.83,94.73,92.16,89.68,1.0,15,3
3,8,100,10,2048,94.21000000000001,91.32666666666667,90.40333333333334,89.9,0.5,15,3
7,3,100,30,2048,96.39333333333333,93.82333333333334,90.16,89.72,0.5,15,3
4,5,100,10,128,94.67999999999999,90.4,90.86,90.18,1.0,15,3
6,5,100,40,5000,96.92,95.33,90.42,90.42,1.0,15,3
6,5,100,20,7000,96.88,93.82000000000001,90.42,90.42,1.0,1,1
2,1,100,30,1024,95.28,91.17,91.31,89.68,1.0,15,3
3,3,100,20,4096,95.78,92.88,90.89,89.9,1.0,15,3
3,5,100,80,5000,95.25,93.13,89.9,89.9,1.0,15,3
4,5,100,30,5000,95.36,93.25,90.7,90.18,1.0,15,3
2,1,100,30,1024,94.86666666666666,91.25666666666666,90.71333333333334,89.68,0.5,15,3
7,5,100,20,7000,96.95,94.59,91.17,89.72,1.0,1,1
3,1,100,10,4096,94.21000000000001,90.28666666666668,90.55333333333333,89.9,0.5,15,3
3,8,100,10,1024,95.07,91.94,90.74,89.9,1.0,15,3
0,5,100,40,5000,97.94,93.4,90.31,90.2,1.0,15,3
2,2,100,30,4096,95.56,94.67999999999999,90.91,89.68,1.0,15,3
3,3,100,30,2048,94.72666666666667,92.17999999999999,90.40333333333334,89.9,0.5,15,3
7,5,100,40,1024,96.76,94.94,90.01,89.72,1.0,15,3
2,10,100,20,1024,95.64,94.38,92.16,89.68,1.0,15,3
3,5,100,100,7000,94.95,92.67,89.9,89.9,1.0,1,1
5,3,100,20,64,92.23,91.08000000000001,91.08000000000001,91.08000000000001,1.0,15,3
3,2,100,10,2048,95.46,91.62,90.86999999999999,89.9,1.0,15,3
2,5,100,30,32,90.92333333333333,91.68666666666667,90.10000000000001,89.68,0.5,15,3
9,5,100,20,7000,92.9,90.82000000000001,90.03,89.91,1.0,1,1
3,8,100,30,4096,95.27,93.67999999999999,90.36,89.9,1.0,15,3
0,5,100,10,5000,98.03,91.93,90.2,90.2,1.0,15,3
7,5,100,40,1024,96.87,95.17999999999999,89.92999999999999,89.72,1.0,15,3
2,3,100,30,4096,95.16,93.86333333333333,90.57,89.68,0.5,15,3
3,2,100,30,2048,94.71000000000001,90.81666666666666,90.2,89.9,0.5,15,3
1,5,100,30,7000,97.89999999999999,90.60000000000001,88.64999999999999,88.64999999999999,1.0,1,1
3,5,100,50,5000,95.19,93.58,90.0,89.9,1.0,15,3
0,5,100,20,5000,98.05,92.35,90.24,90.2,1.0,15,3
2,5,100,30,1024,95.3,94.5,91.12,89.68,1.0,15,3
3,5,100,80,7000,95.05,93.07,89.9,89.9,1.0,1,1
2,1,100,30,5000,95.43,91.19,90.95,89.68,1.0,1,1
5,5,100,10,7000,93.49,91.25,91.11,91.08000000000001,1.0,1,1
2,5,100,10,512,95.67999999999999,93.39,91.83,89.68,1.0,15,3
1,10,100,10,1024,97.82333333333332,88.79,89.36,88.64999999999999,0.75,15,3
2,3,100,20,4096,95.76,94.66,91.83,89.68,1.0,15,3
6,5,100,20,5000,96.59,93.55,90.42,90.42,1.0,1,1
1,5,100,60,7000,97.92999999999999,93.54,88.64999999999999,88.64999999999999,1.0,1,1
7,8,100,20,2048,96.31,93.81333333333333,90.35666666666667,89.72,0.5,15,3
7,1,100,60,16,90.85333333333332,89.73333333333333,89.72,89.72,0.5,15,3
1,5,100,50,5000,98.02,93.33,88.64999999999999,88.64999999999999,1.0,15,3
2,5,100,100,5000,95.77,92.32000000000001,89.68,89.68,1.0,15,3
3,5,100,50,7000,95.02000000000001,93.95,89.9,89.9,1.0,1,1
8,5,100,20,7000,93.19,90.88000000000001,90.25999999999999,90.25999999999999,1.0,1,1
6,5,100,50,5000,97.11999999999999,94.98,90.42,90.42,1.0,15,3
5,5,100,80,7000,93.08999999999999,92.15,91.08000000000001,91.08000000000001,1.0,1,1
3,5,100,10,32,91.99666666666667,89.90666666666667,90.14999999999999,89.9,0.5,15,3
0,5,100,10,5000,97.56,91.17,90.2,90.2,1.0,1,1
2,5,100,20,5000,95.41,94.13,91.60000000000001,89.68,1.0,1,1
2,5,100,10,64,93.85,92.11,90.86,89.68,1.0,15,3
1,5,100,50,5000,97.96000000000001,93.53,88.64999999999999,88.64999999999999,1.0,15,3
5,5,100,60,7000,93.22,92.7,91.08000000000001,91.08000000000001,1.0,1,1
9,5,100,50,7000,92.44,92.07,89.91,89.91,1.0,1,1
3,10,100,50,256,92.42333333333333,92.12,89.9,89.9,0.25,15,3
1,5,100,40,1024,98.14,92.01,88.64999999999999,88.64999999999999,1.0,15,3
1,5,100,30,1024,97.91,90.48,88.69,88.64999999999999,1.0,15,3
5,5,100,40,1024,93.28,92.19000000000001,91.08000000000001,91.08000000000001,1.0,15,3
6,5,100,10,5000,96.39999999999999,91.92,90.46,90.42,1.0,1,1
4,2,100,30,2048,95.03,92.56,90.69,90.18,1.0,15,3
4,2,100,20,2048,95.33,92.96,91.21000000000001,90.18,1.0,15,3
2,5,100,60,7000,95.02000000000001,94.19999999999999,89.68,89.68,1.0,1,1
2,2,100,30,4096,95.08,92.88666666666666,90.59,89.68,0.5,15,3
7,5,100,50,7000,96.74000000000001,94.8,89.73,89.72,1.0,1,1
7,5,100,10,512,96.85000000000001,93.01,91.33,89.72,1.0,15,3
3,1,100,30,1024,94.54333333333334,90.19666666666667,90.19,89.9,0.5,15,3
4,1,100,10,256,94.34,91.2,91.04333333333334,90.18,0.75,15,3
7,1,100,30,4096,97.11,90.78,90.25,89.72,1.0,15,3
2,5,100,20,32,90.61333333333333,91.0,90.40333333333334,89.68,0.5,15,3
4,5,100,50,7000,94.64,92.01,90.18,90.18,1.0,1,1
4,10,100,10,4096,95.54,92.86999999999999,91.4,90.18,1.0,15,3
2,5,100,10,128,94.22,90.98,91.19,89.68,1.0,15,3
8,5,100,60,5000,92.81,91.05,90.25999999999999,90.25999999999999,1.0,1,1
1,5,100,30,5000,98.08,91.53,88.64999999999999,88.64999999999999,1.0,15,3
3,5,100,50,7000,95.16,93.88,89.9,89.9,1.0,1,1
4,5,100,40,5000,94.71000000000001,92.44,90.19,90.18,1.0,1,1
2,5,100,30,5000,95.74000000000001,95.25,91.29,89.68,1.0,15,3
2,2,100,30,5000,95.25,93.00333333333334,90.58,89.68,0.5,10,3
9,5,100,10,5000,93.06,90.55,89.97,89.91,1.0,15,3
4,1,100,30,2048,94.19333333333333,90.29333333333334,90.32333333333334,90.18,0.5,15,3
5,5,100,60,7000,93.30000000000001,92.36,91.08000000000001,91.08000000000001,1.0,1,1
1,5,100,100,5000,98.28,92.42,88.64999999999999,88.64999999999999,1.0,15,3
2,8,100,10,1024,94.66333333333333,92.90666666666667,91.06,89.68,0.5,15,3
3,1,100,20,2048,95.15,90.67,91.22,89.9,1.0,15,3
3,3,100,30,4096,95.33,93.01,90.52,89.9,1.0,15,3
6,5,100,30,7000,97.0,94.3,90.42,90.42,1.0,1,1
6,5,100,30,7000,96.72,94.26,90.42,90.42,1.0,1,1
2,5,100,20,512,95.06,93.7,91.81,89.68,1.0,15,3
3,5,100,40,7000,94.85,93.61,90.05,89.9,1.0,1,1
7,2,100,20,4096,96.39,92.69333333333334,90.45666666666666,89.72,0.5,15,3
4,1,100,10,2048,93.73333333333333,90.74333333333333,90.77,90.18,0.5,15,3
2,2,100,10,4096,95.96000000000001,93.94,92.0,89.68,1.0,15,3
5,5,100,40,7000,93.46,92.32000000000001,91.08000000000001,91.08000000000001,1.0,1,1
6,5,100,100,7000,97.02,92.52,90.42,90.42,1.0,1,1
3,3,100,20,1024,94.33333333333334,91.61333333333333,90.64333333333333,89.9,0.5,15,3
6,5,100,50,5000,96.36,94.41000000000001,90.42,90.42,1.0,1,1
3,5,100,30,16,90.02,89.91,89.91,89.9,0.5,15,3
4,1,100,30,2048,94.25,90.38666666666667,90.40333333333334,90.18,0.5,15,3
2,5,100,20,64,91.96666666666667,91.44666666666666,90.66,89.68,0.5,15,3
7,5,100,40,5000,97.0,95.28,90.0,89.72,1.0,15,3
7,1,100,30,4096,96.89,91.11,90.28,89.72,1.0,15,3
2,5,100,100,7000,95.63000000000001,91.47999999999999,89.68,89.68,1.0,1,1
9,5,100,100,7000,92.93,91.36,89.91,89.91,1.0,1,1
2,5,100,10,7000,95.65,93.78999999999999,92.27,89.68,1.0,1,1
2,1,100,80,5000,95.37,89.74,89.68,89.68,1.0,1,1
7,5,100,10,256,95.60666666666665,92.01,90.62666666666667,89.72,0.5,15,3
4,8,100,10,1024,95.39999999999999,92.36,91.49000000000001,90.18,1.0,15,3
7,5,100,80,7000,96.89,93.84,89.72,89.72,1.0,1,1
7,5,100,30,1024,96.12,93.92333333333333,90.07,89.72,0.5,15,3
7,5,100,30,256,96.19,92.97999999999999,90.53,89.72,1.0,15,3
5,1,100,40,512,92.32000000000001,91.08000000000001,91.08000000000001,91.08000000000001,0.5,15,3
5,1,100,20,5000,93.37,91.08000000000001,91.08000000000001,91.08000000000001,1.0,1,1
0,3,100,20,128,96.83,90.5,90.21666666666667,90.2,0.75,15,3
7,5,100,40,5000,97.09,95.37,89.98,89.72,1.0,15,3
3,5,100,40,7000,95.03,93.81,89.95,89.9,1.0,1,1
6,2,100,60,64,93.99333333333333,91.10333333333334,90.42,90.42,0.75,15,3
1,5,100,10,5000,98.11999999999999,89.37,89.4,88.64999999999999,1.0,15,3
3,3,100,30,2048,95.24000000000001,93.44,90.42,89.9,1.0,15,3
3,5,100,40,7000,95.14,93.95,89.95,89.9,1.0,1,1
3,5,100,20,512,95.19999999999999,92.29,90.79,89.9,1.0,15,3
4,1,100,20,2048,94.26333333333334,90.77666666666667,90.59666666666666,90.18,0.5,15,3
5,1,100,30,5000,93.37,91.08000000000001,91.08000000000001,91.08000000000001,1.0,1,1
4,2,100,10,1024,93.88333333333333,91.44333333333333,90.84,90.18,0.5,15,3
3,5,100,20,16,90.32666666666667,89.9,89.97,89.9,0.5,15,3
1,5,100,20,5000,98.26,89.56,89.44,88.64999999999999,1.0,15,3
0,5,100,60,7000,97.71,92.53,90.2,90.2,1.0,1,1
7,5,100,60,5000,96.97,94.83,89.72,89.72,1.0,15,3
2,1,100,30,1024,94.96333333333334,91.18,90.58666666666667,89.68,0.5,15,3
3,2,100,20,2048,95.17999999999999,92.16,90.96,89.9,1.0,15,3
2,5,100,10,512,94.51666666666667,92.55,91.03666666666666,89.68,0.5,15,3
8,5,100,30,5000,92.78,90.94,90.25999999999999,90.25999999999999,1.0,1,1
3,3,100,30,2048,94.74333333333334,92.34666666666666,90.25666666666666,89.9,0.5,15,3
2,2,100,30,5958,95.8,94.22,90.89,89.68,1.0,3,3
5,5,100,80,7000,93.15,92.22,91.08000000000001,91.08000000000001,1.0,1,1
3,5,100,30,128,93.04666666666667,91.41,90.21000000000001,89.9,0.5,15,3
2,2,100,20,2048,95.26,93.44333333333333,91.25,89.68,0.5,15,3
6,1,100,40,5000,96.69,90.42,90.42,90.42,1.0,1,1
9,2,100,40,16,89.94333333333333,89.91,89.91,89.91,0.5,15,3
6,5,100,40,16,91.97999999999999,90.42999999999999,90.42,90.42,1.0,15,3
3,5,100,50,7000,95.00999999999999,93.77,89.91,89.9,1.0,1,1
3,5,100,30,1024,95.09,93.24,90.53999999999999,89.9,1.0,15,3
3,8,100,10,4096,95.82000000000001,92.47999999999999,90.95,89.9,1.0,15,3
3,2,100,30,2048,94.78666666666666,91.66,90.17333333333333,89.9,0.5,15,3
4,5,100,80,5000,95.44,91.45,90.18,90.18,1.0,15,3
1,5,100,40,7000,97.92,92.16,88.64999999999999,88.64999999999999,1.0,1,1
3,8,100,20,2048,95.36,92.97999999999999,90.91,89.9,1.0,15,3
3,5,100,50,7000,95.04,93.58999999999999,89.9,89.9,1.0,1,1
0,1,100,40,5000,97.67,90.24,90.25,90.2,1.0,1,1
8,5,100,20,5000,93.47999999999999,90.73,90.25999999999999,90.25999999999999,1.0,15,3
7,5,100,20,128,95.81,91.60000000000001,90.49000000000001,89.72,1.0,15,3
2,5,100,40,7000,95.46,94.87,90.13,89.68,1.0,1,1
3,5,100,20,5000,95.67,93.11,91.06,89.9,1.0,15,3
7,5,100,40,5000,96.99,95.35,89.87,89.72,1.0,15,3
2,2,100,10,1024,94.64,92.44666666666667,91.02333333333334,89.68,0.5,15,3
1,5,100,80,7000,97.87,94.0,88.64999999999999,88.64999999999999,1.0,1,1
0,1,100,20,128,96.72,90.21333333333334,90.20333333333333,90.2,0.75,15,3
8,2,100,100,128,91.86,90.38000000000001,90.25999999999999,90.25999999999999,1.0,15,3
7,5,100,30,1024,96.25,93.60000000000001,90.12,89.72,0.5,15,3
7,5,100,30,16,93.08999999999999,89.85,89.72,89.72,1.0,15,3
1,5,100,20,5000,98.18,89.05999999999999,89.24,88.64999999999999,1.0,15,3
1,1,100,100,5000,97.96000000000001,88.64999999999999,88.64999999999999,88.64999999999999,1.0,1,1
4,5,100,20,128,92.71000000000001,91.07333333333332,90.92666666666666,90.18,0.5,15,3
3,3,100,80,256,93.56666666666666,91.66666666666666,89.9,89.9,0.5,15,3
2,3,100,30,2048,95.45,94.64,91.09,89.68,1.0,15,3
1,5,100,40,5000,98.17,93.17,88.64999999999999,88.64999999999999,1.0,15,3
0,3,100,80,256,97.15333333333334,90.62333333333333,90.20333333333333,90.2,0.75,15,3
2,5,100,30,512,94.92,94.07,90.97,89.68,1.0,15,3
2,2,100,20,2048,94.93,93.02333333333334,91.24,89.68,0.5,15,3
3,5,100,20,7000,95.28,92.66,91.24,89.9,1.0,1,1
5,5,100,40,7000,93.24,92.35,91.08000000000001,91.08000000000001,1.0,1,1
2,5,100,80,7000,95.25,92.78,89.69,89.68,1.0,1,1
3,5,100,40,7000,95.12,93.73,89.95,89.9,1.0,1,1
5,5,100,40,5000,93.76,92.81,91.08000000000001,91.08000000000001,1.0,15,3
5,5,100,30,7000,93.46,91.81,91.09,91.08000000000001,1.0,1,1
4,8,100,20,2048,94.14666666666666,91.72333333333333,90.62333333333333,90.18,0.5,15,3
9,5,100,80,7000,92.69,91.71000000000001,89.91,89.91,1.0,1,1
4,5,100,40,7000,94.89,92.25,90.18,90.18,1.0,1,1
2,1,100,20,4096,96.00999999999999,92.86999999999999,91.85,89.68,1.0,15,3
8,5,100,50,5000,93.38,91.05,90.25999999999999,90.25999999999999,1.0,15,3
9,10,100,60,256,90.42666666666666,90.40333333333334,89.91,89.91,0.25,15,3
4,5,100,20,1024,94.07,91.73666666666666,90.80333333333334,90.18,0.5,15,3
6,5,100,30,7000,96.78,94.13,90.42,90.42,1.0,1,1
2,5,100,40,1024,95.73,95.24000000000001,90.44,89.68,1.0,15,3
7,5,100,30,512,96.05,93.69333333333333,90.24666666666667,89.72,0.5,15,3
2,5,100,10,1024,95.67,93.7,92.0,89.68,1.0,15,3
9,1,100,50,128,90.90666666666667,89.91333333333333,89.91,89.91,0.75,15,3
0,5,100,10,7000,97.88,91.09,90.2,90.2,1.0,1,1
2,2,100,20,1024,95.89,94.3,92.03,89.68,1.0,15,3
2,10,100,10,4096,96.09,94.39999999999999,92.16,89.68,1.0,15,3
7,5,100,30,64,93.43333333333334,92.06666666666666,89.83333333333333,89.72,0.5,15,3
3,5,100,10,16,90.61,89.92999999999999,89.97,89.9,0.5,15,3
0,5,100,10,5000,97.71,90.94,90.2,90.2,1.0,1,1
8,5,100,40,7000,93.0,90.73,90.25999999999999,90.25999999999999,1.0,1,1
6,3,100,30,16,92.35,90.42,90.42,90.42,1.0,15,3
4,5,100,60,7000,95.17999999999999,91.45,90.18,90.18,1.0,1,1
7,5,100,100,7000,96.75,92.97999999999999,89.72,89.72,1.0,1,1
4,2,100,30,4096,94.27,91.56666666666666,90.35666666666667,90.18,0.5,15,3
0,5,100,10,5000,98.21,92.0,90.2,90.2,1.0,15,3
1,5,100,80,7000,98.17,93.64,88.64999999999999,88.64999999999999,1.0,1,1
6,5,100,50,5000,97.38,95.55,90.42,90.42,1.0,15,3
7,5,100,80,7000,96.67999999999999,93.91000000000001,89.72,89.72,1.0,1,1
5,5,100,30,7000,93.44,91.8,91.09,91.08000000000001,1.0,1,1
4,5,100,40,5000,95.3,92.9,90.2,90.18,1.0,15,3
7,5,100,20,512,96.83,93.31,90.95,89.72,1.0,15,3
2,1,100,10,2048,94.70666666666666,91.48333333333333,91.06333333333333,89.68,0.5,15,3
4,8,100,30,4096,95.44,92.86,90.56,90.18,1.0,15,3
2,3,100,30,1024,94.66,93.53,90.68666666666667,89.68,0.5,15,3
0,5,100,20,5000,98.07000000000001,91.69,90.27,90.2,1.0,15,3
8,2,100,100,64,90.29666666666667,90.27666666666667,90.25999999999999,90.25999999999999,0.25,15,3
7,5,100,10,5000,96.91,93.32000000000001,90.89,89.72,1.0,1,1
2,3,100,30,1024,94.67,93.59333333333333,90.47333333333333,89.68,0.5,15,3
6,5,100,40,5000,97.0,95.24000000000001,90.42,90.42,1.0,15,3
2,5,100,20,16,91.64999999999999,91.5,90.89,89.68,1.0,15,3
7,5,100,20,16,91.17666666666666,89.89666666666668,89.72,89.72,0.5,15,3
5,5,100,30,5000,93.77,92.4,91.08000000000001,91.08000000000001,1.0,15,3
7,2,100,20,1024,96.13333333333334,92.61666666666667,90.43666666666667,89.72,0.5,15,3
2,8,100,20,2048,95.5,94.62,91.67999999999999,89.68,1.0,15,3
4,5,100,10,32,91.09,90.18333333333334,90.36999999999999,90.18,0.5,15,3
0,5,100,60,5000,98.08,92.24,90.2,90.2,1.0,15,3
2,5,100,30,512,94.91000000000001,93.97999999999999,90.9,89.68,1.0,15,3
3,5,100,40,7000,95.0,93.78999999999999,89.94,89.9,1.0,1,1
9,5,100,40,5000,92.75,92.11,89.91,89.91,1.0,15,3
7,1,100,50,128,93.02,89.74333333333333,89.72333333333333,89.72,0.25,15,3
2,1,100,20,2048,95.54,92.94,91.91,89.68,1.0,15,3
4,5,100,10,256,94.82000000000001,91.17,91.05,90.18,1.0,15,3
4,2,100,10,4096,95.28999999999999,92.66,91.52,90.18,1.0,15,3
1,5,100,40,5000,98.35000000000001,92.12,88.64999999999999,88.64999999999999,1.0,15,3
2,3,100,20,1024,95.39999999999999,94.41000000000001,91.77,89.68,1.0,15,3
8,5,100,80,7000,92.97999999999999,90.67,90.25999999999999,90.25999999999999,1.0,1,1
0,5,100,100,5000,98.27,91.47999999999999,90.2,90.2,1.0,15,3
1,5,100,10,7000,97.88,88.75999999999999,89.53999999999999,88.64999999999999,1.0,1,1
4,5,100,80,5000,95.45,91.44,90.18,90.18,1.0,15,3
3,5,100,20,7000,95.19,92.51,91.06,89.9,1.0,1,1
6,5,100,10,5000,97.00999999999999,92.80000000000001,90.42,90.42,1.0,15,3
3,2,100,10,5000,94.80333333333333,91.42,90.82000000000001,89.9,0.75,15,3
2,10,100,30,1024,94.99,94.62,90.82000000000001,89.68,1.0,15,3
2,5,100,20,1024,94.64666666666666,93.52333333333334,91.00333333333333,89.68,0.5,15,3
7,5,100,60,7000,96.54,94.56,89.72,89.72,1.0,1,1
2,1,100,10,4096,95.15333333333334,92.29333333333334,91.58,89.68,0.75,15,3
2,5,100,50,7000,95.37,94.31,89.79,89.68,1.0,1,1
5,5,100,30,7000,93.63,91.79,91.11,91.08000000000001,1.0,1,1
6,5,100,30,2048,97.11,94.51,90.42,90.42,1.0,15,3
4,1,100,30,4096,94.48,90.22666666666666,90.29,90.18,0.5,15,3
3,5,100,40,7000,95.04,93.53,89.9,89.9,1.0,1,1
4,3,100,30,2048,95.09,92.75,90.59,90.18,1.0,15,3
4,3,100,30,2048,95.3,92.58,90.48,90.18,1.0,15,3
4,5,100,30,512,94.8,92.34,90.47,90.18,1.0,15,3
8,5,100,40,7000,92.71000000000001,91.10000000000001,90.25999999999999,90.25999999999999,1.0,1,1
3,3,100,20,4096,94.78999999999999,91.85666666666667,90.47333333333333,89.9,0.5,15,3
4,8,100,30,2048,95.32000000000001,93.08,90.73,90.18,1.0,15,3
3,5,100,10,32,93.55,90.01,89.67,89.9,1.0,15,3
6,5,100,50,7000,96.65,94.48,90.42,90.42,1.0,1,1
3,1,100,40,4096,95.30999999999999,90.03999999999999,90.2,89.9,1.0,15,3
8,5,100,40,7000,92.66,91.06,90.25999999999999,90.25999999999999,1.0,1,1
4,5,100,20,1024,94.11,91.72333333333333,90.68333333333334,90.18,0.5,15,3
4,5,100,30,2048,94.45666666666666,91.87666666666667,90.42666666666666,90.18,0.5,15,3
4,5,100,10,1024,95.27,92.19000000000001,91.12,90.18,1.0,15,3
2,5,100,30,16,89.73333333333333,90.59333333333333,89.78,89.68,0.5,15,3
7,5,100,10,2048,96.19,92.56666666666666,90.51333333333334,89.72,0.5,15,3
0,5,100,30,7000,97.71,92.62,90.23,90.2,1.0,1,1
2,3,100,20,4096,95.89999999999999,94.75,91.94,89.68,1.0,15,3
9,5,100,50,7000,92.49000000000001,91.93,89.91,89.91,1.0,1,1
7,5,100,30,2048,96.37666666666667,94.04333333333334,90.03,89.72,0.5,15,3
3,8,100,30,2048,95.30999999999999,93.26,90.59,89.9,1.0,15,3
4,5,100,10,16,91.12666666666667,90.18666666666667,90.3,90.18,0.5,15,3
2,1,100,30,1024,95.27,91.43,91.13,89.68,1.0,15,3
7,5,100,40,1024,96.67999999999999,95.06,90.06,89.72,1.0,15,3
7,5,100,30,16,90.47666666666667,89.98,89.73,89.72,0.5,15,3
9,1,100,100,2048,92.21000000000001,89.91,89.91,89.91,0.5,15,3
7,5,100,20,128,95.82000000000001,91.14999999999999,90.53,89.72,1.0,15,3
8,5,100,30,5000,93.12,91.16,90.25999999999999,90.25999999999999,1.0,15,3
2,1,100,30,2048,95.25,91.67999999999999,91.09,89.68,1.0,15,3
0,5,100,80,5000,98.05,91.75999999999999,90.21000000000001,90.2,1.0,15,3
9,5,100,50,7000,92.30000000000001,92.44,89.91,89.91,1.0,1,1
1,5,100,10,7000,97.87,88.79,89.34,88.64999999999999,1.0,1,1
3,5,100,40,7000,94.83,93.55,89.9,89.9,1.0,1,1
4,2,100,30,1024,94.21666666666667,91.75999999999999,90.54666666666667,90.18,0.5,15,3
2,1,100,60,5000,94.81,89.72666666666666,89.68,89.68,0.25,15,3
2,5,100,10,1024,94.50666666666667,92.51,90.87333333333333,89.68,0.5,15,3
2,5,100,20,7000,95.81,94.35,92.12,89.68,1.0,1,1
9,5,100,60,5000,92.93,92.74,89.92,89.91,1.0,15,3
3,5,100,80,7000,94.83,93.08,89.9,89.9,1.0,1,1
3,5,100,20,1024,94.65,92.17333333333333,90.58666666666667,89.9,0.5,15,3
2,5,100,80,7000,95.09,92.47,89.68,89.68,1.0,1,1
3,1,100,30,1024,95.14,90.4,90.53,89.9,1.0,15,3
6,5,100,40,5000,97.03,95.07,90.42,90.42,1.0,15,3
3,5,100,20,1024,94.76,91.88,90.53999999999999,89.9,0.5,15,3
0,5,100,40,5000,97.58,92.69,90.25999999999999,90.2,1.0,1,1
9,5,100,50,5000,93.08,92.56,89.91,89.91,1.0,15,3
2,5,100,30,7000,95.28,94.51,91.07,89.68,1.0,1,1
4,8,100,30,2048,95.02000000000001,93.17,90.9,90.18,1.0,15,3
2,8,100,10,2048,94.58,92.95333333333333,91.10000000000001,89.68,0.5,15,3
3,8,100,30,1024,95.37,93.49,90.60000000000001,89.9,1.0,15,3
3,5,100,20,512,94.8,91.73,90.66,89.9,1.0,15,3
6,5,100,100,7000,96.83,92.17,90.42,90.42,1.0,1,1
3,5,100,30,4096,95.28999999999999,93.43,90.44,89.9,1.0,15,3
7,10,100,20,256,95.76,93.14333333333333,90.43666666666667,89.72,0.75,15,3
3,5,100,40,7000,95.1,93.51,89.97,89.9,1.0,1,1
0,5,100,20,5000,98.22999999999999,92.21000000000001,90.2,90.2,1.0,15,3
6,5,100,20,5000,96.75,93.37,90.42,90.42,1.0,1,1
7,8,100,20,2048,96.45333333333333,94.05333333333333,90.52333333333334,89.72,0.5,15,3
9,5,100,60,5000,93.33,92.5,89.91,89.91,1.0,15,3
8,3,100,20,64,91.61,90.25999999999999,90.25999999999999,90.25999999999999,1.0,15,3
8,5,100,80,5000,93.31,91.03,90.25999999999999,90.25999999999999,1.0,15,3
3,5,100,30,1024,94.34666666666666,92.44,90.19,89.9,0.5,15,3
7,5,100,30,16,91.64,90.3,89.74333333333333,89.72,0.5,15,3
2,5,100,20,16,90.46333333333332,90.32666666666667,89.93333333333334,89.68,0.5,15,3
4,5,100,10,5000,94.84,91.64999999999999,91.02,90.18,1.0,1,1
2,1,100,30,5000,95.06,91.00666666666667,90.60333333333334,89.68,0.5,10,3
4,5,100,20,16,90.39666666666668,90.24333333333333,90.34333333333333,90.18,0.5,15,3
2,5,100,10,1024,94.81,92.92333333333333,91.15666666666667,89.68,0.5,15,3
5,5,100,40,5000,93.78,93.06,91.08000000000001,91.08000000000001,1.0,15,3
7,2,100,20,4096,96.30333333333333,92.64666666666666,90.41333333333334,89.72,0.5,15,3
7,5,100,10,7000,96.91,93.31,91.25,89.72,1.0,1,1
2,8,100,20,4096,96.19,95.36,91.78,89.68,1.0,15,3
2,5,100,30,5000,95.49,95.15,91.06,89.68,1.0,10,3
2,5,100,60,7000,95.55,93.96,89.68,89.68,1.0,1,1
2,5,100,40,7000,95.15,94.75,90.24,89.68,1.0,1,1
3,10,100,20,2048,95.32000000000001,93.02,90.89,89.9,1.0,15,3
3,5,100,30,16,90.36999999999999,90.14999999999999,89.95333333333333,89.9,0.5,15,3
2,5,100,40,5000,95.38,94.59,90.2,89.68,1.0,1,1
5,5,100,20,7000,93.49,91.53,91.09,91.08000000000001,1.0,1,1
4,1,100,20,5000,94.97,91.2,91.10000000000001,90.18,1.0,1,1
2,5,100,10,256,94.28999999999999,92.48666666666666,90.93666666666667,89.68,0.5,15,3
3,5,100,20,256,94.43,90.83,90.83,89.9,1.0,15,3
7,3,100,50,64,94.56,91.84333333333333,89.72,89.72,0.75,15,3
2,5,100,20,256,95.33,93.32000000000001,92.14,89.68,1.0,15,3
7,1,100,30,5000,96.5,90.89,90.05,89.72,1.0,1,1
5,5,100,40,1024,93.22,92.67999999999999,91.08000000000001,91.08000000000001,1.0,15,3
2,5,100,10,5000,95.17,93.10000000000001,91.82000000000001,89.68,1.0,1,1
2,10,100,30,1024,94.76333333333334,93.96333333333334,90.59666666666666,89.68,0.5,15,3
3,2,100,20,1024,94.78333333333333,91.53999999999999,90.96333333333332,89.9,0.75,15,3
0,5,100,50,5000,98.13,93.07,90.22,90.2,1.0,15,3
0,3,100,60,32,94.87666666666667,90.36666666666666,90.2,90.2,0.5,15,3
7,5,100,40,1024,96.92,95.07,89.97,89.72,1.0,15,3
2,1,100,10,1024,94.42666666666668,91.14,90.91,89.68,0.5,15,3
3,2,100,30,6131,95.12,91.47666666666666,90.43666666666667,89.9,0.5,3,3
7,5,100,20,32,94.49,92.07,89.91,89.72,1.0,15,3
6,5,100,20,5000,97.14,94.17,90.42,90.42,1.0,15,3
4,5,100,30,16,90.83,90.25,90.27,90.18,1.0,15,3
4,5,100,10,1024,94.19999999999999,91.56,90.65333333333334,90.18,0.5,15,3
3,5,100,50,7000,94.88,93.60000000000001,89.9,89.9,1.0,1,1
4,5,100,30,16,90.39666666666668,90.19,90.18666666666667,90.18,0.5,15,3
4,5,100,20,5000,94.95,91.99000000000001,91.09,90.18,1.0,1,1
2,5,100,30,64,93.58,90.46,90.24,89.68,1.0,15,3
7,5,100,40,5000,96.66,94.71000000000001,89.77000000000001,89.72,1.0,1,1
4,5,100,20,1024,95.16,92.74,91.24,90.18,1.0,15,3
4,5,100,30,7000,94.89999999999999,92.55,90.48,90.18,1.0,1,1
3,1,100,30,2048,94.82666666666667,90.27,90.24,89.9,0.5,15,3
8,5,100,100,7000,92.83,90.60000000000001,90.25999999999999,90.25999999999999,1.0,1,1
2,5,100,20,7000,95.61,94.49,92.2,89.68,1.0,1,1
7,1,100,30,2048,96.34666666666666,90.7,90.11333333333333,89.72,0.5,15,3
7,1,100,20,4096,96.98,92.19000000000001,90.66,89.72,1.0,15,3
9,1,100,10,1024,92.47999999999999,89.95,89.98666666666666,89.91,0.75,15,3
3,3,100,30,4096,95.28,93.25,90.45,89.9,1.0,15,3
3,5,100,40,5000,95.28999999999999,93.76,90.16999999999999,89.9,1.0,15,3
3,8,100,10,4096,95.67,92.36999999999999,91.0,89.9,1.0,15,3
9,5,100,20,5000,93.69,91.08000000000001,89.94,89.91,1.0,15,3
2,3,100,40,1024,93.74,92.14666666666666,89.89,89.68,0.25,15,3
3,2,100,10,4096,95.7,91.9,91.14,89.9,1.0,15,3
4,1,100,30,2048,95.37,90.85,90.60000000000001,90.18,1.0,15,3
3,3,100,20,1024,95.09,92.39,90.84,89.9,1.0,15,3
3,5,100,20,32,91.96,90.06,89.9,89.9,1.0,15,3
7,5,100,30,5000,96.96000000000001,94.91000000000001,90.29,89.72,1.0,15,3
2,5,100,20,2048,95.13333333333334,93.83333333333333,91.25666666666666,89.68,0.5,15,3
3,5,100,20,512,95.07,92.4,90.63,89.9,1.0,15,3
4,5,100,20,2048,94.13333333333334,91.67999999999999,90.76333333333334,90.18,0.5,15,3
1,5,100,20,5000,98.16,89.63,89.66,88.64999999999999,1.0,15,3
7,5,100,10,256,95.80666666666666,91.97333333333333,90.52666666666667,89.72,0.5,15,3
1,5,100,20,7000,98.06,89.25999999999999,89.61,88.64999999999999,1.0,1,1
2,5,100,20,16,89.92333333333333,90.89666666666668,90.24666666666667,89.68,0.5,15,3
3,5,100,30,5000,94.93,92.73,90.48,89.9,1.0,1,1
7,5,100,40,1024,96.98,94.91000000000001,89.89,89.72,1.0,15,3
2,5,100,30,1024,95.23,94.53,91.03999999999999,89.68,1.0,15,3
9,5,100,80,7000,92.65,92.13,89.91,89.91,1.0,1,1
1,3,100,100,2048,98.02,90.02,88.64999999999999,88.64999999999999,1.0,15,3
1,1,100,20,5000,97.78,88.67,89.1,88.64999999999999,1.0,1,1
3,5,100,80,5000,95.08,93.10000000000001,89.9,89.9,1.0,15,3
5,5,100,80,5000,93.67,92.47,91.08000000000001,91.08000000000001,1.0,15,3
1,5,100,60,7000,98.00999999999999,93.85,88.64999999999999,88.64999999999999,1.0,1,1
3,5,100,20,128,92.84666666666666,90.80333333333334,90.33666666666666,89.9,0.5,15,3
7,5,100,30,1024,96.12,93.76666666666667,90.05,89.72,0.5,15,3
7,5,100,60,5000,96.99,95.05,89.72,89.72,1.0,15,3
3,5,100,50,7000,95.07,93.66,89.9,89.9,1.0,1,1
3,5,100,50,5000,95.28,94.01,89.94,89.9,1.0,15,3
6,5,100,100,5000,97.09,93.10000000000001,90.42,90.42,1.0,15,3
2,5,100,30,1024,94.75,93.93666666666667,90.59,89.68,0.5,15,3
4,5,100,20,5000,95.8,93.39,91.41,90.18,1.0,15,3
3,5,100,10,1024,95.47,91.8,90.85,89.9,1.0,15,3
6,5,100,80,7000,96.67999999999999,93.36,90.42,90.42,1.0,1,1
3,5,100,20,5000,95.07,92.58,90.86,89.9,1.0,1,1
3,5,100,20,256,94.69,90.93,91.08000000000001,89.9,1.0,15,3
1,2,100,20,5000,98.25,89.07000000000001,89.48,88.64999999999999,1.0,15,3
2,3,100,10,1024,94.66,92.74666666666667,91.07333333333332,89.68,0.5,15,3
6,2,100,60,16,90.52666666666667,90.57666666666667,90.42,90.42,0.25,15,3
4,5,100,30,32,91.55,90.24,90.60000000000001,90.18,1.0,15,3
2,8,100,10,1024,95.77,93.63,92.04,89.68,1.0,15,3
6,5,100,20,5000,96.92,94.33,90.42,90.42,1.0,15,3
5,5,100,40,1024,93.24,92.13,91.08000000000001,91.08000000000001,1.0,15,3
3,5,100,20,128,94.07,90.27,90.45,89.9,1.0,15,3
0,5,100,50,5000,98.06,93.27,90.25,90.2,1.0,15,3
4,5,100,30,5000,95.19,93.22,90.81,90.18,1.0,15,3
3,2,100,10,16,90.36333333333333,89.93666666666667,89.97333333333334,89.9,0.5,15,3
8,5,100,10,5000,93.35,90.45,90.25999999999999,90.25999999999999,1.0,15,3
4,5,100,20,32,91.91,90.22,90.18,90.18,1.0,15,3
7,1,100,30,2048,96.37,90.45666666666666,90.10000000000001,89.72,0.5,15,3
4,5,100,20,32,91.21000000000001,90.48333333333333,90.42999999999999,90.18,0.5,15,3
4,8,100,10,2048,93.97333333333333,91.63,90.79333333333334,90.18,0.5,15,3
1,5,100,50,7000,97.75,93.81,88.64999999999999,88.64999999999999,1.0,1,1
3,5,100,30,512,94.3,92.19000000000001,90.26666666666667,89.9,0.5,15,3
2,3,100,10,5000,93.71333333333334,91.31666666666666,90.08666666666667,89.68,0.25,15,3
2,5,100,20,256,93.86333333333333,92.91666666666667,91.21666666666667,89.68,0.5,15,3
3,5,100,20,16,90.44,90.12,90.10333333333334,89.9,0.5,15,3
2,5,100,10,5000,95.89,94.11,91.96,89.68,1.0,15,3
3,3,100,20,1024,94.42666666666668,91.81333333333333,90.49000000000001,89.9,0.5,15,3
8,5,100,20,7000,93.10000000000001,90.64999999999999,90.25999999999999,90.25999999999999,1.0,1,1
5,10,100,40,64,91.7,91.08000000000001,91.08000000000001,91.08000000000001,1.0,15,3
7,2,100,10,4096,97.05,93.82000000000001,90.93,89.72,1.0,15,3
8,5,100,50,5000,93.13,91.14,90.25999999999999,90.25999999999999,1.0,15,3
3,5,100,80,7000,94.91000000000001,93.06,89.9,89.9,1.0,1,1
3,8,100,30,2048,95.0,93.28,90.55,89.9,1.0,15,3
3,5,100,30,128,94.01,90.82000000000001,90.48,89.9,1.0,15,3
3,2,100,10,1024,95.54,91.67,90.64999999999999,89.9,1.0,15,3
4,1,100,20,2048,95.26,91.45,91.14999999999999,90.18,1.0,15,3
5,5,100,40,7000,93.56,92.34,91.08000000000001,91.08000000000001,1.0,1,1
2,3,100,30,2048,94.86333333333333,93.55333333333333,90.47666666666667,89.68,0.5,15,3
4,5,100,20,64,92.74,90.21000000000001,90.55,90.18,1.0,15,3
8,5,100,10,7000,92.84,90.3,90.25999999999999,90.25999999999999,1.0,1,1
8,5,100,60,7000,92.91,90.82000000000001,90.25999999999999,90.25999999999999,1.0,1,1
3,3,100,20,2048,94.58,91.93,90.59333333333333,89.9,0.5,15,3
4,5,100,30,1024,94.12666666666667,91.77666666666666,90.40333333333334,90.18,0.5,15,3
4,5,100,50,5000,95.35,92.78,90.19,90.18,1.0,15,3
3,5,100,50,5000,95.41,93.82000000000001,89.97,89.9,1.0,15,3
2,10,100,20,2048,95.84,94.74000000000001,91.81,89.68,1.0,15,3
0,5,100,100,5000,97.81,91.28,90.21000000000001,90.2,1.0,1,1
9,2,100,40,512,91.99000000000001,90.79,89.91,89.91,1.0,15,3
7,1,100,30,2048,96.97,91.16,90.41,89.72,1.0,15,3
9,5,100,40,7000,92.83,92.14,89.94,89.91,1.0,1,1
1,5,100,50,7000,97.88,93.84,88.64999999999999,88.64999999999999,1.0,1,1
9,5,100,30,7000,92.74,91.47999999999999,89.97,89.91,1.0,1,1
7,5,100,20,16,92.44,90.16999999999999,89.83,89.72,1.0,15,3
9,5,100,10,7000,92.96,90.19,89.99000000000001,89.91,1.0,1,1
2,2,100,30,1024,95.55,94.24,91.14999999999999,89.68,1.0,15,3
3,2,100,20,2048,95.30999999999999,91.99000000000001,91.0,89.9,1.0,15,3
3,1,100,30,4096,94.76,90.05333333333333,90.35666666666667,89.9,0.5,15,3
7,1,100,20,5000,96.93,91.74,90.77,89.72,1.0,1,1
3,5,100,20,1024,95.30999999999999,92.34,91.11,89.9,1.0,15,3
3,2,100,20,2048,95.50999999999999,91.88,91.08000000000001,89.9,1.0,15,3
0,5,100,80,7000,97.75,91.61,90.2,90.2,1.0,1,1
5,1,100,40,5000,93.19,91.08000000000001,91.08000000000001,91.08000000000001,1.0,1,1
2,2,100,30,1024,94.81333333333333,92.88333333333333,90.51,89.68,0.5,15,3
4,5,100,100,7000,94.83,90.62,90.18,90.18,1.0,1,1
9,5,100,50,5000,93.0,92.51,89.91,89.91,1.0,15,3
2,5,100,30,2048,94.96,94.1,90.64666666666666,89.68,0.5,15,3
2,1,100,30,2048,95.50999999999999,91.52,90.88000000000001,89.68,1.0,15,3
2,2,100,10,2048,94.47666666666666,92.53666666666666,90.98,89.68,0.5,15,3
0,5,100,30,7000,97.72999999999999,93.07,90.24,90.2,1.0,1,1
0,5,100,20,5000,98.16,91.95,90.24,90.2,1.0,15,3
3,5,100,50,7000,95.02000000000001,93.71000000000001,89.9,89.9,1.0,1,1
3,5,100,10,2048,95.35,92.10000000000001,91.03999999999999,89.9,1.0,15,3
2,8,100,30,1024,95.17999999999999,94.81,91.09,89.68,1.0,15,3
4,5,100,80,7000,94.91000000000001,90.98,90.18,90.18,1.0,1,1
3,5,100,40,1024,94.63000000000001,93.65,90.16999999999999,89.9,1.0,15,3
9,5,100,20,5000,93.4,91.17,90.02,89.91,1.0,15,3
7,5,100,30,5000,96.97,94.97,90.31,89.72,1.0,15,3
2,5,100,30,2048,95.65,94.89,90.88000000000001,89.68,1.0,15,3
4,2,100,10,1024,94.89,92.25999999999999,91.21000000000001,90.18,1.0,15,3
3,3,100,10,2048,94.10333333333334,91.24333333333333,90.56666666666666,89.9,0.5,15,3
8,5,100,100,5000,93.07,90.74,90.25999999999999,90.25999999999999,1.0,15,3
2,10,100,30,5958,95.67,95.34,90.92,89.68,1.0,3,3
4,8,100,10,1024,95.32000000000001,92.25999999999999,91.33,90.18,1.0,15,3
1,5,100,60,7000,98.1,94.37,88.64999999999999,88.64999999999999,1.0,1,1
2,5,100,30,64,93.71000000000001,90.59,89.98,89.68,1.0,15,3
6,1,100,80,5000,96.6,90.42,90.42,90.42,1.0,1,1
8,5,100,40,5000,93.0,91.23,90.25999999999999,90.25999999999999,1.0,15,3
4,5,100,30,128,92.59666666666668,91.12,90.39,90.18,0.5,15,3
4,5,100,30,32,90.79333333333334,90.27333333333334,90.18333333333334,90.18,0.5,15,3
1,5,100,100,5000,97.96000000000001,93.03,88.64999999999999,88.64999999999999,1.0,1,1
2,5,100,10,256,94.24333333333334,92.24333333333334,91.27333333333333,89.68,0.5,15,3
2,5,100,20,512,94.65,93.68666666666667,90.93666666666667,89.68,0.5,15,3
7,5,100,10,2048,97.2,93.89,91.16,89.72,1.0,15,3
7,2,100,10,2048,97.0,93.42,91.03999999999999,89.72,1.0,15,3
2,5,100,10,512,94.52333333333334,92.55666666666667,90.81333333333333,89.68,0.5,15,3
2,2,100,40,1024,94.66,92.60333333333334,90.16333333333333,89.68,0.5,15,3
4,5,100,30,32,91.74,90.18,90.18,90.18,1.0,15,3
4,1,100,30,1024,94.05,90.47,90.41,90.18,0.5,15,3
3,3,100,30,2048,94.59,92.16666666666666,90.21666666666667,89.9,0.5,15,3
0,5,100,20,5000,97.72999999999999,91.16,90.25,90.2,1.0,1,1
4,5,100,100,7000,95.0,90.64999999999999,90.18,90.18,1.0,1,1
4,1,100,30,2048,94.29333333333332,90.5,90.3,90.18,0.5,15,3
4,5,100,50,7000,94.78,91.89,90.19,90.18,1.0,1,1
2,5,100,10,7000,95.58,93.51,92.04,89.68,1.0,1,1
7,5,100,100,7000,96.77,92.92,89.72,89.72,1.0,1,1
5,1,100,100,512,92.72333333333333,91.08000000000001,91.08000000000001,91.08000000000001,0.75,15,3
0,3,100,100,512,97.19333333333333,90.64666666666666,90.2,90.2,0.5,15,3
0,5,100,10,7000,97.69,91.14,90.2,90.2,1.0,1,1
3,5,100,30,1024,95.11,93.39,90.51,89.9,1.0,15,3
3,5,100,20,5000,95.00999999999999,92.58,90.74,89.9,1.0,1,1
2,5,100,10,4096,95.86,94.39,91.83,89.68,1.0,15,3
3,2,100,20,2048,95.63000000000001,92.25,90.94,89.9,1.0,15,3
6,5,100,60,5000,96.96000000000001,94.69999999999999,90.42,90.42,1.0,15,3
4,1,100,10,4096,93.51666666666667,90.64,90.75666666666666,90.18,0.5,15,3
3,1,100,20,4096,94.59333333333333,90.60000000000001,90.52666666666667,89.9,0.5,15,3
1,5,100,60,7000,97.95,94.39,88.64999999999999,88.64999999999999,1.0,1,1
5,5,100,40,5000,94.08,92.91,91.08000000000001,91.08000000000001,1.0,15,3
2,5,100,20,32,92.06,90.93,89.89,89.68,1.0,15,3
3,2,100,30,6131,95.07,92.65,90.47,89.9,1.0,3,3
1,5,100,20,5000,98.21,90.39,89.52,88.64999999999999,1.0,15,3
5,5,100,80,7000,93.33,92.21000000000001,91.08000000000001,91.08000000000001,1.0,1,1
3,2,100,30,2048,95.14,92.27,90.53999999999999,89.9,1.0,15,3
3,2,100,30,2048,95.28,92.51,90.45,89.9,1.0,15,3
3,5,100,20,32,93.03,89.94,89.9,89.9,1.0,15,3
7,5,100,30,512,95.8,93.95,90.20666666666666,89.72,0.5,15,3
7,5,100,10,16,93.76,90.72,90.42999999999999,89.72,1.0,15,3
3,5,100,20,16,90.83,89.92999999999999,90.13,89.9,1.0,15,3
3,10,100,10,2048,95.45,92.23,90.89,89.9,1.0,15,3
2,5,100,20,5000,96.02000000000001,95.09,92.15,89.68,1.0,15,3
0,1,100,20,5000,97.86,90.2,90.22,90.2,1.0,1,1
4,3,100,10,2048,95.48,92.61,91.43,90.18,1.0,15,3
7,5,100,10,64,95.83,91.95,90.42999999999999,89.72,1.0,15,3
1,5,100,60,7000,97.94,94.07,88.64999999999999,88.64999999999999,1.0,1,1
3,1,100,40,5000,95.28999999999999,90.03,90.07,89.9,1.0,15,3
4,5,100,40,1024,95.07,92.75999999999999,90.29,90.18,1.0,15,3
4,10,100,50,64,91.88333333333333,90.72666666666666,90.18,90.18,0.5,15,3
0,5,100,60,5000,97.68,92.43,90.2,90.2,1.0,1,1
3,5,100,30,4096,95.62,93.23,90.34,89.9,1.0,15,3
5,5,100,50,7000,93.47999999999999,92.34,91.08000000000001,91.08000000000001,1.0,1,1
9,5,100,40,5000,93.13,92.19000000000001,89.92999999999999,89.91,1.0,15,3
2,1,100,20,1024,95.12,92.45,91.66,89.68,1.0,15,3
5,5,100,50,7000,93.44,92.73,91.08000000000001,91.08000000000001,1.0,1,1
9,5,100,100,7000,92.71000000000001,91.34,89.91,89.91,1.0,1,1
0,5,100,60,5000,97.74000000000001,93.01,90.21000000000001,90.2,1.0,1,1
1,5,100,20,5000,98.29,89.68,89.14,88.64999999999999,1.0,15,3
7,8,100,10,2048,97.27,93.86,91.14,89.72,1.0,15,3
3,8,100,10,2048,94.28333333333333,91.45333333333333,90.50666666666667,89.9,0.5,15,3
7,5,100,80,5000,97.00999999999999,94.58,89.72,89.72,1.0,15,3
7,2,100,20,2048,96.37333333333333,92.86999999999999,90.60333333333334,89.72,0.5,15,3
1,5,100,80,5000,98.33,94.21000000000001,88.64999999999999,88.64999999999999,1.0,15,3
3,5,100,30,128,93.83,90.57,90.42999999999999,89.9,1.0,15,3
3,5,100,20,512,94.14333333333333,92.30666666666667,90.68666666666667,89.9,0.5,15,3
3,5,100,20,256,93.85,91.27333333333333,90.42666666666666,89.9,0.5,15,3
4,5,100,100,7000,95.04,90.52,90.18,90.18,1.0,1,1
7,1,100,30,16,92.07333333333332,89.86333333333333,89.82,89.72,0.75,15,3
5,2,100,80,512,92.29333333333334,91.13333333333333,91.08000000000001,91.08000000000001,0.75,15,3
2,5,100,10,64,93.14333333333333,91.47666666666666,91.34,89.68,0.5,15,3
7,5,100,30,512,96.8,94.35,90.36,89.72,1.0,15,3
2,5,100,10,7000,95.56,93.58999999999999,92.05,89.68,1.0,1,1
6,5,100,50,7000,96.66,94.58,90.42,90.42,1.0,1,1
5,5,100,50,7000,93.46,92.78999999999999,91.08000000000001,91.08000000000001,1.0,1,1
2,1,100,20,2048,94.87666666666667,91.67999999999999,91.07333333333332,89.68,0.5,15,3
5,5,100,100,7000,93.28999999999999,91.75,91.08000000000001,91.08000000000001,1.0,1,1
3,5,100,10,2048,95.61,92.25999999999999,90.99000000000001,89.9,1.0,15,3
0,5,100,100,5000,98.24000000000001,91.51,90.2,90.2,1.0,15,3
4,5,100,20,7000,95.04,92.29,91.25,90.18,1.0,1,1
6,10,100,50,512,95.87666666666667,94.49333333333333,90.42,90.42,0.5,15,3
2,5,100,20,128,93.37333333333333,92.49333333333334,90.61,89.68,0.5,15,3
2,3,100,20,2048,95.63000000000001,94.63000000000001,91.86,89.68,1.0,15,3
2,3,100,10,1024,94.57333333333334,92.53666666666666,91.11333333333333,89.68,0.5,15,3
3,5,100,30,32,92.21000000000001,89.91,89.9,89.9,1.0,15,3
4,5,100,20,5000,95.78999999999999,93.5,91.17,90.18,1.0,15,3
2,2,100,20,2048,95.87,94.17999999999999,91.83,89.68,1.0,15,3
7,1,100,30,1024,96.72,90.72,90.21000000000001,89.72,1.0,15,3
0,5,100,20,5000,97.64,90.94,90.24,90.2,1.0,1,1
4,2,100,30,2048,94.53666666666666,91.2,90.45333333333333,90.18,0.5,15,3
2,2,100,10,1024,94.58,92.57,91.06333333333333,89.68,0.5,15,3
2,2,100,10,4096,95.91,93.85,92.16,89.68,1.0,15,3
6,5,100,30,5000,97.03,94.91000000000001,90.42,90.42,1.0,15,3
2,5,100,20,64,92.25,91.51333333333334,91.07333333333332,89.68,0.5,15,3
7,1,100,20,2048,96.32666666666667,91.67666666666666,90.45333333333333,89.72,0.5,15,3
5,5,100,40,7000,93.62,92.4,91.08000000000001,91.08000000000001,1.0,1,1
1,5,100,30,7000,97.96000000000001,91.25,88.64999999999999,88.64999999999999,1.0,1,1
3,2,100,20,2048,95.19,91.88,90.93,89.9,1.0,15,3
2,1,100,20,2048,95.16666666666667,91.55,91.14,89.68,0.5,15,3
5,5,100,30,7000,93.45,92.0,91.08000000000001,91.08000000000001,1.0,1,1
3,3,100,20,1024,95.50999999999999,92.69,90.95,89.9,1.0,15,3
4,5,100,20,4096,95.26,92.99,91.12,90.18,1.0,15,3
7,5,100,100,7000,96.54,92.64,89.72,89.72,1.0,1,1
2,1,100,20,4096,95.92,93.21000000000001,92.15,89.68,1.0,15,3
6,5,100,10,7000,96.82,92.52,90.42,90.42,1.0,1,1
9,5,100,20,7000,93.10000000000001,90.67,89.94,89.91,1.0,1,1
3,5,100,30,7000,95.19,93.04,90.5,89.9,1.0,1,1
3,5,100,20,16,91.91,90.2,90.12,89.9,1.0,15,3
4,5,100,10,2048,93.92666666666668,91.67999999999999,90.73333333333333,90.18,0.5,15,3
6,5,100,40,7000,96.66,94.78,90.42,90.42,1.0,1,1
9,5,100,40,7000,92.72,92.34,89.91,89.91,1.0,1,1
2,1,100,10,4096,96.1,93.08,92.08,89.68,1.0,15,3
2,5,100,50,5000,95.75,95.13000000000001,90.0,89.68,1.0,15,3
8,5,100,20,5000,93.58999999999999,91.21000000000001,90.25999999999999,90.25999999999999,1.0,15,3
9,1,100,10,1024,93.07,89.92999999999999,89.92,89.91,1.0,15,3
1,5,100,50,5000,98.05,94.01,88.64999999999999,88.64999999999999,1.0,15,3
1,5,100,60,1024,97.91,94.81,88.64999999999999,88.64999999999999,1.0,15,3
3,1,100,30,6131,95.26333333333334,90.21000000000001,90.27666666666667,89.9,0.5,3,3
1,5,100,30,5000,98.19,90.35,88.64999999999999,88.64999999999999,1.0,15,3
2,3,100,50,2048,94.98666666666666,93.23666666666666,89.94666666666666,89.68,0.5,15,3
2,8,100,20,2048,95.16666666666667,93.94333333333333,91.14,89.68,0.5,15,3
2,5,100,10,512,95.21,92.83,91.75,89.68,1.0,15,3
3,1,100,30,4096,95.46,90.5,90.53,89.9,1.0,15,3
3,5,100,80,7000,95.02000000000001,93.33,89.9,89.9,1.0,1,1
2,5,100,10,16,91.02,90.05,89.75999999999999,89.68,1.0,15,3
4,5,100,100,7000,94.99,90.71000000000001,90.18,90.18,1.0,1,1
7,5,100,40,5000,96.87,95.28999999999999,89.88000000000001,89.72,1.0,15,3
4,5,100,10,128,92.88,91.05666666666666,90.67333333333333,90.18,0.5,15,3
3,5,100,10,32,91.94333333333333,89.97,90.08666666666667,89.9,0.5,15,3
9,5,100,40,7000,92.53,91.96,89.91,89.91,1.0,1,1
6,5,100,40,7000,96.61999999999999,94.56,90.42,90.42,1.0,1,1
5,5,100,80,5000,93.77,92.75,91.08000000000001,91.08000000000001,1.0,15,3
2,5,100,20,256,94.03,93.16,91.36,89.68,0.5,15,3
3,5,100,30,1024,95.05,93.11,90.62,89.9,1.0,15,3
2,5,100,30,1024,95.1,94.39999999999999,90.89,89.68,1.0,15,3
2,5,100,20,7000,95.50999999999999,94.28,92.36,89.68,1.0,1,1
3,5,100,40,5000,95.15,94.26,90.14999999999999,89.9,1.0,15,3
3,5,100,100,5000,95.26,92.75,89.9,89.9,1.0,15,3
8,2,100,20,4096,93.63,90.35,90.25999999999999,90.25999999999999,1.0,15,3
2,8,100,20,1024,95.50999999999999,94.55,91.79,89.68,1.0,15,3
3,5,100,10,7000,95.32000000000001,91.71000000000001,90.81,89.9,1.0,1,1
2,2,100,10,1024,95.52000000000001,93.47,91.91,89.68,1.0,15,3
3,5,100,50,7000,95.07,93.42,89.9,89.9,1.0,1,1
2,8,100,10,4096,95.95,94.27,92.17,89.68,1.0,15,3
6,5,100,100,7000,96.91,92.02,90.42,90.42,1.0,1,1
3,5,100,20,32,91.21333333333334,89.92999999999999,90.06,89.9,0.5,15,3
3,3,100,30,5000,95.50999999999999,93.05,90.45,89.9,1.0,15,3
8,1,100,10,5000,92.77,90.25999999999999,90.25999999999999,90.25999999999999,1.0,1,1
3,5,100,40,5000,95.6,93.47,90.22,89.9,1.0,15,3
8,3,100,60,512,90.94333333333333,90.31,90.25999999999999,90.25999999999999,0.25,15,3
7,5,100,40,5000,97.05,95.3,90.02,89.72,1.0,15,3
9,5,100,50,7000,92.39,92.5,89.91,89.91,1.0,1,1
8,5,100,60,5000,92.9,90.99000000000001,90.25999999999999,90.25999999999999,1.0,1,1
2,5,100,10,256,95.27,91.95,91.63,89.68,1.0,15,3
8,5,100,100,7000,92.95,90.7,90.25999999999999,90.25999999999999,1.0,1,1
7,5,100,30,32,94.32000000000001,89.74,89.72,89.72,1.0,15,3
0,2,100,100,2048,98.06,90.99000000000001,90.21000000000001,90.2,1.0,15,3
6,5,100,60,5000,96.85000000000001,95.16,90.42,90.42,1.0,15,3
7,5,100,40,1024,96.69,94.98,90.02,89.72,1.0,15,3
3,5,100,50,7000,94.97,93.49,89.9,89.9,1.0,1,1
3,5,100,40,7000,95.25,93.83,89.95,89.9,1.0,1,1
4,2,100,10,1024,95.25,92.23,91.46,90.18,1.0,15,3
3,5,100,60,7000,94.88,93.43,89.9,89.9,1.0,1,1
3,10,100,20,4096,95.58,93.47,91.16,89.9,1.0,15,3
5,5,100,20,5000,94.22,91.86999999999999,91.08000000000001,91.08000000000001,1.0,15,3
9,5,100,10,5000,93.44,90.53999999999999,90.02,89.91,1.0,15,3
3,1,100,30,2048,94.73333333333333,90.15333333333334,90.19333333333334,89.9,0.5,15,3
6,5,100,50,7000,96.67999999999999,94.82000000000001,90.42,90.42,1.0,1,1
4,10,100,30,512,93.98666666666666,91.84666666666666,90.42999999999999,90.18,0.5,15,3
9,5,100,20,5000,93.39,91.0,89.91,89.91,1.0,15,3
4,5,100,20,5000,95.3,92.9,91.05,90.18,1.0,15,3
7,8,100,30,2048,96.93,94.81,90.22,89.72,1.0,15,3
2,5,100,10,64,94.19,90.91,91.25,89.68,1.0,15,3
4,5,100,10,5000,95.8,92.57,91.44,90.18,1.0,15,3
9,5,100,30,7000,92.78,91.52,89.92,89.91,1.0,1,1
0,5,100,80,7000,97.89999999999999,92.06,90.2,90.2,1.0,1,1
4,5,100,30,1024,95.03,92.80000000000001,90.53999999999999,90.18,1.0,15,3
8,5,100,40,5000,92.58,91.0,90.25999999999999,90.25999999999999,1.0,1,1
3,5,100,40,7000,94.95,93.75,89.96,89.9,1.0,1,1
4,2,100,30,2048,94.26333333333334,91.19,90.44666666666666,90.18,0.5,15,3
7,5,100,40,5000,96.85000000000001,95.48,89.88000000000001,89.72,1.0,15,3
2,5,100,10,64,93.17333333333333,91.07666666666667,90.95333333333333,89.68,0.5,15,3
4,2,100,10,2048,93.62666666666667,91.25666666666666,90.68666666666667,90.18,0.5,15,3
3,5,100,20,64,93.46,90.24,90.10000000000001,89.9,1.0,15,3
2,2,100,30,4096,95.48,94.28,91.11,89.68,1.0,15,3
3,5,100,20,512,94.96,92.17,90.72,89.9,1.0,15,3
2,1,100,20,4096,95.89999999999999,92.99,91.9,89.68,1.0,15,3
0,5,100,60,7000,97.94,92.01,90.23,90.2,1.0,1,1
4,10,100,20,1024,94.07666666666667,91.85666666666667,90.67,90.18,0.5,15,3
4,5,100,50,5000,95.42,92.78999999999999,90.18,90.18,1.0,15,3
9,5,100,40,5000,93.16,92.52,89.92,89.91,1.0,15,3
2,5,100,10,64,92.75,91.35666666666667,91.03999999999999,89.68,0.5,15,3
0,5,100,50,5000,98.09,92.05,90.21000000000001,90.2,1.0,15,3
3,5,100,40,7000,95.13000000000001,93.8,89.91,89.9,1.0,1,1
4,5,100,30,64,91.91333333333334,91.00333333333333,90.29666666666667,90.18,0.5,15,3
6,5,100,80,5000,96.82,94.0,90.42,90.42,1.0,15,3
7,3,100,20,2048,96.44,93.28,90.38666666666667,89.72,0.5,15,3
9,5,100,20,7000,92.86999999999999,90.55,89.91,89.91,1.0,1,1
2,1,100,30,2048,95.19333333333333,91.15666666666667,90.58333333333334,89.68,0.5,15,3
2,8,100,20,2048,94.82666666666667,93.86666666666666,91.25333333333333,89.68,0.5,15,3
7,5,100,20,5000,97.06,95.08,90.62,89.72,1.0,15,3
8,5,100,80,7000,93.10000000000001,91.06,90.25999999999999,90.25999999999999,1.0,1,1
5,5,100,30,256,92.47999999999999,91.43,91.08000000000001,91.08000000000001,1.0,15,3
4,5,100,30,1024,94.83,92.73,90.55,90.18,1.0,15,3
7,5,100,100,5000,96.69,92.67,89.72,89.72,1.0,1,1
2,5,100,30,16,90.25333333333333,90.52,89.89,89.68,0.5,15,3
2,1,100,20,4096,95.08666666666666,91.55333333333333,91.29,89.68,0.5,15,3
3,5,100,20,5000,95.26,92.89,90.82000000000001,89.9,1.0,15,3
2,5,100,30,5000,95.50999999999999,94.99,91.07,89.68,1.0,10,3
3,5,100,30,16,90.60000000000001,89.98,89.91,89.9,1.0,15,3
3,5,100,20,512,94.77,92.4,91.25,89.9,1.0,15,3
2,1,100,10,1024,95.58,92.66,92.17,89.68,1.0,15,3
9,3,100,40,64,90.20666666666666,90.13333333333333,89.91333333333333,89.91,0.5,15,3
1,5,100,40,1024,97.92,91.64999999999999,88.64999999999999,88.64999999999999,1.0,15,3
7,3,100,60,512,96.56,93.13,89.72,89.72,1.0,15,3
2,5,100,30,16,89.95,89.68,89.68,89.68,1.0,15,3
4,8,100,20,2048,95.24000000000001,92.95,91.11,90.18,1.0,15,3
2,10,100,20,2048,95.54,94.8,92.03,89.68,1.0,15,3
2,5,100,20,2048,94.94666666666667,93.79333333333332,91.11,89.68,0.5,15,3
1,5,100,20,7000,98.07000000000001,89.25999999999999,89.53,88.64999999999999,1.0,1,1
1,1,100,20,5000,97.86,88.67,89.18,88.64999999999999,1.0,1,1
3,5,100,30,1024,95.09,92.84,90.56,89.9,1.0,15,3
2,5,100,20,5000,95.93,94.8,91.74,89.68,1.0,15,3
4,5,100,30,128,92.99333333333333,90.88666666666667,90.32333333333334,90.18,0.75,15,3
2,2,100,30,1024,94.58666666666666,92.84,90.51666666666667,89.68,0.5,15,3
3,1,100,30,1024,95.14,90.31,90.46,89.9,1.0,15,3
7,5,100,20,16,91.36,91.04666666666667,90.31333333333333,89.72,0.5,15,3
7,5,100,20,16,93.62,89.84,90.21000000000001,89.72,1.0,15,3
2,5,100,40,5000,95.67999999999999,95.05,90.51,89.68,1.0,15,3
1,3,100,50,128,97.47,89.39,88.64999999999999,88.64999999999999,1.0,15,3
7,5,100,30,16,91.18333333333334,90.77,89.97,89.72,0.5,15,3
8,5,100,50,7000,92.82000000000001,91.03999999999999,90.25999999999999,90.25999999999999,1.0,1,1
7,1,100,30,4096,96.84,90.82000000000001,90.36,89.72,1.0,15,3
2,10,100,30,5000,95.72,95.27,90.97,89.68,1.0,10,3
8,2,100,10,64,90.64333333333333,90.26666666666667,90.25999999999999,90.25999999999999,0.5,15,3
4,5,100,10,7000,95.13000000000001,92.52,91.57,90.18,1.0,1,1
2,8,100,20,1024,94.64,93.65666666666667,91.22333333333333,89.68,0.5,15,3
7,1,100,30,2048,96.92,91.10000000000001,90.22,89.72,1.0,15,3
3,10,100,30,2048,94.98,93.21000000000001,90.64,89.9,1.0,15,3
8,5,100,60,7000,93.0,90.82000000000001,90.25999999999999,90.25999999999999,1.0,1,1
3,1,100,10,5000,95.3,90.28,90.91,89.9,1.0,1,1
6,5,100,40,1024,96.6,94.85,90.42,90.42,1.0,15,3
2,5,100,40,7000,95.39999999999999,95.08,90.21000000000001,89.68,1.0,1,1
6,1,100,30,512,96.31,90.53999999999999,90.42,90.42,1.0,15,3
2,8,100,30,2048,95.5,95.05,91.14999999999999,89.68,1.0,15,3
7,3,100,30,2048,96.32333333333334,93.41666666666667,90.12333333333333,89.72,0.5,15,3
8,5,100,60,7000,92.60000000000001,90.82000000000001,90.25999999999999,90.25999999999999,1.0,1,1
4,5,100,10,1024,93.97333333333333,91.40666666666667,90.80666666666667,90.18,0.5,15,3
7,5,100,60,7000,96.78999999999999,95.05,89.72,89.72,1.0,1,1
2,2,100,20,2048,94.95666666666666,93.0,91.23333333333333,89.68,0.5,15,3
7,5,100,50,5000,96.84,95.27,89.8,89.72,1.0,15,3
0,5,100,50,7000,97.68,93.17999999999999,90.2,90.2,1.0,1,1
1,5,100,100,5000,98.09,92.28,88.64999999999999,88.64999999999999,1.0,1,1
2,3,100,10,1024,95.54,93.85,92.21000000000001,89.68,1.0,15,3
3,5,100,30,1024,94.39666666666666,92.55666666666667,90.36999999999999,89.9,0.5,15,3
8,5,100,30,7000,92.75,90.89,90.25999999999999,90.25999999999999,1.0,1,1
4,5,100,10,2048,94.16,91.52666666666667,90.71666666666667,90.18,0.5,15,3
8,5,100,40,5000,92.49000000000001,91.01,90.25999999999999,90.25999999999999,1.0,1,1
2,5,100,40,1024,95.12,94.72,90.22,89.68,1.0,15,3
7,5,100,80,5000,96.91,94.55,89.72,89.72,1.0,15,3
6,1,100,40,64,91.62333333333333,90.42,90.42,90.42,0.25,15,3
3,5,100,50,7000,94.75,93.54,89.9,89.9,1.0,1,1
4,5,100,30,128,93.41000000000001,90.38000000000001,90.5,90.18,1.0,15,3
9,5,100,10,5000,93.31,90.79,89.91,89.91,1.0,15,3
7,5,100,20,5000,96.89,94.55,90.8,89.72,1.0,15,3
1,5,100,20,7000,97.81,88.96,89.58,88.64999999999999,1.0,1,1
0,5,100,100,5000,98.22999999999999,91.41,90.2,90.2,1.0,15,3
2,5,100,30,5000,95.62,95.09,90.8,89.68,1.0,15,3
2,5,100,20,4096,95.97,94.94,92.06,89.68,1.0,15,3
4,5,100,20,4096,95.65,92.97999999999999,91.12,90.18,1.0,15,3
3,5,100,50,7000,94.97,93.73,89.91,89.9,1.0,1,1
2,5,100,100,5000,95.27,91.92,89.68,89.68,1.0,1,1
7,8,100,30,1024,96.81,94.65,90.09,89.72,1.0,15,3
9,5,100,80,7000,92.75,91.97999999999999,89.91,89.91,1.0,1,1
6,5,100,20,7000,96.85000000000001,93.65,90.42,90.42,1.0,1,1
0,5,100,40,7000,97.71,92.96,90.25999999999999,90.2,1.0,1,1
2,3,100,10,2048,95.78,93.77,92.05,89.68,1.0,15,3
2,5,100,20,32,91.48666666666668,90.94333333333333,90.9,89.68,0.5,15,3
3,1,100,10,2048,94.40333333333332,90.33,90.53333333333333,89.9,0.5,15,3
3,5,100,20,32,91.63333333333334,90.19333333333334,90.28666666666668,89.9,0.5,15,3
8,10,100,50,256,92.53,90.69,90.25999999999999,90.25999999999999,1.0,15,3
3,5,100,10,64,93.03666666666666,90.16999999999999,90.25999999999999,89.9,0.5,15,3
2,3,100,20,2048,94.83,93.46666666666667,90.92,89.68,0.5,15,3
2,2,100,20,2048,95.15,93.37666666666667,91.14,89.68,0.5,15,3
8,10,100,40,128,91.61,90.33,90.25999999999999,90.25999999999999,0.75,15,3
2,3,100,10,1024,95.89,93.7,92.23,89.68,1.0,15,3
4,5,100,50,7000,95.00999999999999,92.33,90.18,90.18,1.0,1,1
2,5,100,80,7000,95.28999999999999,92.82000000000001,89.69,89.68,1.0,1,1
3,5,100,40,5000,94.8,93.15,89.9,89.9,1.0,1,1
0,2,100,60,2048,97.72999999999999,90.49333333333334,90.21000000000001,90.2,0.75,15,3
1,1,100,30,4096,98.15,88.64999999999999,88.66000000000001,88.64999999999999,1.0,15,3
7,5,100,20,64,93.95,92.27,90.17666666666668,89.72,0.5,15,3
7,2,100,20,1024,96.69,93.71000000000001,90.77,89.72,1.0,15,3
4,5,100,20,64,91.87333333333333,90.77,90.82666666666667,90.18,0.5,15,3
2,3,100,30,1024,95.25,94.58,90.77,89.68,1.0,15,3
4,5,100,20,16,91.34,90.35,90.34,90.18,1.0,15,3
1,5,100,20,5000,98.28,90.35,89.2,88.64999999999999,1.0,15,3
3,5,100,40,7000,95.08,93.71000000000001,90.09,89.9,1.0,1,1
2,10,100,30,1024,94.65,93.87666666666667,90.52333333333334,89.68,0.5,15,3
2,5,100,20,16,90.08,89.92,90.33,89.68,1.0,15,3
8,5,100,50,5000,93.46,91.25999999999999,90.25999999999999,90.25999999999999,1.0,15,3
3,2,100,10,2048,94.25333333333333,91.03999999999999,90.61333333333333,89.9,0.5,15,3
4,5,100,100,7000,94.95,90.75999999999999,90.18,90.18,1.0,1,1
2,1,100,40,5000,95.50999999999999,90.52,90.32,89.68,1.0,15,3
5,5,100,100,7000,93.38,91.63,91.08000000000001,91.08000000000001,1.0,1,1
5,5,100,80,7000,93.42,91.92,91.08000000000001,91.08000000000001,1.0,1,1
3,10,100,20,2048,95.13000000000001,93.19,90.78,89.9,1.0,15,3
7,5,100,50,7000,96.89,94.62,89.74,89.72,1.0,1,1
8,5,100,40,5000,92.66,91.02,90.25999999999999,90.25999999999999,1.0,1,1
7,5,100,10,7000,96.88,93.27,91.21000000000001,89.72,1.0,1,1
0,5,100,80,7000,97.89,91.17,90.2,90.2,1.0,1,1
7,1,100,40,5000,96.48,90.09,89.75999999999999,89.72,1.0,1,1
6,5,100,30,7000,96.78,94.43,90.42,90.42,1.0,1,1
7,5,100,100,5000,96.91,93.39,89.72,89.72,1.0,1,1
7,5,100,10,5000,97.07000000000001,93.89,91.18,89.72,1.0,15,3
4,1,100,20,2048,94.46333333333334,90.82000000000001,90.85,90.18,0.5,15,3
3,5,100,40,7000,94.84,93.35,89.94,89.9,1.0,1,1
3,5,100,10,256,93.68666666666667,90.64,90.25,89.9,0.5,15,3
3,3,100,30,2048,95.15,93.27,90.53999999999999,89.9,1.0,15,3
7,5,100,20,7000,96.76,94.11,91.02,89.72,1.0,1,1
6,5,100,60,7000,96.55,94.13,90.42,90.42,1.0,1,1
7,5,100,50,7000,96.58,94.81,89.75,89.72,1.0,1,1
6,2,100,40,2048,96.73333333333333,93.19666666666667,90.42,90.42,0.75,15,3
2,5,100,10,7000,95.66,93.51,92.4,89.68,1.0,1,1
3,3,100,20,2048,94.38666666666666,91.56666666666666,90.47666666666667,89.9,0.5,15,3
3,5,100,20,1024,94.75,92.58,90.93,89.9,1.0,15,3
2,5,100,20,64,93.17,90.05,89.99000000000001,89.68,1.0,15,3
2,3,100,30,4096,95.36,93.95,90.71666666666667,89.68,0.5,15,3
4,5,100,20,5000,94.88,92.36,91.17,90.18,1.0,1,1
8,5,100,60,7000,92.99,91.07,90.25999999999999,90.25999999999999,1.0,1,1
2,2,100,30,2048,95.47,94.31,90.97,89.68,1.0,15,3
3,5,100,10,4096,95.34,92.34,90.72,89.9,1.0,15,3
4,5,100,20,32,90.89,90.40333333333334,90.36666666666666,90.18,0.5,15,3
5,1,100,50,5000,93.06,91.08000000000001,91.08000000000001,91.08000000000001,1.0,1,1
8,5,100,50,5000,93.14,91.39,90.25999999999999,90.25999999999999,1.0,15,3
4,5,100,10,1024,93.99333333333333,91.43,90.81666666666666,90.18,0.5,15,3
7,5,100,30,32,94.02000000000001,89.74,89.73,89.72,1.0,15,3
3,8,100,30,1024,94.56333333333333,92.81666666666666,90.23,89.9,0.5,15,3
0,5,100,80,7000,97.86,91.81,90.2,90.2,1.0,1,1
7,3,100,10,2048,95.95666666666666,92.67333333333333,90.44333333333333,89.72,0.5,15,3
7,8,100,20,2048,96.43666666666667,94.17999999999999,90.42333333333333,89.72,0.5,15,3
2,2,100,20,2048,94.95333333333333,92.82333333333334,91.23333333333333,89.68,0.5,15,3
2,5,100,40,1024,95.32000000000001,94.66,90.56,89.68,1.0,15,3
7,2,100,20,2048,96.30666666666666,92.76666666666667,90.64666666666666,89.72,0.5,15,3
1,10,100,60,32,95.84,88.64999999999999,88.64999999999999,88.64999999999999,1.0,15,3
2,2,100,30,5000,95.94,94.62,90.95,89.68,1.0,10,3
7,3,100,30,2048,97.0,94.77,90.22,89.72,1.0,15,3
0,5,100,10,7000,97.99,91.43,90.2,90.2,1.0,1,1
4,8,100,30,2048,95.37,93.13,90.68,90.18,1.0,15,3
3,5,100,30,5000,95.03,92.9,90.35,89.9,1.0,1,1
6,2,100,30,512,96.82,93.44,90.42,90.42,1.0,15,3
6,5,100,40,7000,96.75,94.86,90.42,90.42,1.0,1,1
2,5,100,20,1024,95.39999999999999,94.73,91.88,89.68,1.0,15,3
4,3,100,80,64,92.94,90.25999999999999,90.18,90.18,1.0,15,3
3,5,100,30,6131,94.98333333333333,92.90333333333334,90.34333333333333,89.9,0.5,3,3
2,5,100,100,7000,95.6,91.67,89.68,89.68,1.0,1,1
2,5,100,30,16,90.05666666666666,90.53,89.83,89.68,0.5,15,3
2,3,100,30,2048,95.43,94.69999999999999,90.94,89.68,1.0,15,3
4,10,100,20,4096,95.46,93.36,91.3,90.18,1.0,15,3
3,5,100,50,7000,94.98,93.4,89.9,89.9,1.0,1,1
3,5,100,40,5000,95.21,93.58,90.13,89.9,1.0,15,3
2,5,100,20,128,93.09333333333333,92.36,90.82000000000001,89.68,0.5,15,3
3,5,100,10,16,91.07,89.90666666666667,89.97666666666667,89.9,0.5,15,3
8,2,100,40,5000,92.42,90.26666666666667,90.25999999999999,90.25999999999999,0.5,15,3
6,10,100,80,1024,96.89999999999999,93.89999999999999,90.42,90.42,1.0,15,3
7,3,100,100,512,96.19666666666666,91.79666666666667,89.72,89.72,0.75,15,3
2,5,100,10,4096,95.99,94.08,92.14,89.68,1.0,15,3
3,5,100,100,7000,95.00999999999999,92.51,89.9,89.9,1.0,1,1
8,5,100,80,7000,92.85,91.01,90.25999999999999,90.25999999999999,1.0,1,1
0,5,100,20,5000,97.86,92.08,90.25,90.2,1.0,15,3
2,1,100,20,4096,95.75,92.75999999999999,91.79,89.68,1.0,15,3
4,5,100,10,2048,93.92,91.59333333333333,90.83,90.18,0.5,15,3
3,5,100,10,2048,94.30333333333333,91.47999999999999,90.37666666666667,89.9,0.5,15,3
7,5,100,30,5000,96.59,93.89999999999999,90.06,89.72,1.0,1,1
4,5,100,80,7000,94.89999999999999,91.36999999999999,90.18,90.18,1.0,1,1
6,5,100,40,5000,97.05,94.76,90.42,90.42,1.0,15,3
4,2,100,40,512,94.78999999999999,91.78,90.19,90.18,1.0,15,3
4,1,100,20,4096,95.44,91.5,91.34,90.18,1.0,15,3
2,5,100,10,256,95.14,91.27,91.41,89.68,1.0,15,3
3,5,100,40,7000,95.04,94.05,89.95,89.9,1.0,1,1
3,2,100,20,4096,95.61,91.99000000000001,91.06,89.9,1.0,15,3
9,5,100,80,5000,92.60000000000001,92.17,89.91,89.91,1.0,15,3
2,5,100,60,7000,95.33,93.55,89.69,89.68,1.0,1,1
2,8,100,20,1024,95.56,94.54,91.89,89.68,1.0,15,3
3,5,100,50,7000,94.85,93.05,89.9,89.9,1.0,1,1
1,5,100,60,5000,98.17,95.02000000000001,88.64999999999999,88.64999999999999,1.0,15,3
1,1,100,40,32,96.35000000000001,88.64999999999999,88.64999999999999,88.64999999999999,1.0,15,3
3,5,100,50,7000,94.94,93.73,89.9,89.9,1.0,1,1
0,3,100,10,5000,97.69333333333333,91.06333333333333,90.2,90.2,0.75,15,3
8,5,100,10,7000,93.02,90.36,90.25999999999999,90.25999999999999,1.0,1,1
7,10,100,30,2048,96.82,94.57,90.09,89.72,1.0,15,3
4,10,100,20,2048,95.44,92.92,90.97,90.18,1.0,15,3
2,5,100,10,2048,95.93,94.51,91.91,89.68,1.0,15,3
4,2,100,20,4096,95.56,92.60000000000001,91.2,90.18,1.0,15,3
2,5,100,10,32,91.73333333333333,90.79333333333334,90.70666666666666,89.68,0.5,15,3
3,5,100,10,1024,94.23666666666666,91.27,90.53,89.9,0.5,15,3
3,5,100,50,7000,95.1,93.58999999999999,89.9,89.9,1.0,1,1
2,5,100,10,512,95.56,92.95,91.75999999999999,89.68,1.0,15,3
4,5,100,30,1024,94.16,91.86999999999999,90.37333333333333,90.18,0.5,15,3
2,5,100,20,16,90.53333333333333,91.21000000000001,90.2,89.68,0.5,15,3
7,5,100,10,2048,96.04333333333334,92.60666666666667,90.56333333333333,89.72,0.5,15,3
3,3,100,20,4096,94.62,91.75333333333333,90.59666666666666,89.9,0.5,15,3
4,5,100,30,32,90.96333333333332,90.28333333333333,90.19,90.18,0.5,15,3
2,5,100,50,7000,95.28,95.00999999999999,89.81,89.68,1.0,1,1
7,2,100,10,2048,96.82,93.82000000000001,91.14,89.72,1.0,15,3
5,5,100,40,5000,93.85,92.73,91.10000000000001,91.08000000000001,1.0,15,3
4,5,100,20,7000,95.1,92.38,91.14,90.18,1.0,1,1
6,5,100,20,7000,96.83,93.75,90.42,90.42,1.0,1,1
4,5,100,50,7000,95.00999999999999,92.04,90.18,90.18,1.0,1,1
2,5,100,20,32,92.35,91.2,90.01,89.68,1.0,15,3
4,2,100,20,4096,94.21000000000001,91.53,90.66,90.18,0.5,15,3
9,5,100,10,5000,93.45,90.36999999999999,89.91,89.91,1.0,15,3
2,5,100,20,1024,94.67999999999999,93.35333333333334,91.03333333333333,89.68,0.5,15,3
4,5,100,30,1024,94.05666666666667,91.79333333333334,90.34666666666666,90.18,0.5,15,3
8,5,100,10,5000,93.19,90.34,90.25999999999999,90.25999999999999,1.0,15,3
5,5,100,20,7000,93.58,91.47999999999999,91.08000000000001,91.08000000000001,1.0,1,1
2,5,100,20,7000,95.61,94.26,92.29,89.68,1.0,1,1
4,5,100,10,128,94.46,90.85,91.03999999999999,90.18,1.0,15,3
7,5,100,30,128,95.83,90.73,90.08,89.72,1.0,15,3
1,5,100,40,5000,97.68,91.08000000000001,88.64999999999999,88.64999999999999,1.0,1,1
6,5,100,40,5000,96.84,95.42,90.42,90.42,1.0,15,3
5,5,100,30,7000,93.53,92.19000000000001,91.08000000000001,91.08000000000001,1.0,1,1
3,5,100,30,5000,95.44,93.67999999999999,90.57,89.9,1.0,15,3
3,3,100,10,4096,95.65,91.89,90.86999999999999,89.9,1.0,15,3
4,5,100,10,5000,95.37,92.72,91.47999999999999,90.18,1.0,15,3
2,5,100,100,5000,95.76,92.03,89.68,89.68,1.0,15,3
7,5,100,40,5000,97.02,95.41,89.89,89.72,1.0,15,3
7,5,100,30,5000,96.78,95.06,90.06,89.72,1.0,15,3
5,5,100,10,5000,94.05,91.47999999999999,91.10000000000001,91.08000000000001,1.0,15,3
3,5,100,30,1024,94.99,93.22,90.64999999999999,89.9,1.0,15,3
4,5,100,50,7000,94.97,92.21000000000001,90.19,90.18,1.0,1,1
3,5,100,60,5000,95.42,93.93,89.9,89.9,1.0,15,3
7,5,100,30,32,92.25666666666666,91.30333333333334,89.94333333333333,89.72,0.5,15,3
0,5,100,40,7000,97.94,92.93,90.22,90.2,1.0,1,1
2,5,100,50,7000,95.41,94.54,89.75999999999999,89.68,1.0,1,1
4,2,100,10,2048,95.66,92.62,91.28,90.18,1.0,15,3
9,5,100,40,5000,92.92,93.14,89.92999999999999,89.91,1.0,15,3
9,1,100,50,64,90.05,89.94,89.92999999999999,89.91,1.0,15,3
3,5,100,30,64,92.91,89.99000000000001,90.05,89.9,1.0,15,3
7,5,100,10,128,96.1,90.82000000000001,90.63,89.72,1.0,15,3
3,5,100,10,256,95.16,90.59,90.9,89.9,1.0,15,3
7,5,100,20,5000,97.15,94.76,90.69,89.72,1.0,15,3
4,1,100,30,5000,94.95,90.75,90.4,90.18,1.0,1,1
0,5,100,20,7000,97.78999999999999,91.5,90.21000000000001,90.2,1.0,1,1
1,5,100,50,5000,98.08,93.77,88.64999999999999,88.64999999999999,1.0,15,3
2,1,100,10,2048,95.94,92.83,91.92,89.68,1.0,15,3
6,5,100,40,7000,96.89999999999999,94.87,90.42,90.42,1.0,1,1
8,1,100,80,5000,92.74,90.25999999999999,90.25999999999999,90.25999999999999,1.0,1,1
6,5,100,100,5000,97.3,92.85,90.42,90.42,1.0,15,3
4,5,100,30,16,90.44666666666666,90.21666666666667,90.19,90.18,0.5,15,3
7,8,100,20,2048,97.09,94.92,90.8,89.72,1.0,15,3
3,5,100,30,1024,95.13000000000001,93.37,90.67,89.9,1.0,15,3
4,5,100,20,512,94.8,92.14,90.95,90.18,1.0,15,3
3,1,100,30,2048,95.39,90.4,90.52,89.9,1.0,15,3
3,1,100,10,1024,94.41000000000001,90.14333333333333,90.48666666666666,89.9,0.5,15,3
7,5,100,10,32,94.30333333333333,90.71666666666667,90.17333333333333,89.72,0.5,15,3
5,5,100,40,7000,93.44,92.61,91.08000000000001,91.08000000000001,1.0,1,1
9,5,100,100,7000,92.54,91.22,89.91,89.91,1.0,1,1
6,5,100,30,5000,96.89,95.3,90.42,90.42,1.0,15,3
8,1,100,30,5000,92.64,90.27,90.25999999999999,90.25999999999999,1.0,1,1
7,2,100,30,4096,96.64,93.21666666666667,90.18,89.72,0.75,15,3
3,1,100,30,4096,95.28,90.23,90.55,89.9,1.0,15,3
2,1,100,60,2048,94.36666666666666,89.69,89.69333333333334,89.68,0.25,15,3
6,5,100,20,5000,97.3,94.3,90.42,90.42,1.0,15,3
4,1,100,30,5000,94.78,90.60000000000001,90.39,90.18,1.0,1,1
9,5,100,60,7000,92.53,91.59,89.91,89.91,1.0,1,1
7,5,100,20,5000,96.53,93.66,90.75,89.72,1.0,1,1
8,1,100,20,256,92.56,90.25999999999999,90.25999999999999,90.25999999999999,1.0,15,3
2,5,100,30,2048,95.52000000000001,94.84,90.91,89.68,1.0,15,3
9,5,100,50,7000,92.72,92.03,89.91,89.91,1.0,1,1
7,5,100,30,5000,97.28999999999999,94.77,90.39,89.72,1.0,15,3
1,5,100,10,7000,97.88,88.69,89.35,88.64999999999999,1.0,1,1
4,5,100,10,1024,95.26,92.46,91.47999999999999,90.18,1.0,15,3
2,5,100,80,7000,95.21,92.85,89.69,89.68,1.0,1,1
4,1,100,80,5000,94.74000000000001,90.18,90.18,90.18,1.0,1,1
3,5,100,30,7000,95.26,93.47999999999999,90.47,89.9,1.0,1,1
2,3,100,30,2048,95.01666666666667,93.8,90.60000000000001,89.68,0.5,15,3
2,8,100,10,2048,94.71000000000001,93.26333333333334,91.24333333333333,89.68,0.5,15,3
9,2,100,30,64,89.92333333333333,89.91333333333333,89.91333333333333,89.91,0.25,15,3
7,5,100,20,1024,96.67999999999999,94.31,90.68,89.72,1.0,15,3
3,5,100,30,1024,94.42,92.78333333333333,90.31,89.9,0.5,15,3
2,5,100,30,128,93.02666666666667,93.25666666666666,90.64,89.68,0.5,15,3
4,5,100,30,2048,94.30666666666667,91.78,90.38000000000001,90.18,0.5,15,3
6,5,100,20,5000,96.89,94.17,90.42,90.42,1.0,15,3
4,5,100,30,7000,94.65,92.14,90.45,90.18,1.0,1,1
3,5,100,60,7000,95.05,93.33,89.9,89.9,1.0,1,1
4,1,100,30,2048,95.06,90.81,90.64999999999999,90.18,1.0,15,3
5,5,100,20,5000,93.33,91.46,91.09,91.08000000000001,1.0,1,1
2,1,100,10,5000,95.30999999999999,91.74,92.03,89.68,1.0,1,1
1,1,100,50,5000,97.74000000000001,88.64999999999999,88.64999999999999,88.64999999999999,1.0,1,1
2,5,100,20,5000,95.97,94.75,91.75,89.68,1.0,15,3
3,3,100,30,2048,94.67999999999999,92.32000000000001,90.29666666666667,89.9,0.5,15,3
2,5,100,30,64,92.2,92.32000000000001,90.53333333333333,89.68,0.5,15,3
8,5,100,40,7000,92.88,90.96,90.25999999999999,90.25999999999999,1.0,1,1
8,3,100,30,16,90.25999999999999,90.25999999999999,90.25999999999999,90.25999999999999,0.5,15,3
4,8,100,20,2048,94.31,92.07,90.82000000000001,90.18,0.5,15,3
0,1,100,20,5000,97.85000000000001,90.22,90.22,90.2,1.0,1,1
2,1,100,10,1024,94.64,91.06666666666666,91.18333333333334,89.68,0.5,15,3
7,5,100,40,1024,96.93,94.97,90.08,89.72,1.0,15,3
4,1,100,10,2048,95.3,91.57,91.42,90.18,1.0,15,3
5,5,100,20,32,91.12,91.08000000000001,91.08000000000001,91.08000000000001,1.0,15,3
9,5,100,20,5000,93.39,91.34,90.0,89.91,1.0,15,3
4,3,100,20,2048,95.42,92.78,91.21000000000001,90.18,1.0,15,3
3,5,100,40,1024,95.16,93.42,90.2,89.9,1.0,15,3
6,5,100,30,5000,96.55,93.47,90.42,90.42,1.0,1,1
4,1,100,30,2048,94.33333333333334,90.49666666666667,90.30666666666667,90.18,0.5,15,3
3,8,100,10,2048,95.6,92.03,90.75,89.9,1.0,15,3
3,2,100,30,2048,94.77,91.62333333333333,90.34333333333333,89.9,0.5,15,3
4,5,100,40,5000,95.09,92.42,90.19,90.18,1.0,1,1
2,8,100,10,4096,96.08,94.67,91.89,89.68,1.0,15,3
2,10,100,10,1024,95.77,94.02000000000001,92.19000000000001,89.68,1.0,15,3
6,5,100,40,1024,96.89,94.85,90.42,90.42,1.0,15,3
7,1,100,10,64,95.48333333333333,90.36,90.43333333333334,89.72,0.75,15,3
2,5,100,30,5958,95.42666666666668,94.1,90.63666666666667,89.68,0.5,3,3
5,5,100,40,1024,93.15,92.36,91.09,91.08000000000001,1.0,15,3
2,3,100,20,2048,95.00333333333333,93.53666666666666,91.19333333333334,89.68,0.5,15,3
2,1,100,10,2048,95.72,92.47,91.95,89.68,1.0,15,3
3,5,100,50,7000,95.02000000000001,93.30000000000001,89.9,89.9,1.0,1,1
2,5,100,20,256,94.84,92.84,92.02,89.68,1.0,15,3
2,2,100,20,4096,96.04,94.28999999999999,92.21000000000001,89.68,1.0,15,3
2,2,100,10,1024,94.73,92.61333333333333,90.82666666666667,89.68,0.5,15,3
2,2,100,60,256,94.02000000000001,91.66333333333333,89.7,89.68,0.75,15,3
0,5,100,40,5000,98.03,93.45,90.29,90.2,1.0,15,3
2,5,100,30,128,92.91333333333334,92.24333333333334,90.30333333333334,89.68,0.5,15,3
2,5,100,20,32,93.11,91.74,89.91,89.68,1.0,15,3
3,5,100,50,7000,94.77,93.72,89.9,89.9,1.0,1,1
3,2,100,100,512,94.86333333333333,90.95666666666666,89.9,89.9,0.75,15,3
2,5,100,10,16,90.84,90.42666666666666,89.96,89.68,0.5,15,3
3,8,100,10,2048,94.31,91.67999999999999,90.33333333333333,89.9,0.5,15,3
7,5,100,100,5000,97.05,93.57,89.72,89.72,1.0,15,3
9,5,100,10,7000,92.83,90.25999999999999,89.92,89.91,1.0,1,1
2,10,100,30,5000,95.92,95.17999999999999,90.88000000000001,89.68,1.0,10,3
2,1,100,30,4096,95.07333333333334,91.27666666666666,90.61666666666667,89.68,0.5,15,3
3,2,100,20,2048,94.59,91.16333333333333,90.34,89.9,0.5,15,3
2,5,100,20,256,94.72,92.64,91.36,89.68,1.0,15,3
0,1,100,50,128,97.1,90.2,90.2,90.2,1.0,15,3
2,2,100,20,2048,95.06333333333333,93.30666666666667,91.14333333333333,89.68,0.5,15,3
3,5,100,30,5000,95.36,93.61,90.36,89.9,1.0,15,3
8,5,100,10,7000,93.26,90.28,90.25999999999999,90.25999999999999,1.0,1,1
2,5,100,20,32,91.64,91.25333333333333,90.34666666666666,89.68,0.5,15,3
6,5,100,60,5000,97.11999999999999,94.94,90.42,90.42,1.0,15,3
2,3,100,20,2048,95.04333333333334,93.64333333333333,91.18,89.68,0.5,15,3
2,5,100,10,7000,95.67999999999999,93.5,92.10000000000001,89.68,1.0,1,1
3,5,100,50,5000,94.97,93.73,89.9,89.9,1.0,1,1
2,2,100,20,1024,94.94,93.22,91.32666666666667,89.68,0.5,15,3
2,8,100,20,2048,95.7,94.71000000000001,92.02,89.68,1.0,15,3
6,5,100,40,7000,96.69,94.98,90.42,90.42,1.0,1,1
1,5,100,20,5000,98.3,89.63,89.25,88.64999999999999,1.0,15,3
6,5,100,50,128,95.98,91.59,90.42,90.42,1.0,15,3
3,5,100,20,256,93.67999999999999,91.2,90.38000000000001,89.9,0.5,15,3
5,5,100,40,5000,94.02000000000001,93.16,91.08000000000001,91.08000000000001,1.0,15,3
4,5,100,30,32,92.5,90.25,90.19,90.18,1.0,15,3
3,5,100,40,5000,95.30999999999999,93.87,90.18,89.9,1.0,15,3
0,5,100,20,5000,98.14,92.22,90.2,90.2,1.0,15,3
4,5,100,10,7000,94.96,91.83,91.32000000000001,90.18,1.0,1,1
7,5,100,10,7000,96.97,93.35,91.17,89.72,1.0,1,1
3,3,100,30,1024,95.06,92.84,90.49000000000001,89.9,1.0,15,3
2,1,100,30,5000,95.73,91.88,91.12,89.68,1.0,10,3
2,5,100,10,256,95.09,91.55,91.47999999999999,89.68,1.0,15,3
3,2,100,10,2048,94.46666666666667,90.96,90.58333333333334,89.9,0.5,15,3
3,3,100,10,4096,94.08333333333333,91.26333333333334,90.53333333333333,89.9,0.5,15,3
2,1,100,30,2048,95.47,91.69,91.09,89.68,1.0,15,3
2,5,100,10,256,94.98,91.59,91.79,89.68,1.0,15,3
4,1,100,20,2048,95.30999999999999,91.13,91.25,90.18,1.0,15,3
4,5,100,30,256,93.00333333333334,91.2,90.28,90.18,0.5,15,3
5,5,100,40,128,91.42666666666666,91.49666666666667,91.08000000000001,91.08000000000001,0.5,15,3
7,5,100,10,128,96.0,91.14,91.36999999999999,89.72,1.0,15,3
7,5,100,50,7000,96.71,95.02000000000001,89.75,89.72,1.0,1,1
2,5,100,20,16,91.36999999999999,90.94,90.24,89.68,1.0,15,3
5,1,100,40,5000,93.30000000000001,91.08000000000001,91.08000000000001,91.08000000000001,1.0,1,1
2,5,100,40,1024,95.36,95.05,90.60000000000001,89.68,1.0,15,3
2,5,100,20,5000,95.75,94.84,91.96,89.68,1.0,15,3
6,5,100,20,7000,96.8,93.82000000000001,90.42,90.42,1.0,1,1
5,5,100,40,7000,93.62,92.30000000000001,91.08000000000001,91.08000000000001,1.0,1,1
8,5,100,60,7000,92.92,90.91,90.25999999999999,90.25999999999999,1.0,1,1
8,5,100,10,7000,92.86999999999999,90.35,90.25999999999999,90.25999999999999,1.0,1,1
5,5,100,50,5000,93.7,92.67,91.08000000000001,91.08000000000001,1.0,15,3
0,2,100,80,256,97.27,90.49000000000001,90.2,90.2,1.0,15,3
2,3,100,10,2048,94.64666666666666,92.7,91.15333333333334,89.68,0.5,15,3
4,2,100,30,5000,93.38666666666666,90.45,90.21000000000001,90.18,0.25,15,3
3,5,100,20,16,90.60333333333334,90.06333333333333,90.08666666666667,89.9,0.5,15,3
2,5,100,10,16,91.14,90.25999999999999,90.18,89.68,0.5,15,3
6,5,100,30,5000,97.02,94.88,90.42,90.42,1.0,15,3
3,5,100,30,7000,95.11,93.11,90.72,89.9,1.0,1,1
3,8,100,20,2048,94.55333333333333,92.34,90.60000000000001,89.9,0.5,15,3
3,3,100,20,2048,95.19999999999999,92.78999999999999,90.97,89.9,1.0,15,3
7,3,100,30,2048,96.98,94.38,90.2,89.72,1.0,15,3
8,1,100,20,1024,93.27,90.25999999999999,90.25999999999999,90.25999999999999,1.0,15,3
2,1,100,10,2048,94.82000000000001,91.05333333333333,91.02333333333334,89.68,0.5,15,3
3,5,100,40,5000,95.22,93.41000000000001,90.10000000000001,89.9,1.0,15,3
3,5,100,30,4096,95.28999999999999,93.64,90.42,89.9,1.0,15,3
4,5,100,20,1024,94.1,91.88333333333333,90.91333333333334,90.18,0.5,15,3
7,5,100,30,16,91.02,91.39,89.96666666666667,89.72,0.5,15,3
4,1,100,60,5000,94.78999999999999,90.18,90.18,90.18,1.0,1,1
9,5,100,30,7000,92.67,91.58,89.91,89.91,1.0,1,1
4,5,100,30,256,94.28999999999999,91.61,90.46,90.18,1.0,15,3
2,5,100,20,2048,95.14,93.72666666666667,91.17666666666666,89.68,0.5,15,3
5,5,100,80,5000,93.85,92.89,91.08000000000001,91.08000000000001,1.0,15,3
7,5,100,20,32,93.19333333333333,91.35333333333332,90.17333333333333,89.72,0.5,15,3
2,10,100,10,1024,94.48,92.58999999999999,90.86999999999999,89.68,0.5,15,3
6,5,100,30,7000,96.66,93.97,90.42,90.42,1.0,1,1
4,10,100,60,2048,95.15,92.10000000000001,90.18,90.18,1.0,15,3
0,5,100,40,5000,97.98,93.64,90.24,90.2,1.0,15,3
1,2,100,40,16,92.85666666666667,88.64999999999999,88.64999999999999,88.64999999999999,0.5,15,3
3,5,100,20,16,90.05666666666666,89.90333333333334,89.94,89.9,0.5,15,3
2,10,100,20,4096,95.75,95.04,91.92,89.68,1.0,15,3
4,2,100,50,256,91.83666666666667,90.41666666666667,90.18,90.18,0.25,15,3
3,8,100,30,2048,95.22,93.43,90.45,89.9,1.0,15,3
4,5,100,40,5000,95.25,93.37,90.23,90.18,1.0,15,3
3,5,100,30,1024,94.83,93.04,90.5,89.9,1.0,15,3
0,5,100,80,5000,98.16,91.47999999999999,90.21000000000001,90.2,1.0,15,3
2,5,100,30,5958,95.44,94.98,91.09,89.68,1.0,3,3
3,10,100,10,4096,95.58,92.31,90.97,89.9,1.0,15,3
6,5,100,50,7000,96.39,94.59,90.42,90.42,1.0,1,1
4,5,100,30,5000,95.28,93.04,90.53,90.18,1.0,15,3
8,1,100,40,5000,92.80000000000001,90.25999999999999,90.25999999999999,90.25999999999999,1.0,1,1
1,5,100,10,7000,97.95,88.94999999999999,89.49000000000001,88.64999999999999,1.0,1,1
3,5,100,100,5000,95.13000000000001,92.97999999999999,89.9,89.9,1.0,15,3
7,5,100,30,5000,97.05,95.22,90.23,89.72,1.0,15,3
4,2,100,20,1024,94.05,91.42333333333333,90.61,90.18,0.5,15,3
1,1,100,10,5000,97.96000000000001,88.68,89.3,88.64999999999999,1.0,1,1
9,5,100,50,7000,92.58,92.0,89.91,89.91,1.0,1,1
7,2,100,20,2048,96.89999999999999,94.19999999999999,90.51,89.72,1.0,15,3
5,5,100,80,7000,93.19,92.25,91.08000000000001,91.08000000000001,1.0,1,1
2,10,100,30,5000,95.49,95.24000000000001,91.25,89.68,1.0,10,3
3,10,100,10,2048,95.38,92.07,90.83,89.9,1.0,15,3
2,5,100,20,16,91.43,91.60000000000001,90.36999999999999,89.68,1.0,15,3
5,5,100,10,7000,93.32000000000001,91.2,91.10000000000001,91.08000000000001,1.0,1,1
9,2,100,40,16,89.93333333333334,89.91,89.91,89.91,0.5,15,3
2,3,100,10,2048,95.89999999999999,93.95,91.97,89.68,1.0,15,3
2,1,100,30,1024,95.53,91.96,90.85,89.68,1.0,15,3
2,3,100,20,512,95.00999999999999,93.53,92.09,89.68,1.0,15,3
3,1,100,10,2048,95.82000000000001,90.7,90.88000000000001,89.9,1.0,15,3
4,5,100,80,5000,94.67,91.52,90.18,90.18,1.0,1,1
5,5,100,100,7000,93.39,91.56,91.08000000000001,91.08000000000001,1.0,1,1
4,10,100,10,1024,94.05,91.52,90.75333333333333,90.18,0.5,15,3
4,5,100,30,2048,95.13000000000001,93.17999999999999,90.48,90.18,1.0,15,3
5,5,100,40,5000,93.77,92.61,91.08000000000001,91.08000000000001,1.0,15,3
7,5,100,40,1024,96.89999999999999,94.76,89.92999999999999,89.72,1.0,15,3
7,5,100,80,7000,96.87,93.97,89.73,89.72,1.0,1,1
7,5,100,30,16,90.97333333333333,89.86,89.74,89.72,0.5,15,3
0,5,100,40,7000,97.71,93.4,90.22,90.2,1.0,1,1
1,5,100,50,64,95.46333333333334,89.87666666666667,88.64999999999999,88.64999999999999,0.5,15,3
2,5,100,30,64,92.13,91.58333333333334,90.45,89.68,0.5,15,3
8,1,100,20,5000,93.05,90.25999999999999,90.25999999999999,90.25999999999999,1.0,1,1
1,1,100,20,16,93.59333333333333,88.64999999999999,88.64999999999999,88.64999999999999,0.5,15,3
9,5,100,40,5000,93.10000000000001,92.25,89.96,89.91,1.0,15,3
0,5,100,80,5000,98.26,91.51,90.21000000000001,90.2,1.0,15,3
6,5,100,30,5000,97.18,94.81,90.42,90.42,1.0,15,3
0,1,100,60,5000,97.78999999999999,90.21000000000001,90.22,90.2,1.0,1,1
5,5,100,40,64,91.19333333333334,91.52,91.08000000000001,91.08000000000001,0.5,15,3
2,5,100,10,5000,95.82000000000001,94.13,92.17,89.68,1.0,15,3
7,5,100,40,7000,96.7,95.07,89.75999999999999,89.72,1.0,1,1
7,5,100,20,5000,97.00999999999999,94.56,90.62,89.72,1.0,15,3
3,5,100,30,1024,94.33666666666667,92.42666666666666,90.27666666666667,89.9,0.5,15,3
3,8,100,20,2048,95.36,93.17,90.98,89.9,1.0,15,3
4,5,100,30,64,91.39,90.91,90.38000000000001,90.18,0.5,15,3
5,1,100,100,5000,93.24,91.09,91.08000000000001,91.08000000000001,1.0,1,1
0,5,100,40,5000,97.94,93.83,90.21000000000001,90.2,1.0,15,3
2,5,100,30,16,90.91,90.82000000000001,89.8,89.68,1.0,15,3
4,5,100,10,7000,95.07,92.2,91.34,90.18,1.0,1,1
3,8,100,20,2048,95.15,92.82000000000001,90.9,89.9,1.0,15,3
5,5,100,10,5000,94.03,91.41,91.09,91.08000000000001,1.0,15,3
3,8,100,30,4096,95.48,93.97,90.57,89.9,1.0,15,3
3,5,100,60,5000,95.28999999999999,93.8,89.9,89.9,1.0,15,3
4,5,100,50,256,93.14333333333333,91.3,90.18,90.18,0.5,15,3
7,10,100,20,32,90.72666666666666,90.04333333333334,89.98666666666666,89.72,0.25,15,3
4,1,100,10,2048,93.78333333333333,90.46666666666667,90.60666666666667,90.18,0.5,15,3
6,5,100,30,7000,96.76,94.36,90.42,90.42,1.0,1,1
7,1,100,10,2048,97.13000000000001,91.73,91.32000000000001,89.72,1.0,15,3
2,8,100,20,2048,95.00666666666666,93.77,91.3,89.68,0.5,15,3
6,1,100,20,5000,96.57,90.45,90.42,90.42,1.0,1,1
2,5,100,20,1024,94.84,93.57,91.31666666666666,89.68,0.5,15,3
2,10,100,20,2048,95.85000000000001,94.77,92.0,89.68,1.0,15,3
3,5,100,20,2048,95.19,92.62,91.21000000000001,89.9,1.0,15,3
3,5,100,20,128,94.3,90.36,90.71000000000001,89.9,1.0,15,3
3,5,100,30,256,93.59666666666666,92.23666666666666,90.24333333333333,89.9,0.5,15,3
3,5,100,20,512,94.22,91.83666666666667,90.68666666666667,89.9,0.5,15,3
7,5,100,10,5000,96.71,93.35,90.8,89.72,1.0,1,1
3,3,100,10,4096,94.32000000000001,91.05,90.52,89.9,0.5,15,3
6,5,100,40,5000,97.03,95.25,90.42,90.42,1.0,15,3
4,5,100,40,1024,94.67999999999999,92.60000000000001,90.22,90.18,1.0,15,3
9,5,100,20,5000,93.58,91.41,89.99000000000001,89.91,1.0,15,3
4,5,100,40,1024,94.85,93.08,90.2,90.18,1.0,15,3
3,1,100,20,2048,95.06,91.05,91.07,89.9,1.0,15,3
3,5,100,50,7000,94.73,94.05,89.91,89.9,1.0,1,1
9,5,100,40,1024,92.69,91.83,89.91,89.91,1.0,15,3
2,2,100,30,2048,94.99,92.83333333333333,90.63666666666667,89.68,0.5,15,3
7,5,100,30,2048,96.41,94.16333333333333,90.07333333333334,89.72,0.5,15,3
0,5,100,60,5000,97.94,92.33,90.23,90.2,1.0,15,3
6,5,100,80,5000,96.43,93.21000000000001,90.42,90.42,1.0,1,1
3,5,100,10,64,94.21000000000001,90.14999999999999,90.25999999999999,89.9,1.0,15,3
7,5,100,20,256,96.26,93.28,90.79,89.72,1.0,15,3
3,5,100,40,5000,94.99,93.46,89.92999999999999,89.9,1.0,1,1
2,8,100,20,2048,94.92333333333333,93.78333333333333,91.26666666666667,89.68,0.5,15,3
4,5,100,40,1024,94.89,93.12,90.23,90.18,1.0,15,3
9,5,100,40,1024,92.14,91.75,89.94,89.91,1.0,15,3
2,3,100,20,512,93.05666666666667,91.84666666666666,90.30333333333334,89.68,0.25,15,3
4,5,100,20,16,90.51333333333334,90.43333333333334,90.30666666666667,90.18,0.5,15,3
2,3,100,20,2048,95.96000000000001,95.00999999999999,92.01,89.68,1.0,15,3
2,1,100,10,1024,95.75,92.39,91.77,89.68,1.0,15,3
2,5,100,20,512,94.51666666666667,93.49666666666666,91.09,89.68,0.5,15,3
9,5,100,40,7000,92.54,92.2,89.92999999999999,89.91,1.0,1,1
5,5,100,40,1024,93.11,92.43,91.09,91.08000000000001,1.0,15,3
9,5,100,80,7000,92.60000000000001,91.81,89.91,89.91,1.0,1,1
0,5,100,80,5000,98.08,91.45,90.2,90.2,1.0,15,3
9,2,100,10,2048,90.73,89.91,89.91,89.91,0.25,15,3
2,3,100,30,1024,94.51333333333334,93.60333333333334,90.60666666666667,89.68,0.5,15,3
8,5,100,100,7000,92.95,90.48,90.25999999999999,90.25999999999999,1.0,1,1
0,5,100,40,5000,98.13,93.55,90.32,90.2,1.0,15,3
3,3,100,40,2048,94.54666666666667,92.57333333333332,90.06333333333333,89.9,0.5,15,3
4,5,100,100,5000,95.55,91.24,90.18,90.18,1.0,15,3
2,8,100,20,2048,95.04333333333334,93.76666666666667,91.19666666666667,89.68,0.5,15,3
9,5,100,60,5000,92.67,92.53,89.91,89.91,1.0,15,3
3,2,100,20,256,94.45,90.97,90.83,89.9,1.0,15,3
3,10,100,10,2048,95.5,92.25999999999999,90.99000000000001,89.9,1.0,15,3
7,5,100,20,32,94.74000000000001,90.10000000000001,89.86,89.72,1.0,15,3
8,5,100,50,7000,92.74,90.94,90.25999999999999,90.25999999999999,1.0,1,1
1,5,100,40,7000,97.96000000000001,92.17,88.64999999999999,88.64999999999999,1.0,1,1
7,3,100,40,64,91.18,91.54666666666667,89.74333333333333,89.72,0.25,15,3
2,5,100,20,128,93.41333333333334,92.45666666666666,91.28666666666668,89.68,0.5,15,3
2,5,100,10,1024,94.5,92.84333333333333,91.00333333333333,89.68,0.5,15,3
2,5,100,10,2048,94.68333333333334,92.99333333333333,91.10333333333334,89.68,0.5,15,3
7,5,100,40,5000,96.99,95.39999999999999,89.92,89.72,1.0,15,3
3,3,100,30,2048,94.70666666666666,92.53666666666666,90.25333333333333,89.9,0.5,15,3
7,5,100,20,5000,97.25,94.64,90.64999999999999,89.72,1.0,15,3
2,1,100,30,2048,95.62,91.34,91.03999999999999,89.68,1.0,15,3
2,1,100,20,1024,95.48,93.07,91.79,89.68,1.0,15,3
4,5,100,50,7000,95.0,92.28,90.18,90.18,1.0,1,1
2,5,100,10,2048,95.96000000000001,94.21000000000001,92.17,89.68,1.0,15,3
4,5,100,10,5000,95.50999999999999,92.67999999999999,91.38,90.18,1.0,15,3
7,8,100,20,2048,96.37333333333333,93.84333333333333,90.54666666666667,89.72,0.5,15,3
8,5,100,80,5000,93.27,90.93,90.25999999999999,90.25999999999999,1.0,15,3
7,3,100,20,2048,96.92,94.57,90.84,89.72,1.0,15,3
4,5,100,20,256,93.47,91.26666666666667,90.49666666666667,90.18,0.5,15,3
9,5,100,60,7000,92.71000000000001,92.11,89.91,89.91,1.0,1,1
8,5,100,80,7000,92.60000000000001,90.89,90.25999999999999,90.25999999999999,1.0,1,1
2,5,100,20,512,94.37333333333333,93.33,91.23333333333333,89.68,0.5,15,3
8,5,100,100,7000,93.10000000000001,90.46,90.25999999999999,90.25999999999999,1.0,1,1
4,10,100,20,2048,95.25,92.95,91.28,90.18,1.0,15,3
0,5,100,40,1024,97.74000000000001,92.77,90.25,90.2,1.0,15,3
0,5,100,80,5000,98.17,91.78,90.24,90.2,1.0,15,3
3,2,100,10,2048,94.46333333333334,90.89,90.46,89.9,0.5,15,3
2,1,100,20,2048,94.93666666666667,91.84666666666666,91.25999999999999,89.68,0.5,15,3
4,5,100,20,16,90.53,90.21666666666667,90.19,90.18,0.5,15,3
2,1,100,20,2048,95.17999999999999,91.53666666666666,91.17333333333333,89.68,0.5,15,3
7,2,100,20,2048,96.85000000000001,93.97999999999999,90.67,89.72,1.0,15,3
7,1,100,20,2048,96.24666666666667,91.08000000000001,90.57666666666667,89.72,0.5,15,3
1,5,100,20,5000,98.39,89.95,89.14999999999999,88.64999999999999,1.0,15,3
3,5,100,30,1024,95.07,93.14,90.61,89.9,1.0,15,3
1,5,100,60,5000,97.77,94.03,88.64999999999999,88.64999999999999,1.0,1,1
3,5,100,40,5000,95.21,93.83,90.23,89.9,1.0,15,3
2,1,100,20,1024,95.42,93.07,92.0,89.68,1.0,15,3
3,5,100,30,2048,94.44666666666667,92.57666666666667,90.32333333333334,89.9,0.5,15,3
2,5,100,10,16,91.84,90.10000000000001,90.03,89.68,1.0,15,3
1,5,100,30,5000,98.24000000000001,90.33,88.69,88.64999999999999,1.0,15,3
7,1,100,20,4096,96.23333333333333,90.86666666666666,90.43333333333334,89.72,0.5,15,3
4,5,100,60,7000,94.77,91.64,90.18,90.18,1.0,1,1
1,5,100,80,7000,97.86,93.94,88.64999999999999,88.64999999999999,1.0,1,1
8,5,100,40,1024,92.99,91.28,90.25999999999999,90.25999999999999,1.0,15,3
7,5,100,60,7000,96.78999999999999,94.35,89.74,89.72,1.0,1,1
0,5,100,100,7000,97.8,91.41,90.2,90.2,1.0,1,1
8,5,100,100,7000,93.12,90.4,90.25999999999999,90.25999999999999,1.0,1,1
7,5,100,10,128,95.37333333333333,91.44,90.68333333333334,89.72,0.5,15,3
7,5,100,20,5000,97.09,94.76,90.89,89.72,1.0,15,3
5,10,100,40,16,91.08666666666667,91.08000000000001,91.08000000000001,91.08000000000001,0.75,15,3
1,5,100,50,7000,97.72,94.06,88.64999999999999,88.64999999999999,1.0,1,1
7,1,100,50,5000,96.69,89.84,89.75999999999999,89.72,1.0,1,1
3,5,100,10,5000,95.44,92.34,90.94,89.9,1.0,15,3
3,5,100,10,512,94.01,91.08333333333334,90.45666666666666,89.9,0.5,15,3
8,5,100,40,7000,92.74,91.07,90.25999999999999,90.25999999999999,1.0,1,1
2,5,100,20,1024,95.58,94.08999999999999,91.57,89.68,1.0,15,3
5,5,100,100,7000,93.43,91.99000000000001,91.08000000000001,91.08000000000001,1.0,1,1
2,5,100,30,1024,94.75666666666666,93.84666666666666,90.57,89.68,0.5,15,3
1,1,100,40,16,90.54333333333334,88.64999999999999,88.64999999999999,88.64999999999999,0.25,15,3
3,3,100,10,4096,94.23,91.03999999999999,90.46666666666667,89.9,0.5,15,3
1,5,100,20,5000,97.99,89.7,88.94999999999999,88.64999999999999,1.0,1,1
3,5,100,40,7000,94.92,93.53,89.92999999999999,89.9,1.0,1,1
9,5,100,20,5000,93.17,90.89,89.97,89.91,1.0,15,3
4,5,100,60,7000,94.87,91.58,90.18,90.18,1.0,1,1
6,5,100,20,5000,97.07000000000001,94.56,90.42,90.42,1.0,15,3
6,5,100,100,7000,96.97,92.64,90.42,90.42,1.0,1,1
3,2,100,20,2048,94.79333333333332,91.15666666666667,90.65333333333334,89.9,0.5,15,3
2,3,100,20,1024,95.28999999999999,94.25,91.94,89.68,1.0,15,3
2,1,100,30,2048,95.62,91.93,91.13,89.68,1.0,15,3
3,5,100,30,32,90.82000000000001,90.11,89.91666666666667,89.9,0.5,15,3
5,1,100,20,256,92.24,91.09,91.08000000000001,91.08000000000001,1.0,15,3
3,5,100,60,7000,94.89,93.64,89.9,89.9,1.0,1,1
9,3,100,10,16,90.18,89.91,89.91,89.91,1.0,15,3
8,5,100,20,7000,93.01,90.75999999999999,90.25999999999999,90.25999999999999,1.0,1,1
9,5,100,80,64,90.46,90.32,89.91,89.91,0.75,15,3
1,5,100,40,5000,97.84,90.91,88.64999999999999,88.64999999999999,1.0,1,1
3,8,100,30,4096,95.45,93.56,90.71000000000001,89.9,1.0,15,3
0,1,100,50,5000,97.89999999999999,90.2,90.23,90.2,1.0,1,1
2,1,100,40,128,92.90666666666667,90.03,90.17333333333333,89.68,0.5,15,3
8,1,100,60,5000,92.52,90.27,90.25999999999999,90.25999999999999,1.0,1,1
4,10,100,30,1024,95.05,93.17,90.59,90.18,1.0,15,3
2,8,100,30,1024,94.88333333333333,93.76333333333334,90.52666666666667,89.68,0.5,15,3
4,1,100,20,2048,94.06333333333333,90.66666666666666,90.74333333333333,90.18,0.5,15,3
3,10,100,30,6131,95.63000000000001,93.71000000000001,90.44,89.9,1.0,3,3
7,2,100,10,4096,96.01666666666667,92.37333333333333,90.63666666666667,89.72,0.5,15,3
6,5,100,80,7000,96.59,93.31,90.42,90.42,1.0,1,1
6,5,100,30,7000,96.76,94.51,90.42,90.42,1.0,1,1
4,5,100,30,32,90.73666666666666,90.54666666666667,90.35666666666667,90.18,0.5,15,3
4,5,100,20,1024,94.20333333333333,92.03666666666666,90.91333333333334,90.18,0.5,15,3
2,3,100,30,2048,95.04666666666667,93.82666666666667,90.53,89.68,0.5,15,3
5,5,100,20,7000,93.65,91.64999999999999,91.08000000000001,91.08000000000001,1.0,1,1
4,5,100,10,256,93.47666666666666,90.95,90.79,90.18,0.5,15,3
3,5,100,20,512,94.44,91.80666666666667,90.45666666666666,89.9,0.5,15,3
8,5,100,80,5000,93.11,90.86,90.25999999999999,90.25999999999999,1.0,15,3
2,1,100,20,1024,94.64,91.63666666666667,91.19333333333334,89.68,0.5,15,3
4,8,100,10,1024,95.38,92.39,91.21000000000001,90.18,1.0,15,3
4,5,100,10,5000,94.75,91.66,91.0,90.18,1.0,1,1
6,5,100,50,5000,96.93,95.28999999999999,90.42,90.42,1.0,15,3
3,5,100,80,7000,95.03,93.49,89.9,89.9,1.0,1,1
3,5,100,20,16,90.65333333333334,90.08333333333334,89.95666666666666,89.9,0.5,15,3
2,5,100,10,16,91.95,91.23,90.21000000000001,89.68,1.0,15,3
7,5,100,30,16,90.56333333333333,90.18333333333334,89.75,89.72,0.5,15,3
2,10,100,10,1024,94.86,92.92,91.03,89.68,0.5,15,3
3,5,100,10,256,93.47999999999999,90.60333333333334,90.46333333333332,89.9,0.5,15,3
3,5,100,10,5000,95.82000000000001,92.21000000000001,90.89,89.9,1.0,15,3
7,5,100,100,5000,97.07000000000001,93.41000000000001,89.72,89.72,1.0,15,3
2,1,100,10,1024,95.69,93.19,92.0,89.68,1.0,15,3
7,1,100,40,4096,96.82,90.04666666666667,89.94,89.72,0.75,15,3
3,8,100,30,4096,95.28,93.77,90.47,89.9,1.0,15,3
7,2,100,20,16,93.08999999999999,91.47999999999999,91.01,89.72,1.0,15,3
6,1,100,10,5000,96.52,90.52,90.49000000000001,90.42,1.0,1,1
3,5,100,10,1024,95.19999999999999,91.64,90.91,89.9,1.0,15,3
7,5,100,50,5000,96.96000000000001,95.33,89.77000000000001,89.72,1.0,15,3
0,5,100,20,5000,97.89,92.38,90.22,90.2,1.0,15,3
2,8,100,10,4096,96.05,94.35,91.96,89.68,1.0,15,3
2,5,100,10,32,93.17999999999999,89.96,89.68,89.68,1.0,15,3
1,5,100,100,5000,98.54,92.63,88.64999999999999,88.64999999999999,1.0,15,3
0,5,100,60,5000,98.15,92.29,90.25999999999999,90.2,1.0,15,3
1,1,100,100,16,91.30666666666667,88.64999999999999,88.64999999999999,88.64999999999999,0.25,15,3
4,1,100,10,4096,95.33,91.31,91.22,90.18,1.0,15,3
5,5,100,100,5000,93.38,91.92,91.08000000000001,91.08000000000001,1.0,1,1
2,2,100,10,1024,94.61,92.47,91.01666666666667,89.68,0.5,15,3
6,5,100,10,5000,97.27,93.01,90.42,90.42,1.0,15,3
2,1,100,40,5000,94.97,90.81,90.09,89.68,1.0,1,1
5,5,100,30,7000,94.04,92.01,91.08000000000001,91.08000000000001,1.0,1,1
0,2,100,10,16,96.23,90.2,90.2,90.2,1.0,15,3
3,5,100,30,7000,95.22,93.15,90.88000000000001,89.9,1.0,1,1
4,5,100,20,512,94.8,92.25999999999999,91.38,90.18,1.0,15,3
4,5,100,20,5000,95.39,93.0,91.02,90.18,1.0,15,3
0,10,100,40,32,95.56,90.2,90.2,90.2,1.0,15,3
2,1,100,30,4096,95.19333333333333,91.13666666666667,90.62333333333333,89.68,0.5,15,3
2,5,100,30,5000,95.78,94.99,91.06,89.68,1.0,10,3
7,5,100,40,7000,96.7,94.77,89.83,89.72,1.0,1,1
3,5,100,40,7000,94.85,93.77,89.92,89.9,1.0,1,1
8,1,100,50,5000,91.92333333333333,90.25999999999999,90.25999999999999,90.25999999999999,0.25,15,3
2,5,100,20,256,94.99,93.05,91.53999999999999,89.68,1.0,15,3
5,5,100,10,5000,93.89,91.49000000000001,91.08000000000001,91.08000000000001,1.0,15,3
4,5,100,20,16,90.62,90.51,90.89,90.18,1.0,15,3
4,3,100,10,2048,93.84666666666666,91.42333333333333,90.66333333333333,90.18,0.5,15,3
6,1,100,60,5000,96.53,90.42,90.42,90.42,1.0,1,1
3,5,100,30,512,94.19999999999999,92.39,90.22333333333333,89.9,0.5,15,3
2,5,100,30,7000,95.33,94.83,91.4,89.68,1.0,1,1
0,5,100,20,5000,98.04,92.86,90.24,90.2,1.0,15,3
0,5,100,80,5000,97.7,91.35,90.2,90.2,1.0,1,1
2,5,100,100,5000,95.64,92.16,89.68,89.68,1.0,15,3
4,1,100,30,4096,94.33333333333334,90.48,90.23333333333333,90.18,0.5,15,3
2,5,100,30,7000,95.72,94.39999999999999,91.08000000000001,89.68,1.0,1,1
4,5,100,10,1024,93.97999999999999,91.21333333333334,90.53,90.18,0.5,15,3
4,5,100,30,16,90.62,90.18,90.18,90.18,1.0,15,3
2,2,100,10,4096,94.72333333333334,92.81666666666666,91.05333333333333,89.68,0.5,15,3
3,5,100,20,7000,95.22,92.62,91.16,89.9,1.0,1,1
4,5,100,100,7000,95.16,90.75999999999999,90.18,90.18,1.0,1,1
2,5,100,10,16,90.64999999999999,90.70666666666666,90.25999999999999,89.68,0.5,15,3
3,5,100,60,5000,95.41,93.89999999999999,89.9,89.9,1.0,15,3
2,5,100,100,5000,95.78999999999999,91.86,89.69,89.68,1.0,15,3
4,5,100,50,5000,95.7,92.88,90.19,90.18,1.0,15,3
5,5,100,80,5000,93.15,92.22,91.08000000000001,91.08000000000001,1.0,1,1
3,5,100,20,32,91.18666666666667,90.35333333333332,90.10000000000001,89.9,0.5,15,3
4,5,100,20,16,90.57,90.46666666666667,90.28,90.18,0.5,15,3
2,5,100,100,7000,95.35,91.47,89.68,89.68,1.0,1,1
0,5,100,20,5000,98.14,91.74,90.24,90.2,1.0,15,3
4,2,100,10,2048,93.56,91.40666666666667,90.73,90.18,0.5,15,3
5,5,100,100,7000,93.28999999999999,91.63,91.08000000000001,91.08000000000001,1.0,1,1
7,5,100,10,512,97.14,92.86999999999999,91.11,89.72,1.0,15,3
2,1,100,20,2048,94.96333333333334,91.63333333333334,91.24333333333333,89.68,0.5,15,3
2,5,100,80,7000,95.35,92.73,89.69,89.68,1.0,1,1
4,3,100,10,2048,93.85333333333334,91.33,90.65666666666667,90.18,0.5,15,3
3,5,100,100,5000,95.45,92.97999999999999,89.9,89.9,1.0,15,3
3,5,100,10,1024,94.63666666666667,91.15333333333334,90.58666666666667,89.9,0.5,15,3
2,5,100,20,128,93.17333333333333,92.33666666666667,90.96,89.68,0.5,15,3
2,5,100,10,32,93.08,90.79,90.22,89.68,1.0,15,3
3,3,100,20,5000,94.48666666666666,91.81,90.57333333333332,89.9,0.5,15,3
7,5,100,10,16,91.85,90.28,89.99000000000001,89.72,1.0,15,3
2,10,100,30,1024,95.09,94.76,90.91,89.68,1.0,15,3
2,3,100,20,4096,96.02000000000001,94.85,92.24,89.68,1.0,15,3
3,3,100,30,1024,95.22,92.78,90.78,89.9,1.0,15,3
2,5,100,40,1024,95.41,94.45,90.48,89.68,1.0,15,3
5,1,100,10,5000,93.37,91.08000000000001,91.08000000000001,91.08000000000001,1.0,1,1
2,5,100,30,5000,95.24000000000001,94.0,91.19,89.68,1.0,1,1
7,5,100,30,128,95.89999999999999,91.16,90.16,89.72,1.0,15,3
9,5,100,40,5000,93.07,92.58999999999999,89.92999999999999,89.91,1.0,15,3
4,1,100,100,32,90.75,90.18,90.18,90.18,0.5,15,3
2,3,100,30,4096,95.29666666666667,93.7,90.63666666666667,89.68,0.5,15,3
3,5,100,100,5000,95.56,92.92,89.9,89.9,1.0,15,3
7,5,100,10,1024,97.1,93.22,91.02,89.72,1.0,15,3
5,5,100,40,7000,93.52000000000001,92.58999999999999,91.08000000000001,91.08000000000001,1.0,1,1
7,5,100,30,16,91.3,90.09333333333333,89.74333333333333,89.72,0.5,15,3
3,5,100,40,7000,94.84,93.27,89.95,89.9,1.0,1,1
5,2,100,40,128,91.49333333333334,91.29666666666667,91.08000000000001,91.08000000000001,0.5,15,3
2,5,100,100,7000,95.27,91.57,89.68,89.68,1.0,1,1
9,5,100,60,7000,92.65,92.17999999999999,89.91,89.91,1.0,1,1
5,5,100,40,1024,93.16,92.31,91.08000000000001,91.08000000000001,1.0,15,3
8,5,100,100,7000,92.96,90.49000000000001,90.25999999999999,90.25999999999999,1.0,1,1
4,5,100,30,256,94.38,91.82000000000001,90.74,90.18,1.0,15,3
7,2,100,10,4096,95.92666666666668,92.24,90.63,89.72,0.5,15,3
2,3,100,20,4096,94.93666666666667,93.63666666666667,91.23333333333333,89.68,0.5,15,3
3,2,100,30,6131,94.68666666666667,91.45666666666666,90.28333333333333,89.9,0.5,3,3
3,1,100,10,4096,94.02666666666667,90.0,90.42,89.9,0.5,15,3
4,2,100,10,1024,94.07,91.29333333333334,90.75666666666666,90.18,0.5,15,3
3,5,100,30,256,93.98333333333333,92.10000000000001,90.22666666666666,89.9,0.5,15,3
0,5,100,60,7000,97.94,92.62,90.21000000000001,90.2,1.0,1,1
3,1,100,10,2048,95.45,90.60000000000001,90.91,89.9,1.0,15,3
3,5,100,20,256,93.85,91.08000000000001,90.36,89.9,0.5,15,3
8,5,100,20,5000,92.9,90.53,90.25999999999999,90.25999999999999,1.0,1,1
7,5,100,20,128,94.83666666666667,92.4,90.45,89.72,0.5,15,3
8,1,100,100,5000,92.89,90.25999999999999,90.25999999999999,90.25999999999999,1.0,1,1
6,1,100,20,5000,96.85000000000001,90.77,90.42,90.42,1.0,1,1
9,5,100,40,7000,92.7,91.93,89.91,89.91,1.0,1,1
3,5,100,20,1024,95.21,92.66,91.16,89.9,1.0,15,3
5,2,100,100,16,91.08000000000001,91.08000000000001,91.08000000000001,91.08000000000001,1.0,15,3
3,10,100,30,1024,95.28999999999999,93.34,90.51,89.9,1.0,15,3
5,5,100,80,5000,93.77,92.34,91.08000000000001,91.08000000000001,1.0,15,3
7,5,100,100,5000,97.13000000000001,93.01,89.72,89.72,1.0,15,3
9,5,100,30,5000,93.19,91.08000000000001,89.92999999999999,89.91,1.0,15,3
2,5,100,20,32,90.88666666666667,91.25333333333333,90.55666666666666,89.68,0.5,15,3
6,5,100,100,5000,97.17,93.14,90.42,90.42,1.0,15,3
2,1,100,30,1024,94.76333333333334,90.82000000000001,90.64,89.68,0.5,15,3
7,5,100,40,5000,96.78999999999999,95.46,89.92999999999999,89.72,1.0,15,3
1,5,100,20,5000,98.17,89.5,89.44,88.64999999999999,1.0,15,3
6,5,100,50,5000,97.1,95.34,90.42,90.42,1.0,15,3
2,8,100,30,1024,95.28,94.88,91.01,89.68,1.0,15,3
5,5,100,40,1024,92.95,92.46,91.08000000000001,91.08000000000001,1.0,15,3
3,5,100,20,128,93.16,90.83333333333333,90.56666666666666,89.9,0.5,15,3
6,5,100,40,1024,96.75,95.16,90.42,90.42,1.0,15,3
6,5,100,60,5000,97.04,94.53,90.42,90.42,1.0,15,3
9,5,100,50,5000,92.74,92.66,89.91,89.91,1.0,15,3
4,1,100,40,5000,94.89,90.33,90.18,90.18,1.0,1,1
3,5,100,20,1024,94.97,92.27,91.01,89.9,1.0,15,3
3,8,100,10,4096,95.59,92.25999999999999,91.19,89.9,1.0,15,3
5,10,100,80,4096,93.62,92.97999999999999,91.08000000000001,91.08000000000001,1.0,15,3
3,2,100,20,4096,94.50333333333333,91.45666666666666,90.56333333333333,89.9,0.5,15,3
4,5,100,30,16,90.55333333333333,90.18,90.18333333333334,90.18,0.5,15,3
6,5,100,40,7000,96.78999999999999,95.04,90.42,90.42,1.0,1,1
5,5,100,60,5000,93.67,92.62,91.08000000000001,91.08000000000001,1.0,15,3
8,5,100,100,5000,93.17,90.59,90.25999999999999,90.25999999999999,1.0,15,3
7,3,100,30,2048,96.83,94.73,90.23,89.72,1.0,15,3
3,3,100,30,1024,94.91000000000001,92.78,90.75999999999999,89.9,1.0,15,3
7,5,100,80,5000,97.11,94.45,89.72,89.72,1.0,15,3
4,5,100,10,7000,94.91000000000001,92.07,91.24,90.18,1.0,1,1
5,5,100,80,7000,93.14,92.31,91.08000000000001,91.08000000000001,1.0,1,1
5,5,100,100,5000,93.47999999999999,92.0,91.08000000000001,91.08000000000001,1.0,15,3
3,5,100,20,16,90.94,90.06,89.96,89.9,0.5,15,3
8,5,100,40,1024,92.88,91.02,90.25999999999999,90.25999999999999,1.0,15,3
4,5,100,30,512,94.69999999999999,92.60000000000001,90.58,90.18,1.0,15,3
4,5,100,30,1024,93.98666666666666,91.67999999999999,90.29666666666667,90.18,0.5,15,3
4,5,100,30,7000,94.94,92.94,90.52,90.18,1.0,1,1
4,1,100,20,1024,95.09,91.19,90.96,90.18,1.0,15,3
4,2,100,20,2048,95.22,92.61,90.86,90.18,1.0,15,3
5,5,100,30,7000,93.32000000000001,91.93,91.09,91.08000000000001,1.0,1,1
7,5,100,10,1024,96.96000000000001,93.28999999999999,91.27,89.72,1.0,15,3
2,5,100,20,64,92.09,91.72,90.78333333333333,89.68,0.5,15,3
5,5,100,30,5000,93.34,91.89,91.08000000000001,91.08000000000001,1.0,1,1
8,5,100,20,7000,92.81,90.75,90.25999999999999,90.25999999999999,1.0,1,1
2,3,100,10,2048,95.76,93.71000000000001,91.92,89.68,1.0,15,3
1,1,100,80,5000,97.92999999999999,88.64999999999999,88.64999999999999,88.64999999999999,1.0,1,1
9,5,100,30,5000,93.43,91.91,89.95,89.91,1.0,15,3
3,3,100,30,4096,95.24000000000001,93.5,90.44,89.9,1.0,15,3
5,1,100,20,5000,93.56,91.08000000000001,91.08000000000001,91.08000000000001,1.0,1,1
2,10,100,60,64,93.17999999999999,90.06,89.71000000000001,89.68,1.0,15,3
2,5,100,10,2048,95.92,93.95,92.08,89.68,1.0,15,3
7,5,100,20,1024,96.27,93.61,90.50333333333333,89.72,0.5,15,3
9,5,100,80,7000,92.51,92.03,89.91,89.91,1.0,1,1
3,2,100,10,2048,95.32000000000001,91.63,90.82000000000001,89.9,1.0,15,3
0,5,100,10,5000,98.04,91.77,90.2,90.2,1.0,15,3
3,5,100,10,64,92.72333333333333,90.08,90.10666666666667,89.9,0.5,15,3
4,1,100,20,1024,95.0,91.4,91.07,90.18,1.0,15,3
2,3,100,10,2048,94.76666666666667,92.91,91.02666666666667,89.68,0.5,15,3
4,2,100,30,4096,94.57333333333334,91.45,90.35333333333332,90.18,0.5,15,3
4,5,100,30,16,91.14999999999999,90.18,90.19,90.18,1.0,15,3
3,5,100,10,5000,95.07,91.8,91.02,89.9,1.0,1,1
3,1,100,10,5000,95.27,90.22,90.81,89.9,1.0,1,1
2,5,100,30,1024,94.71666666666667,94.04666666666667,90.64333333333333,89.68,0.5,15,3
3,5,100,50,7000,94.96,93.7,89.91,89.9,1.0,1,1
6,5,100,20,5000,97.00999999999999,94.39,90.42,90.42,1.0,15,3
5,2,100,80,128,91.53,91.18333333333334,91.08000000000001,91.08000000000001,0.5,15,3
8,5,100,100,5000,92.99,90.7,90.25999999999999,90.25999999999999,1.0,15,3
4,5,100,30,16,90.68333333333334,90.20666666666666,90.19666666666667,90.18,0.5,15,3
4,5,100,80,7000,94.83,90.95,90.18,90.18,1.0,1,1
4,5,100,20,4096,95.66,93.27,91.05,90.18,1.0,15,3
5,1,100,60,5000,93.45,91.08000000000001,91.08000000000001,91.08000000000001,1.0,1,1
9,2,100,30,128,90.27333333333334,89.91333333333333,89.91333333333333,89.91,0.25,15,3
3,3,100,10,4096,95.64,91.77,90.97,89.9,1.0,15,3
3,5,100,40,7000,95.0,93.56,90.03999999999999,89.9,1.0,1,1
6,5,100,20,7000,96.85000000000001,93.58999999999999,90.44,90.42,1.0,1,1
3,8,100,20,2048,95.35,93.13,90.92,89.9,1.0,15,3
3,3,100,80,256,93.65333333333334,91.36,89.9,89.9,0.5,15,3
7,8,100,10,1024,97.02,93.74,91.07,89.72,1.0,15,3
4,3,100,20,2048,95.04,93.17,91.19,90.18,1.0,15,3
2,1,100,30,5000,95.02000000000001,91.25999999999999,90.72,89.68,1.0,1,1
2,2,100,10,4096,95.89999999999999,93.78,91.96,89.68,1.0,15,3
2,5,100,30,128,94.33,91.12,90.36,89.68,1.0,15,3
3,5,100,50,7000,95.17,93.78999999999999,89.9,89.9,1.0,1,1
7,5,100,100,5000,96.98,93.56,89.72,89.72,1.0,15,3
2,2,100,30,5000,95.63000000000001,94.53,91.05,89.68,1.0,10,3
2,5,100,20,2048,95.03333333333333,93.65333333333334,91.14666666666666,89.68,0.5,15,3
7,5,100,30,5000,96.93,95.30999999999999,90.23,89.72,1.0,15,3
3,1,100,10,4096,94.39,90.11,90.56,89.9,0.5,15,3
7,5,100,40,5000,96.89999999999999,95.24000000000001,90.14,89.72,1.0,15,3
3,5,100,100,5000,95.03,92.5,89.9,89.9,1.0,1,1
4,5,100,10,2048,93.78,91.39333333333333,90.71000000000001,90.18,0.5,15,3
7,1,100,30,4096,96.25333333333333,90.65666666666667,90.12,89.72,0.5,15,3
7,5,100,10,5000,97.02,94.1,91.11,89.72,1.0,15,3
3,8,100,20,2048,94.64333333333333,92.40666666666667,90.49000000000001,89.9,0.5,15,3
2,2,100,30,1024,95.24000000000001,93.94,90.97,89.68,1.0,15,3
1,5,100,20,5000,98.39,90.01,89.22,88.64999999999999,1.0,15,3
0,5,100,100,5000,98.35000000000001,91.35,90.21000000000001,90.2,1.0,15,3
2,5,100,20,5000,96.02000000000001,95.0,91.97999999999999,89.68,1.0,15,3
5,5,100,20,7000,93.89999999999999,91.53999999999999,91.08000000000001,91.08000000000001,1.0,1,1
3,8,100,30,1024,95.17999999999999,93.03,90.72,89.9,1.0,15,3
7,5,100,40,5000,96.54,94.72,89.74,89.72,1.0,1,1
4,5,100,20,16,90.41666666666667,90.33,90.32,90.18,0.5,15,3
3,3,100,30,2048,94.55,92.26333333333334,90.21666666666667,89.9,0.5,15,3
3,3,100,20,2048,95.46,92.29,90.98,89.9,1.0,15,3
2,5,100,20,64,92.82666666666667,92.65333333333334,90.83333333333333,89.68,0.5,15,3
2,2,100,30,2048,95.54,94.69999999999999,91.03,89.68,1.0,15,3
3,1,100,20,2048,95.37,90.85,91.05,89.9,1.0,15,3
4,10,100,50,1024,94.55666666666667,92.48333333333333,90.18,90.18,0.75,15,3
3,5,100,50,7000,95.02000000000001,93.45,89.9,89.9,1.0,1,1
3,2,100,10,1024,95.34,91.23,91.02,89.9,1.0,15,3
2,1,100,30,5958,95.31333333333333,90.78333333333333,90.62333333333333,89.68,0.5,3,3
3,5,100,40,1024,95.17,93.76,90.21000000000001,89.9,1.0,15,3
2,1,100,20,2048,95.11333333333334,91.64999999999999,91.31666666666666,89.68,0.5,15,3
6,1,100,10,5000,96.89999999999999,90.67,90.42,90.42,1.0,1,1
4,2,100,10,2048,93.98666666666666,91.24666666666667,90.67333333333333,90.18,0.5,15,3
6,5,100,10,5000,96.96000000000001,92.91,90.42,90.42,1.0,15,3
2,3,100,20,4096,95.88,94.78999999999999,91.88,89.68,1.0,15,3
2,5,100,20,16,89.88333333333334,89.97333333333334,89.9,89.68,0.5,15,3
6,5,100,10,5000,97.15,93.25,90.42,90.42,1.0,15,3
7,1,100,30,128,95.39999999999999,90.50333333333333,90.05,89.72,0.75,15,3
9,5,100,50,5000,92.86,92.57,89.92,89.91,1.0,15,3
3,5,100,100,7000,95.19,92.33,89.9,89.9,1.0,1,1
3,5,100,20,16,92.36999999999999,90.21000000000001,90.14999999999999,89.9,1.0,15,3
4,5,100,30,7000,94.88,92.57,90.4,90.18,1.0,1,1
7,5,100,20,1024,96.14666666666668,93.47999999999999,90.49333333333334,89.72,0.5,15,3
3,5,100,20,32,90.86333333333333,90.19,90.09,89.9,0.5,15,3
7,5,100,20,16,91.25,90.01,90.07333333333334,89.72,0.5,15,3
1,5,100,40,5000,97.81,90.75999999999999,88.64999999999999,88.64999999999999,1.0,1,1
3,5,100,10,5000,95.71,92.39,91.35,89.9,1.0,15,3
8,5,100,20,5000,93.33,90.98,90.25999999999999,90.25999999999999,1.0,15,3
7,5,100,20,128,96.2,91.47999999999999,90.55,89.72,1.0,15,3
7,5,100,100,5000,97.2,93.34,89.72,89.72,1.0,15,3
2,5,100,20,2048,94.84666666666666,93.59666666666666,91.10333333333334,89.68,0.5,15,3
7,5,100,30,128,95.63000000000001,90.75,90.09,89.72,1.0,15,3
3,2,100,20,1024,94.57333333333334,91.47333333333333,90.64333333333333,89.9,0.5,15,3
6,5,100,20,7000,96.95,93.81,90.42,90.42,1.0,1,1
3,5,100,20,32,91.71333333333334,90.26666666666667,90.32666666666667,89.9,0.5,15,3
2,1,100,30,5000,95.69,91.55,91.11,89.68,1.0,10,3
7,5,100,20,7000,96.83,94.13,90.96,89.72,1.0,1,1
2,1,100,10,1024,94.67999999999999,91.10333333333334,91.16666666666666,89.68,0.5,15,3
3,5,100,40,7000,95.14,93.76,90.09,89.9,1.0,1,1
4,5,100,20,7000,95.09,92.42,91.27,90.18,1.0,1,1
4,3,100,20,2048,95.41,92.86,91.08000000000001,90.18,1.0,15,3
4,5,100,30,256,93.22,91.23666666666666,90.33666666666666,90.18,0.5,15,3
4,5,100,30,16,91.01,90.18,90.18,90.18,1.0,15,3
3,5,100,20,32,91.16,90.44333333333333,90.22666666666666,89.9,0.5,15,3
3,2,100,20,1024,94.57,91.07666666666667,90.50333333333333,89.9,0.5,15,3
0,5,100,30,5000,97.94,92.67,90.21000000000001,90.2,1.0,15,3
4,5,100,20,16,90.74,90.30333333333334,90.25666666666666,90.18,0.5,15,3
3,5,100,50,7000,94.94,93.49,89.9,89.9,1.0,1,1
4,5,100,10,16,92.02,90.18,90.53999999999999,90.18,1.0,15,3
3,5,100,30,32,91.25999999999999,90.42666666666666,90.10000000000001,89.9,0.5,15,3
2,1,100,10,2048,94.66333333333333,91.17666666666666,91.05,89.68,0.5,15,3
8,5,100,50,7000,92.85,91.06,90.25999999999999,90.25999999999999,1.0,1,1
3,5,100,40,7000,95.22,93.22,89.9,89.9,1.0,1,1
3,5,100,30,16,91.02,89.91,89.9,89.9,1.0,15,3
9,5,100,20,5000,93.30000000000001,91.0,89.96,89.91,1.0,15,3
5,5,100,50,5000,93.85,93.07,91.08000000000001,91.08000000000001,1.0,15,3
9,5,100,10,7000,92.96,90.31,89.92,89.91,1.0,1,1
3,5,100,20,32,92.82000000000001,89.92999999999999,89.9,89.9,1.0,15,3
3,5,100,40,7000,95.3,93.47999999999999,89.98,89.9,1.0,1,1
4,5,100,40,7000,94.8,92.34,90.24,90.18,1.0,1,1
5,5,100,40,5000,93.99,92.93,91.08000000000001,91.08000000000001,1.0,15,3
3,5,100,60,7000,95.14,93.22,89.9,89.9,1.0,1,1
4,5,100,20,5000,95.5,93.23,91.14999999999999,90.18,1.0,15,3
7,2,100,30,2048,96.81,93.58,90.22,89.72,1.0,15,3
7,5,100,60,5000,96.84,95.12,89.72,89.72,1.0,15,3
0,5,100,20,7000,97.63,91.85,90.24,90.2,1.0,1,1
3,2,100,30,2048,95.19999999999999,91.7,90.52,89.9,1.0,15,3
7,5,100,40,7000,96.86,94.92,89.75999999999999,89.72,1.0,1,1
2,2,100,10,2048,94.63000000000001,92.70666666666668,91.13666666666667,89.68,0.5,15,3
6,5,100,100,7000,96.8,92.25999999999999,90.42,90.42,1.0,1,1
0,5,100,100,7000,97.87,91.39,90.2,90.2,1.0,1,1
3,1,100,20,2048,94.64666666666666,90.47,90.5,89.9,0.5,15,3
2,5,100,10,64,93.32333333333334,91.50333333333333,91.19,89.68,0.5,15,3
6,1,100,40,64,94.64333333333333,90.48,90.42,90.42,0.75,15,3
2,1,100,30,4096,95.65,91.96,90.98,89.68,1.0,15,3
4,1,100,20,2048,95.30999999999999,91.75,91.18,90.18,1.0,15,3
5,5,100,60,7000,93.25,92.67999999999999,91.08000000000001,91.08000000000001,1.0,1,1
3,5,100,10,512,94.28666666666666,90.95333333333333,90.36999999999999,89.9,0.5,15,3
3,8,100,30,2048,95.1,93.38,90.46,89.9,1.0,15,3
0,5,100,60,7000,97.82,92.36999999999999,90.2,90.2,1.0,1,1
3,5,100,10,5000,95.46,92.35,90.82000000000001,89.9,1.0,15,3
2,5,100,80,5000,95.78999999999999,92.66,89.69,89.68,1.0,15,3
5,5,100,50,7000,93.58,92.46,91.08000000000001,91.08000000000001,1.0,1,1
1,5,100,80,5000,98.28,93.53,88.64999999999999,88.64999999999999,1.0,15,3
2,5,100,20,32,91.33666666666667,91.64333333333333,90.78,89.68,0.5,15,3
2,5,100,10,256,95.46,91.59,92.11,89.68,1.0,15,3
2,5,100,20,512,95.06,93.06,92.02,89.68,1.0,15,3
8,3,100,10,256,92.91,90.28,90.25999999999999,90.25999999999999,1.0,15,3
3,5,100,20,256,94.74000000000001,91.24,90.92,89.9,1.0,15,3
1,1,100,80,512,97.22333333333333,88.64999999999999,88.64999999999999,88.64999999999999,0.5,15,3
1,5,100,30,7000,97.99,91.45,88.64999999999999,88.64999999999999,1.0,1,1
7,5,100,40,1024,96.71,95.03,89.92999999999999,89.72,1.0,15,3
4,5,100,10,4096,95.67,92.82000000000001,91.42,90.18,1.0,15,3
0,5,100,30,7000,97.92,92.75,90.25,90.2,1.0,1,1
2,5,100,30,16,89.94,90.95333333333333,89.82,89.68,0.5,15,3
3,5,100,40,7000,95.26,93.5,89.92,89.9,1.0,1,1
3,1,100,10,4096,94.15333333333334,90.21000000000001,90.43666666666667,89.9,0.5,15,3
7,3,100,30,2048,96.95,94.69999999999999,90.33,89.72,1.0,15,3
0,5,100,40,5000,98.00999999999999,92.75,90.25999999999999,90.2,1.0,15,3
4,3,100,30,1024,93.98333333333333,91.71000000000001,90.40333333333334,90.18,0.5,15,3
9,5,100,40,64,90.36999999999999,89.96,89.92,89.91,1.0,15,3
0,5,100,80,5000,98.27,91.89,90.2,90.2,1.0,15,3
3,10,100,30,1024,94.83,93.28,90.36,89.9,1.0,15,3
4,5,100,30,7000,94.98,92.39,90.49000000000001,90.18,1.0,1,1
7,5,100,20,512,96.06333333333333,93.27,90.36,89.72,0.5,15,3
3,5,100,50,7000,95.26,93.57,89.9,89.9,1.0,1,1
2,3,100,20,4096,95.74000000000001,94.76,91.88,89.68,1.0,15,3
1,5,100,40,5000,98.13,92.75999999999999,88.64999999999999,88.64999999999999,1.0,15,3
7,10,100,50,16,92.61,89.87,88.94,89.72,1.0,15,3
9,5,100,50,7000,92.66,92.13,89.91,89.91,1.0,1,1
2,3,100,30,2048,95.09,93.87666666666667,90.59,89.68,0.5,15,3
2,2,100,30,1024,95.25,94.17999999999999,91.28,89.68,1.0,15,3
2,3,100,20,16,90.30666666666667,91.07,90.45666666666666,89.68,0.5,15,3
0,2,100,60,128,96.80666666666666,90.30333333333334,90.21000000000001,90.2,0.75,15,3
3,1,100,30,2048,94.71333333333334,90.12666666666667,90.26666666666667,89.9,0.5,15,3
9,5,100,80,7000,92.46,92.0,89.91,89.91,1.0,1,1
4,2,100,30,2048,95.14,92.14,90.7,90.18,1.0,15,3
4,5,100,10,256,93.46,91.08666666666667,90.82666666666667,90.18,0.5,15,3
2,1,100,20,4096,95.06666666666666,91.67333333333333,91.13666666666667,89.68,0.5,15,3
7,5,100,20,1024,96.07666666666667,93.29333333333332,90.39333333333333,89.72,0.5,15,3
3,5,100,10,1024,95.28999999999999,91.7,90.91,89.9,1.0,15,3
2,2,100,20,2048,95.59,94.15,92.05,89.68,1.0,15,3
4,8,100,30,1024,95.24000000000001,92.56,90.59,90.18,1.0,15,3
3,5,100,30,512,94.96,92.11,90.42,89.9,1.0,15,3
4,1,100,10,4096,95.5,91.46,91.51,90.18,1.0,15,3
4,10,100,30,2048,95.25,92.77,90.5,90.18,1.0,15,3
2,5,100,40,5000,95.75,95.35,90.42999999999999,89.68,1.0,15,3
2,1,100,20,2048,95.07666666666667,92.02,91.16333333333333,89.68,0.5,15,3
4,3,100,20,2048,94.31,91.78333333333333,90.73,90.18,0.5,15,3
2,2,100,30,4096,95.64,94.04,91.25,89.68,1.0,15,3
5,2,100,100,256,92.55,91.3,91.08000000000001,91.08000000000001,1.0,15,3
7,1,100,10,2048,97.1,92.65,91.24,89.72,1.0,15,3
2,2,100,100,512,94.99,90.95,89.68,89.68,1.0,15,3
4,5,100,10,256,93.50333333333334,91.12666666666667,90.80666666666667,90.18,0.5,15,3
6,5,100,60,7000,97.03,94.33,90.42,90.42,1.0,1,1
2,5,100,30,1024,94.74000000000001,94.08,90.70333333333333,89.68,0.5,15,3
1,5,100,30,64,96.69,88.64999999999999,88.64999999999999,88.64999999999999,1.0,15,3
7,5,100,30,32,92.84333333333333,91.90666666666667,90.05333333333333,89.72,0.5,15,3
9,3,100,100,64,90.41333333333334,90.02666666666667,89.91,89.91,0.75,15,3
7,5,100,80,5000,96.89,94.08999999999999,89.72,89.72,1.0,15,3
7,5,100,20,256,95.36333333333333,93.00333333333334,90.59333333333333,89.72,0.5,15,3
2,5,100,30,4096,95.61,94.94,91.19,89.68,1.0,15,3
2,5,100,10,256,95.17999999999999,91.38,91.91,89.68,1.0,15,3
2,8,100,10,2048,94.53,93.03666666666666,91.09333333333333,89.68,0.5,15,3
6,5,100,10,7000,96.81,92.73,90.42999999999999,90.42,1.0,1,1
1,5,100,40,1024,98.04,91.92,88.64999999999999,88.64999999999999,1.0,15,3
2,3,100,30,2048,95.32000000000001,94.35,91.25999999999999,89.68,1.0,15,3
4,5,100,40,1024,95.09,92.88,90.3,90.18,1.0,15,3
8,5,100,20,5000,93.31,90.7,90.25999999999999,90.25999999999999,1.0,15,3
3,1,100,10,2048,95.42,90.77,91.0,89.9,1.0,15,3
3,5,100,30,7000,95.11,93.23,90.60000000000001,89.9,1.0,1,1
0,5,100,100,5000,97.89999999999999,91.12,90.2,90.2,1.0,1,1
2,2,100,10,512,95.21333333333334,93.14333333333333,91.56333333333333,89.68,0.75,15,3
6,5,100,100,5000,97.3,93.02,90.42,90.42,1.0,15,3
3,2,100,10,1024,95.21,91.27,90.84,89.9,1.0,15,3
2,5,100,20,128,93.11666666666667,92.33,90.81666666666666,89.68,0.5,15,3
3,5,100,40,7000,95.09,93.16,90.0,89.9,1.0,1,1
3,10,100,30,1024,94.24,92.81,90.22666666666666,89.9,0.5,15,3
4,5,100,20,32,92.0,90.73,90.18,90.18,1.0,15,3
2,5,100,30,1024,94.80666666666666,94.02000000000001,90.64333333333333,89.68,0.5,15,3
4,3,100,30,2048,94.19333333333333,91.66333333333333,90.34,90.18,0.5,15,3
2,5,100,30,32,91.23333333333333,91.86999999999999,90.07333333333334,89.68,0.5,15,3
6,5,100,20,7000,96.95,93.58,90.49000000000001,90.42,1.0,1,1
2,5,100,30,5000,95.66,95.06,91.09,89.68,1.0,15,3
3,5,100,40,7000,94.92,93.45,89.99000000000001,89.9,1.0,1,1
3,2,100,30,1024,94.32333333333334,91.31,90.26333333333334,89.9,0.5,15,3
3,10,100,20,2048,95.32000000000001,93.42,91.10000000000001,89.9,1.0,15,3
4,5,100,20,512,94.55,92.09,90.89,90.18,1.0,15,3
9,5,100,40,5000,93.12,92.08,89.91,89.91,1.0,15,3
5,5,100,40,7000,93.89999999999999,92.5,91.08000000000001,91.08000000000001,1.0,1,1
5,5,100,50,7000,93.47,92.57,91.08000000000001,91.08000000000001,1.0,1,1
1,5,100,30,1024,97.86333333333333,90.29666666666667,88.7,88.64999999999999,0.75,15,3
2,5,100,40,7000,95.43,95.30999999999999,90.21000000000001,89.68,1.0,1,1
9,5,100,50,7000,92.84,91.88,89.92,89.91,1.0,1,1
6,10,100,40,128,94.83,93.32000000000001,90.42333333333333,90.42,0.5,15,3
6,10,100,100,16,90.52666666666667,90.43666666666667,90.42,90.42,0.5,15,3
5,5,100,100,7000,93.0,91.59,91.08000000000001,91.08000000000001,1.0,1,1
4,5,100,100,7000,94.87,90.78,90.18,90.18,1.0,1,1
3,5,100,30,32,92.15,89.9,89.9,89.9,1.0,15,3
3,5,100,10,512,94.19999999999999,91.19666666666667,90.5,89.9,0.5,15,3
7,5,100,60,7000,96.75,94.56,89.73,89.72,1.0,1,1
2,1,100,20,1024,95.24000000000001,92.47,91.86,89.68,1.0,15,3
4,5,100,20,2048,95.23,92.9,91.14,90.18,1.0,15,3
8,5,100,20,7000,93.26,90.9,90.25999999999999,90.25999999999999,1.0,1,1
3,2,100,30,2048,94.78,91.70333333333333,90.37666666666667,89.9,0.5,15,3
4,3,100,30,2048,94.35,91.69333333333334,90.41666666666667,90.18,0.5,15,3
7,1,100,10,1024,96.36333333333333,90.89,90.53333333333333,89.72,0.5,15,3
6,5,100,20,5000,96.88,93.71000000000001,90.42,90.42,1.0,15,3
5,5,100,80,7000,93.63,92.11,91.08000000000001,91.08000000000001,1.0,1,1
2,5,100,50,7000,95.26,94.76,89.86,89.68,1.0,1,1
2,5,100,60,5000,95.63000000000001,94.46,89.8,89.68,1.0,15,3
7,5,100,30,1024,96.93,94.61,90.2,89.72,1.0,15,3
4,5,100,10,64,92.11333333333333,90.63666666666667,90.64666666666666,90.18,0.5,15,3
3,5,100,20,32,92.61,90.19,89.9,89.9,1.0,15,3
6,5,100,100,7000,96.95,92.53,90.42,90.42,1.0,1,1
2,5,100,20,512,94.78999999999999,93.38,91.72,89.68,1.0,15,3
4,5,100,30,16,91.27,90.3,90.19,90.18,1.0,15,3
3,5,100,10,2048,94.20666666666668,91.42333333333333,90.47666666666667,89.9,0.5,15,3
7,8,100,30,2048,96.78999999999999,94.69999999999999,90.3,89.72,1.0,15,3
4,5,100,40,7000,95.03,92.24,90.18,90.18,1.0,1,1
3,3,100,10,2048,94.66,91.11333333333333,90.48666666666666,89.9,0.5,15,3
3,1,100,20,2048,94.82000000000001,90.46333333333332,90.61666666666667,89.9,0.5,15,3
6,1,100,50,4096,96.85333333333334,90.5,90.42,90.42,0.75,15,3
2,5,100,10,512,95.39,92.92,91.69,89.68,1.0,15,3
7,5,100,40,5000,97.03,95.14,89.89,89.72,1.0,15,3
2,1,100,20,1024,94.86666666666666,91.88333333333333,91.14999999999999,89.68,0.5,15,3
3,5,100,20,4096,95.47,93.27,91.08000000000001,89.9,1.0,15,3
3,5,100,100,5000,95.55,92.47999999999999,89.9,89.9,1.0,15,3
0,5,100,10,7000,97.81,91.49000000000001,90.2,90.2,1.0,1,1
8,5,100,20,5000,92.85,90.47,90.25999999999999,90.25999999999999,1.0,1,1
4,1,100,100,5000,94.89,90.18,90.18,90.18,1.0,1,1
8,5,100,40,7000,92.80000000000001,91.23,90.25999999999999,90.25999999999999,1.0,1,1
8,5,100,40,5000,92.61,90.66,90.25999999999999,90.25999999999999,1.0,1,1
1,5,100,100,5000,98.07000000000001,92.36,88.64999999999999,88.64999999999999,1.0,15,3
0,5,100,60,5000,98.06,92.24,90.21000000000001,90.2,1.0,15,3
8,5,100,30,5000,93.10000000000001,91.07,90.25999999999999,90.25999999999999,1.0,15,3
4,5,100,20,7000,95.04,92.69,91.39,90.18,1.0,1,1
3,5,100,20,512,94.96,92.15,90.74,89.9,1.0,15,3
6,10,100,80,16,91.28,90.42,90.38000000000001,90.42,1.0,15,3
4,5,100,30,128,94.24,91.01,90.56,90.18,1.0,15,3
2,5,100,30,128,93.82000000000001,91.67,90.73,89.68,1.0,15,3
0,5,100,100,5000,98.11999999999999,91.25999999999999,90.2,90.2,1.0,15,3
7,5,100,50,5000,96.72,95.21,89.74,89.72,1.0,15,3
3,5,100,50,7000,94.99,94.04,89.9,89.9,1.0,1,1
2,3,100,30,2048,95.43,94.83,91.0,89.68,1.0,15,3
3,5,100,30,16,90.97,89.92,89.8,89.9,1.0,15,3
3,8,100,10,1024,95.36,92.11,90.97,89.9,1.0,15,3
2,5,100,30,16,90.69,89.72,89.71000000000001,89.68,1.0,15,3
2,3,100,30,2048,95.16333333333333,93.87333333333333,90.67666666666668,89.68,0.5,15,3
3,5,100,10,2048,95.48,91.93,90.88000000000001,89.9,1.0,15,3
2,5,100,20,16,91.67,91.56,91.0,89.68,1.0,15,3
2,5,100,60,5000,95.69,94.32000000000001,89.78,89.68,1.0,15,3
0,5,100,50,7000,97.74000000000001,92.89,90.21000000000001,90.2,1.0,1,1
3,5,100,20,128,94.21000000000001,90.29,90.41,89.9,1.0,15,3
4,10,100,10,2048,95.39999999999999,92.67999999999999,91.28,90.18,1.0,15,3
2,5,100,20,32,92.72,91.56,89.79,89.68,1.0,15,3
3,5,100,20,512,94.81,91.49000000000001,90.79,89.9,1.0,15,3
4,1,100,40,128,93.07,90.26666666666667,90.21000000000001,90.18,0.75,15,3
7,3,100,10,32,91.37333333333333,89.92999999999999,89.87666666666667,89.72,0.25,15,3
7,5,100,30,1024,96.93,94.93,90.31,89.72,1.0,15,3
8,5,100,60,5000,93.2,91.32000000000001,90.25999999999999,90.25999999999999,1.0,15,3
1,5,100,20,5000,98.29,89.96,89.22,88.64999999999999,1.0,15,3
2,5,100,30,16,90.91,90.07,89.72,89.68,1.0,15,3
8,5,100,40,256,91.52333333333334,90.42,90.25999999999999,90.25999999999999,0.5,15,3
6,1,100,80,5000,96.49,90.42,90.42,90.42,1.0,1,1
7,5,100,10,512,95.87666666666667,92.17999999999999,90.48,89.72,0.5,15,3
2,8,100,30,4096,95.67,95.23,90.96,89.68,1.0,15,3
2,2,100,20,1024,95.46,93.85,92.13,89.68,1.0,15,3
2,5,100,10,32,93.38,91.75,90.16,89.68,1.0,15,3
6,5,100,20,7000,96.82,93.74,90.42999999999999,90.42,1.0,1,1
7,5,100,60,5000,97.02,95.3,89.72,89.72,1.0,15,3
9,5,100,40,1024,92.35,91.75,89.96,89.91,1.0,15,3
2,2,100,20,1024,95.46,94.28999999999999,91.93,89.68,1.0,15,3
2,2,100,20,4096,95.07,93.05666666666667,91.29666666666667,89.68,0.5,15,3
7,2,100,10,1024,97.00999999999999,93.04,90.81,89.72,1.0,15,3
6,5,100,20,7000,96.75,93.63,90.42,90.42,1.0,1,1
1,3,100,30,256,95.85333333333334,88.68666666666667,88.64999999999999,88.64999999999999,0.25,15,3
3,5,100,20,128,92.9,90.96666666666667,90.46,89.9,0.5,15,3
7,10,100,50,512,96.57,94.84,89.81,89.72,1.0,15,3
3,5,100,20,64,92.89,90.8,90.25333333333333,89.9,0.5,15,3
6,2,100,40,16,90.59,90.42,90.42,90.42,0.5,15,3
5,5,100,10,7000,93.41000000000001,91.14,91.09,91.08000000000001,1.0,1,1
7,5,100,40,5000,96.95,95.38,89.92,89.72,1.0,15,3
2,5,100,30,5000,95.46,95.26,90.96,89.68,1.0,15,3
1,5,100,30,7000,97.72999999999999,90.14,88.64999999999999,88.64999999999999,1.0,1,1
3,5,100,10,32,93.30000000000001,89.9,89.92,89.9,1.0,15,3
3,5,100,40,7000,94.97,93.54,90.02,89.9,1.0,1,1
3,5,100,60,5000,95.27,94.13,89.9,89.9,1.0,15,3
7,5,100,30,16,91.64999999999999,89.72,89.72,89.72,1.0,15,3
7,1,100,30,128,93.41000000000001,90.08666666666667,90.02333333333333,89.72,0.25,15,3
3,5,100,20,1024,94.73333333333333,91.81,90.46666666666667,89.9,0.5,15,3
2,5,100,10,512,94.47333333333333,92.29666666666667,91.00666666666667,89.68,0.5,15,3
2,5,100,20,128,94.28,91.82000000000001,92.17999999999999,89.68,1.0,15,3
5,5,100,80,5000,93.8,92.84,91.08000000000001,91.08000000000001,1.0,15,3
1,3,100,40,1024,96.76333333333334,89.33,88.64999999999999,88.64999999999999,0.25,15,3
2,5,100,30,64,93.15,90.39,90.32,89.68,1.0,15,3
0,5,100,20,7000,97.57000000000001,91.83,90.21000000000001,90.2,1.0,1,1
0,5,100,40,5000,98.14,93.52000000000001,90.25999999999999,90.2,1.0,15,3
2,5,100,80,5000,95.42,92.81,89.68,89.68,1.0,1,1
4,1,100,20,2048,95.21,91.61,91.11,90.18,1.0,15,3
3,1,100,30,1024,95.0,90.28,90.67,89.9,1.0,15,3
7,5,100,30,64,93.57333333333332,92.49666666666667,89.90666666666667,89.72,0.5,15,3
4,1,100,20,64,92.19666666666667,90.44666666666666,90.36666666666666,90.18,0.75,15,3
2,3,100,30,1024,94.97,93.97,91.19,89.68,1.0,15,3
6,5,100,40,5000,96.96000000000001,94.8,90.42,90.42,1.0,15,3
4,8,100,20,2048,94.28666666666666,91.69666666666667,90.64,90.18,0.5,15,3
2,5,100,20,1024,95.37,94.41000000000001,91.63,89.68,1.0,15,3
3,5,100,40,7000,95.08,93.51,89.92999999999999,89.9,1.0,1,1
3,5,100,20,512,94.57,92.03,90.84,89.9,1.0,15,3
9,5,100,50,7000,92.91,92.15,89.91,89.91,1.0,1,1
7,8,100,20,2048,96.88,94.64,90.58,89.72,1.0,15,3
3,2,100,20,2048,94.61,91.18333333333334,90.48666666666666,89.9,0.5,15,3
6,5,100,50,7000,96.83,94.81,90.42,90.42,1.0,1,1
4,5,100,30,1024,94.24666666666667,91.75999999999999,90.46666666666667,90.18,0.5,15,3
3,5,100,20,16,91.94,90.51,90.06,89.9,1.0,15,3
3,1,100,30,2048,94.61,90.13666666666667,90.20666666666666,89.9,0.5,15,3
2,2,100,10,2048,94.55333333333333,92.73333333333333,91.22,89.68,0.5,15,3
4,5,100,40,1024,94.94,92.80000000000001,90.19,90.18,1.0,15,3
4,5,100,10,1024,94.08333333333333,91.23333333333333,90.66,90.18,0.5,15,3
4,5,100,10,32,91.60000000000001,90.42,90.70666666666666,90.18,0.5,15,3
3,3,100,10,2048,95.34,91.92,90.93,89.9,1.0,15,3
3,2,100,30,1024,95.05,92.41,90.32,89.9,1.0,15,3
2,5,100,40,5000,95.62,95.25,90.46,89.68,1.0,15,3
5,5,100,50,5000,93.28,92.94,91.08000000000001,91.08000000000001,1.0,15,3
2,5,100,20,128,94.21000000000001,91.75,91.11,89.68,1.0,15,3
1,5,100,60,7000,97.98,94.47,88.64999999999999,88.64999999999999,1.0,1,1
3,5,100,60,7000,94.97,93.81,89.9,89.9,1.0,1,1
3,3,100,20,2048,95.47,92.4,91.24,89.9,1.0,15,3
2,5,100,20,512,95.24000000000001,93.71000000000001,91.97999999999999,89.68,1.0,15,3
8,1,100,10,5000,93.04,90.25999999999999,90.27,90.25999999999999,1.0,1,1
4,5,100,10,2048,95.37,92.64,91.36,90.18,1.0,15,3
2,5,100,20,512,94.55333333333333,93.39333333333333,91.10000000000001,89.68,0.5,15,3
4,5,100,30,1024,94.19333333333333,91.75999999999999,90.30666666666667,90.18,0.5,15,3
3,3,100,10,1024,94.44666666666667,91.16,90.42333333333333,89.9,0.5,15,3
2,5,100,30,1024,95.42,94.69999999999999,90.93,89.68,1.0,15,3
3,8,100,30,4096,95.49,93.69,90.48,89.9,1.0,15,3
7,5,100,20,7000,96.81,94.15,91.06,89.72,1.0,1,1
2,5,100,40,7000,95.48,94.6,90.22,89.68,1.0,1,1
2,5,100,20,512,95.26,93.81,92.10000000000001,89.68,1.0,15,3
3,5,100,20,128,93.27,91.27333333333333,90.57666666666667,89.9,0.5,15,3
2,5,100,20,7000,95.89,94.32000000000001,92.13,89.68,1.0,1,1
2,5,100,100,5000,95.69,92.08,89.69,89.68,1.0,15,3
4,5,100,30,512,93.5,91.97,90.38000000000001,90.18,0.5,15,3
1,5,100,10,5000,98.09,89.29,89.42999999999999,88.64999999999999,1.0,15,3
3,5,100,10,16,92.17999999999999,89.99000000000001,89.9,89.9,1.0,15,3
8,5,100,10,5000,93.16,90.38000000000001,90.25999999999999,90.25999999999999,1.0,15,3
4,5,100,30,5000,94.86,92.07,90.42999999999999,90.18,1.0,1,1
7,5,100,30,1024,96.84,94.44,90.21000000000001,89.72,1.0,15,3
0,5,100,30,7000,97.77,92.77,90.2,90.2,1.0,1,1
2,1,100,10,1024,95.67,92.95,91.85,89.68,1.0,15,3
4,5,100,60,7000,94.82000000000001,91.61,90.18,90.18,1.0,1,1
4,5,100,30,16,90.69,90.18,90.18,90.18,1.0,15,3
2,5,100,80,5000,95.87,93.60000000000001,89.69,89.68,1.0,15,3
2,1,100,20,128,94.53,91.21000000000001,91.34,89.68,1.0,15,3
4,5,100,30,32,90.95333333333333,90.45666666666666,90.26333333333334,90.18,0.5,15,3
8,5,100,100,5000,93.15,90.69,90.25999999999999,90.25999999999999,1.0,15,3
0,5,100,50,5000,98.11999999999999,93.12,90.25999999999999,90.2,1.0,15,3
2,5,100,50,7000,95.22,94.46,89.8,89.68,1.0,1,1
7,3,100,30,2048,96.32333333333334,93.74,90.15333333333334,89.72,0.5,15,3
8,5,100,30,5000,93.55,91.03,90.25999999999999,90.25999999999999,1.0,15,3
7,1,100,50,5000,96.57,89.82,89.72,89.72,1.0,1,1
6,5,100,40,5000,97.11999999999999,95.17999999999999,90.42,90.42,1.0,15,3
8,3,100,40,2048,92.99,90.8,90.25999999999999,90.25999999999999,1.0,15,3
8,3,100,80,512,90.96,90.27333333333334,90.25999999999999,90.25999999999999,0.25,15,3
2,8,100,20,2048,95.78999999999999,94.77,92.04,89.68,1.0,15,3
5,5,100,20,5000,93.37,91.64,91.10000000000001,91.08000000000001,1.0,1,1
7,2,100,10,4096,95.83666666666667,92.42666666666666,90.60666666666667,89.72,0.5,15,3
7,5,100,80,5000,96.7,94.1,89.72,89.72,1.0,1,1
2,5,100,10,64,92.92,91.88333333333333,91.23333333333333,89.68,0.5,15,3
3,1,100,20,4096,94.67999999999999,90.47,90.68,89.9,0.5,15,3
1,5,100,100,5000,97.97,92.78,88.64999999999999,88.64999999999999,1.0,15,3
1,5,100,60,5000,97.8,93.89999999999999,88.64999999999999,88.64999999999999,1.0,1,1
2,1,100,40,5000,95.25,90.69,90.23,89.68,1.0,1,1
8,5,100,80,7000,92.88,90.68,90.25999999999999,90.25999999999999,1.0,1,1
3,3,100,30,1024,94.85,92.66,90.64,89.9,1.0,15,3
0,3,100,40,1024,97.72,92.0,90.24,90.2,1.0,15,3
3,5,100,40,7000,95.09,93.44,89.98,89.9,1.0,1,1
3,1,100,20,2048,94.64,90.23333333333333,90.63666666666667,89.9,0.5,15,3
4,3,100,20,4096,94.11666666666667,91.75666666666666,90.74333333333333,90.18,0.5,15,3
7,2,100,20,2048,96.17333333333333,92.69333333333334,90.52,89.72,0.5,15,3
3,5,100,20,16,90.69,89.92333333333333,90.02333333333333,89.9,0.5,15,3
6,5,100,100,5000,97.34,93.04,90.42,90.42,1.0,15,3
2,5,100,20,512,95.35,93.87,91.72,89.68,1.0,15,3
7,2,100,30,2048,96.18333333333334,92.92333333333333,90.10000000000001,89.72,0.5,15,3
7,3,100,50,2048,96.75,94.53,89.83,89.72,1.0,15,3
6,5,100,60,7000,96.83,94.46,90.42,90.42,1.0,1,1
7,5,100,30,64,93.54333333333334,92.23,89.99666666666667,89.72,0.5,15,3
4,5,100,80,7000,94.98,90.83,90.18,90.18,1.0,1,1
5,5,100,20,5000,94.32000000000001,91.78,91.08000000000001,91.08000000000001,1.0,15,3
9,5,100,30,5000,92.93,91.84,89.95,89.91,1.0,15,3
8,2,100,40,512,92.07,90.28333333333333,90.25999999999999,90.25999999999999,0.5,15,3
7,5,100,50,5000,96.85000000000001,95.02000000000001,89.72,89.72,1.0,15,3
3,5,100,20,5000,95.39999999999999,93.10000000000001,90.89,89.9,1.0,15,3
2,5,100,40,5000,95.98,94.82000000000001,90.51,89.68,1.0,15,3
3,5,100,10,128,94.08,90.14,90.49000000000001,89.9,1.0,15,3
6,5,100,60,5000,97.02,94.41000000000001,90.42,90.42,1.0,15,3
3,3,100,10,4096,95.46,92.2,91.07,89.9,1.0,15,3
5,1,100,80,5000,93.15,91.08000000000001,91.08000000000001,91.08000000000001,1.0,1,1
3,5,100,10,1024,95.19,91.64,90.82000000000001,89.9,1.0,15,3
2,8,100,20,1024,95.33,94.39999999999999,91.89,89.68,1.0,15,3
0,5,100,20,5000,97.96000000000001,92.58,90.2,90.2,1.0,15,3
0,5,100,100,7000,97.98,91.53999999999999,90.21000000000001,90.2,1.0,1,1
2,10,100,10,4096,95.87,94.14,91.97,89.68,1.0,15,3
4,1,100,30,2048,94.38666666666666,90.54666666666667,90.42333333333333,90.18,0.5,15,3
1,1,100,60,5000,97.74000000000001,88.64999999999999,88.64999999999999,88.64999999999999,1.0,1,1
4,5,100,20,1024,94.04,92.00666666666667,90.75,90.18,0.5,15,3
4,10,100,30,1024,94.94,93.05,90.85,90.18,1.0,15,3
4,3,100,30,2048,95.00999999999999,92.78999999999999,90.68,90.18,1.0,15,3
6,5,100,100,5000,97.11,93.15,90.42,90.42,1.0,15,3
2,10,100,10,1024,95.38,93.77,91.84,89.68,1.0,15,3
7,5,100,60,5000,96.67999999999999,94.87,89.74,89.72,1.0,1,1
2,5,100,20,1024,94.97333333333333,93.54333333333334,91.14,89.68,0.5,15,3
6,5,100,20,7000,96.78999999999999,93.61,90.42,90.42,1.0,1,1
9,5,100,20,5000,93.11,91.16,89.97,89.91,1.0,15,3
7,5,100,60,5000,96.47,94.58,89.74,89.72,1.0,1,1
7,5,100,10,128,95.36333333333333,91.45666666666666,90.31666666666666,89.72,0.5,15,3
6,5,100,10,2048,96.09666666666666,91.86333333333333,90.42,90.42,0.5,15,3
0,5,100,60,5000,98.11999999999999,92.56,90.22,90.2,1.0,15,3
3,1,100,20,5000,95.06,90.77,91.22,89.9,1.0,1,1
4,1,100,10,1024,94.89999999999999,91.32000000000001,91.44,90.18,1.0,15,3
7,5,100,100,7000,96.84,93.11,89.72,89.72,1.0,1,1
7,5,100,20,1024,96.76,94.14,90.71000000000001,89.72,1.0,15,3
3,5,100,30,128,92.75,91.23666666666666,90.25999999999999,89.9,0.5,15,3
2,5,100,10,128,94.74000000000001,90.79,90.77,89.68,1.0,15,3
1,5,100,40,7000,98.02,92.14,88.64999999999999,88.64999999999999,1.0,1,1
5,1,100,30,5000,93.03,91.08000000000001,91.08000000000001,91.08000000000001,1.0,1,1
2,5,100,100,7000,95.5,91.63,89.68,89.68,1.0,1,1
2,2,100,20,4096,95.8,94.46,91.99000000000001,89.68,1.0,15,3
3,10,100,30,4096,95.17,93.71000000000001,90.35,89.9,1.0,15,3
4,1,100,30,4096,94.47333333333333,90.45333333333333,90.35,90.18,0.5,15,3
7,5,100,20,1024,96.77,94.61,90.72,89.72,1.0,15,3
2,5,100,20,64,93.42,91.03999999999999,91.18,89.68,1.0,15,3
2,5,100,20,128,93.29666666666667,92.24,91.02333333333334,89.68,0.5,15,3
5,5,100,40,7000,93.49,92.17,91.09,91.08000000000001,1.0,1,1
3,8,100,10,2048,95.59,92.25,90.86999999999999,89.9,1.0,15,3
9,5,100,60,7000,92.65,92.10000000000001,89.91,89.91,1.0,1,1
5,1,100,40,5000,93.30000000000001,91.11,91.08000000000001,91.08000000000001,1.0,1,1
9,5,100,30,7000,92.38,91.24,89.92,89.91,1.0,1,1
3,5,100,50,7000,94.86,93.28999999999999,89.9,89.9,1.0,1,1
7,5,100,30,7000,96.89999999999999,94.96,90.27,89.72,1.0,1,1
8,5,100,50,7000,92.97,91.02,90.25999999999999,90.25999999999999,1.0,1,1
4,5,100,10,512,93.66,91.10000000000001,90.88000000000001,90.18,0.5,15,3
6,5,100,10,5000,96.94,92.78999999999999,90.42,90.42,1.0,15,3
5,5,100,20,5000,94.08,92.07,91.09,91.08000000000001,1.0,15,3
7,3,100,20,2048,96.19333333333333,93.16,90.53333333333333,89.72,0.5,15,3
3,5,100,20,32,90.89,90.03333333333333,90.07666666666667,89.9,0.5,15,3
2,1,100,20,1024,94.99333333333333,91.79333333333334,91.34666666666666,89.68,0.5,15,3
5,5,100,20,7000,93.92,91.53999999999999,91.08000000000001,91.08000000000001,1.0,1,1
3,5,100,20,64,92.84333333333333,91.07333333333332,90.67666666666668,89.9,0.5,15,3
3,5,100,10,256,94.73,90.64,90.60000000000001,89.9,1.0,15,3
3,5,100,40,5000,95.45,93.46,90.06,89.9,1.0,15,3
2,8,100,20,1024,95.54,94.42,91.8,89.68,1.0,15,3
6,5,100,60,7000,96.74000000000001,94.13,90.42,90.42,1.0,1,1
2,1,100,100,5000,95.43,89.68,89.68,89.68,1.0,1,1
3,3,100,20,2048,94.67333333333333,91.77,90.71000000000001,89.9,0.5,15,3
5,3,100,60,1024,93.47,92.07,91.08000000000001,91.08000000000001,1.0,15,3
3,5,100,20,128,94.28,90.10000000000001,90.38000000000001,89.9,1.0,15,3
8,1,100,20,5000,92.71000000000001,90.25999999999999,90.25999999999999,90.25999999999999,1.0,1,1
1,5,100,60,5000,98.16,94.45,88.64999999999999,88.64999999999999,1.0,15,3
3,5,100,20,1024,94.44,92.23333333333333,90.46,89.9,0.5,15,3
5,5,100,20,7000,93.8,91.61,91.09,91.08000000000001,1.0,1,1
7,8,100,20,2048,96.21666666666667,93.85,90.5,89.72,0.5,15,3
5,5,100,60,7000,93.13,92.27,91.08000000000001,91.08000000000001,1.0,1,1
0,5,100,50,7000,97.72999999999999,93.04,90.22,90.2,1.0,1,1
6,3,100,20,128,95.46333333333334,92.43666666666667,90.42333333333333,90.42,0.75,15,3
3,8,100,30,1024,94.56333333333333,92.71000000000001,90.28,89.9,0.5,15,3
1,1,100,40,5000,97.72,88.64999999999999,88.64999999999999,88.64999999999999,1.0,1,1
2,5,100,30,16,89.87333333333333,89.87333333333333,89.73666666666666,89.68,0.5,15,3
3,5,100,20,5000,95.45,93.02,91.18,89.9,1.0,15,3
2,1,100,20,4096,95.89,92.97,92.06,89.68,1.0,15,3
7,5,100,10,32,92.60666666666667,89.87,90.16333333333333,89.72,0.5,15,3
5,5,100,80,7000,93.45,92.29,91.08000000000001,91.08000000000001,1.0,1,1
0,2,100,40,1000,97.49,90.52,90.24,90.2,1.0,1,1
7,5,100,60,5000,96.89999999999999,95.07,89.72,89.72,1.0,15,3
0,5,100,20,7000,97.94,91.7,90.22,90.2,1.0,1,1
0,5,100,50,7000,97.81,93.58,90.23,90.2,1.0,1,1
4,5,100,10,16,90.98333333333333,90.25333333333333,90.64,90.18,0.5,15,3
7,5,100,50,5000,96.41999999999999,94.73,89.75999999999999,89.72,1.0,1,1
4,5,100,20,256,93.32666666666667,91.52333333333334,90.70666666666666,90.18,0.5,15,3
5,2,100,40,512,92.75,91.59,91.08000000000001,91.08000000000001,1.0,15,3
3,3,100,30,2048,95.26,92.62,90.27,89.9,1.0,15,3
3,3,100,10,2048,95.39999999999999,91.8,90.78,89.9,1.0,15,3
2,1,100,10,1024,95.56,92.17,92.02,89.68,1.0,15,3
1,5,100,20,7000,97.82,89.39,89.25,88.64999999999999,1.0,1,1
2,1,100,30,1024,95.34,92.08,91.18,89.68,1.0,15,3
0,5,100,20,5000,97.72999999999999,91.52,90.25999999999999,90.2,1.0,1,1
8,1,100,20,5000,93.47,90.25999999999999,90.25999999999999,90.25999999999999,1.0,15,3
5,5,100,60,7000,93.26,92.15,91.08000000000001,91.08000000000001,1.0,1,1
2,1,100,30,2048,95.48,91.73,91.18,89.68,1.0,15,3
7,5,100,30,7000,96.92,94.62,90.10000000000001,89.72,1.0,1,1
2,5,100,20,64,93.92,91.58,90.63,89.68,1.0,15,3
2,5,100,10,1024,95.59,93.28999999999999,91.62,89.68,1.0,15,3
4,5,100,60,7000,94.69999999999999,91.18,90.18,90.18,1.0,1,1
0,5,100,40,7000,97.72999999999999,93.41000000000001,90.2,90.2,1.0,1,1
0,5,100,40,5000,97.55,93.23,90.24,90.2,1.0,1,1
2,5,100,20,5000,95.15,94.03,91.93,89.68,1.0,1,1
4,8,100,20,2048,95.45,93.08,91.2,90.18,1.0,15,3
5,5,100,40,7000,93.58999999999999,92.38,91.08000000000001,91.08000000000001,1.0,1,1
7,5,100,40,1024,96.96000000000001,94.96,89.89,89.72,1.0,15,3
0,1,100,30,128,97.22,90.21000000000001,90.22,90.2,1.0,15,3
5,5,100,60,5000,93.58999999999999,92.94,91.08000000000001,91.08000000000001,1.0,15,3
5,5,100,40,5000,93.73,92.94,91.08000000000001,91.08000000000001,1.0,15,3
4,3,100,30,2048,94.92,92.46,90.47,90.18,1.0,15,3
7,2,100,20,2048,96.26333333333334,92.81666666666666,90.47666666666667,89.72,0.5,15,3
7,1,100,20,2048,97.1,92.0,90.48,89.72,1.0,15,3
3,8,100,20,4096,95.62,93.24,90.96,89.9,1.0,15,3
1,5,100,50,5000,97.65,93.07,88.64999999999999,88.64999999999999,1.0,1,1
7,1,100,40,5000,96.61999999999999,90.22,89.77000000000001,89.72,1.0,1,1
5,5,100,50,7000,93.46,92.75,91.08000000000001,91.08000000000001,1.0,1,1
2,5,100,40,5000,95.73,95.08,90.44,89.68,1.0,15,3
7,5,100,10,32,93.55,90.26666666666667,90.22333333333333,89.72,0.5,15,3
2,1,100,10,4096,94.49,91.27666666666666,91.00666666666667,89.68,0.5,15,3
3,5,100,40,7000,95.00999999999999,93.47999999999999,89.91,89.9,1.0,1,1
3,2,100,20,2048,94.53,91.06333333333333,90.5,89.9,0.5,15,3
2,3,100,50,64,93.42,90.01,89.72,89.68,1.0,15,3
4,5,100,30,1024,93.94333333333333,91.49333333333334,90.33,90.18,0.5,15,3
4,8,100,10,2048,93.81666666666668,91.36999999999999,90.66,90.18,0.5,15,3
4,2,100,30,4096,94.50666666666667,91.37666666666667,90.33666666666666,90.18,0.5,15,3
1,5,100,80,7000,97.91,93.69,88.64999999999999,88.64999999999999,1.0,1,1
6,5,100,20,5000,97.2,93.95,90.42,90.42,1.0,15,3
4,2,100,10,4096,93.58,91.14666666666666,90.72666666666666,90.18,0.5,15,3
7,5,100,20,16,91.64,91.48666666666668,90.64333333333333,89.72,0.5,15,3
3,5,100,10,16,92.60000000000001,89.96,89.9,89.9,1.0,15,3
4,2,100,30,2048,95.34,92.57,90.53,90.18,1.0,15,3
4,5,100,50,5000,94.99,92.86,90.18,90.18,1.0,15,3
3,5,100,80,7000,94.89999999999999,93.49,89.9,89.9,1.0,1,1
3,5,100,20,5000,95.56,93.26,90.72,89.9,1.0,15,3
7,3,100,50,256,94.0,92.0,89.72,89.72,0.25,15,3
0,5,100,10,7000,97.94,90.96,90.21000000000001,90.2,1.0,1,1
3,5,100,30,16,91.14,90.47,90.0,89.9,1.0,15,3
4,10,100,10,2048,95.46,92.29,91.27,90.18,1.0,15,3
3,5,100,20,7000,95.47,92.53,91.05,89.9,1.0,1,1
4,5,100,40,5000,95.15,92.74,90.3,90.18,1.0,15,3
2,10,100,30,1024,95.41,94.74000000000001,90.91,89.68,1.0,15,3
9,5,100,60,5000,92.95,92.44,89.91,89.91,1.0,15,3
9,5,100,100,5000,92.80000000000001,91.63,89.91,89.91,1.0,15,3
3,2,100,20,4096,94.59666666666666,91.41,90.68666666666667,89.9,0.5,15,3
0,5,100,50,7000,97.92,93.07,90.24,90.2,1.0,1,1
4,5,100,10,16,91.4,90.38000000000001,90.51,90.18,1.0,15,3
7,2,100,50,64,94.74333333333334,91.21000000000001,89.72666666666666,89.72,0.75,15,3
2,3,100,20,2048,95.13000000000001,93.71000000000001,91.23666666666666,89.68,0.5,15,3
9,5,100,80,5000,93.15,92.06,89.91,89.91,1.0,15,3
1,5,100,20,5000,98.19,89.85,89.28,88.64999999999999,1.0,15,3
3,5,100,50,7000,95.03,93.54,89.9,89.9,1.0,1,1
2,1,100,30,2048,94.93666666666667,90.92,90.70666666666666,89.68,0.5,15,3
0,5,100,80,7000,97.85000000000001,91.92,90.2,90.2,1.0,1,1
3,5,100,10,7000,95.14,91.62,91.0,89.9,1.0,1,1
3,5,100,50,7000,94.74000000000001,93.72,89.9,89.9,1.0,1,1
4,2,100,30,2048,94.46,91.27,90.32,90.18,0.5,15,3
3,10,100,30,1024,94.25333333333333,92.66,90.21666666666667,89.9,0.5,15,3
1,5,100,20,5000,98.28,90.36999999999999,89.25,88.64999999999999,1.0,15,3
4,5,100,30,32,92.55,90.34,90.21000000000001,90.18,1.0,15,3
3,3,100,10,1024,95.48,91.47999999999999,90.79,89.9,1.0,15,3
4,3,100,10,4096,93.67666666666666,91.50333333333333,90.69,90.18,0.5,15,3
7,3,100,80,32,94.17999999999999,89.86,89.72,89.72,1.0,15,3
3,1,100,30,2048,94.61,90.27,90.31333333333333,89.9,0.5,15,3
6,5,100,50,5000,96.84,95.21,90.42,90.42,1.0,15,3
2,1,100,30,4096,95.18666666666667,90.95,90.54333333333334,89.68,0.5,15,3
4,5,100,20,1024,95.35,92.49000000000001,91.11,90.18,1.0,15,3
3,2,100,20,2048,95.61,92.19000000000001,91.02,89.9,1.0,15,3
2,2,100,10,1024,95.62,93.89,91.8,89.68,1.0,15,3
3,5,100,20,32,92.44,90.02,89.9,89.9,1.0,15,3
9,3,100,20,512,92.22,90.03,89.91,89.91,1.0,15,3
7,5,100,30,7000,96.83,94.61,90.16,89.72,1.0,1,1
7,5,100,20,5000,97.22,94.96,90.75,89.72,1.0,15,3
2,5,100,30,16,90.05666666666666,90.88333333333334,89.78666666666668,89.68,0.5,15,3
4,5,100,100,5000,95.62,90.99000000000001,90.18,90.18,1.0,15,3
5,5,100,40,1024,93.21000000000001,92.75,91.08000000000001,91.08000000000001,1.0,15,3
4,5,100,40,7000,95.08,92.12,90.19,90.18,1.0,1,1
6,5,100,40,1024,96.83,94.48,90.42,90.42,1.0,15,3
3,5,100,40,7000,95.0,93.72,89.92,89.9,1.0,1,1
2,5,100,60,5000,95.87,94.83,89.75,89.68,1.0,15,3
4,3,100,20,1024,95.24000000000001,93.0,91.42,90.18,1.0,15,3
1,5,100,10,5000,98.28,89.25999999999999,89.42,88.64999999999999,1.0,15,3
3,5,100,30,2048,95.17999999999999,93.01,90.42,89.9,1.0,15,3
3,5,100,40,7000,94.83,93.60000000000001,89.92999999999999,89.9,1.0,1,1
4,5,100,40,5000,95.61,93.22,90.24,90.18,1.0,15,3
3,2,100,20,4096,95.5,92.02,90.98,89.9,1.0,15,3
3,3,100,30,1024,95.13000000000001,92.73,90.59,89.9,1.0,15,3
5,5,100,40,512,92.58,92.29666666666667,91.08000000000001,91.08000000000001,0.5,15,3
7,10,100,40,256,96.13000000000001,93.14,89.92,89.72,1.0,15,3
4,1,100,30,2048,95.39,90.94,90.53999999999999,90.18,1.0,15,3
2,1,100,30,2048,95.67,91.61,91.38,89.68,1.0,15,3
4,5,100,40,1024,95.3,92.66,90.29,90.18,1.0,15,3
4,5,100,100,5000,95.08,91.06,90.18,90.18,1.0,15,3
5,5,100,100,7000,93.47,91.67999999999999,91.08000000000001,91.08000000000001,1.0,1,1
3,5,100,30,1024,94.91000000000001,92.75,90.64999999999999,89.9,1.0,15,3
7,2,100,20,2048,96.92,94.08999999999999,90.63,89.72,1.0,15,3
3,5,100,30,7000,95.14,93.14,90.64999999999999,89.9,1.0,1,1
2,1,100,20,2048,95.69,92.64,91.84,89.68,1.0,15,3
2,8,100,30,1024,94.88333333333333,94.04666666666667,90.53,89.68,0.5,15,3
2,1,100,10,2048,94.58,91.35333333333332,91.09333333333333,89.68,0.5,15,3
9,5,100,30,5000,93.10000000000001,91.61,89.92999999999999,89.91,1.0,15,3
2,1,100,30,2048,95.52000000000001,91.47999999999999,91.11,89.68,1.0,15,3
2,5,100,20,256,93.70333333333333,93.20666666666668,90.96,89.68,0.5,15,3
2,5,100,20,128,94.44,91.14999999999999,91.60000000000001,89.68,1.0,15,3
0,1,100,100,5000,98.00999999999999,90.2,90.2,90.2,1.0,1,1
4,2,100,30,2048,95.32000000000001,91.91,90.53,90.18,1.0,15,3
2,5,100,40,7000,95.09,94.92,90.24,89.68,1.0,1,1
2,3,100,30,4096,95.04333333333334,93.93666666666667,90.58333333333334,89.68,0.5,15,3
2,2,100,20,4096,95.88,94.38,91.94,89.68,1.0,15,3
3,3,100,30,2048,94.64333333333333,92.4,90.25333333333333,89.9,0.5,15,3
4,5,100,10,1024,95.27,91.91,91.33,90.18,1.0,15,3
3,5,100,30,16,90.12666666666667,89.92,89.90333333333334,89.9,0.5,15,3
2,5,100,20,16,90.23,90.53666666666666,90.46666666666667,89.68,0.5,15,3
2,2,100,10,4096,95.94,94.08,92.14,89.68,1.0,15,3
3,5,100,30,16,90.35333333333332,90.03333333333333,89.92999999999999,89.9,0.5,15,3
6,5,100,50,7000,96.75,94.77,90.42,90.42,1.0,1,1
8,5,100,100,128,91.57,90.27,90.25999999999999,90.25999999999999,1.0,15,3
4,5,100,30,2048,95.15,93.24,90.66,90.18,1.0,15,3
4,3,100,30,1024,93.93666666666667,91.73333333333333,90.45,90.18,0.5,15,3
2,2,100,20,2048,95.12666666666667,93.13,91.13,89.68,0.5,15,3
2,5,100,40,5000,95.8,95.54,90.47,89.68,1.0,15,3
3,8,100,10,4096,95.48,92.25999999999999,90.99000000000001,89.9,1.0,15,3
3,5,100,100,7000,95.08,91.96,89.9,89.9,1.0,1,1
7,5,100,30,256,96.15,93.12,90.05,89.72,1.0,15,3
9,5,100,100,7000,92.56,91.32000000000001,89.91,89.91,1.0,1,1
3,5,100,30,128,93.86,90.31,90.45,89.9,1.0,15,3
7,5,100,10,5000,97.32,94.01,91.11,89.72,1.0,15,3
3,5,100,20,7000,95.33,92.72,91.12,89.9,1.0,1,1
5,5,100,60,5000,93.95,92.75999999999999,91.08000000000001,91.08000000000001,1.0,15,3
5,5,100,40,5000,93.42,92.21000000000001,91.08000000000001,91.08000000000001,1.0,1,1
2,1,100,30,2048,95.23,91.52,91.14,89.68,1.0,15,3
9,5,100,40,1024,92.36,91.75,89.95,89.91,1.0,15,3
3,5,100,10,16,92.31,89.9,89.98,89.9,1.0,15,3
3,5,100,20,1024,94.77666666666667,92.08333333333333,90.70333333333333,89.9,0.5,15,3
1,5,100,60,5000,97.99,94.33,88.64999999999999,88.64999999999999,1.0,15,3
7,5,100,20,16,92.48666666666666,90.79,89.93333333333334,89.72,0.5,15,3
9,5,100,40,5000,93.0,92.57,89.94,89.91,1.0,15,3
2,10,100,30,4096,96.03,94.97,91.11,89.68,1.0,15,3
4,1,100,30,2048,94.23333333333333,90.45666666666666,90.42999999999999,90.18,0.5,15,3
7,5,100,10,64,94.55666666666667,91.23666666666666,90.57333333333332,89.72,0.5,15,3
3,5,100,20,1024,94.62,91.93666666666667,90.71000000000001,89.9,0.5,15,3
3,5,100,20,7000,95.24000000000001,92.5,91.05,89.9,1.0,1,1
5,5,100,60,7000,93.69,92.52,91.08000000000001,91.08000000000001,1.0,1,1
2,5,100,80,5000,95.95,93.08,89.69,89.68,1.0,15,3
3,5,100,100,5000,95.19999999999999,93.25,89.9,89.9,1.0,15,3
2,5,100,20,128,92.95,92.82666666666667,91.00333333333333,89.68,0.5,15,3
3,5,100,100,5000,95.00999999999999,92.85,89.9,89.9,1.0,1,1
7,5,100,60,7000,96.77,94.88,89.73,89.72,1.0,1,1
4,2,100,10,1024,93.76333333333334,91.23333333333333,90.71666666666667,90.18,0.5,15,3
0,5,100,30,5000,97.98,92.91,90.28,90.2,1.0,15,3
3,5,100,30,7000,95.26,93.30000000000001,90.88000000000001,89.9,1.0,1,1
2,5,100,10,64,92.57666666666667,90.97666666666666,90.31666666666666,89.68,0.5,15,3
4,5,100,20,5000,94.85,92.42,91.03999999999999,90.18,1.0,1,1
5,5,100,100,7000,93.38,91.60000000000001,91.08000000000001,91.08000000000001,1.0,1,1
9,5,100,80,5000,92.99,92.01,89.91,89.91,1.0,15,3
9,5,100,40,1024,92.93,92.17999999999999,89.92,89.91,1.0,15,3
2,5,100,60,7000,95.09,93.78999999999999,89.7,89.68,1.0,1,1
0,5,100,40,5000,98.07000000000001,92.61,90.2,90.2,1.0,15,3
8,5,100,40,7000,93.11,91.47999999999999,90.25999999999999,90.25999999999999,1.0,1,1
4,5,100,60,5000,95.12,92.36999999999999,90.18,90.18,1.0,15,3
4,5,100,60,7000,94.93,91.63,90.18,90.18,1.0,1,1
9,5,100,50,7000,92.58,92.25,89.91,89.91,1.0,1,1
4,2,100,10,2048,93.94666666666667,91.42333333333333,90.67666666666668,90.18,0.5,15,3
6,5,100,30,1024,96.49,94.38333333333333,90.42,90.42,0.75,15,3
3,5,100,10,7000,95.15,91.84,91.03999999999999,89.9,1.0,1,1
2,5,100,20,512,94.35666666666667,93.32333333333334,91.0,89.68,0.5,15,3
3,3,100,30,1024,95.14,92.45,90.48,89.9,1.0,15,3
2,1,100,80,5000,95.16,89.68,89.68,89.68,1.0,1,1
3,5,100,30,1024,95.12,93.0,90.47,89.9,1.0,15,3
3,5,100,10,16,90.89333333333333,90.13333333333333,90.10666666666667,89.9,0.5,15,3
6,5,100,100,7000,97.03,92.36999999999999,90.42,90.42,1.0,1,1
3,2,100,80,1024,94.91000000000001,91.74,89.9,89.9,1.0,15,3
1,10,100,80,16,94.84,88.64999999999999,88.64999999999999,88.64999999999999,1.0,15,3
4,8,100,20,2048,95.91,93.44,91.16,90.18,1.0,15,3
2,1,100,30,4096,95.11333333333334,91.10333333333334,90.59333333333333,89.68,0.5,15,3
1,5,100,20,7000,97.98,89.63,89.33,88.64999999999999,1.0,1,1
5,5,100,100,7000,93.28,91.89,91.08000000000001,91.08000000000001,1.0,1,1
3,5,100,50,7000,95.0,93.39,89.9,89.9,1.0,1,1
3,5,100,20,16,90.91,90.22,89.9,89.9,1.0,15,3
9,5,100,40,128,91.10000000000001,89.92,89.91,89.91,1.0,15,3
2,5,100,20,256,94.08999999999999,93.15,90.97,89.68,0.5,15,3
2,1,100,30,1024,94.61,90.93,90.59,89.68,0.5,15,3
2,5,100,30,5000,95.78,95.11,90.96,89.68,1.0,15,3
3,2,100,30,6131,95.28,92.60000000000001,90.61,89.9,1.0,3,3
4,5,100,20,7000,95.53,92.78,91.25999999999999,90.18,1.0,1,1
5,5,100,10,7000,93.69,91.12,91.08000000000001,91.08000000000001,1.0,1,1
3,5,100,10,16,90.57,89.97,90.11,89.9,0.5,15,3
7,5,100,20,128,94.93,92.42333333333333,90.35333333333332,89.72,0.5,15,3
6,5,100,20,5000,96.77,93.63,90.42,90.42,1.0,1,1
7,5,100,10,1024,96.11333333333333,92.75333333333333,90.72666666666666,89.72,0.5,15,3
5,5,100,60,7000,93.37,92.12,91.08000000000001,91.08000000000001,1.0,1,1
8,5,100,30,5000,92.66,90.64,90.25999999999999,90.25999999999999,1.0,1,1
2,5,100,40,5000,95.67999999999999,95.07,90.42999999999999,89.68,1.0,15,3
4,2,100,10,2048,93.65666666666667,91.23666666666666,90.7,90.18,0.5,15,3
4,1,100,30,512,92.68333333333332,90.21333333333334,90.21666666666667,90.18,0.25,15,3
2,10,100,20,2048,95.46,94.78999999999999,91.89,89.68,1.0,15,3
0,5,100,40,5000,98.0,93.31,90.25999999999999,90.2,1.0,15,3
7,5,100,30,1024,96.26,93.94,90.20666666666666,89.72,0.5,15,3
3,8,100,20,2048,94.74000000000001,92.32000000000001,90.58,89.9,0.5,15,3
6,10,100,60,64,93.58666666666666,92.11333333333333,90.42,90.42,0.5,15,3
6,5,100,20,5000,96.61999999999999,93.5,90.42,90.42,1.0,1,1
3,1,100,60,5000,94.64,89.91,89.9,89.9,1.0,1,1
6,5,100,80,7000,96.92,93.08999999999999,90.42,90.42,1.0,1,1
5,5,100,80,7000,93.22,92.4,91.08000000000001,91.08000000000001,1.0,1,1
7,5,100,60,7000,96.63000000000001,94.71000000000001,89.72,89.72,1.0,1,1
4,3,100,30,4096,94.54,91.76666666666667,90.41,90.18,0.5,15,3
7,1,100,20,512,94.87,90.11,90.12,89.72,0.25,15,3
2,5,100,10,256,95.05,91.27,91.79,89.68,1.0,15,3
3,5,100,30,256,93.50333333333334,91.60000000000001,90.10000000000001,89.9,0.5,15,3
7,1,100,30,5000,96.63000000000001,90.71000000000001,90.16,89.72,1.0,1,1
3,5,100,40,7000,95.1,93.57,89.92,89.9,1.0,1,1
3,5,100,40,5000,95.36,93.49,90.16999999999999,89.9,1.0,15,3
2,5,100,60,7000,94.89,93.67999999999999,89.68,89.68,1.0,1,1
1,5,100,40,7000,97.96000000000001,92.28,88.64999999999999,88.64999999999999,1.0,1,1
3,1,100,50,5000,94.94,89.96,89.9,89.9,1.0,1,1
1,3,100,50,128,96.29333333333334,89.44,88.64999999999999,88.64999999999999,0.5,15,3
3,2,100,10,2048,94.41666666666667,90.79333333333334,90.53999999999999,89.9,0.5,15,3
8,5,100,50,7000,92.4,91.11,90.25999999999999,90.25999999999999,1.0,1,1
7,5,100,20,5000,97.22,95.00999999999999,90.53999999999999,89.72,1.0,15,3
2,3,100,10,4096,95.74000000000001,94.02000000000001,91.77,89.68,1.0,15,3
3,8,100,10,1024,95.25,92.04,91.09,89.9,1.0,15,3
4,5,100,100,7000,94.83,90.56,90.18,90.18,1.0,1,1
6,5,100,40,1024,96.84,94.45,90.42,90.42,1.0,15,3
3,5,100,30,16,90.36333333333333,90.30333333333334,89.92999999999999,89.9,0.5,15,3
7,5,100,60,7000,97.03,94.97,89.72,89.72,1.0,1,1
3,5,100,20,1024,94.67,92.01333333333334,90.66,89.9,0.5,15,3
6,2,100,60,64,93.58333333333333,91.54666666666667,90.42,90.42,0.5,15,3
3,1,100,40,5000,94.97,90.18,90.03,89.9,1.0,1,1
4,1,100,80,5000,94.99,90.19,90.18,90.18,1.0,1,1
4,5,100,10,32,91.61333333333333,90.25666666666666,90.38666666666667,90.18,0.5,15,3
4,1,100,20,5000,94.94,91.12,90.98,90.18,1.0,1,1
4,2,100,20,2048,94.43333333333334,91.68333333333332,90.9,90.18,0.5,15,3
4,10,100,20,2048,95.35,92.96,91.14999999999999,90.18,1.0,15,3
8,2,100,20,256,92.58,90.29,90.25999999999999,90.25999999999999,1.0,15,3
4,5,100,30,5000,95.28,93.07,90.79,90.18,1.0,15,3
3,5,100,20,5000,95.28999999999999,92.91,90.71000000000001,89.9,1.0,15,3
6,5,100,30,5000,97.02,94.75,90.42,90.42,1.0,15,3
4,5,100,20,32,91.30333333333334,90.30333333333334,90.41,90.18,0.5,15,3
6,5,100,40,7000,96.75,94.66,90.42,90.42,1.0,1,1
2,2,100,20,2048,95.69,94.32000000000001,92.04,89.68,1.0,15,3
5,5,100,60,7000,93.43,92.34,91.08000000000001,91.08000000000001,1.0,1,1
3,5,100,30,6131,95.63000000000001,93.67,90.44,89.9,1.0,3,3
2,2,100,30,4096,95.20666666666666,93.31,90.77,89.68,0.5,15,3
2,3,100,30,1024,94.81333333333333,93.58,90.65666666666667,89.68,0.5,15,3
2,3,100,30,2048,95.33,94.77,91.02,89.68,1.0,15,3
3,5,100,40,5000,95.08,94.08999999999999,90.09,89.9,1.0,15,3
1,1,100,20,32,94.90333333333332,88.64999999999999,88.64999999999999,88.64999999999999,0.5,15,3
6,10,100,100,256,96.09,92.75999999999999,90.42,90.42,1.0,15,3
2,5,100,10,1024,95.30999999999999,93.67999999999999,91.75999999999999,89.68,1.0,15,3
3,5,100,20,256,93.78999999999999,91.62,90.49333333333334,89.9,0.5,15,3
2,5,100,10,32,91.51666666666667,90.16999999999999,90.2,89.68,0.5,15,3
3,10,100,20,4096,95.47,93.46,91.14,89.9,1.0,15,3
7,5,100,50,7000,96.85000000000001,94.89,89.74,89.72,1.0,1,1
3,5,100,10,1024,95.27,91.58,90.86,89.9,1.0,15,3
3,3,100,40,128,91.69,91.2,89.91333333333333,89.9,0.25,15,3
4,1,100,20,5000,94.65,91.23,91.3,90.18,1.0,1,1
0,5,100,30,5000,98.00999999999999,92.58,90.27,90.2,1.0,15,3
4,1,100,10,2048,95.53,91.81,91.31,90.18,1.0,15,3
3,5,100,40,7000,95.02000000000001,93.17,89.97,89.9,1.0,1,1
6,5,100,20,5000,96.93,94.32000000000001,90.42,90.42,1.0,15,3
5,1,100,10,5000,92.49666666666667,91.08000000000001,91.08000000000001,91.08000000000001,0.5,15,3
8,5,100,60,5000,92.93,91.33,90.25999999999999,90.25999999999999,1.0,15,3
2,5,100,10,128,93.36,91.33,90.86,89.68,0.5,15,3
5,5,100,10,5000,94.0,91.47,91.08000000000001,91.08000000000001,1.0,15,3
3,5,100,10,32,92.01666666666667,89.96,90.03,89.9,0.5,15,3
3,1,100,20,4096,95.61,90.97,90.85,89.9,1.0,15,3
3,5,100,50,7000,94.77,93.65,89.9,89.9,1.0,1,1
5,5,100,40,1024,93.21000000000001,92.32000000000001,91.08000000000001,91.08000000000001,1.0,15,3
4,2,100,20,1024,95.19999999999999,92.69,91.08000000000001,90.18,1.0,15,3
6,5,100,50,7000,96.77,94.57,90.42,90.42,1.0,1,1
1,5,100,20,5000,98.2,89.37,89.42,88.64999999999999,1.0,15,3
3,5,100,30,128,93.30666666666667,91.33333333333333,90.24333333333333,89.9,0.5,15,3
3,5,100,20,16,90.66,89.99000000000001,89.9,89.9,1.0,15,3
9,10,100,60,64,89.96333333333332,90.15333333333334,89.91,89.91,0.25,15,3
4,5,100,80,7000,94.8,90.79,90.18,90.18,1.0,1,1
4,5,100,30,16,90.59,90.36,90.22,90.18,1.0,15,3
3,5,100,40,5000,95.07,93.83,90.09,89.9,1.0,15,3
6,5,100,10,5000,96.93,92.88,90.42,90.42,1.0,15,3
4,10,100,10,2048,95.28999999999999,92.32000000000001,91.53999999999999,90.18,1.0,15,3
7,5,100,10,16,93.67999999999999,91.71000000000001,91.03,89.72,1.0,15,3
7,5,100,20,7000,96.89999999999999,94.45,91.02,89.72,1.0,1,1
6,10,100,10,32,93.60333333333334,90.51333333333334,90.42,90.42,0.5,15,3
2,5,100,20,64,92.23666666666666,92.24,90.80333333333334,89.68,0.5,15,3
2,5,100,20,32,92.82000000000001,91.55,90.25,89.68,1.0,15,3
3,2,100,30,1024,95.34,92.19000000000001,90.35,89.9,1.0,15,3
1,5,100,40,5000,98.05,93.11,88.64999999999999,88.64999999999999,1.0,15,3
3,5,100,20,256,94.64,91.34,90.55,89.9,1.0,15,3
2,5,100,10,1024,94.68666666666667,92.97,91.00666666666667,89.68,0.5,15,3
3,5,100,20,7000,95.45,92.46,91.36,89.9,1.0,1,1
7,5,100,10,16,94.59,91.36999999999999,90.36,89.72,1.0,15,3
2,1,100,20,5000,95.59,92.10000000000001,92.0,89.68,1.0,1,1
3,5,100,50,7000,94.92,93.8,89.92,89.9,1.0,1,1
4,10,100,30,2048,95.28,92.84,90.66,90.18,1.0,15,3
2,5,100,10,16,91.52666666666667,90.06,90.45333333333333,89.68,0.5,15,3
3,5,100,20,4096,95.56,92.71000000000001,91.0,89.9,1.0,15,3
7,1,100,20,2048,96.26666666666667,91.05,90.41,89.72,0.5,15,3
7,3,100,20,2048,96.03,93.30333333333334,90.46,89.72,0.5,15,3
4,5,100,30,16,90.25333333333333,90.22666666666666,90.20333333333333,90.18,0.5,15,3
3,5,100,10,16,91.61333333333333,89.94666666666666,89.97,89.9,0.5,15,3
7,5,100,10,256,95.75,92.12,90.48666666666666,89.72,0.5,15,3
0,3,100,50,2048,97.83,91.7,90.23,90.2,1.0,15,3
1,5,100,40,1024,97.99,92.17999999999999,88.64999999999999,88.64999999999999,1.0,15,3
7,5,100,30,256,96.2,92.54,90.41,89.72,1.0,15,3
2,5,100,60,7000,95.38,93.62,89.68,89.68,1.0,1,1
4,5,100,10,32,92.69,91.36999999999999,91.22,90.18,1.0,15,3
0,5,100,20,7000,97.96000000000001,92.04,90.21000000000001,90.2,1.0,1,1
3,8,100,20,2048,94.44,92.30333333333334,90.57666666666667,89.9,0.5,15,3
3,5,100,50,7000,94.93,93.25,89.9,89.9,1.0,1,1
8,3,100,50,512,91.55333333333333,90.38333333333334,90.25999999999999,90.25999999999999,0.5,15,3
7,5,100,20,128,94.94,92.45333333333333,90.32333333333334,89.72,0.5,15,3
9,5,100,10,7000,92.58999999999999,90.23,90.02,89.91,1.0,1,1
4,1,100,10,4096,95.26,91.25,91.12,90.18,1.0,15,3
3,2,100,50,64,93.62,90.03,89.9,89.9,1.0,15,3
7,5,100,30,5000,96.89,94.65,90.16999999999999,89.72,1.0,15,3
2,5,100,30,32,91.62,89.96,89.92,89.68,1.0,15,3
2,5,100,30,4096,95.77,95.35,90.98,89.68,1.0,15,3
3,1,100,20,1024,95.0,90.64,91.02,89.9,1.0,15,3
1,5,100,10,5000,97.92,88.67,89.23,88.64999999999999,1.0,1,1
3,5,100,50,7000,94.93,93.5,89.91,89.9,1.0,1,1
4,8,100,20,2048,94.44666666666667,91.72333333333333,90.71666666666667,90.18,0.5,15,3
7,5,100,30,1024,96.78999999999999,95.05,90.51,89.72,1.0,15,3
4,5,100,10,5000,95.27,92.78,91.36,90.18,1.0,15,3
2,2,100,20,2048,94.94666666666667,93.01666666666667,91.10666666666667,89.68,0.5,15,3
4,5,100,30,1024,94.39,91.81666666666666,90.21666666666667,90.18,0.5,15,3
8,5,100,10,5000,93.33,90.32,90.25999999999999,90.25999999999999,1.0,15,3
0,5,100,100,7000,98.0,91.18,90.2,90.2,1.0,1,1
2,5,100,20,1024,94.67666666666666,93.8,91.33,89.68,0.5,15,3
3,1,100,10,4096,95.5,91.0,91.08000000000001,89.9,1.0,15,3
4,5,100,30,7000,94.87,92.56,90.42999999999999,90.18,1.0,1,1
2,5,100,30,5000,95.0,94.49,91.02,89.68,1.0,1,1
3,3,100,30,1024,94.62333333333333,92.7,90.15333333333334,89.9,0.5,15,3
2,2,100,20,4096,94.96333333333334,93.04666666666667,91.14999999999999,89.68,0.5,15,3
3,10,100,20,2048,95.19999999999999,93.33,91.12,89.9,1.0,15,3
3,5,100,20,1024,94.41666666666667,91.63333333333334,90.53999999999999,89.9,0.5,15,3
3,5,100,30,512,94.10333333333334,92.24,90.2,89.9,0.5,15,3
7,2,100,10,2048,97.24000000000001,93.78999999999999,91.18,89.72,1.0,15,3
5,5,100,60,7000,93.87,92.58,91.08000000000001,91.08000000000001,1.0,1,1
4,5,100,10,1024,95.39,92.7,91.47,90.18,1.0,15,3
3,8,100,10,4096,95.77,92.47999999999999,91.02,89.9,1.0,15,3
2,2,100,10,2048,94.84,92.95,91.18333333333334,89.68,0.5,15,3
3,3,100,30,1024,95.28999999999999,93.22,90.48,89.9,1.0,15,3
3,3,100,20,2048,94.56,91.75333333333333,90.53666666666666,89.9,0.5,15,3
5,1,100,20,5000,93.57,91.08000000000001,91.08000000000001,91.08000000000001,1.0,1,1
7,5,100,20,64,95.50999999999999,90.34,89.99000000000001,89.72,1.0,15,3
1,5,100,40,7000,97.87,92.58,88.64999999999999,88.64999999999999,1.0,1,1
7,5,100,30,16,90.78,89.72,89.72,89.72,1.0,15,3
2,3,100,20,2048,94.94,93.56,91.32333333333334,89.68,0.5,15,3
1,5,100,30,5000,98.22,90.25999999999999,88.74,88.64999999999999,1.0,15,3
2,1,100,10,1024,94.78999999999999,90.84,91.13666666666667,89.68,0.5,15,3
2,2,100,10,2048,94.61666666666667,92.66666666666666,91.11,89.68,0.5,15,3
5,10,100,20,512,92.26333333333334,91.33,91.08000000000001,91.08000000000001,0.25,15,3
2,5,100,30,16,90.81,89.85,89.78,89.68,1.0,15,3
0,5,100,100,5000,98.2,91.49000000000001,90.21000000000001,90.2,1.0,15,3
7,5,100,20,16,91.70333333333333,90.07666666666667,90.16999999999999,89.72,0.5,15,3
4,2,100,10,2048,93.78666666666666,91.33,90.69,90.18,0.5,15,3
7,1,100,20,2048,96.31,90.94333333333333,90.56333333333333,89.72,0.5,15,3
2,5,100,20,1024,95.63000000000001,93.94,91.66,89.68,1.0,15,3
2,5,100,30,7000,95.28,94.34,91.36,89.68,1.0,1,1
8,5,100,30,7000,93.02,91.10000000000001,90.25999999999999,90.25999999999999,1.0,1,1
7,10,100,40,512,95.7,94.62333333333333,89.84333333333333,89.72,0.5,15,3
2,3,100,30,2048,95.23333333333333,93.90333333333334,90.71000000000001,89.68,0.5,15,3
1,5,100,30,7000,98.00999999999999,90.32,88.64999999999999,88.64999999999999,1.0,1,1
3,1,100,30,6131,95.63000000000001,90.52,90.68,89.9,1.0,3,3
5,5,100,20,7000,94.13,91.72,91.08000000000001,91.08000000000001,1.0,1,1
1,5,100,40,7000,98.00999999999999,92.56,88.64999999999999,88.64999999999999,1.0,1,1
1,5,100,50,7000,97.92999999999999,93.8,88.64999999999999,88.64999999999999,1.0,1,1
4,5,100,20,64,91.44666666666666,90.68666666666667,90.46333333333332,90.18,0.5,15,3
6,1,100,50,5000,96.52,90.42,90.42,90.42,1.0,1,1
3,5,100,20,7000,95.19,92.16,91.09,89.9,1.0,1,1
8,1,100,40,5000,92.83,90.27,90.25999999999999,90.25999999999999,1.0,1,1
3,5,100,30,5000,95.28,93.25,90.42999999999999,89.9,1.0,15,3
8,5,100,80,5000,92.97999999999999,90.94,90.25999999999999,90.25999999999999,1.0,15,3
2,5,100,30,7000,95.33,94.76,91.24,89.68,1.0,1,1
3,5,100,40,7000,95.00999999999999,93.5,90.03999999999999,89.9,1.0,1,1
6,5,100,50,7000,96.78999999999999,94.39999999999999,90.42,90.42,1.0,1,1
3,5,100,80,7000,94.83,93.21000000000001,89.9,89.9,1.0,1,1
2,8,100,30,4096,95.89,95.09,90.92,89.68,1.0,15,3
7,5,100,30,64,95.46,90.78,90.14,89.72,1.0,15,3
2,5,100,80,5000,95.5,93.34,89.7,89.68,1.0,15,3
4,8,100,10,1024,93.81666666666668,91.47999999999999,90.73666666666666,90.18,0.5,15,3
4,3,100,20,2048,94.20666666666668,91.82000000000001,90.65666666666667,90.18,0.5,15,3
1,5,100,20,5000,98.16,89.35,89.49000000000001,88.64999999999999,1.0,15,3
4,5,100,10,2048,95.25,92.33,91.17,90.18,1.0,15,3
7,5,100,30,512,95.84333333333333,93.71666666666667,90.0,89.72,0.5,15,3
7,5,100,40,5000,97.08,95.11,90.03999999999999,89.72,1.0,15,3
2,5,100,40,7000,95.23,94.91000000000001,90.19,89.68,1.0,1,1
1,1,100,30,5000,97.78999999999999,88.64999999999999,88.64999999999999,88.64999999999999,1.0,1,1
2,5,100,100,7000,95.09,91.56,89.68,89.68,1.0,1,1
6,5,100,60,5000,96.7,94.56,90.42,90.42,1.0,1,1
4,1,100,10,2048,93.63666666666667,90.62666666666667,90.72,90.18,0.5,15,3
3,5,100,20,7000,95.42,92.63,91.03999999999999,89.9,1.0,1,1
1,5,100,60,5000,98.05,93.89,88.64999999999999,88.64999999999999,1.0,15,3
3,5,100,30,32,92.0,89.97,89.9,89.9,1.0,15,3
2,3,100,10,2048,94.64666666666666,92.94333333333333,91.13666666666667,89.68,0.5,15,3
5,10,100,20,64,91.45333333333333,91.09666666666666,91.08000000000001,91.08000000000001,0.5,15,3
4,8,100,20,2048,94.22333333333334,91.69333333333334,90.62666666666667,90.18,0.5,15,3
7,5,100,50,64,93.39,91.96666666666667,89.72,89.72,0.5,15,3
8,5,100,80,7000,92.56,90.83,90.25999999999999,90.25999999999999,1.0,1,1
3,5,100,100,512,94.26,92.06,89.9,89.9,0.5,15,3
4,1,100,10,4096,93.85,90.60666666666667,90.69333333333334,90.18,0.5,15,3
3,5,100,20,16,90.47666666666667,89.92666666666666,89.9,89.9,0.5,15,3
3,3,100,10,2048,95.5,91.82000000000001,90.9,89.9,1.0,15,3
3,5,100,20,128,94.37,90.42,90.53,89.9,1.0,15,3
2,5,100,10,16,92.31,90.08,90.19,89.68,1.0,15,3
5,2,100,50,512,92.78999999999999,91.72,91.08000000000001,91.08000000000001,1.0,15,3
6,5,100,10,5000,97.03,92.86,90.42,90.42,1.0,15,3
2,5,100,30,2048,95.54,94.73,91.08000000000001,89.68,1.0,15,3
2,5,100,10,32,93.47,90.86999999999999,90.62,89.68,1.0,15,3
2,5,100,30,5958,95.86,95.28999999999999,90.95,89.68,1.0,3,3
7,3,100,30,2048,97.0,94.53,90.25999999999999,89.72,1.0,15,3
3,1,100,20,2048,94.66,90.56,90.63333333333333,89.9,0.5,15,3
8,5,100,30,5000,93.23,91.14,90.25999999999999,90.25999999999999,1.0,15,3
3,10,100,10,2048,95.63000000000001,92.38,91.01,89.9,1.0,15,3
5,5,100,20,2048,93.53333333333333,91.61666666666667,91.08333333333334,91.08000000000001,0.75,15,3
2,5,100,60,7000,95.24000000000001,93.75,89.71000000000001,89.68,1.0,1,1
4,2,100,20,512,93.72,91.18666666666667,90.66333333333333,90.18,0.5,15,3
7,5,100,100,7000,96.88,93.27,89.72,89.72,1.0,1,1
1,5,100,40,5000,98.13,93.34,88.64999999999999,88.64999999999999,1.0,15,3
3,5,100,30,32,91.96,90.05,89.95,89.9,1.0,15,3
7,1,100,80,5000,96.72,89.73,89.73,89.72,1.0,1,1
7,5,100,80,7000,96.75,94.19999999999999,89.72,89.72,1.0,1,1
7,5,100,10,1024,96.11666666666666,92.58999999999999,90.50666666666667,89.72,0.5,15,3
0,5,100,100,5000,98.11,91.17,90.2,90.2,1.0,15,3
4,5,100,20,2048,93.99666666666666,91.81666666666666,90.7,90.18,0.5,15,3
4,5,100,10,64,91.99666666666667,90.49666666666667,90.76666666666667,90.18,0.5,15,3
3,5,100,100,7000,94.95,92.75,89.9,89.9,1.0,1,1
4,5,100,20,1024,94.17,91.82000000000001,90.63333333333333,90.18,0.5,15,3
7,5,100,10,32,94.96,90.99000000000001,89.72,89.72,1.0,15,3
7,5,100,50,5000,96.75,95.35,89.75,89.72,1.0,15,3
8,5,100,80,7000,92.84,90.89,90.25999999999999,90.25999999999999,1.0,1,1
1,5,100,50,7000,98.13,94.33,88.64999999999999,88.64999999999999,1.0,1,1
4,1,100,20,1024,95.15,91.86,91.14,90.18,1.0,15,3
5,1,100,40,5000,93.42,91.08000000000001,91.08000000000001,91.08000000000001,1.0,1,1
4,5,100,10,16,90.63333333333333,90.18,90.32666666666667,90.18,0.5,15,3
3,5,100,30,2048,94.67666666666666,92.67,90.20666666666666,89.9,0.5,15,3
3,8,100,30,4096,95.33,94.01,90.57,89.9,1.0,15,3
1,3,100,40,2048,97.91333333333333,91.20333333333333,88.64999999999999,88.64999999999999,0.75,15,3
2,5,100,30,2048,95.48,94.74000000000001,90.96,89.68,1.0,15,3
4,3,100,10,2048,95.17,92.24,91.47,90.18,1.0,15,3
8,5,100,40,7000,92.96,91.47999999999999,90.25999999999999,90.25999999999999,1.0,1,1
2,8,100,30,2048,95.54,94.99,91.02,89.68,1.0,15,3
3,5,100,100,7000,95.02000000000001,92.63,89.9,89.9,1.0,1,1
4,5,100,30,128,93.47,90.61,90.49000000000001,90.18,1.0,15,3
3,5,100,20,128,94.11,90.09,90.38000000000001,89.9,1.0,15,3
0,5,100,80,5000,98.27,92.25999999999999,90.22,90.2,1.0,15,3
9,5,100,10,7000,93.13,90.46,90.01,89.91,1.0,1,1
4,2,100,20,2048,95.0,92.53,91.19,90.18,1.0,15,3
7,5,100,40,1024,97.11999999999999,95.02000000000001,89.84,89.72,1.0,15,3
3,5,100,50,7000,95.03,93.81,89.9,89.9,1.0,1,1
3,5,100,30,5000,95.14,93.69,90.53999999999999,89.9,1.0,15,3
3,2,100,20,2048,94.76666666666667,91.42,90.50666666666667,89.9,0.5,15,3
9,3,100,10,1024,92.31,90.27,90.03333333333333,89.91,0.75,15,3
4,3,100,40,128,92.29333333333334,90.95,90.18666666666667,90.18,0.5,15,3
8,1,100,80,5000,93.11,90.25999999999999,90.25999999999999,90.25999999999999,1.0,1,1
7,5,100,10,1024,96.22666666666667,92.55333333333333,90.7,89.72,0.5,15,3
7,5,100,40,5000,96.96000000000001,95.52000000000001,90.05,89.72,1.0,15,3
4,8,100,20,2048,95.3,92.95,91.2,90.18,1.0,15,3
8,5,100,20,5000,93.35,90.86,90.25999999999999,90.25999999999999,1.0,15,3
8,5,100,10,7000,93.15,90.4,90.25999999999999,90.25999999999999,1.0,1,1
2,3,100,10,1024,95.74000000000001,94.17,92.0,89.68,1.0,15,3
2,5,100,100,5000,95.82000000000001,91.84,89.68,89.68,1.0,15,3
2,1,100,30,4096,95.82000000000001,91.94,91.03999999999999,89.68,1.0,15,3
0,5,100,60,5000,98.04,92.62,90.24,90.2,1.0,15,3
3,1,100,20,2048,95.37,90.49000000000001,90.84,89.9,1.0,15,3
3,5,100,20,32,91.3,90.29333333333334,90.31333333333333,89.9,0.5,15,3
2,2,100,20,4096,95.05,93.27333333333333,91.09333333333333,89.68,0.5,15,3
0,5,100,20,7000,97.82,91.82000000000001,90.2,90.2,1.0,1,1
3,5,100,20,2048,95.58,92.83,91.03,89.9,1.0,15,3
7,5,100,100,7000,96.93,93.05,89.72,89.72,1.0,1,1
2,5,100,30,1024,94.74333333333334,94.03,90.56333333333333,89.68,0.5,15,3
9,5,100,80,5000,92.82000000000001,91.8,89.91,89.91,1.0,15,3
5,3,100,30,1024,93.32333333333334,91.84333333333333,91.08000000000001,91.08000000000001,0.75,15,3
6,3,100,10,128,95.98,90.74,90.60000000000001,90.42,1.0,15,3
4,5,100,100,5000,95.02000000000001,90.73,90.18,90.18,1.0,1,1
4,5,100,20,5000,95.39999999999999,93.38,91.02,90.18,1.0,15,3
4,5,100,40,7000,94.73,92.67999999999999,90.18,90.18,1.0,1,1
2,5,100,10,256,94.32666666666667,92.33666666666667,91.14999999999999,89.68,0.5,15,3
6,5,100,10,7000,96.87,92.33,90.52,90.42,1.0,1,1
5,1,100,60,5000,93.37,91.08000000000001,91.08000000000001,91.08000000000001,1.0,1,1
7,5,100,30,16,90.97333333333333,90.17333333333333,89.82,89.72,0.5,15,3
7,5,100,10,1024,96.02666666666667,92.60666666666667,90.59333333333333,89.72,0.5,15,3
9,5,100,30,5000,92.93,91.66,89.92,89.91,1.0,15,3
6,5,100,80,7000,96.99,93.97999999999999,90.42,90.42,1.0,1,1
3,5,100,50,7000,95.03,93.77,89.94,89.9,1.0,1,1
2,3,100,30,2048,95.57,94.62,91.10000000000001,89.68,1.0,15,3
7,5,100,100,7000,96.67,93.27,89.72,89.72,1.0,1,1
4,2,100,30,2048,95.34,92.67999999999999,90.44,90.18,1.0,15,3
2,1,100,40,5000,95.15,90.42,90.18,89.68,1.0,1,1
4,3,100,10,4096,95.39999999999999,92.88,91.33,90.18,1.0,15,3
2,10,100,10,1024,94.74333333333334,92.78,90.96,89.68,0.5,15,3
3,5,100,40,7000,95.13000000000001,93.72,89.92,89.9,1.0,1,1
4,5,100,30,64,91.40333333333334,90.67333333333333,90.28666666666668,90.18,0.5,15,3
2,5,100,30,256,94.42,93.26,90.75,89.68,1.0,15,3
8,5,100,60,5000,92.77,91.01,90.25999999999999,90.25999999999999,1.0,15,3
8,5,100,40,5000,93.08999999999999,91.08000000000001,90.25999999999999,90.25999999999999,1.0,15,3
7,2,100,40,2048,96.63000000000001,93.17333333333333,89.83333333333333,89.72,0.75,15,3
4,5,100,10,7000,94.86,91.86999999999999,91.36,90.18,1.0,1,1
0,5,100,50,5000,97.87,93.0,90.22,90.2,1.0,15,3
8,5,100,30,7000,92.75,90.98,90.25999999999999,90.25999999999999,1.0,1,1
2,2,100,10,4096,96.00999999999999,93.99,92.0,89.68,1.0,15,3
2,5,100,10,16,90.92,89.88000000000001,90.29666666666667,89.68,0.5,15,3
3,5,100,10,16,91.31,89.90333333333334,90.02333333333333,89.9,0.5,15,3
4,1,100,30,1024,93.99,90.34333333333333,90.43666666666667,90.18,0.5,15,3
3,2,100,30,2048,95.39999999999999,92.54,90.46,89.9,1.0,15,3
5,5,100,20,7000,93.76,91.53999999999999,91.08000000000001,91.08000000000001,1.0,1,1
0,5,100,100,5000,98.35000000000001,91.45,90.2,90.2,1.0,15,3
4,2,100,20,2048,95.22,92.44,91.21000000000001,90.18,1.0,15,3
3,5,100,20,32,93.06,89.96,89.9,89.9,1.0,15,3
4,2,100,10,1024,95.17999999999999,92.35,91.3,90.18,1.0,15,3
4,3,100,10,1024,95.23,92.44,91.25999999999999,90.18,1.0,15,3
7,5,100,100,7000,96.87,93.55,89.72,89.72,1.0,1,1
3,5,100,20,64,92.81,89.99000000000001,90.06,89.9,1.0,15,3
3,3,100,30,2048,95.13000000000001,92.67,90.53999999999999,89.9,1.0,15,3
5,5,100,20,2048,93.11,91.54666666666667,91.08000000000001,91.08000000000001,0.5,15,3
3,5,100,20,1024,94.31,91.92,90.49666666666667,89.9,0.5,15,3
4,8,100,20,2048,95.45,92.75999999999999,91.07,90.18,1.0,15,3
4,2,100,20,2048,94.35666666666667,91.30333333333334,90.66666666666666,90.18,0.5,15,3
7,5,100,20,256,96.16,92.27,90.78,89.72,1.0,15,3
3,3,100,20,2048,94.98,92.30000000000001,90.92,89.9,1.0,15,3
3,2,100,30,2048,95.25,92.24,90.63,89.9,1.0,15,3
5,5,100,100,7000,93.2,91.63,91.08000000000001,91.08000000000001,1.0,1,1
4,5,100,30,128,92.83666666666667,91.47333333333333,90.46,90.18,0.5,15,3
4,1,100,10,2048,93.81666666666668,90.71333333333334,90.90333333333334,90.18,0.5,15,3
2,5,100,30,512,94.43333333333334,93.82333333333334,90.60333333333334,89.68,0.5,15,3
3,5,100,40,512,95.0,93.01,90.24,89.9,1.0,15,3
6,5,100,30,5000,96.92,94.66,90.42,90.42,1.0,15,3
7,5,100,10,1024,96.98,93.16,91.07,89.72,1.0,15,3
3,5,100,100,7000,94.99,92.61,89.9,89.9,1.0,1,1
7,5,100,20,7000,96.76,94.25,91.03999999999999,89.72,1.0,1,1
3,5,100,20,1024,95.11,92.51,90.82000000000001,89.9,1.0,15,3
3,5,100,60,5000,94.92,93.8,89.9,89.9,1.0,1,1
0,5,100,100,5000,98.22999999999999,91.41,90.2,90.2,1.0,15,3
4,1,100,20,2048,95.11,91.25999999999999,91.14999999999999,90.18,1.0,15,3
2,5,100,20,5000,95.67,94.98,92.05,89.68,1.0,15,3
6,5,100,10,5000,96.54,92.33,90.42,90.42,1.0,1,1
7,5,100,30,32,94.0,89.85,89.72,89.72,1.0,15,3
2,3,100,10,2048,94.72666666666667,92.61,91.08000000000001,89.68,0.5,15,3
4,5,100,20,32,92.2,90.38000000000001,90.23,90.18,1.0,15,3
3,5,100,50,7000,94.85,93.53,89.9,89.9,1.0,1,1
4,5,100,10,16,91.64999999999999,90.85,90.58,90.18,1.0,15,3
2,3,100,20,2048,95.89999999999999,94.45,91.99000000000001,89.68,1.0,15,3
7,5,100,50,1024,96.28,95.00333333333333,89.73666666666666,89.72,0.75,15,3
3,3,100,20,4096,95.59,92.99,90.98,89.9,1.0,15,3
2,8,100,20,2048,95.20333333333333,93.78999999999999,91.13666666666667,89.68,0.5,15,3
2,3,100,20,2048,94.95666666666666,93.52666666666667,91.26333333333334,89.68,0.5,15,3
1,5,100,100,5000,98.13,91.93,88.64999999999999,88.64999999999999,1.0,15,3
4,3,100,100,128,93.65,90.33,90.18,90.18,1.0,15,3
6,5,100,80,5000,96.99,93.74,90.42,90.42,1.0,15,3
2,2,100,30,1024,94.67666666666666,93.22,90.81666666666666,89.68,0.5,15,3
5,5,100,100,128,91.60000000000001,91.71333333333334,91.08000000000001,91.08000000000001,0.5,15,3
4,5,100,30,7000,94.67,92.72,90.36,90.18,1.0,1,1
1,5,100,20,5000,97.78,89.61,89.2,88.64999999999999,1.0,1,1
9,5,100,80,7000,92.47,91.82000000000001,89.91,89.91,1.0,1,1
3,3,100,30,2048,95.17,93.19,90.51,89.9,1.0,15,3
2,5,100,20,5000,95.73,94.92,92.04,89.68,1.0,15,3
4,2,100,20,1024,94.45666666666666,91.58666666666667,90.69666666666667,90.18,0.5,15,3
2,5,100,60,5000,95.62,94.28999999999999,89.83,89.68,1.0,15,3
2,1,100,50,5000,95.26,89.95,89.78,89.68,1.0,1,1
4,5,100,10,16,90.96666666666667,90.56333333333333,90.45333333333333,90.18,0.5,15,3
4,5,100,100,4096,95.48,90.84,90.18,90.18,1.0,15,3
2,5,100,30,1024,94.74000000000001,93.91666666666667,90.54333333333334,89.68,0.5,15,3
5,5,100,30,5000,93.89,92.25999999999999,91.09,91.08000000000001,1.0,15,3
3,2,100,10,2048,94.24333333333334,91.17333333333333,90.56666666666666,89.9,0.5,15,3
9,5,100,20,5000,93.49,91.19,90.03999999999999,89.91,1.0,15,3
4,5,100,40,7000,94.97,92.47999999999999,90.2,90.18,1.0,1,1
8,5,100,10,5000,93.69,90.28,90.25999999999999,90.25999999999999,1.0,15,3
2,5,100,20,32,91.33333333333333,91.64,90.72333333333333,89.68,0.5,15,3
6,5,100,30,7000,96.66,94.31,90.42,90.42,1.0,1,1
8,10,100,80,256,92.77,90.45,90.25999999999999,90.25999999999999,1.0,15,3
2,10,100,30,1024,94.63333333333334,94.15,90.55333333333333,89.68,0.5,15,3
9,5,100,80,5000,92.41,92.07,89.91,89.91,1.0,15,3
2,3,100,10,1024,95.66,93.75,91.94,89.68,1.0,15,3
3,5,100,20,256,93.82000000000001,91.75999999999999,90.56,89.9,0.5,15,3
7,5,100,20,7000,96.92,94.0,90.88000000000001,89.72,1.0,1,1
2,5,100,50,5000,95.36,94.33,90.13,89.68,1.0,15,3
1,1,100,80,5000,97.8,88.64999999999999,88.64999999999999,88.64999999999999,1.0,1,1
5,5,100,20,5000,94.52000000000001,91.8,91.08000000000001,91.08000000000001,1.0,15,3
6,1,100,100,128,96.21,90.42,90.42,90.42,1.0,15,3
2,1,100,10,4096,94.73333333333333,91.71000000000001,91.27666666666666,89.68,0.5,15,3
3,1,100,30,5000,94.86,90.58,90.68,89.9,1.0,1,1
9,5,100,30,7000,92.66,90.86999999999999,89.92,89.91,1.0,1,1
2,3,100,30,2048,95.01333333333334,93.90333333333334,90.73,89.68,0.5,15,3
9,5,100,20,128,90.68,90.12,89.91,89.91,0.5,15,3
0,3,100,40,512,95.87,90.57666666666667,90.2,90.2,0.25,15,3
2,1,100,30,2048,94.97333333333333,91.15333333333334,90.62666666666667,89.68,0.5,15,3
5,5,100,80,5000,93.69,92.36999999999999,91.08000000000001,91.08000000000001,1.0,15,3
3,2,100,20,1024,95.11,91.75,90.77,89.9,1.0,15,3
3,1,100,10,2048,94.49666666666666,90.25333333333333,90.66,89.9,0.5,15,3
3,5,100,30,6131,94.87666666666667,92.88333333333333,90.23,89.9,0.5,3,3
8,1,100,20,5000,92.88,90.25999999999999,90.25999999999999,90.25999999999999,1.0,1,1
1,5,100,40,1024,98.00999999999999,92.58,88.64999999999999,88.64999999999999,1.0,15,3
6,5,100,20,5000,97.26,94.04,90.42,90.42,1.0,15,3
7,5,100,30,64,93.90666666666667,91.70666666666666,89.91666666666667,89.72,0.5,15,3
4,5,100,40,1024,95.12,92.53,90.28,90.18,1.0,15,3
3,2,100,30,6131,95.58,92.29,90.35,89.9,1.0,3,3
3,5,100,20,16,90.44,90.13333333333333,90.05666666666666,89.9,0.5,15,3
3,5,100,30,16,90.75666666666666,90.25999999999999,89.96,89.9,0.5,15,3
0,5,100,80,5000,98.13,91.86,90.2,90.2,1.0,15,3
2,8,100,20,2048,95.02000000000001,94.07666666666667,91.22,89.68,0.5,15,3
3,5,100,30,128,94.52000000000001,90.92,90.53999999999999,89.9,1.0,15,3
8,3,100,20,2048,92.39666666666668,90.51666666666667,90.25999999999999,90.25999999999999,0.5,15,3
1,5,100,60,7000,97.91,94.11,88.64999999999999,88.64999999999999,1.0,1,1
6,5,100,40,1024,96.57,94.87,90.42,90.42,1.0,15,3
2,5,100,30,16,90.05333333333333,91.30333333333334,90.31333333333333,89.68,0.5,15,3
0,3,100,20,5000,97.89,91.96,90.22,90.2,1.0,15,3
4,5,100,20,64,92.20666666666666,90.98333333333333,90.65666666666667,90.18,0.5,15,3
8,1,100,40,5000,92.92,90.25999999999999,90.25999999999999,90.25999999999999,1.0,1,1
5,5,100,80,5000,93.17,92.17999999999999,91.08000000000001,91.08000000000001,1.0,1,1
5,5,100,10,5000,94.06,91.27,91.08000000000001,91.08000000000001,1.0,15,3
2,5,100,20,512,95.23,93.82000000000001,91.67999999999999,89.68,1.0,15,3
6,5,100,10,7000,96.53,92.27,90.42,90.42,1.0,1,1
4,8,100,20,2048,94.14333333333333,91.81666666666666,90.68333333333334,90.18,0.5,15,3
1,5,100,40,5000,98.16,93.37,88.64999999999999,88.64999999999999,1.0,15,3
7,5,100,10,32,93.39666666666666,90.66,90.03999999999999,89.72,0.5,15,3
2,3,100,20,1024,95.23,94.27,91.99000000000001,89.68,1.0,15,3
3,3,100,30,4096,95.13000000000001,93.04,90.44,89.9,1.0,15,3
3,5,100,20,1024,95.09,92.60000000000001,90.98,89.9,1.0,15,3
8,5,100,40,7000,93.03,91.10000000000001,90.25999999999999,90.25999999999999,1.0,1,1
8,5,100,40,1024,92.62,90.79,90.25999999999999,90.25999999999999,1.0,15,3
4,5,100,20,1024,94.8,92.92,91.22,90.18,1.0,15,3
4,5,100,20,128,92.36666666666666,91.37333333333333,90.68,90.18,0.5,15,3
3,8,100,30,1024,95.24000000000001,93.0,90.60000000000001,89.9,1.0,15,3
2,3,100,30,2048,95.78,94.69,91.08000000000001,89.68,1.0,15,3
4,2,100,20,2048,94.19,91.49000000000001,90.75,90.18,0.5,15,3
4,5,100,20,128,94.03,90.84,91.14,90.18,1.0,15,3
2,5,100,20,16,90.10666666666667,90.18666666666667,89.99333333333334,89.68,0.5,15,3
3,2,100,30,6131,94.98666666666666,91.53,90.24,89.9,0.5,3,3
3,1,100,10,2048,95.12,90.67,91.03,89.9,1.0,15,3
3,5,100,10,4096,95.61,92.07,90.7,89.9,1.0,15,3
4,5,100,20,1024,94.12333333333333,91.4,90.71666666666667,90.18,0.5,15,3
4,5,100,30,32,92.5,90.35,90.18,90.18,1.0,15,3
5,5,100,20,7000,93.55,91.36,91.09,91.08000000000001,1.0,1,1
3,5,100,30,512,94.88,92.46,90.36,89.9,1.0,15,3
2,1,100,40,5000,95.28999999999999,90.57,90.16,89.68,1.0,1,1
3,3,100,10,1024,94.20333333333333,91.12666666666667,90.48666666666666,89.9,0.5,15,3
7,5,100,20,16,90.89666666666668,90.36333333333333,89.96,89.72,0.5,15,3
9,3,100,40,32,89.94333333333333,89.92999999999999,89.91,89.91,0.5,15,3
2,5,100,20,16,90.20666666666666,90.14666666666666,90.04666666666667,89.68,0.5,15,3
3,5,100,80,7000,95.00999999999999,92.95,89.9,89.9,1.0,1,1
7,1,100,20,4096,96.41666666666666,91.13,90.61666666666667,89.72,0.5,15,3
6,3,100,40,1024,96.08666666666666,93.69666666666666,90.42,90.42,0.5,15,3
0,5,100,50,7000,97.69,92.4,90.25,90.2,1.0,1,1
2,1,100,20,4096,95.96000000000001,93.02,91.75999999999999,89.68,1.0,15,3
3,5,100,20,128,93.20333333333333,91.31666666666666,90.53666666666666,89.9,0.5,15,3
2,8,100,10,1024,95.72,93.77,92.01,89.68,1.0,15,3
7,5,100,20,1024,96.8,93.8,90.55,89.72,1.0,15,3
7,2,100,30,2048,96.77,93.5,90.41,89.72,1.0,15,3
3,10,100,30,6131,95.5,93.74,90.48,89.9,1.0,3,3
9,5,100,20,7000,92.95,90.63,89.94,89.91,1.0,1,1
4,5,100,40,1024,95.00999999999999,92.77,90.24,90.18,1.0,15,3
0,5,100,60,7000,97.89999999999999,92.43,90.22,90.2,1.0,1,1
9,5,100,100,7000,92.72,91.44,89.91,89.91,1.0,1,1
1,5,100,50,5000,98.16,95.42,88.64999999999999,88.64999999999999,1.0,15,3
7,5,100,50,5000,97.08,95.49,89.8,89.72,1.0,15,3
1,2,100,20,32,92.97999999999999,88.64999999999999,88.64999999999999,88.64999999999999,0.25,15,3
9,1,100,60,64,90.07333333333334,89.91,89.91,89.91,0.25,15,3
4,5,100,30,2048,94.24,91.96,90.42333333333333,90.18,0.5,15,3
3,1,100,10,1024,94.45,90.31333333333333,90.55333333333333,89.9,0.5,15,3
6,5,100,40,1024,96.61999999999999,94.65,90.42,90.42,1.0,15,3
4,5,100,100,128,93.64,90.3,90.18,90.18,1.0,15,3
4,2,100,10,1024,93.87333333333333,91.35333333333332,90.73666666666666,90.18,0.5,15,3
3,5,100,40,7000,94.93,93.84,90.03,89.9,1.0,1,1
6,5,100,50,7000,96.76,95.06,90.42,90.42,1.0,1,1
2,2,100,20,1024,95.34,94.28999999999999,91.96,89.68,1.0,15,3
7,2,100,20,4096,96.92,94.13,90.85,89.72,1.0,15,3
3,5,100,50,7000,95.26,93.72,89.9,89.9,1.0,1,1
4,2,100,10,2048,93.60666666666667,91.36666666666666,90.47666666666667,90.18,0.5,15,3
8,5,100,60,7000,92.60000000000001,90.94,90.25999999999999,90.25999999999999,1.0,1,1
8,3,100,50,256,91.45666666666666,90.39666666666668,90.25999999999999,90.25999999999999,0.5,15,3
2,8,100,20,4096,95.71,95.07,91.8,89.68,1.0,15,3
1,5,100,80,5000,97.82,94.21000000000001,88.64999999999999,88.64999999999999,1.0,1,1
4,1,100,30,2048,94.13,90.41666666666667,90.39666666666668,90.18,0.5,15,3
0,5,100,20,5000,98.03,92.2,90.22,90.2,1.0,15,3
7,1,100,10,1024,97.07000000000001,92.36999999999999,91.14,89.72,1.0,15,3
9,5,100,100,5000,92.81,91.79,89.91,89.91,1.0,15,3
6,5,100,80,7000,96.78999999999999,93.42,90.42,90.42,1.0,1,1
3,5,100,20,512,95.12,91.96,90.75999999999999,89.9,1.0,15,3
2,1,100,10,2048,95.85000000000001,92.67,92.01,89.68,1.0,15,3
3,5,100,40,1024,94.97,93.45,90.19,89.9,1.0,15,3
4,1,100,40,5000,94.52000000000001,90.18,90.2,90.18,1.0,1,1
2,1,100,30,2048,94.96,91.18,90.50333333333333,89.68,0.5,15,3
9,5,100,80,5000,92.67999999999999,92.47999999999999,89.91,89.91,1.0,15,3
3,5,100,30,1024,94.32666666666667,92.45333333333333,90.23333333333333,89.9,0.5,15,3
3,3,100,30,2048,94.71333333333334,92.27,90.32333333333334,89.9,0.5,15,3
2,5,100,60,7000,95.49,93.91000000000001,89.68,89.68,1.0,1,1
7,2,100,10,2048,95.89999999999999,92.35666666666667,90.55,89.72,0.5,15,3
4,2,100,30,4096,94.33,91.36,90.25666666666666,90.18,0.5,15,3
8,5,100,80,7000,93.0,90.8,90.25999999999999,90.25999999999999,1.0,1,1
6,10,100,50,64,95.21,91.06,90.42,90.42,1.0,15,3
7,5,100,20,32,92.28333333333333,89.97666666666667,90.12333333333333,89.72,0.5,15,3
6,5,100,80,7000,96.8,93.35,90.42,90.42,1.0,1,1
3,5,100,20,16,90.48,90.09666666666666,90.24,89.9,0.5,15,3
4,3,100,30,2048,94.37,91.36,90.33666666666666,90.18,0.5,15,3
4,2,100,20,4096,94.31333333333333,91.70666666666666,90.84,90.18,0.5,15,3
1,5,100,20,7000,98.05,89.73,89.33,88.64999999999999,1.0,1,1
4,5,100,40,7000,95.12,91.99000000000001,90.2,90.18,1.0,1,1
4,10,100,10,1024,95.1,92.25999999999999,91.41,90.18,1.0,15,3
2,5,100,10,32,91.75666666666666,90.05666666666666,90.27666666666667,89.68,0.5,15,3
2,5,100,10,512,95.46,93.27,91.83,89.68,1.0,15,3
8,5,100,50,5000,92.77,90.84,90.25999999999999,90.25999999999999,1.0,1,1
4,5,100,10,2048,93.79666666666667,91.57333333333332,90.92,90.18,0.5,15,3
5,5,100,10,7000,93.5,91.09,91.08000000000001,91.08000000000001,1.0,1,1
3,5,100,20,16,90.52,89.95,89.94666666666666,89.9,0.5,15,3
2,5,100,10,16,92.71000000000001,91.57,90.63,89.68,1.0,15,3
5,5,100,40,7000,93.19,92.13,91.08000000000001,91.08000000000001,1.0,1,1
6,5,100,40,5000,96.98,95.19,90.42,90.42,1.0,15,3
4,10,100,30,2048,94.86,92.86,90.85,90.18,1.0,15,3
3,5,100,100,5000,95.25,92.97999999999999,89.9,89.9,1.0,15,3
2,5,100,50,5000,95.69,94.85,90.12,89.68,1.0,15,3
4,3,100,20,2048,94.08999999999999,91.47999999999999,90.68666666666667,90.18,0.5,15,3
4,2,100,10,2048,93.90666666666667,91.49333333333334,90.90666666666667,90.18,0.5,15,3
2,1,100,10,1024,95.25,92.25,91.97,89.68,1.0,15,3
9,5,100,20,7000,93.35,90.98,90.03,89.91,1.0,1,1
2,5,100,20,7000,95.66,94.05,92.17,89.68,1.0,1,1
4,5,100,20,1024,94.09666666666666,91.91,90.76333333333334,90.18,0.5,15,3
3,2,100,30,2048,94.71000000000001,91.37333333333333,90.23,89.9,0.5,15,3
2,5,100,20,5000,95.22,94.51,92.05,89.68,1.0,1,1
8,5,100,40,32,90.34333333333333,90.25999999999999,90.25999999999999,90.25999999999999,0.75,15,3
7,8,100,20,2048,96.98,94.53,90.8,89.72,1.0,15,3
3,2,100,30,2048,94.53333333333333,91.53,90.32,89.9,0.5,15,3
2,8,100,30,2048,95.48,95.28,91.14,89.68,1.0,15,3
5,5,100,10,5000,94.0,91.39,91.10000000000001,91.08000000000001,1.0,15,3
2,3,100,30,2048,95.16333333333333,93.66333333333333,90.57333333333332,89.68,0.5,15,3
0,1,100,60,5000,97.69,90.2,90.22,90.2,1.0,1,1
2,1,100,30,1024,95.47,91.83,90.9,89.68,1.0,15,3
2,5,100,20,5000,95.91,94.81,91.91,89.68,1.0,15,3
7,5,100,20,64,93.92666666666668,91.97,90.36999999999999,89.72,0.5,15,3
2,1,100,30,5958,95.88,91.82000000000001,90.94,89.68,1.0,3,3
3,3,100,20,4096,94.56666666666666,91.66666666666666,90.72333333333333,89.9,0.5,15,3
5,5,100,60,5000,93.75,92.7,91.08000000000001,91.08000000000001,1.0,15,3
4,5,100,20,512,93.64333333333333,91.09333333333333,90.64333333333333,90.18,0.5,15,3
4,1,100,60,32,90.93333333333334,90.18,90.18333333333334,90.18,0.5,15,3
8,5,100,100,5000,93.12,90.62,90.25999999999999,90.25999999999999,1.0,15,3
5,5,100,10,7000,93.27,91.14999999999999,91.08000000000001,91.08000000000001,1.0,1,1
9,5,100,20,5000,93.28999999999999,90.98,89.91,89.91,1.0,15,3
3,3,100,20,2048,95.43,92.56,91.22,89.9,1.0,15,3
9,5,100,50,7000,92.36,92.58,89.91,89.91,1.0,1,1
2,1,100,10,4096,94.45333333333333,91.60333333333334,90.93666666666667,89.68,0.5,15,3
2,2,100,20,1024,95.66,94.21000000000001,91.72,89.68,1.0,15,3
4,5,100,60,5000,94.92,91.63,90.18,90.18,1.0,1,1
5,5,100,100,5000,93.77,92.02,91.08000000000001,91.08000000000001,1.0,15,3
0,5,100,30,7000,97.95,93.13,90.29,90.2,1.0,1,1
3,5,100,30,16,90.16,90.05333333333333,89.9,89.9,0.5,15,3
3,1,100,20,4096,95.54,90.93,91.14999999999999,89.9,1.0,15,3
7,5,100,30,7000,96.63000000000001,94.34,90.13,89.72,1.0,1,1
4,2,100,80,256,93.72666666666667,90.42,90.18333333333334,90.18,0.75,15,3
3,5,100,50,7000,94.91000000000001,93.62,89.91,89.9,1.0,1,1
7,5,100,30,2048,96.38333333333333,93.84666666666666,90.10666666666667,89.72,0.5,15,3
3,5,100,60,7000,95.14,93.58999999999999,89.9,89.9,1.0,1,1
3,5,100,80,7000,95.06,93.51,89.9,89.9,1.0,1,1
7,1,100,10,4096,95.86333333333333,90.16,90.74333333333333,89.72,0.5,15,3
4,8,100,10,1024,95.1,92.38,91.4,90.18,1.0,15,3
3,2,100,50,4096,94.85,91.92,89.94666666666666,89.9,0.75,15,3
2,5,100,10,7000,95.76,93.66,92.11,89.68,1.0,1,1
2,5,100,60,5000,95.52000000000001,94.28999999999999,89.75999999999999,89.68,1.0,15,3
3,8,100,20,1024,95.0,92.13,90.5,89.9,1.0,15,3
2,5,100,10,1024,94.60333333333332,92.95,91.03333333333333,89.68,0.5,15,3
0,3,100,20,16,91.92333333333333,90.2,90.2,90.2,0.25,15,3
1,5,100,30,5000,97.88,90.2,88.64999999999999,88.64999999999999,1.0,1,1
4,1,100,20,2048,94.22,90.66333333333333,90.85333333333332,90.18,0.5,15,3
8,5,100,40,5000,93.0,91.32000000000001,90.25999999999999,90.25999999999999,1.0,15,3
1,5,100,40,5000,98.25,93.47999999999999,88.64999999999999,88.64999999999999,1.0,15,3
7,8,100,30,2048,96.74000000000001,94.97,90.34,89.72,1.0,15,3
3,5,100,20,16,90.16666666666666,89.9,89.91666666666667,89.9,0.5,15,3
7,5,100,20,32,94.47,91.25,89.96,89.72,1.0,15,3
3,5,100,20,32,91.33666666666667,90.55,90.16,89.9,0.5,15,3
4,3,100,20,2048,93.96666666666667,91.62666666666667,90.74666666666667,90.18,0.5,15,3
2,5,100,20,128,93.35,92.66,91.21333333333334,89.68,0.5,15,3
8,5,100,80,5000,92.72,90.52,90.25999999999999,90.25999999999999,1.0,1,1
9,5,100,40,7000,92.9,91.83,89.94,89.91,1.0,1,1
9,5,100,20,7000,92.73,90.94,89.95,89.91,1.0,1,1
6,5,100,40,5000,96.84,94.82000000000001,90.42,90.42,1.0,15,3
4,5,100,10,5000,95.62,92.96,91.32000000000001,90.18,1.0,15,3
1,1,100,100,4096,98.07333333333334,88.64999999999999,88.64999999999999,88.64999999999999,0.75,15,3
4,5,100,10,1024,95.09,92.60000000000001,91.24,90.18,1.0,15,3
0,5,100,50,7000,97.44,93.56,90.22,90.2,1.0,1,1
7,5,100,30,32,91.79333333333334,91.35,90.02333333333333,89.72,0.5,15,3
3,5,100,20,64,92.31,90.27,90.36666666666666,89.9,0.5,15,3
9,5,100,40,7000,92.97,91.94,89.91,89.91,1.0,1,1
6,5,100,30,7000,96.78999999999999,94.15,90.42,90.42,1.0,1,1
7,8,100,20,2048,96.19333333333333,93.91666666666667,90.42999999999999,89.72,0.5,15,3
6,5,100,50,7000,96.59,94.71000000000001,90.42,90.42,1.0,1,1
9,5,100,30,7000,92.63,91.02,89.92999999999999,89.91,1.0,1,1
2,8,100,20,2048,95.33,94.62,91.75,89.68,1.0,15,3
1,5,100,80,7000,97.97,93.97999999999999,88.64999999999999,88.64999999999999,1.0,1,1
5,10,100,60,512,92.28333333333333,92.41,91.08000000000001,91.08000000000001,0.5,15,3
3,5,100,50,7000,95.17,93.69,89.9,89.9,1.0,1,1
7,1,100,20,2048,96.40666666666667,91.26666666666667,90.46666666666667,89.72,0.5,15,3
2,5,100,10,1024,95.27,93.66,91.9,89.68,1.0,15,3
6,5,100,40,1024,96.77,94.99,90.42,90.42,1.0,15,3
0,5,100,20,7000,97.59,91.79,90.24,90.2,1.0,1,1
2,5,100,20,32,91.04333333333334,90.90333333333334,90.25,89.68,0.5,15,3
3,5,100,10,7000,95.14,91.93,90.94,89.9,1.0,1,1
1,5,100,10,5000,98.22999999999999,89.31,89.69,88.64999999999999,1.0,15,3
4,5,100,30,16,90.67666666666668,90.19,90.20333333333333,90.18,0.5,15,3
6,1,100,30,5000,96.39,90.61,90.42,90.42,1.0,1,1
4,5,100,80,5000,95.13000000000001,91.53,90.18,90.18,1.0,15,3
0,2,100,60,4096,98.29,90.27,90.25999999999999,90.2,1.0,15,3
7,5,100,30,16,92.77,90.18,90.23,89.72,1.0,15,3
8,5,100,40,7000,92.91,90.99000000000001,90.25999999999999,90.25999999999999,1.0,1,1
2,5,100,20,16,90.21333333333334,90.35666666666667,89.84333333333333,89.68,0.5,15,3
1,5,100,40,5000,98.11,92.62,88.64999999999999,88.64999999999999,1.0,15,3
0,5,100,60,5000,98.19,92.31,90.21000000000001,90.2,1.0,15,3
3,2,100,30,4096,94.72,91.63,90.39666666666668,89.9,0.5,15,3
7,5,100,10,128,95.28333333333333,91.76666666666667,90.57333333333332,89.72,0.5,15,3
2,3,100,30,2048,95.14666666666668,93.78666666666666,90.57666666666667,89.68,0.5,15,3
4,5,100,20,7000,95.25,92.74,91.25999999999999,90.18,1.0,1,1
1,5,100,10,5000,97.67,88.69,89.4,88.64999999999999,1.0,1,1
0,10,100,50,32,96.06,90.2,90.2,90.2,1.0,15,3
2,5,100,20,32,93.12,91.16,90.38000000000001,89.68,1.0,15,3
0,5,100,100,5000,98.32,91.35,90.21000000000001,90.2,1.0,15,3
4,1,100,20,4096,95.53,91.4,91.16,90.18,1.0,15,3
6,1,100,40,5000,96.61999999999999,90.64,90.42,90.42,1.0,1,1
3,5,100,40,7000,95.11,93.58,89.92,89.9,1.0,1,1
1,1,100,10,4096,96.28666666666666,88.64999999999999,88.69666666666667,88.64999999999999,0.25,15,3
7,2,100,20,2048,96.48333333333333,92.86999999999999,90.61333333333333,89.72,0.5,15,3
7,5,100,20,1024,96.12333333333333,93.33333333333333,90.46666666666667,89.72,0.5,15,3
2,5,100,20,512,94.48666666666666,93.39,90.85666666666667,89.68,0.5,15,3
6,5,100,60,5000,97.11,95.11,90.42,90.42,1.0,15,3
7,10,100,60,512,95.82000000000001,94.32000000000001,89.72,89.72,0.5,15,3
3,5,100,20,7000,95.63000000000001,92.72,91.23,89.9,1.0,1,1
5,5,100,40,7000,93.21000000000001,92.33,91.08000000000001,91.08000000000001,1.0,1,1
0,2,100,40,32,96.27,90.2,90.2,90.2,1.0,15,3
2,5,100,10,64,93.85,91.17,91.09,89.68,1.0,15,3
9,2,100,20,256,91.44666666666666,90.03,89.92,89.91,0.5,15,3
2,5,100,20,64,92.61,91.97666666666666,91.25999999999999,89.68,0.5,15,3
7,5,100,30,128,94.76666666666667,93.09666666666668,90.11,89.72,0.5,15,3
3,5,100,20,16,92.36,89.92,90.13,89.9,1.0,15,3
3,3,100,20,2048,94.74333333333334,91.86666666666666,90.59333333333333,89.9,0.5,15,3
3,5,100,20,16,91.67999999999999,90.81,89.9,89.9,1.0,15,3
0,5,100,30,7000,97.74000000000001,92.91,90.21000000000001,90.2,1.0,1,1
4,5,100,20,16,90.41666666666667,90.24,90.45333333333333,90.18,0.5,15,3
0,5,100,40,5000,97.92999999999999,92.97,90.22,90.2,1.0,15,3
3,2,100,30,2048,95.24000000000001,91.97999999999999,90.42,89.9,1.0,15,3
5,1,100,40,1024,92.96,91.08000000000001,91.08000000000001,91.08000000000001,0.5,15,3
0,5,100,80,7000,97.89,92.06,90.2,90.2,1.0,1,1
2,2,100,30,5000,95.28666666666666,92.75666666666666,90.53,89.68,0.5,10,3
4,10,100,10,2048,95.45,92.75999999999999,91.35,90.18,1.0,15,3
9,5,100,80,7000,92.28,91.9,89.91,89.91,1.0,1,1
7,5,100,20,32,92.42333333333333,91.5,90.02333333333333,89.72,0.5,15,3
6,5,100,40,7000,96.77,94.97,90.42,90.42,1.0,1,1
6,5,100,20,7000,96.78999999999999,93.81,90.42,90.42,1.0,1,1
1,10,100,40,32,95.39999999999999,88.88666666666667,88.64999999999999,88.64999999999999,0.75,15,3
0,1,100,40,5000,97.72999999999999,90.2,90.23,90.2,1.0,1,1
7,5,100,30,512,96.67999999999999,93.8,90.55,89.72,1.0,15,3
2,5,100,20,64,92.42333333333333,91.27,90.89,89.68,0.5,15,3
5,5,100,40,1024,93.4,92.66,91.08000000000001,91.08000000000001,1.0,15,3
9,2,100,10,32,90.53999999999999,89.91,89.92,89.91,0.75,15,3
2,2,100,30,4096,95.18666666666667,93.13666666666667,90.67,89.68,0.5,15,3
3,5,100,30,32,92.36999999999999,89.98,89.9,89.9,1.0,15,3
3,5,100,40,7000,95.06,93.72,89.99000000000001,89.9,1.0,1,1
5,5,100,30,5000,94.04,92.35,91.08000000000001,91.08000000000001,1.0,15,3
7,5,100,40,7000,96.75,95.17999999999999,89.78,89.72,1.0,1,1
7,5,100,30,1024,96.67,94.53,90.18,89.72,1.0,15,3
7,1,100,100,5000,96.89,89.74,89.72,89.72,1.0,1,1
7,8,100,10,2048,97.11999999999999,94.19,91.07,89.72,1.0,15,3
9,5,100,30,7000,92.7,91.27,89.92999999999999,89.91,1.0,1,1
7,5,100,30,5000,97.33000000000001,95.19999999999999,90.16999999999999,89.72,1.0,15,3
7,5,100,20,7000,96.86,94.57,91.11,89.72,1.0,1,1
3,8,100,20,4096,95.52000000000001,93.35,91.05,89.9,1.0,15,3
2,1,100,30,5958,95.82000000000001,91.61,91.27,89.68,1.0,3,3
3,5,100,30,7000,94.97,92.97999999999999,90.79,89.9,1.0,1,1
8,1,100,100,5000,92.62,90.25999999999999,90.25999999999999,90.25999999999999,1.0,1,1
2,5,100,10,1024,94.56333333333333,92.64666666666666,91.03999999999999,89.68,0.5,15,3
8,5,100,50,5000,93.15,91.47999999999999,90.25999999999999,90.25999999999999,1.0,15,3
2,1,100,20,1024,94.92666666666668,91.37333333333333,91.32666666666667,89.68,0.5,15,3
7,5,100,30,1024,96.25,93.97333333333333,90.15333333333334,89.72,0.5,15,3
0,5,100,40,5000,97.94,93.75,90.28,90.2,1.0,15,3
2,3,100,20,2048,95.69,94.78999999999999,91.91,89.68,1.0,15,3
4,5,100,100,5000,95.3,91.22,90.18,90.18,1.0,15,3
2,10,100,20,1024,94.85666666666667,93.93333333333334,91.39666666666668,89.68,0.5,15,3
9,5,100,30,5000,93.31,91.93,89.96,89.91,1.0,15,3
3,5,100,20,32,93.37,90.31,89.92,89.9,1.0,15,3
4,5,100,20,32,92.67,90.18,90.2,90.18,1.0,15,3
3,2,100,30,2048,94.70666666666666,91.09,90.18666666666667,89.9,0.5,15,3
2,3,100,30,1024,94.89999999999999,93.95,90.81333333333333,89.68,0.5,15,3
6,5,100,10,7000,96.72,92.47999999999999,90.42,90.42,1.0,1,1
7,5,100,20,7000,96.76,94.19999999999999,90.89,89.72,1.0,1,1
2,3,100,10,1024,94.92333333333333,92.61,91.03333333333333,89.68,0.5,15,3
5,5,100,40,5000,93.43,91.93,91.08000000000001,91.08000000000001,1.0,1,1
7,5,100,10,5000,97.02,94.03,90.94,89.72,1.0,15,3
3,5,100,30,512,95.12,92.9,90.45,89.9,1.0,15,3
4,1,100,20,4096,94.34,90.81,90.71666666666667,90.18,0.5,15,3
3,5,100,10,1024,94.57666666666667,91.4,90.50666666666667,89.9,0.5,15,3
3,5,100,20,4096,95.58,93.21000000000001,90.78,89.9,1.0,15,3
9,5,100,40,7000,92.77,91.82000000000001,89.92,89.91,1.0,1,1
8,5,100,50,128,91.96,90.56,90.27,90.25999999999999,1.0,15,3
2,1,100,20,2048,95.10333333333332,91.64999999999999,91.27,89.68,0.5,15,3
2,5,100,30,64,91.45333333333333,91.44,90.45333333333333,89.68,0.5,15,3
7,3,100,10,2048,96.95,93.89,91.14,89.72,1.0,15,3
2,5,100,10,256,94.78,92.10000000000001,92.03,89.68,1.0,15,3
6,1,100,30,5000,96.57,90.42,90.42,90.42,1.0,1,1
7,5,100,20,5000,97.08,94.76,90.79,89.72,1.0,15,3
8,5,100,30,7000,93.06,90.95,90.25999999999999,90.25999999999999,1.0,1,1
0,5,100,20,5000,98.14,92.65,90.21000000000001,90.2,1.0,15,3
3,5,100,50,7000,94.97,93.44,89.9,89.9,1.0,1,1
7,2,100,10,4096,95.87666666666667,92.33,90.64333333333333,89.72,0.5,15,3
4,3,100,10,4096,93.51333333333334,91.47,90.69,90.18,0.5,15,3
2,8,100,20,4096,95.93,94.97,91.94,89.68,1.0,15,3
4,5,100,60,7000,95.09,91.79,90.18,90.18,1.0,1,1
3,1,100,20,2048,95.32000000000001,90.64,90.99000000000001,89.9,1.0,15,3
2,8,100,30,1024,95.23,94.49,90.94,89.68,1.0,15,3
6,5,100,100,5000,96.77,92.58999999999999,90.42,90.42,1.0,1,1
3,5,100,20,1024,94.5,92.13666666666667,90.70333333333333,89.9,0.5,15,3
3,1,100,80,5000,95.03,89.9,89.9,89.9,1.0,1,1
7,5,100,20,5000,97.02,94.74000000000001,90.69,89.72,1.0,15,3
6,5,100,50,7000,96.54,94.5,90.42,90.42,1.0,1,1
7,5,100,10,32,94.85,91.75999999999999,90.11,89.72,1.0,15,3
7,5,100,10,256,96.61,90.84,90.82000000000001,89.72,1.0,15,3
4,1,100,20,1024,94.10666666666667,90.78333333333333,90.76333333333334,90.18,0.5,15,3
4,1,100,60,5000,94.73,90.18,90.18,90.18,1.0,1,1
8,2,100,60,4096,92.87333333333333,90.42999999999999,90.25999999999999,90.25999999999999,0.75,15,3
7,5,100,30,2048,96.8,95.0,90.25,89.72,1.0,15,3
2,2,100,10,4096,95.91,93.8,92.14,89.68,1.0,15,3
4,3,100,30,2048,95.11,92.96,90.85,90.18,1.0,15,3
0,5,100,40,1024,97.89,93.08999999999999,90.25,90.2,1.0,15,3
3,5,100,20,128,94.19999999999999,89.99000000000001,90.78,89.9,1.0,15,3
4,3,100,30,2048,94.29333333333332,91.70333333333333,90.46,90.18,0.5,15,3
2,2,100,20,1024,95.3,93.88,91.88,89.68,1.0,15,3
2,2,100,10,1024,95.83,93.72,91.97999999999999,89.68,1.0,15,3
4,1,100,20,1024,94.15333333333334,90.85666666666667,90.89666666666668,90.18,0.5,15,3
9,5,100,10,7000,92.73,90.34,89.94,89.91,1.0,1,1
3,5,100,40,7000,95.04,93.37,89.94,89.9,1.0,1,1
2,5,100,20,16,90.66666666666666,90.34333333333333,90.09666666666666,89.68,0.5,15,3
3,5,100,30,32,91.45333333333333,90.34666666666666,90.12666666666667,89.9,0.5,15,3
4,2,100,20,2048,95.28,92.63,91.19,90.18,1.0,15,3
2,5,100,20,7000,95.8,94.47,92.25999999999999,89.68,1.0,1,1
3,1,100,30,4096,94.90333333333332,90.19,90.22333333333333,89.9,0.5,15,3
4,3,100,20,2048,94.28333333333333,91.62,90.71000000000001,90.18,0.5,15,3
2,5,100,40,5000,95.64,95.14,90.4,89.68,1.0,15,3
7,1,100,20,2048,96.78999999999999,92.33,90.75,89.72,1.0,15,3
6,5,100,50,5000,97.07000000000001,95.37,90.42,90.42,1.0,15,3
4,5,100,80,5000,95.17,91.67999999999999,90.18,90.18,1.0,15,3
2,5,100,20,256,94.03333333333333,92.78999999999999,91.28333333333333,89.68,0.5,15,3
2,3,100,10,2048,95.62,93.58999999999999,92.03,89.68,1.0,15,3
6,5,100,30,7000,96.72,94.16,90.42,90.42,1.0,1,1
8,5,100,40,1024,92.78,91.14,90.25999999999999,90.25999999999999,1.0,15,3
2,5,100,80,5000,95.58,93.04,89.69,89.68,1.0,15,3
7,5,100,20,1024,96.81,94.21000000000001,90.52,89.72,1.0,15,3
0,5,100,10,5000,98.22,91.9,90.2,90.2,1.0,15,3
4,5,100,20,1024,95.03,92.69,91.32000000000001,90.18,1.0,15,3
4,5,100,20,16,90.42999999999999,90.18,90.19,90.18,1.0,15,3
1,1,100,20,5000,98.06,88.66000000000001,89.12,88.64999999999999,1.0,1,1
4,5,100,30,32,90.41333333333334,90.22666666666666,90.21333333333334,90.18,0.5,15,3
2,2,100,20,32,90.03999999999999,90.57,89.97333333333334,89.68,0.25,15,3
8,1,100,100,64,90.42,90.25999999999999,90.25999999999999,90.25999999999999,1.0,15,3
5,5,100,50,7000,93.21000000000001,92.19000000000001,91.08000000000001,91.08000000000001,1.0,1,1
9,5,100,80,7000,92.56,91.97,89.91,89.91,1.0,1,1
3,1,100,30,6131,95.39999999999999,90.13,90.47,89.9,1.0,3,3
2,2,100,20,1024,94.92,93.16333333333333,91.49666666666667,89.68,0.5,15,3
7,1,100,100,5000,96.66,89.73,89.72,89.72,1.0,1,1
1,5,100,40,5000,98.11,92.97999999999999,88.64999999999999,88.64999999999999,1.0,15,3
4,10,100,30,4096,95.39,93.10000000000001,90.58,90.18,1.0,15,3
2,2,100,30,5958,95.20666666666666,93.14333333333333,90.70333333333333,89.68,0.5,3,3
3,5,100,20,64,92.47333333333333,90.63666666666667,90.60666666666667,89.9,0.5,15,3
7,5,100,10,5000,97.19,93.81,91.01,89.72,1.0,15,3
3,3,100,10,4096,94.08333333333333,91.30666666666667,90.43666666666667,89.9,0.5,15,3
4,5,100,30,32,92.36,90.29,90.18,90.18,1.0,15,3
1,5,100,50,5000,98.25,93.84,88.64999999999999,88.64999999999999,1.0,15,3
2,10,100,10,1024,94.86666666666666,93.12,91.04333333333334,89.68,0.5,15,3
7,8,100,10,2048,95.97666666666666,92.75333333333333,90.61333333333333,89.72,0.5,15,3
5,5,100,40,5000,93.84,92.83,91.08000000000001,91.08000000000001,1.0,15,3
1,5,100,20,5000,98.28,89.77000000000001,89.33,88.64999999999999,1.0,15,3
1,1,100,50,5000,97.74000000000001,88.64999999999999,88.64999999999999,88.64999999999999,1.0,1,1
6,5,100,60,7000,96.83,94.28999999999999,90.42,90.42,1.0,1,1
2,1,100,50,32,92.07,89.68,89.68,89.68,1.0,15,3
3,3,100,20,2048,94.68333333333334,91.73,90.60666666666667,89.9,0.5,15,3
2,1,100,20,2048,95.7,92.97999999999999,91.89,89.68,1.0,15,3
7,5,100,30,16,93.33,89.72,89.72,89.72,1.0,15,3
5,5,100,20,5000,93.35,91.39,91.08000000000001,91.08000000000001,1.0,1,1
1,5,100,40,7000,98.0,92.54,88.64999999999999,88.64999999999999,1.0,1,1
1,5,100,30,7000,98.03,89.55,88.64999999999999,88.64999999999999,1.0,1,1
2,8,100,20,2048,95.82000000000001,94.92,92.36999999999999,89.68,1.0,15,3
3,5,100,30,1024,95.02000000000001,93.08,90.63,89.9,1.0,15,3
2,8,100,10,2048,94.69666666666666,93.03333333333333,91.21666666666667,89.68,0.5,15,3
3,5,100,20,7000,95.32000000000001,92.5,90.93,89.9,1.0,1,1
4,5,100,10,512,94.04333333333334,91.33,90.92333333333333,90.18,0.5,15,3
9,5,100,10,5000,93.37,90.46,89.92999999999999,89.91,1.0,15,3
3,1,100,10,4096,95.67999999999999,90.63,90.92,89.9,1.0,15,3
4,2,100,30,1024,95.02000000000001,92.36999999999999,90.45,90.18,1.0,15,3
5,5,100,60,7000,93.41000000000001,92.63,91.08000000000001,91.08000000000001,1.0,1,1
7,3,100,10,2048,97.04,93.85,90.94,89.72,1.0,15,3
1,1,100,40,5000,97.83,88.64999999999999,88.64999999999999,88.64999999999999,1.0,1,1
8,5,100,10,7000,93.08,90.31,90.25999999999999,90.25999999999999,1.0,1,1
7,5,100,30,32,93.28999999999999,89.8,89.72,89.72,1.0,15,3
2,5,100,30,1024,95.32000000000001,94.65,91.12,89.68,1.0,15,3
4,5,100,50,7000,94.99,92.08,90.18,90.18,1.0,1,1
7,5,100,20,64,93.56,91.50666666666667,90.38333333333334,89.72,0.5,15,3
2,2,100,50,5000,95.58,93.33,90.05,89.68,1.0,15,3
3,10,100,20,2048,95.27,93.04,91.19,89.9,1.0,15,3
6,5,100,30,5000,97.0,94.76,90.42,90.42,1.0,15,3
2,2,100,20,2048,95.59,94.11,91.79,89.68,1.0,15,3
0,1,100,10,5000,97.88,90.2,90.2,90.2,1.0,1,1
9,5,100,40,5000,93.12,92.07,89.94,89.91,1.0,15,3
3,5,100,10,256,94.42,90.44,90.92,89.9,1.0,15,3
4,5,100,50,7000,94.85,91.75999999999999,90.19,90.18,1.0,1,1
2,2,100,30,2048,95.5,94.16,90.75999999999999,89.68,1.0,15,3
9,5,100,40,1024,91.87666666666667,91.39333333333333,89.91,89.91,0.5,15,3
5,5,100,60,7000,93.58,92.66,91.08000000000001,91.08000000000001,1.0,1,1
2,5,100,10,32,92.67999999999999,92.25,89.89,89.68,1.0,15,3
3,5,100,30,256,94.78,91.8,90.48,89.9,1.0,15,3
5,5,100,40,1024,93.27,92.43,91.08000000000001,91.08000000000001,1.0,15,3
4,5,100,40,5000,94.62,92.21000000000001,90.18,90.18,1.0,1,1
7,5,100,60,5000,96.89999999999999,95.15,89.73,89.72,1.0,15,3
0,5,100,20,5000,97.92999999999999,92.04,90.22,90.2,1.0,15,3
7,3,100,40,32,90.92333333333333,91.51333333333334,89.74,89.72,0.25,15,3
2,10,100,30,5000,95.81,95.02000000000001,90.8,89.68,1.0,10,3
3,10,100,60,16,89.94333333333333,90.06666666666666,89.9,89.9,0.25,15,3
4,5,100,20,1024,93.85,91.82000000000001,90.83333333333333,90.18,0.5,15,3
4,5,100,30,128,93.17999999999999,90.56,90.31,90.18,1.0,15,3
3,8,100,20,2048,95.32000000000001,92.94,90.85,89.9,1.0,15,3
2,5,100,20,128,94.49,91.95,91.42,89.68,1.0,15,3
3,3,100,30,2048,94.61,92.35,90.34,89.9,0.5,15,3
4,5,100,30,1024,94.12333333333333,91.66666666666666,90.34666666666666,90.18,0.5,15,3
7,5,100,30,256,95.10666666666665,93.22666666666667,90.07666666666667,89.72,0.5,15,3
6,5,100,30,5000,97.1,95.04,90.42,90.42,1.0,15,3
2,5,100,20,2048,95.07666666666667,93.64333333333333,91.09333333333333,89.68,0.5,15,3
7,5,100,40,5000,97.16,95.30999999999999,89.99000000000001,89.72,1.0,15,3
6,5,100,100,7000,96.87,92.01,90.42,90.42,1.0,1,1
3,5,100,60,5000,95.23,93.73,89.9,89.9,1.0,15,3
3,8,100,20,2048,95.64,93.13,90.85,89.9,1.0,15,3
3,5,100,20,256,94.5,91.07,91.03999999999999,89.9,1.0,15,3
8,5,100,80,5000,92.7,90.62,90.25999999999999,90.25999999999999,1.0,1,1
2,5,100,20,64,92.30333333333334,91.86666666666666,90.7,89.68,0.5,15,3
3,5,100,50,5000,95.22,94.23,89.97,89.9,1.0,15,3
3,5,100,40,7000,94.98,93.76,89.92,89.9,1.0,1,1
3,5,100,50,7000,95.13000000000001,93.53,89.9,89.9,1.0,1,1
4,2,100,40,512,93.53333333333333,91.25999999999999,90.18666666666667,90.18,0.5,15,3
2,1,100,30,2048,95.22,91.55666666666666,90.72333333333333,89.68,0.5,15,3
4,8,100,30,2048,94.89,92.83,90.58,90.18,1.0,15,3
2,5,100,20,512,94.30333333333333,93.67666666666666,90.98333333333333,89.68,0.5,15,3
4,5,100,100,5000,95.41,91.27,90.18,90.18,1.0,15,3
1,5,100,60,7000,97.89,94.0,88.64999999999999,88.64999999999999,1.0,1,1
2,1,100,10,5000,95.7,91.79,91.75999999999999,89.68,1.0,1,1
1,3,100,20,256,96.86,88.69,88.79666666666667,88.64999999999999,0.5,15,3
3,5,100,10,16,92.95,89.9,90.02,89.9,1.0,15,3
3,3,100,100,16,90.47,89.9,89.9,89.9,1.0,15,3
1,1,100,40,5000,97.87,88.64999999999999,88.64999999999999,88.64999999999999,1.0,1,1
3,1,100,60,5000,94.81,89.9,89.9,89.9,1.0,1,1
7,5,100,30,7000,96.67,94.69999999999999,90.07,89.72,1.0,1,1
2,1,100,30,5000,95.23333333333333,91.00666666666667,90.60333333333334,89.68,0.5,10,3
2,5,100,30,128,93.64,92.57,90.67,89.68,1.0,15,3
4,5,100,80,7000,94.78999999999999,90.99000000000001,90.18,90.18,1.0,1,1
2,5,100,50,32,90.48333333333333,90.70666666666666,89.71666666666667,89.68,0.5,15,3
7,5,100,50,7000,96.72,94.86,89.74,89.72,1.0,1,1
0,5,100,60,7000,97.65,92.39,90.21000000000001,90.2,1.0,1,1
7,5,100,50,7000,96.38,94.27,89.74,89.72,1.0,1,1
3,5,100,40,7000,95.07,93.87,89.92,89.9,1.0,1,1
2,2,100,30,1024,95.3,93.99,90.99000000000001,89.68,1.0,15,3
2,5,100,40,5000,95.58,95.39,90.44,89.68,1.0,15,3
8,5,100,60,7000,93.01,91.03999999999999,90.25999999999999,90.25999999999999,1.0,1,1
6,5,100,20,7000,96.73,93.92,90.42,90.42,1.0,1,1
4,5,100,20,16,91.27,90.18,90.2,90.18,1.0,15,3
7,8,100,20,2048,97.06,94.78999999999999,90.78,89.72,1.0,15,3
5,5,100,60,7000,93.44,92.64,91.08000000000001,91.08000000000001,1.0,1,1
3,1,100,30,1024,94.12666666666667,90.13666666666667,90.25999999999999,89.9,0.5,15,3
7,2,100,20,2048,96.36666666666667,92.48666666666666,90.47333333333333,89.72,0.5,15,3
2,5,100,20,16,91.94,91.8,91.24,89.68,1.0,15,3
9,5,100,50,7000,92.61,91.97,89.91,89.91,1.0,1,1
3,5,100,60,7000,94.86,93.43,89.9,89.9,1.0,1,1
3,5,100,30,32,91.03,90.18333333333334,89.9,89.9,0.5,15,3
0,5,100,20,5000,98.15,92.31,90.23,90.2,1.0,15,3
3,3,100,20,4096,95.58,93.24,91.22,89.9,1.0,15,3
7,5,100,30,16,91.55,90.81,89.72,89.72,1.0,15,3
3,8,100,20,2048,95.33,92.85,90.9,89.9,1.0,15,3
6,5,100,20,5000,97.06,94.15,90.42,90.42,1.0,15,3
7,5,100,40,1024,96.75,95.11,89.97,89.72,1.0,15,3
3,5,100,40,1024,95.26,93.76,89.95,89.9,1.0,15,3
9,5,100,40,5000,93.08,92.75999999999999,89.92,89.91,1.0,15,3
5,5,100,50,7000,93.35,92.67999999999999,91.08000000000001,91.08000000000001,1.0,1,1
2,8,100,10,2048,95.92,94.37,92.2,89.68,1.0,15,3
7,5,100,10,16,92.30666666666667,90.48,89.99000000000001,89.72,0.5,15,3
7,1,100,40,5000,96.67,90.07,89.78,89.72,1.0,1,1
3,5,100,20,512,94.39333333333333,92.01,90.64999999999999,89.9,0.5,15,3
3,5,100,50,7000,94.73,93.45,89.9,89.9,1.0,1,1
3,5,100,30,16,91.13,89.9,89.9,89.9,1.0,15,3
6,5,100,20,5000,97.0,94.56,90.42,90.42,1.0,15,3
2,1,100,30,5958,95.73,91.9,91.07,89.68,1.0,3,3
1,5,100,20,5000,98.00999999999999,89.75,89.25,88.64999999999999,1.0,15,3
2,2,100,30,2048,95.22333333333334,93.30333333333334,90.60333333333334,89.68,0.5,15,3
2,1,100,30,5958,95.26333333333334,91.06666666666666,90.58333333333334,89.68,0.5,3,3
4,5,100,30,5000,95.41,93.46,90.75999999999999,90.18,1.0,15,3
3,5,100,20,512,94.64333333333333,92.04333333333334,90.85333333333332,89.9,0.75,15,3
0,5,100,30,7000,97.78999999999999,92.7,90.23,90.2,1.0,1,1
1,5,100,20,5000,97.92999999999999,89.27000000000001,89.19,88.64999999999999,1.0,1,1
2,5,100,100,5000,94.99,91.93,89.68,89.68,1.0,1,1
2,2,100,20,4096,96.2,94.37,92.13,89.68,1.0,15,3
1,1,100,40,4096,97.31,88.64999999999999,88.64999999999999,88.64999999999999,0.25,15,3
2,5,100,30,16,90.01333333333334,89.73333333333333,89.81333333333333,89.68,0.5,15,3
2,5,100,20,1024,95.39,94.35,92.06,89.68,1.0,15,3
7,5,100,80,7000,96.75,94.32000000000001,89.72,89.72,1.0,1,1
6,5,100,100,5000,97.28,92.47999999999999,90.42,90.42,1.0,15,3
2,1,100,20,1024,94.67,91.51666666666667,91.08333333333334,89.68,0.5,15,3
2,5,100,10,2048,95.78999999999999,94.14,92.15,89.68,1.0,15,3
7,5,100,80,5000,96.89999999999999,94.31,89.72,89.72,1.0,15,3
9,5,100,100,5000,92.96,92.19000000000001,89.91,89.91,1.0,15,3
2,1,100,30,2048,95.01666666666667,90.81,90.62,89.68,0.5,15,3
4,5,100,10,7000,94.93,92.17,91.24,90.18,1.0,1,1
7,5,100,20,512,95.78333333333333,92.89666666666668,90.29,89.72,0.5,15,3
2,1,100,20,2048,95.73,92.67,91.94,89.68,1.0,15,3
8,5,100,10,5000,93.17999999999999,90.34,90.25999999999999,90.25999999999999,1.0,15,3
3,1,100,40,5000,94.96,90.03999999999999,89.91,89.9,1.0,1,1
0,5,100,40,7000,97.74000000000001,92.93,90.23,90.2,1.0,1,1
3,2,100,50,1024,93.60000000000001,90.36666666666666,89.9,89.9,0.25,15,3
2,5,100,40,1024,95.32000000000001,95.07,90.48,89.68,1.0,15,3
4,5,100,100,7000,95.00999999999999,90.69,90.18,90.18,1.0,1,1
4,8,100,30,2048,94.99,92.64,90.52,90.18,1.0,15,3
6,5,100,20,7000,96.97,93.72,90.42999999999999,90.42,1.0,1,1
4,5,100,20,1024,94.76,92.65,91.14,90.18,1.0,15,3
3,5,100,10,32,92.5,89.94,89.99000000000001,89.9,1.0,15,3
9,1,100,40,512,90.67666666666668,89.91,89.91333333333333,89.91,0.25,15,3
2,5,100,20,128,93.13333333333334,92.75,91.09333333333333,89.68,0.5,15,3
3,10,100,20,1024,94.96,92.61,90.63,89.9,1.0,15,3
7,5,100,20,7000,96.78999999999999,94.73,90.88000000000001,89.72,1.0,1,1
8,5,100,20,5000,93.46,90.74,90.25999999999999,90.25999999999999,1.0,15,3
3,5,100,20,64,92.16333333333333,90.96333333333332,90.4,89.9,0.5,15,3
3,5,100,40,7000,95.36,93.49,89.92999999999999,89.9,1.0,1,1
4,5,100,30,256,94.26,91.82000000000001,90.7,90.18,1.0,15,3
7,2,100,10,4096,97.08,93.7,90.89,89.72,1.0,15,3
2,5,100,20,128,94.35,91.39,91.47999999999999,89.68,1.0,15,3
5,1,100,60,1024,92.71000000000001,91.08000000000001,91.08000000000001,91.08000000000001,0.5,15,3
9,5,100,60,5000,92.88,92.74,89.91,89.91,1.0,15,3
2,5,100,10,7000,95.55,93.71000000000001,92.10000000000001,89.68,1.0,1,1
4,5,100,10,256,94.63000000000001,90.77,91.34,90.18,1.0,15,3
5,5,100,100,7000,93.2,91.84,91.08000000000001,91.08000000000001,1.0,1,1
4,2,100,10,4096,95.47,92.36,91.41,90.18,1.0,15,3
2,8,100,10,2048,94.53666666666666,92.81333333333333,91.04666666666667,89.68,0.5,15,3
3,5,100,100,7000,94.97,92.39,89.9,89.9,1.0,1,1
4,2,100,20,4096,94.24666666666667,91.66666666666666,90.64333333333333,90.18,0.5,15,3
6,10,100,80,128,93.09333333333333,91.98333333333333,90.42,90.42,0.25,15,3
9,2,100,50,256,91.84,90.22,89.92,89.91,1.0,15,3
1,5,100,30,5000,97.94,89.45,88.64999999999999,88.64999999999999,1.0,1,1
7,5,100,20,32,92.54666666666667,91.25333333333333,90.09333333333333,89.72,0.5,15,3
2,5,100,20,5000,95.52000000000001,93.93,91.95,89.68,1.0,1,1
3,10,100,10,1024,95.25,91.95,90.88000000000001,89.9,1.0,15,3
3,1,100,30,2048,95.14,90.57,90.52,89.9,1.0,15,3
3,5,100,30,128,92.68666666666667,91.37666666666667,90.08666666666667,89.9,0.5,15,3
2,2,100,30,2048,94.83333333333334,92.92333333333333,90.53999999999999,89.68,0.5,15,3
3,5,100,20,7000,95.28999999999999,92.82000000000001,91.28,89.9,1.0,1,1
7,5,100,80,5000,97.04,94.87,89.72,89.72,1.0,15,3
7,5,100,30,64,93.89666666666666,92.15,89.92,89.72,0.5,15,3
5,10,100,60,4096,93.67,93.49,91.08000000000001,91.08000000000001,1.0,15,3
0,10,100,20,4096,98.16,92.21000000000001,90.24,90.2,1.0,15,3
2,5,100,100,5000,95.58,92.23,89.68,89.68,1.0,15,3
2,5,100,60,5000,95.47,94.05,89.68,89.68,1.0,1,1
2,3,100,80,1024,93.85,91.14999999999999,89.68333333333334,89.68,0.25,15,3
2,1,100,80,32,90.53333333333333,89.69,89.68,89.68,0.5,15,3
4,5,100,50,5000,95.3,92.86999999999999,90.2,90.18,1.0,15,3
9,5,100,40,128,91.02333333333334,90.77,89.91,89.91,0.75,15,3
2,1,100,30,1024,94.60666666666665,90.88000000000001,90.67666666666668,89.68,0.5,15,3
9,3,100,40,256,91.60000000000001,90.98333333333333,89.91,89.91,0.75,15,3
8,5,100,40,7000,93.01,91.05,90.25999999999999,90.25999999999999,1.0,1,1
3,3,100,20,4096,95.13000000000001,92.91,90.97,89.9,1.0,15,3
4,1,100,20,1024,94.06333333333333,90.75333333333333,90.75,90.18,0.5,15,3
3,5,100,50,7000,95.08,93.55,89.9,89.9,1.0,1,1
3,5,100,30,128,92.85666666666667,91.52666666666667,90.13,89.9,0.5,15,3
4,5,100,60,5000,94.61,92.03,90.18,90.18,1.0,1,1
3,2,100,20,4096,95.64,92.34,91.19,89.9,1.0,15,3
3,5,100,20,1024,94.39,92.25,90.53999999999999,89.9,0.5,15,3
1,10,100,20,16,93.61666666666667,88.64999999999999,88.64999999999999,88.64999999999999,0.5,15,3
2,5,100,30,128,94.34,91.08000000000001,90.71000000000001,89.68,1.0,15,3
3,3,100,10,1024,94.48333333333333,91.14,90.52,89.9,0.5,15,3
1,5,100,40,5000,98.1,92.13,88.64999999999999,88.64999999999999,1.0,15,3
8,2,100,100,512,92.05333333333333,90.25999999999999,90.25999999999999,90.25999999999999,0.5,15,3
8,5,100,60,7000,92.93,91.3,90.25999999999999,90.25999999999999,1.0,1,1
5,5,100,60,7000,93.49,92.74,91.08000000000001,91.08000000000001,1.0,1,1
0,5,100,20,2048,97.64333333333333,91.82333333333334,90.2,90.2,0.75,15,3
3,5,100,40,7000,95.21,93.84,89.94,89.9,1.0,1,1
8,5,100,100,7000,92.60000000000001,90.53999999999999,90.25999999999999,90.25999999999999,1.0,1,1
8,5,100,40,1024,92.36999999999999,91.18,90.25999999999999,90.25999999999999,1.0,15,3
8,5,100,20,5000,92.81,90.58,90.25999999999999,90.25999999999999,1.0,1,1
4,5,100,60,5000,95.12,92.35,90.18,90.18,1.0,15,3
0,5,100,50,7000,97.87,92.67,90.24,90.2,1.0,1,1
3,5,100,100,7000,95.17999999999999,92.42,89.9,89.9,1.0,1,1
7,5,100,10,7000,96.69,93.36,91.41,89.72,1.0,1,1
6,5,100,100,5000,97.16,93.12,90.42,90.42,1.0,15,3
2,3,100,30,2048,94.92666666666668,93.72666666666667,90.58666666666667,89.68,0.5,15,3
7,5,100,80,5000,97.02,94.76,89.72,89.72,1.0,15,3
7,5,100,20,256,95.49333333333333,92.50666666666667,90.36999999999999,89.72,0.5,15,3
9,5,100,60,7000,92.57,91.91,89.91,89.91,1.0,1,1
2,3,100,20,4096,96.06,94.55,91.75999999999999,89.68,1.0,15,3
4,10,100,80,5000,95.34,91.66,90.18,90.18,1.0,15,3
8,5,100,20,5000,93.39,90.99000000000001,90.25999999999999,90.25999999999999,1.0,15,3
7,5,100,80,7000,96.65,93.89,89.72,89.72,1.0,1,1
4,3,100,20,2048,93.16666666666666,90.75333333333333,90.39,90.18,0.25,15,3
3,5,100,10,16,92.09,89.92,90.14999999999999,89.9,1.0,15,3
2,3,100,10,4096,94.39,92.70666666666668,91.07,89.68,0.5,15,3
3,1,100,30,5000,94.91000000000001,90.51,90.82000000000001,89.9,1.0,1,1
5,5,100,100,5000,93.87,92.15,91.08000000000001,91.08000000000001,1.0,15,3
1,5,100,40,5000,98.26,93.17999999999999,88.64999999999999,88.64999999999999,1.0,15,3
2,5,100,30,256,94.69,92.78999999999999,90.88000000000001,89.68,1.0,15,3
4,2,100,10,2048,93.86333333333333,91.24,90.68,90.18,0.5,15,3
3,5,100,10,2048,94.32333333333334,91.59,90.45333333333333,89.9,0.5,15,3
3,1,100,30,4096,94.75,90.16999999999999,90.22666666666666,89.9,0.5,15,3
5,5,100,80,5000,93.89,92.47999999999999,91.08000000000001,91.08000000000001,1.0,15,3
8,1,100,40,5000,92.81,90.25999999999999,90.25999999999999,90.25999999999999,1.0,1,1
2,3,100,20,4096,95.92,94.71000000000001,92.10000000000001,89.68,1.0,15,3
7,5,100,10,16,90.58333333333334,89.72666666666666,89.86333333333333,89.72,0.5,15,3
7,5,100,30,16,90.91666666666667,90.05,89.86666666666666,89.72,0.5,15,3
3,2,100,10,2048,94.26,90.72666666666666,90.47666666666667,89.9,0.5,15,3
5,5,100,30,7000,93.66,91.84,91.08000000000001,91.08000000000001,1.0,1,1
4,10,100,10,4096,95.45,92.88,91.27,90.18,1.0,15,3
4,2,100,30,2048,94.28333333333333,91.17,90.34333333333333,90.18,0.5,15,3

================================================================================
