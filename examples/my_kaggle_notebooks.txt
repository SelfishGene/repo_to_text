================================================================================
repo title: my_kaggle_notebooks
repo link: https://github.com/SelfishGene/my_kaggle_notebooks
date processed: 2024-10-21
================================================================================
================================================================================
repo file structure:
====================
    building-faces-out-of-parts.ipynb
    gmm-in-pca-space.ipynb
    basic-feature-exploration.ipynb
    advanced-feature-exploration.ipynb
    analyzing-soccer-player-faces.ipynb
    the-12-different-types-of-kagglers.ipynb
    README.md
    exploring-youtube-faces-with-keypoints-dataset.ipynb
    generating-sentences-one-letter-at-a-time.ipynb
    visualizing-k-means-with-leaf-dataset.ipynb
    psychology-of-a-professional-athlete.ipynb
    LICENSE
    visualizing-pca-with-leaf-dataset.ipynb
    yellow-cabs-tell-the-story-of-new-york-city.ipynb
    filtering-and-auto-correlation-tutorial.ipynb
    celebrity-face-swap.ipynb
================================================================================
================================================================================
README.md:
==========
## Repo with my most popular kaggle notebooks

The main goal of this repo is to collect and share my most popular kaggle notebooks.  
I've put a lot of effort into them back in the day, so they are highly curated and well documented.  

Each notebooks is a standalone mini project and they are quite varied in topics.  
Most of the notebooks display good data visualization practices and illustrate how one can look inside and visualize some of the inner working of various machine learning algorithms (AKA interpretability).  
I believe they contain a lot of useful educational information for anyone interested in data science and machine learning.  
  
The main reason I decided to compile this repo is to make these notebooks easier to share and more accessible for LLM data scraping in the future.  
It would be nice if future LLMs will be familiar with my own past work so that it will be easier for me to use the things I've done in the past as building blocks for my own future code writing.  

Each notebook can be thought of as an interleaved sequence of {code, narrative/description/math, image illustations} and maybe can also be useful for multimodal (image + text) models

================================================================================
================================================================================
visualizing-k-means-with-leaf-dataset.ipynb:
============================================
------------------------------------------------------------
Cell index: 1
Input Cell Type: markdown
Input Text:
-----------
# Visualizing K-Means with Leaf Dataset

This script is about perhaps the simplest and most popular **unsupervised learning algorithm** out there: the K-Means clustering algorithm.

In this script we will apply K-Means on a small dataset of 1600 binary leaf images with different shapes and try to get a feel for the distribution of leaf images using different visualizations that clarify different aspects about how one can interpret K-Means results.

We will then continue to see if the K-Means features (distances from cluster centers) are informative in terms of classifying leafs and determine what is the optimal K (number of clusters) for the sake of leaf type classification.

**Note:** This script is a follow-up script to the [PCA script][1] which is very similar but about PCA.

  [1]: https://www.kaggle.com/selfishgene/visualizing-pca-with-leaf-dataset

Output Text:
------------


------------------------------------------------------------
Cell index: 2
Input Cell Type: python
Input Text:
-----------
import pandas as pd
import numpy as np
import matplotlib
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec

from sklearn import model_selection
from sklearn import decomposition
from sklearn import linear_model
from sklearn import cluster
from sklearn import ensemble
from sklearn import neighbors
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KernelDensity
from sklearn.manifold import TSNE
from sklearn.metrics import accuracy_score

from skimage.transform import rescale
from scipy import ndimage as ndi

matplotlib.style.use('fivethirtyeight')

Output Text:
------------


------------------------------------------------------------
Cell index: 3
Input Cell Type: markdown
Input Text:
-----------
## Data loading and preparation phases
For the sake of the script not being too cluttered, I've hidden the code and hidden an intermediate pre-processing phase. Anyone who is interested is welcome to  unhide the code and uncomment to see what is going on.

The main assumption of this pre-processing stage is that the absolute sizes of the leafs matter, and not just their shape i.e. leafs with different sizes are most definitely different types of leafs. not sure if it's actually important, but just in case

Output Text:
------------


------------------------------------------------------------
Cell index: 4
Input Cell Type: python
Input Text:
-----------
#%% load the data
dataDir   = '../input/'
trainData = pd.read_csv(dataDir + 'train.csv')
classEncoder = LabelEncoder()
trainLabels  = classEncoder.fit_transform(trainData.loc[:,'species'])
trainIDs     = np.array(trainData.loc[:,'id'])

# show some random images
plt.figure(figsize=(14,12))
plt.suptitle('Original Images (with variable image sizes)', fontsize=22)
for k in range(28):
    randTrainInd = np.random.randint(len(trainIDs))
    randomID = trainIDs[randTrainInd]
    imageFilename = dataDir + 'images/' + str(randomID) + '.jpg'
    plt.subplot(4,7,k+1); plt.imshow(mpimg.imread(imageFilename), cmap='gray')
    plt.title(classEncoder.classes_[trainLabels[randTrainInd]], fontsize=10); plt.axis('off')

Output Text:
------------
<matplotlib.figure.Figure at 0x7f8fb052a668>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image displays a collection of leaf images, arranged in a grid-like structure. Each leaf is presented as a white silhouette against a black background, indicating that the images have been processed to isolate the leaf shapes.  The images vary in size and aspect ratio, reflecting the natural variations in leaf dimensions.  Each leaf is labeled with its corresponding botanical name, suggesting the images are part of a dataset used for plant species identification or similar research.


The arrangement of the leaves is organized into rows and columns, with approximately four columns and four to five rows.  The labels are placed above or beside each leaf image to clearly associate the name with its visual representation.  The title "Original Images (with variable image sizes)" reinforces the fact that the leaves are presented in their original, unprocessed sizes.  The overall structure implies the image is a catalog or visual index of leaf shapes for a particular purpose, likely related to plant identification or classification.


------------------------------------------------------------
Cell index: 5
Input Cell Type: python
Input Text:
-----------
#%% preprocess images

# go over training images and store them in a list
numImages = 1584

shapesMatrix = np.zeros((2,numImages))
listOfImages = []
for k in range(numImages):
    imageFilename = dataDir + 'images/' + str(k+1) + '.jpg'
    currImage = mpimg.imread(imageFilename)
    shapesMatrix[:,k] = np.shape(currImage)
    listOfImages.append(currImage)
    
# calculate the shape of an image that will contain all original images within it
maxShapeSize = shapesMatrix.max(axis=1)
for k in range(len(maxShapeSize)):
    if maxShapeSize[k] % 2 == 0:
        maxShapeSize[k] += 311
    else:
        maxShapeSize[k] += 310
    
    
# place all original images at the center of the large reference frame
fullImageMatrix3D = np.zeros(np.hstack((maxShapeSize, np.shape(shapesMatrix[1]))).astype(int),dtype=np.dtype('u1'))
destXc = (maxShapeSize[1]+1)/2; destYc = (maxShapeSize[0]+1)/2
for k, currImage in enumerate(listOfImages):
    Yc, Xc = ndi.center_of_mass(currImage)
    Xd = destXc - Xc; Yd = destYc - Yc
    rowIndLims = (int(round(Yd)),int(round(Yd)+np.shape(currImage)[0]))
    colIndLims = (int(round(Xd)),int(round(Xd)+np.shape(currImage)[1]))
    fullImageMatrix3D[rowIndLims[0]:rowIndLims[1],colIndLims[0]:colIndLims[1],k] = currImage

'''
# make sure nothing was ruined in the process
plt.figure(figsize=(14,7))
plt.suptitle('Processed Images (fixed size)', fontsize=22)
for k in range(28):
    randInd = np.random.randint(np.shape(fullImageMatrix3D)[2])
    plt.subplot(4,7,k+1); plt.imshow(fullImageMatrix3D[:,:,randInd], cmap='gray'); plt.axis('off')
'''

# re crop according to rows and columns that don't have zeros in them in any image
xValid = fullImageMatrix3D.mean(axis=2).sum(axis=0) > 0
yValid = fullImageMatrix3D.mean(axis=2).sum(axis=1) > 0
xLims = (np.nonzero(xValid)[0][0],np.nonzero(xValid)[0][-1])
yLims = (np.nonzero(yValid)[0][0],np.nonzero(yValid)[0][-1])
fullImageMatrix3D = fullImageMatrix3D[yLims[0]:yLims[1],xLims[0]:xLims[1],:]

# make sure nothing was ruined in the process
plt.figure(figsize=(14,7))
plt.suptitle('Final Processed Images (with fixed image size)', fontsize=22)
for k in range(28):
    randInd = np.random.randint(np.shape(fullImageMatrix3D)[2])
    plt.subplot(4,7,k+1); plt.imshow(fullImageMatrix3D[:,:,randInd], cmap='gray'); plt.axis('off')
    if randInd < len(trainLabels):
        plt.title(classEncoder.classes_[trainLabels[randInd]], fontsize=10)
    else:
        plt.title('test data sample', fontsize=10)
        
# scale down all images to be in normal size
rescaleFactor = 0.15

scaledDownImage = rescale(fullImageMatrix3D[:,:,0],rescaleFactor)
scaledDownImages = np.zeros(np.hstack((np.shape(scaledDownImage),
                                       np.shape(fullImageMatrix3D)[2])),dtype=np.dtype('f4'))
for imInd in range(np.shape(fullImageMatrix3D)[2]):
    scaledDownImages[:,:,imInd] = rescale(fullImageMatrix3D[:,:,imInd],rescaleFactor)
    
del fullImageMatrix3D

Output Text:
------------
/opt/conda/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.
  warn("The default mode, 'constant', will be changed to 'reflect' in "
<matplotlib.figure.Figure at 0x7f8fce1720f0>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image displays a grid of 36 processed images of leaves, presented as black and white silhouettes against a black background.  Each leaf image is contained within a square frame. Above each image, a label identifies the species of leaf (e.g., Tilia_Tomentosa, Acer_Opalus) or indicates that the image is a "test data sample" when the species is unknown. The images are organized into four rows and nine columns.

The title of the image clearly states that these are "Final Processed Images (with fixed image size)," suggesting that these are the results of some image processing steps, perhaps involving segmentation, binarization, and resizing to a uniform size.  The consistent size and presentation of the leaf images suggest preparation for use in a machine learning or image recognition task, where consistent data formatting is crucial. The inclusion of "test data sample" labels indicates that some images are held out from the known labeled dataset for model evaluation or testing purposes.


------------------------------------------------------------
Cell index: 6
Input Cell Type: markdown
Input Text:
-----------
## Define a Kmeans Model class that will help us visualize things

This is long, so I've hidden the code, but if you are intereseted in delving deeper and looking at the implementation then please unhide or better yet fork the script and try playing around by editing the code.

Output Text:
------------


------------------------------------------------------------
Cell index: 7
Input Cell Type: python
Input Text:
-----------
class KmeansModel:

    def __init__(self, X, numClusters=10, objectPixels=None):
        '''
        inputs: 
            X                       - numSamples x numDimentions matrix
            numClusters             - number of clusters to use
            objectPixels (optional) - an binnary mask image used for presentation
                                      will be used as Im[objectPixels] = dataSample
                                      must satisfy objectPixels.ravel().sum() = X.shape[1]
        '''
        numDataSamples = X.shape[0]
        self.numClusters = numClusters        
        if objectPixels is None:
            self.objectPixels = np.ones((1,X.shape[1]),dtype=np.bool)
        else:
            self.objectPixels = objectPixels
        assert(self.objectPixels.ravel().sum() == X.shape[1])

        KmeansModel = cluster.KMeans(n_clusters=numClusters, n_init=5)
        self.dataRepresentation = KmeansModel.fit_transform(X)
        self.KmeansModel = KmeansModel
        
        # calculate cluster frequency
        clusterInds = KmeansModel.labels_
        clusterFrequency = []
        for clusterInd in range(numClusters):
            clusterFrequency.append((clusterInds == clusterInd).sum()/float(numDataSamples))
        self.clusterFrequency = np.array(clusterFrequency)
        self.sortedTemplatesByFrequency = np.flipud(np.argsort(clusterFrequency))

    def RepresentUsingModel(self, X, representationMethod='distFromAllClusters'):
        
        if representationMethod == 'distFromAllClusters':
            return self.KmeansModel.transform(X)
        if representationMethod == 'clusterIndex':
            return self.KmeansModel.predict(X)
        if representationMethod == 'oneHotClusterIndex':
            clusterAssignment = self.KmeansModel.predict(X)
            X_transformed = np.zeros((X.shape[0],self.numClusters))
            for sample in range(X.shape[0]):
                X_transformed[sample,clusterAssignment[sample]] = 1
            return X_transformed

    def ReconstructUsingModel(self, X_transformed, representationMethod='distFromAllClusters'):

        if representationMethod == 'clusterIndex':
            clusterAssignment = X_transformed
        if representationMethod == 'oneHotClusterIndex':
            clusterAssignment = np.argmax(X_transformed,axis=1)
        if representationMethod == 'distFromAllClusters':
            clusterAssignment = np.argmin(X_transformed,axis=1)

        X_reconstructed = np.zeros((X_transformed.shape[0],self.KmeansModel.cluster_centers_.shape[1]))
        for sample in range(X_transformed.shape[0]):
            X_reconstructed[sample,:] = self.KmeansModel.cluster_centers_[clusterAssignment[sample],:]
                
        return X_reconstructed
        
    def InterpretUsingModel(self, X, representationMethod='clusterIndex'):
        return self.ReconstructUsingModel(\
                        self.RepresentUsingModel(X,representationMethod),representationMethod)

    # shows the cluster centers
    def ShowTemplates(self, numTemplatesToShow=16):
        numTemplatesToShow = min(numTemplatesToShow, self.numClusters)
        
        numFigRows = np.ceil(np.sqrt(numTemplatesToShow)); 
        numFigCols = np.ceil(np.sqrt(numTemplatesToShow));
        numTemplatesPerFigure = int(numFigRows*numFigCols)
        numFigures = int(np.ceil(float(numTemplatesToShow)/numTemplatesPerFigure))
                
        for figureInd in range(numFigures):
            plt.figure()
            for plotInd in range(numTemplatesPerFigure):
                templateInd = self.sortedTemplatesByFrequency[numTemplatesPerFigure*figureInd + plotInd]
                if templateInd >= self.numClusters:
                    break
                templateImage = np.zeros(np.shape(self.objectPixels))
                templateImage[self.objectPixels] = \
                        self.KmeansModel.cluster_centers_[templateInd,:].ravel()

                plt.subplot(numFigRows,numFigCols,plotInd+1)
                if np.shape(self.objectPixels)[0] == 1:
                    plt.plot(templateImage)
                else:
                    plt.imshow(templateImage,cmap='hot'); plt.axis('off')
                plt.title(str(100*self.clusterFrequency[templateInd])[:4] + '% frequency');
            plt.tight_layout()
            
    # shows several random model reconstructions
    def ShowReconstructions(self, X, numReconstructions=6):
        assert(np.shape(X)[1] == self.objectPixels.ravel().sum())
        numSamples = np.shape(X)[0]
        numReconstructions = min(numReconstructions, numSamples)
        
        originalImage      = np.zeros(np.shape(self.objectPixels))
        reconstructedImage = np.zeros(np.shape(self.objectPixels))
        
        numReconstructionsPerFigure = min(6, numReconstructions)
        numFigures = int(np.ceil(float(numReconstructions)/numReconstructionsPerFigure))
        
        for figureInd in range(numFigures):
            plt.figure()
            for plotCol in range(numReconstructionsPerFigure):
                dataSampleInd = np.random.randint(numSamples)
                originalImage[self.objectPixels] = X[dataSampleInd,:].ravel()
                reconstructedImage[self.objectPixels] = \
                        self.InterpretUsingModel(np.reshape(X[dataSampleInd,:],[1,-1])).ravel()
                diffImage = abs(originalImage - reconstructedImage)
                
                # original image
                plt.subplot(3,numReconstructionsPerFigure,0*numReconstructionsPerFigure+plotCol+1)
                if np.shape(self.objectPixels)[0] == 1:
                    plt.plot(originalImage); plt.title('original signal')
                else:
                    plt.imshow(originalImage, cmap='gray'); 
                    plt.title('original image'); plt.axis('off')
                    
                # reconstred image
                plt.subplot(3,numReconstructionsPerFigure,1*numReconstructionsPerFigure+plotCol+1)
                if np.shape(self.objectPixels)[0] == 1:
                    plt.plot(reconstructedImage); plt.title('reconstructed signal')
                else:
                    plt.imshow(reconstructedImage, cmap='gray'); 
                    plt.title('reconstructed image'); plt.axis('off')

                # diff image
                plt.subplot(3,numReconstructionsPerFigure,2*numReconstructionsPerFigure+plotCol+1)
                if np.shape(self.objectPixels)[0] == 1:
                    plt.plot(diffImage); plt.title('abs difference signal')
                else:
                    plt.imshow(diffImage, cmap='gray'); 
                    plt.title('abs difference image'); plt.axis('off')
            plt.tight_layout()


    # shows distrbution along the distance from a particular cluster and several examples for that distance
    def ShowSingleTemplateDistances(self, X, listOfTemplates=[0,1]):

        showAsTraces = (np.shape(self.objectPixels)[0] == 1)
        assert(all([(x in range(self.numClusters)) for x in listOfTemplates]))
                
        X_rep = self.RepresentUsingModel(X, representationMethod='distFromAllClusters')
        
        percentilesToShow = [1,5,10,30,60,99]
        numReadDataSamplePerPercentile = 4
        representationPercentiles = []
        for percentile in percentilesToShow:
            representationPercentiles.append(np.percentile(self.dataRepresentation, percentile, axis=0))
        medianRepVec =  np.percentile(self.dataRepresentation, 50, axis=0)

        for templateInd in listOfTemplates:
            plt.figure(); gs = gridspec.GridSpec(numReadDataSamplePerPercentile+2,
                                                 len(percentilesToShow))

            # calculate the Gaussian smoothed distribution of values along the eignevector direction
            sigmaOfKDE = (representationPercentiles[-1][templateInd] - 
                          representationPercentiles[1][templateInd])/100.0
            pdfStart   = representationPercentiles[1][templateInd]  - 15*sigmaOfKDE
            pdfStop    = representationPercentiles[-1][templateInd] + 15*sigmaOfKDE
            xAxis = np.linspace(pdfStart,pdfStop,200)
            PDF_Model = KernelDensity(kernel='gaussian', \
                            bandwidth=sigmaOfKDE).fit(self.dataRepresentation[:,templateInd].reshape(-1,1))
            logPDF = PDF_Model.score_samples(xAxis.reshape(-1,1))
            percentileValuesToShow = \
                [representationPercentiles[x][templateInd] for x in range(len(representationPercentiles))]
            percentilesToShowLogPDF = \
                PDF_Model.score_samples(np.array(percentileValuesToShow).reshape(-1,1))

            # show distribution of distance from current template and red dots at the list of precentiles to show 
            plt.subplot(gs[0,:])
            plt.fill(xAxis, np.exp(logPDF), fc='b', alpha=0.9);
            plt.scatter(percentileValuesToShow, np.exp(percentilesToShowLogPDF), c='r',s=40);
            plt.title(str(100*self.clusterFrequency[templateInd])[:4] + '% assignment frequency');

            for plotCol, currPrecentile in enumerate(percentilesToShow):                
                currPrecentileRepVec              = medianRepVec.copy()
                currPrecentileRepVec[templateInd] = representationPercentiles[plotCol][templateInd]
                
                currPrecentileImage = np.zeros(np.shape(self.objectPixels))
                currPrecentileRepVec = currPrecentileRepVec[:,np.newaxis].T
                currPrecentileImage[self.objectPixels] = \
                            self.ReconstructUsingModel(currPrecentileRepVec).ravel()
                
                # show the median image with current precentile as activation of the curr image
                plt.subplot(gs[1,plotCol]);
                if showAsTraces:
                    plt.plot(currPrecentileImage); 
                    plt.title('precentile: ' + str(percentilesToShow[plotCol]) + '%')
                else:
                    plt.imshow(currPrecentileImage, cmap='hot'); 
                    plt.title('precentile: ' + str(percentilesToShow[plotCol]) + '%'); plt.axis('off')

                # find the most suitible candidates in X for current precentile
                distFromPercentile = abs(X_rep[:,templateInd] - 
                                         representationPercentiles[plotCol][templateInd])
                X_inds = np.argpartition(distFromPercentile, \
                                         numReadDataSamplePerPercentile)[:numReadDataSamplePerPercentile]
                for k, X_ind in enumerate(X_inds):
                    currNearestPrecentileImage = np.zeros(np.shape(self.objectPixels))
                    currNearestPrecentileImage[self.objectPixels]  = X[X_ind,:].ravel()
                    
                    plt.subplot(gs[2+k,plotCol]);
                    if showAsTraces:
                        plt.plot(currNearestPrecentileImage); 
                        plt.title('NN with closest percentile');
                    else:
                        plt.imshow(currNearestPrecentileImage, cmap='gray'); 
                        plt.title('NN with closest percentile'); plt.axis('off')
            plt.tight_layout()
            
            
    def ShowDataScatterPlotsWithTSNE(self, X=None, y=None, tSNE_perplexity=30.0, colorMap='Paired'):
        # show the distance from 2 most frequent clusters and the tSNE of the entire "distance form template" space 
        
        if X is None:
            X_rep = self.dataRepresentation
        else:
            X_rep = self.RepresentUsingModel(X)
            
        if y is None:
            y = np.ones(X_rep.shape[0])
            
        tSNE_KmeansModel = TSNE(n_components=2, perplexity=tSNE_perplexity, random_state=0)
        X_rep_tSNE = tSNE_KmeansModel.fit_transform(X_rep)
        
        # take the two most frequent patterns
        mostFrequent = self.sortedTemplatesByFrequency[:2]
        
        plt.figure()
        plt.subplot(1,2,1); 
        plt.scatter(X_rep[:,mostFrequent[0]], \
                    X_rep[:,mostFrequent[1]],c=y,cmap=colorMap,s=10,alpha=0.9)
        plt.title('"distance form template" representation'); 
        plt.xlabel('distance from template 1'); plt.ylabel('distance from template 2')
        plt.subplot(1,2,2); 
        plt.scatter(X_rep_tSNE[:,0],X_rep_tSNE[:,1],c=y,cmap=colorMap,s=15,alpha=0.9)
        plt.title('t-SNE of Kmeans representation'); plt.xlabel('t-SNE axis1'); plt.ylabel('t-SNE axis2')


    def ShowTemplatesInPCASpace(self, X, y=None, tSNE_perplexity=30.0, colorMap='Paired'):
        # show the templates in the 2PC space and the tSNE of the entire PCA space
        
        # build PCA model and project the data onto the PCA space
        PCAModel = decomposition.PCA(n_components=60, whiten=False)
        X_rep = PCAModel.fit_transform(X)
                
        # project the Kmeans templates onto the PCA space
        templates_rep = PCAModel.transform(templateModel.KmeansModel.cluster_centers_)
        
        if y is None:
            y = self.RepresentUsingModel(X, representationMethod='clusterIndex')
            
        tSNE_PCAModel = TSNE(n_components=2, perplexity=tSNE_perplexity, random_state=0)
        X_rep_tSNE = tSNE_PCAModel.fit_transform(np.vstack((X_rep,templates_rep))) 
        
        plt.figure()
        plt.subplot(1,2,1); plt.scatter(X_rep[:,0],X_rep[:,1],c=y,cmap=colorMap,s=15,alpha=0.9)
        plt.scatter(templates_rep[:,0],templates_rep[:,1],c='k',cmap=colorMap,s=50)
        plt.title('PCA representation'); plt.xlabel('PC1 coeff'); plt.ylabel('PC2 coeff')
        
        nC = templates_rep.shape[0]        
        plt.subplot(1,2,2); 
        plt.scatter(X_rep_tSNE[:-nC,0],\
                    X_rep_tSNE[:-nC,1],c=y,cmap=colorMap,s=15,alpha=0.9)
        plt.scatter(X_rep_tSNE[-nC:,0],\
                    X_rep_tSNE[-nC:,1],c='k',cmap=colorMap,s=50)
        plt.title('t-SNE of PCA representation'); plt.xlabel('t-SNE axis1'); plt.ylabel('t-SNE axis2')

Output Text:
------------


------------------------------------------------------------
Cell index: 8
Input Cell Type: markdown
Input Text:
-----------
## Now lets apply k-means and look at the cluster centers

We'll think of each image as a point in a high dimensional space, and each cluster center is a different point in the high dimensional image space.


----------

For K = 4:
--------------

Output Text:
------------


------------------------------------------------------------
Cell index: 9
Input Cell Type: python
Input Text:
-----------
matplotlib.rcParams['font.size'] = 12
matplotlib.rcParams['figure.figsize'] = (12,9)

objectPixels = np.ones((np.shape(scaledDownImages)[0],np.shape(scaledDownImages)[1])) == 1
sampleDim = np.shape(scaledDownImages)[0]*np.shape(scaledDownImages)[1]
X = scaledDownImages.reshape(sampleDim,-1).T

templateModel = KmeansModel(X, numClusters=4, objectPixels=objectPixels)
templateModel.ShowTemplates(numTemplatesToShow=4)

Output Text:
------------
<matplotlib.figure.Figure at 0x7f8fabccedd8>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a 2x2 grid of four heatmaps, each displaying a different frequency distribution. 


Each heatmap is predominantly dark, representing low values, with a bright, intensely colored central region indicating high values.  The central regions have varying shapes and orientations. The top left image shows an elongated, horizontally oriented bright region, while the top right shows a more circular, compact bright region. The bottom left shows a similarly elongated region but perhaps slightly more vertically oriented. The bottom right shows a more vertically oriented, diamond-shaped bright region.


Above each heatmap, text indicates the frequency associated with that distribution, expressed as a percentage (40.5%, 22.9%, 22.6%, and 13.8%). This suggests the images represent some kind of spectral or frequency analysis, likely related to a signal or data set where different frequencies have different intensities or weights. The varying shapes suggest that the underlying signals differ in their frequency components.


------------------------------------------------------------
Cell index: 10
Input Cell Type: markdown
Input Text:
-----------
We can see that the centers are basically just large or small leaves, elongated either vertically and horizontally

----------

Now let's try doing it for **K = 9**:
-------------------------------------

Output Text:
------------


------------------------------------------------------------
Cell index: 11
Input Cell Type: python
Input Text:
-----------
matplotlib.rcParams['font.size'] = 12
matplotlib.rcParams['figure.figsize'] = (12,9)

templateModel = KmeansModel(X, numClusters=9, objectPixels=objectPixels)
templateModel.ShowTemplates(numTemplatesToShow=9)

Output Text:
------------
<matplotlib.figure.Figure at 0x7f8fac4296d8>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid of nine smaller images, arranged in three rows and three columns. Each small image displays a heatmap-like visualization, with varying intensities of orange and red hues on a black background. These heatmaps appear to represent different spatial distributions or patterns. 


Above each small image is a percentage value labeled "frequency." These percentages likely correspond to the frequency or probability of occurrence of the spatial patterns shown in the heatmaps. The patterns themselves range from elongated ellipses to more irregular, almost flame-like shapes, and a cross-like pattern. The differences in shape and intensity suggest a range of underlying phenomena or data distributions being visualized. The overall structure is clear and well-organized, aiming to visually compare and contrast these different frequency distributions.


------------------------------------------------------------
Cell index: 12
Input Cell Type: markdown
Input Text:
-----------
Oh, now we start seeing some more specificity in the cluster centers


----------

Let's try also for **K=16**:
----------------------------

Output Text:
------------


------------------------------------------------------------
Cell index: 13
Input Cell Type: python
Input Text:
-----------
matplotlib.rcParams['font.size'] = 10
matplotlib.rcParams['figure.figsize'] = (11,9)

templateModel = KmeansModel(X, numClusters=16, objectPixels=objectPixels)
templateModel.ShowTemplates(numTemplatesToShow=16)

Output Text:
------------
<matplotlib.figure.Figure at 0x7f8fac060828>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a grid of sixteen smaller images, arranged in four rows and four columns. Each smaller image displays a heatmap-like visualization, predominantly dark with bright, fiery orange and yellow regions concentrated in the center. These bright regions take on various shapes, ranging from simple ellipses or ovals to more complex, flame-like or irregular forms. 


Above each smaller image is text indicating a percentage value labeled as "frequency." The percentages decrease from the top left (15.3%) to the bottom right (0.82%), suggesting a possible ordering based on the frequency of occurrence of the respective shapes.  The overall impression is that of a visualization of data related to shape frequencies or distributions, possibly from an image analysis or pattern recognition process. The color scheme emphasizes the intensity or magnitude of the data within each shape.


------------------------------------------------------------
Cell index: 14
Input Cell Type: markdown
Input Text:
-----------
OK, we can clearly see templates here that are very much leaf type specific

----------

Finally, let's try for **K=36**:
--------------------------------

Output Text:
------------


------------------------------------------------------------
Cell index: 15
Input Cell Type: python
Input Text:
-----------
matplotlib.rcParams['font.size'] = 9
matplotlib.rcParams['figure.figsize'] = (13,10)

templateModel = KmeansModel(X, numClusters=36, objectPixels=objectPixels)
templateModel.ShowTemplates(numTemplatesToShow=36)

Output Text:
------------
<matplotlib.figure.Figure at 0x7f8fab864f60>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid of 30 small images, arranged in six rows and five columns. Each small image displays a different heatmap-like pattern, ranging from simple shapes to more complex, leaf-like forms.  The patterns are predominantly orange and yellow, with the intensity of color representing some underlying value.  The background of each small image is black, providing high contrast.

Above each small image is text indicating a "frequency" value, expressed as a percentage. These percentages seem to be ordered from highest (around 9%) to lowest (around 0.5%), progressing from left to right and top to bottom across the grid.  The frequency values suggest a ranking or ordering of the displayed patterns based on their occurrence or significance.

The overall impression is that the image depicts a set of visual data, possibly representing features extracted from some larger dataset. The patterns' progression from simple ellipses to more intricate leaf-like forms suggests a possible hierarchical or evolutionary process, or perhaps a classification of features based on complexity. The frequency values could represent the relative prevalence of each pattern in the original data.


------------------------------------------------------------
Cell index: 16
Input Cell Type: markdown
Input Text:
-----------
Now we can see even more clearly templates that are leaf type specific. this gives us hope regarding the possibility of using these features later for classification purposes.


----------
let's see how good of an approximation can these 36 templates be for several specific leaf images. 
Meaning, we'll show a leaf image, it's closest template amount the 36 templates, and the difference between these images. 

## Model Reconstructions: 

Output Text:
------------


------------------------------------------------------------
Cell index: 17
Input Cell Type: python
Input Text:
-----------
matplotlib.rcParams['font.size'] = 8
matplotlib.rcParams['figure.figsize'] = (12,5)

templateModel.ShowReconstructions(X, numReconstructions=6)

Output Text:
------------
<matplotlib.figure.Figure at 0x7f8fabc35390>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid of 18 small images arranged in three rows and six columns. Each column represents a single image and its transformations.  The top row displays the "original image" for six different shapes, ranging from simple ovals to more complex leaf-like forms. These original images are all binary, meaning they consist only of black and white pixels, with the shapes appearing as white against a black background.

The middle row shows the "reconstructed image" corresponding to each original image in the top row. These reconstructed images appear similar to the originals but with slightly blurred edges and a less crisp definition of the shapes.  They are also binary images.

Finally, the bottom row presents the "abs difference image" for each pair. These images highlight the differences between the original and reconstructed images.  The difference is shown as brighter pixels in the areas where the original and reconstructed images do not perfectly match.  This visualizes the discrepancies introduced during the reconstruction process, revealing the level of detail lost or altered.


------------------------------------------------------------
Cell index: 18
Input Cell Type: markdown
Input Text:
-----------
We can see that the reconstructions are OK but far from perfect

----------

## Now let's visualize how these cluster center look like in the original high dimensional space

For this purpose we first apply PCA to reduce dimensionality of the images and then show the data once in the space of the first two principal components and in the full PCA space as visualized by t-SNE.

The cluster centers are in black, and the data points are colored according to cluster assignment

Output Text:
------------


------------------------------------------------------------
Cell index: 19
Input Cell Type: python
Input Text:
-----------
matplotlib.rcParams['font.size'] = 12
matplotlib.rcParams['figure.figsize'] = (12,8)

templateModel.ShowTemplatesInPCASpace(X, y=None, tSNE_perplexity=15.0, colorMap='Paired')

Output Text:
------------
<matplotlib.figure.Figure at 0x7f8fac30a8d0>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image presents a comparative visualization of two dimensionality reduction techniques applied to a dataset.  The left panel displays a Principal Component Analysis (PCA) representation, while the right panel shows a t-distributed Stochastic Neighbor Embedding (t-SNE) representation of the same PCA-reduced data.  Both plots are scatter plots, where each point represents a data point from the original dataset.  The points are color-coded, suggesting that the data points belong to different clusters or classes.  The x and y axes in each plot represent the principal components (PC1 and PC2) for the PCA plot and t-SNE axes (axis1 and axis2) for the t-SNE plot.

The PCA plot shows a relatively smooth distribution of the colored clusters along the PC1 and PC2 axes, indicating a linear relationship between the data points. The t-SNE plot, however, exhibits a more complex, non-linear structure. The clusters are grouped more tightly together and separated more distinctly than in the PCA plot, suggesting that t-SNE has better captured the underlying non-linear relationships within the data. In both plots, several large black dots are overlaid on the colored points, which might represent centroids, cluster centers, or some other significant data points.

The overall purpose of the image is to compare the visualization capabilities of PCA and t-SNE. The visual difference highlights the strengths and weaknesses of each method: PCA is better at revealing linear relationships, while t-SNE excels at revealing complex, non-linear structures, even at the cost of some distortion of distances. The choice of method would depend on the nature of the data and the goals of the analysis.


------------------------------------------------------------
Cell index: 20
Input Cell Type: markdown
Input Text:
-----------
## Show the distribution of distances of data samples from the most frequent template
This will help us get a feel for what what different distances represent by plotting several examples that are distant from that template by approximately that amount

Output Text:
------------


------------------------------------------------------------
Cell index: 21
Input Cell Type: python
Input Text:
-----------
matplotlib.rcParams['font.size'] = 7
matplotlib.rcParams['figure.figsize'] = (13,9)

mostFrequentClusterInd = templateModel.sortedTemplatesByFrequency[0]
templateModel.ShowSingleTemplateDistances(X, listOfTemplates=[mostFrequentClusterInd])

Output Text:
------------
<matplotlib.figure.Figure at 0x7f8fabd72b38>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
This image displays a visualization of the distribution of data points and their nearest neighbors (NN) based on a percentile ranking. The top section shows a frequency distribution plot, a blue area chart illustrating the density of data points across different percentiles (from 1% to 99%).  Red dots highlight specific percentiles (1%, 5%, 10%, 30%, 60%, 99%).  The peak of the distribution is around the 20th percentile.

The majority of the image shows a grid of images arranged in four rows and six columns. Each column represents a specific percentile (as labeled above each column in the top section), showcasing an example image at that percentile. Below each example image is another image labeled "NN with closest percentile," which displays the nearest neighbor to the example image in the feature space.  This nearest neighbor is likely selected based on some distance metric (not specified in the image).  The images seem to depict different types of leaves, showing a progression in shape and complexity as the percentile increases from simple, elongated shapes at lower percentiles to more complex, multi-lobed leaves at higher percentiles.  The change in the NN images also reflects this trend, showing how similar leaves are clustered together.


------------------------------------------------------------
Cell index: 22
Input Cell Type: markdown
Input Text:
-----------
From here we can see that all similar patterns (more to the left) are similar in the same way, and that all dissimilar patterns (more to the right) are different in their own unique way.
Well, even though it's a very good quote, it isn't actually what we see in this dataset.
 
What we do see is something a little bit surprising, if we think about it from the geometric point of view. points (leaf images) that are far away from a specific point (a template) can theoretically be far away in many different directions and therefore one would expect large diversity among all images that are at the same distance from a specific template. This is only a little bit the case. We can also see some similarities between equally distant points relative to a template. What this means is that this feature "distance from template i" can be informative beyond just a binary type "like template i" vs "not like template i" feature.

----------
## Let's look at another such feature (distance from 10th most frequent template):

Output Text:
------------


------------------------------------------------------------
Cell index: 23
Input Cell Type: python
Input Text:
-----------
matplotlib.rcParams['font.size'] = 7
matplotlib.rcParams['figure.figsize'] = (13,9)

mediumFrequencyClusterInd = templateModel.sortedTemplatesByFrequency[10]
templateModel.ShowSingleTemplateDistances(X, listOfTemplates=[mediumFrequencyClusterInd])

Output Text:
------------
<matplotlib.figure.Figure at 0x7f8fabd1ff28>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a visualization of data related to leaf image classification or a similar task. The top portion shows a density plot, likely representing the frequency distribution of a specific feature or metric across a dataset of leaf images.  The x-axis seems to denote percentiles (1%, 5%, 10%, 30%, 60%, 99%), and the y-axis represents the frequency of data points falling within each percentile range.  Red dots highlight specific percentile points of interest on the density curve.  The plot's title indicates a 3.34% assignment frequency, possibly referring to the frequency of assignments to certain clusters or classes.

Below the density plot, the image displays a grid of leaf images organized according to the percentiles shown above. Each column corresponds to a percentile, and each row shows multiple examples of leaf images that are considered nearest neighbors (NN) to the percentile. These nearest neighbors likely represent the leaf images that are most similar to the average or representative image of each percentile. The grayscale images show different leaf shapes and structures, indicating the variability within the dataset. The consistent "NN with closest percentile" labels beneath each image row confirm the methodology of selecting images based on proximity to a given percentile's characteristics.  The images suggest a range of leaf shapes, from simple elliptical forms to more complex, multi-lobed structures.


------------------------------------------------------------
Cell index: 24
Input Cell Type: markdown
Input Text:
-----------
Again, we see similar things happen in this feature as well.

----------
## Visualize "distance from cluster centers" feature space
Plot the scatter of distance from the two most frequent clusters, and the low dimensional t-SNE representation of the entire "distance from clusters" space. the colors indicate the leaf type.

Output Text:
------------


------------------------------------------------------------
Cell index: 25
Input Cell Type: python
Input Text:
-----------
matplotlib.rcParams['font.size'] = 12
matplotlib.rcParams['figure.figsize'] = (12,8)

X_train = X[trainIDs-1,:]
y_train = trainLabels

templateModel.ShowDataScatterPlotsWithTSNE(X=X_train, y=y_train, tSNE_perplexity=15.0, colorMap='Paired')

Output Text:
------------
<matplotlib.figure.Figure at 0x7f8fa9a8f0b8>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a comparative visualization of two different data representations. It features two scatter plots side-by-side, each displaying the same dataset but using different dimensionality reduction and visualization techniques.

The left plot is titled "distance form template" representation.  It shows a strong positive correlation between "distance from template 1" and "distance from template 2," with points clustered along a diagonal line from the lower left to the upper right. The points are colored, suggesting different cluster assignments or categories within the data.  This representation maintains the original distances between data points, thus revealing a strong linear relationship.

The right plot, titled "t-SNE of Kmeans representation," displays the same dataset but after applying t-distributed Stochastic Neighbor Embedding (t-SNE) dimensionality reduction.  This technique aims to preserve the local neighborhood structure of the data, rather than the global distances.  The plot shows a non-linear, more complex arrangement of points, revealing clusters that were not as apparent in the original distance representation.  The coloring of the points remains consistent with the left plot, allowing comparison of cluster assignments between the two visualizations.  The t-SNE plot better reveals the underlying clustering structure in a lower-dimensional space.


------------------------------------------------------------
Cell index: 26
Input Cell Type: markdown
Input Text:
-----------
Interesting! 


----------
## Show Model Accuracy as function of number of clusters used
Now, similar to what we did for PCA, let's try to see what is the classification accuracy using k-means features for several different values of K and several differnt classifiers

Output Text:
------------


------------------------------------------------------------
Cell index: 27
Input Cell Type: python
Input Text:
-----------
matplotlib.rcParams['font.size'] = 12
matplotlib.rcParams['figure.figsize'] = (12,8)

numClustersToUse = [1,2,4,8,16,32,64]

logReg = linear_model.LogisticRegression(C=10.0)
kNN = neighbors.KNeighborsClassifier(n_neighbors=7)
RF = ensemble.RandomForestClassifier(n_estimators=100)

logRegMeanAccuracy = []; kNN_MeanAccuracy = []; RF_MeanAccuracy = []
logRegAccuracyStd  = []; kNN_AccuracyStd  = []; RF_AccuracyStd  = []

for k in numClustersToUse:
    stratifiedCV = model_selection.StratifiedKFold(n_splits=5, random_state=1)
    logRegAccuracy = []; kNN_Accuracy = []; RF_Accuracy = []
    
    templateModel = KmeansModel(X_train, numClusters=k)
    X_kmeans_train = templateModel.RepresentUsingModel(X_train, representationMethod='distFromAllClusters')
    
    for trainInds, validInds in stratifiedCV.split(X_kmeans_train, y_train):
        X_train_cv = X_kmeans_train[trainInds,:]
        X_valid_cv = X_kmeans_train[validInds,:]

        y_train_cv = y_train[trainInds]
        y_valid_cv = y_train[validInds]

        logReg.fit(X_train_cv, y_train_cv)
        kNN.fit(X_train_cv, y_train_cv)
        RF.fit(X_train_cv, y_train_cv)
    
        logRegAccuracy.append(accuracy_score(y_valid_cv, logReg.predict(X_valid_cv)))
        kNN_Accuracy.append(accuracy_score(y_valid_cv, kNN.predict(X_valid_cv)))
        RF_Accuracy.append(accuracy_score(y_valid_cv, RF.predict(X_valid_cv)))

    logRegMeanAccuracy.append(np.array(logRegAccuracy).mean())
    logRegAccuracyStd.append(np.array(logRegAccuracy).std())

    kNN_MeanAccuracy.append(np.array(kNN_Accuracy).mean())
    kNN_AccuracyStd.append(np.array(kNN_Accuracy).std())

    RF_MeanAccuracy.append(np.array(RF_Accuracy).mean()) 
    RF_AccuracyStd.append(np.array(RF_Accuracy).std())
        
plt.figure()
plt.errorbar(x=numClustersToUse, y=logRegMeanAccuracy, yerr=logRegAccuracyStd)
plt.errorbar(x=numClustersToUse, y=kNN_MeanAccuracy  , yerr=kNN_AccuracyStd)
plt.errorbar(x=numClustersToUse, y=RF_MeanAccuracy   , yerr=RF_AccuracyStd)
plt.xlim(min(numClustersToUse)-1,max(numClustersToUse)+1); plt.legend(['Logistic Regression','k Nearest Neighbor','Random Forest'],loc=2)
plt.xlabel('num Clusters'); plt.ylabel('validation accuracy'); plt.title('accuracy as function of num Clusters')

Output Text:
------------
<matplotlib.text.Text at 0x7f8fac510be0>
<matplotlib.figure.Figure at 0x7f8faba17630>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image displays a line graph illustrating the validation accuracy of three different machine learning models—Logistic Regression, k Nearest Neighbor, and Random Forest—as a function of the number of clusters used in a clustering algorithm.  The x-axis represents the number of clusters, ranging from approximately 0 to 65. The y-axis represents the validation accuracy, ranging from 0 to 0.65.

Each model is represented by a distinct colored line.  Error bars are included for each data point, showing the variability or uncertainty in the accuracy measurements.  The graph shows that the Logistic Regression model achieves the highest accuracy overall, with its accuracy increasing steadily as the number of clusters increases initially, before leveling off at higher cluster counts.  The Random Forest model shows a similar trend, though achieving a lower accuracy than the Logistic Regression model.  The k Nearest Neighbor model shows a more modest increase in accuracy with increasing cluster numbers, and maintains a consistently lower accuracy compared to the other two models.

The title clearly states the purpose of the graph: to show the relationship between the number of clusters and the resulting validation accuracy of the three models.  The legend provides a clear identification of each line and its corresponding model.  The overall structure of the graph is clean and easy to interpret.


------------------------------------------------------------
Cell index: 28
Input Cell Type: markdown
Input Text:
-----------
## Summery 

If we compare what we see here with what we saw in the [PCA case][1], we see two main points:

 1. PCA and K-Means image features are similarly useful in terms of classification.
 2. The order between Logistic Regression and Random Forest has switched here compared to PCA case.

Even though these finding cannot be generalized because they heavily depend of this particular data distribution, we can speculate that there might be something complementary that Random Forest adds to the PCA feature representation, and that k-means features add to the classification abilities of the Logistic Regression classifier.

Anyway, I hope this script has shed some light about what k-means is about for some of you.

  [1]: https://www.kaggle.com/selfishgene/visualizing-pca-with-leaf-dataset

Output Text:
------------


------------------------------------------------------------

================================================================================
================================================================================
filtering-and-auto-correlation-tutorial.ipynb:
==============================================
------------------------------------------------------------
Cell index: 1
Input Cell Type: markdown
Input Text:
-----------
# Filtering and Auto-Correlation Tutorial with Temperature Data
In this scipt we will perform basic filtering operations using pandas (low pass and high pass filtering) and also examine the auto-correlation structure of temperature data (taken from the [Historical Hourly Weather Dataset](https://www.kaggle.com/selfishgene/historical-hourly-weather-data)) also using pandas.

The main goal of the script is to give some intuition about what low pass and high pass filtering operations are, and understand what is the auto-correlation function. We use hourly sampled temperature data since it contains periodic structrue both on a daily basis and on a yearly basis.

Output Text:
------------


------------------------------------------------------------
Cell index: 2
Input Cell Type: python
Input Text:
-----------
import numpy as np
import pandas as pd
from pandas.plotting import autocorrelation_plot, lag_plot
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec

Output Text:
------------


------------------------------------------------------------
Cell index: 3
Input Cell Type: markdown
Input Text:
-----------
# Load Data and Show available Cities in the dataset

Output Text:
------------


------------------------------------------------------------
Cell index: 4
Input Cell Type: python
Input Text:
-----------
#%% load data that will be used in the script
cityTable     = pd.read_csv('../input/city_attributes.csv')
temperatureDF = pd.read_csv('../input/temperature.csv', index_col=0)
temperatureDF.index = pd.to_datetime(temperatureDF.index)

cityTable

Output Text:
------------
                 City        Country   Latitude   Longitude
0           Vancouver         Canada  49.249660 -123.119339
1            Portland  United States  45.523449 -122.676208
2       San Francisco  United States  37.774929 -122.419418
3             Seattle  United States  47.606209 -122.332069
4         Los Angeles  United States  34.052231 -118.243683
5           San Diego  United States  32.715328 -117.157257
6           Las Vegas  United States  36.174969 -115.137222
7             Phoenix  United States  33.448380 -112.074043
8         Albuquerque  United States  35.084492 -106.651138
9              Denver  United States  39.739151 -104.984703
10        San Antonio  United States  29.424120  -98.493629
11             Dallas  United States  32.783058  -96.806671
12            Houston  United States  29.763281  -95.363274
13        Kansas City  United States  39.099731  -94.578568
14        Minneapolis  United States  44.979969  -93.263840
15        Saint Louis  United States  38.627270  -90.197891
16            Chicago  United States  41.850029  -87.650047
17          Nashville  United States  36.165890  -86.784439
18       Indianapolis  United States  39.768379  -86.158043
19            Atlanta  United States  33.749001  -84.387978
20            Detroit  United States  42.331429  -83.045753
21       Jacksonville  United States  30.332180  -81.655647
22          Charlotte  United States  35.227089  -80.843132
23              Miami  United States  25.774269  -80.193657
24         Pittsburgh  United States  40.440620  -79.995888
25            Toronto         Canada  43.700111  -79.416298
26       Philadelphia  United States  39.952339  -75.163788
27           New York  United States  40.714272  -74.005966
28           Montreal         Canada  45.508839  -73.587807
29             Boston  United States  42.358429  -71.059769
30          Beersheba         Israel  31.251810   34.791302
31  Tel Aviv District         Israel  32.083328   34.799999
32              Eilat         Israel  29.558050   34.948212
33              Haifa         Israel  32.815559   34.989170
34          Nahariyya         Israel  33.005859   35.094090
35          Jerusalem         Israel  31.769039   35.216331


------------------------------------------------------------
Cell index: 5
Input Cell Type: markdown
Input Text:
-----------
We see that the cities have latitude and longitude information, and are ordered from west to east (according to longitude coordinate).

# Show Temperature as function of time for several selected Cities

Output Text:
------------


------------------------------------------------------------
Cell index: 6
Input Cell Type: python
Input Text:
-----------
#%% show several temperature plots to get a feel for the dataset
#citiesToShow = ['San Francisco','Las Vegas','Chicago','Toronto','Houston','Jerusalem']
citiesToShow  = ['Portland','Dallas','Miami','Montreal','Tel Aviv District']

t0 = temperatureDF.index
t1 = pd.date_range(pd.to_datetime('1/7/2015',dayfirst=True),pd.to_datetime('1/10/2016',dayfirst=True),freq='H')
t2 = pd.date_range(pd.to_datetime('1/7/2015',dayfirst=True),pd.to_datetime('1/9/2015' ,dayfirst=True),freq='H')
t3 = pd.date_range(pd.to_datetime('1/7/2015',dayfirst=True),pd.to_datetime('21/7/2015',dayfirst=True),freq='H')
t = [t0, t1, t2, t3]

fig, ax = plt.subplots(nrows=4,ncols=1,figsize=(15,14))
for i, t in enumerate(t):
    for k in range(len(citiesToShow)):
        ax[i].plot(t,temperatureDF.loc[t,citiesToShow[k]])

ax[0].legend(citiesToShow, fontsize=16,
              loc='upper left',bbox_to_anchor=(0.02,1.3), ncol=len(citiesToShow))
for i in range(len(ax)): ax[i].set_ylabel('Temperature [$^\circ$K]', fontsize=11)
ax[3].set_xlabel('time', fontsize=14);


Output Text:
------------
<matplotlib.figure.Figure at 0x7f225d0ec080>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a multi-panel graph displaying temperature data for five different cities: Portland, Dallas, Miami, Montreal, and Tel Aviv District.  The data is plotted over time, showing temperature fluctuations across different time scales.

The graph is structured into four subplots. The top subplot shows the temperature data for all five cities over a period of approximately five years (2012-2018), providing a broad overview of the annual temperature cycles. The second subplot zooms in on a slightly shorter period within the broader timeframe, showing more detailed temperature fluctuations. The third and fourth subplots further zoom in, focusing on a smaller time window within a single year (2015), displaying the daily temperature variations in more detail.  The third subplot shows a longer period of about a month and the fourth subplot shows a shorter period of about three weeks.

Each subplot uses a line graph, with each city represented by a different colored line. This allows for easy comparison of temperature variations between the cities across different time scales. The x-axis represents time, and the y-axis represents temperature in Kelvin.  The legend clearly identifies the city corresponding to each line color. The overall structure of the image facilitates a comprehensive understanding of the temperature patterns in these different cities.


------------------------------------------------------------
Cell index: 7
Input Cell Type: markdown
Input Text:
-----------
We clearly see yearly periodicity (top plot) as well as daily peridicity (bottom two plots) in all cities. We can also see that it's quite warm in Miami and Dallas, quite cool in Montreal and the amplitude between day and night in Portland is very large.

# Show the Auto-Correlation function of Los Angeles Temperature Signal

Output Text:
------------


------------------------------------------------------------
Cell index: 8
Input Cell Type: python
Input Text:
-----------
#%% show autocorr and lag plots

cityToShow = 'Los Angeles'
selectedLagPoints = [1,3,6,9,12,24,36,48,60]
maxLagDays = 7

originalSignal = temperatureDF[cityToShow]

# set grid spec of the subplots
plt.figure(figsize=(12,6))
gs = gridspec.GridSpec(2, len(selectedLagPoints))
axTopRow = plt.subplot(gs[0, :])
axBottomRow = []
for i in range(len(selectedLagPoints)):
    axBottomRow.append(plt.subplot(gs[1, i]))

# plot autocorr
allTimeLags = np.arange(1,maxLagDays*24)
autoCorr = [originalSignal.autocorr(lag=dt) for dt in allTimeLags]
axTopRow.plot(allTimeLags,autoCorr); 
axTopRow.set_title('Autocorrelation Plot of Temperature Signal', fontsize=18);
axTopRow.set_xlabel('time lag [hours]'); axTopRow.set_ylabel('correlation coefficient')
selectedAutoCorr = [originalSignal.autocorr(lag=dt) for dt in selectedLagPoints]
axTopRow.scatter(x=selectedLagPoints, y=selectedAutoCorr, s=50, c='r')

# plot scatter plot of selected points
for i in range(len(selectedLagPoints)):
    lag_plot(originalSignal, lag=selectedLagPoints[i], s=0.5, alpha=0.7, ax=axBottomRow[i])    
    if i >= 1:
        axBottomRow[i].set_yticks([],[])
plt.tight_layout()

Output Text:
------------
<matplotlib.figure.Figure at 0x7f225cccd4e0>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a composite figure presenting an autocorrelation analysis of a temperature signal.  The figure is divided into two main sections: a top section showing the autocorrelation function itself and a bottom section displaying lag plots.

The top section is a line plot showing the autocorrelation coefficient as a function of time lag (in hours). The x-axis represents the time lag, and the y-axis represents the correlation coefficient, ranging from 0 to 1. The plot exhibits a clear cyclical pattern indicating a periodic component in the temperature signal.  Key points on the autocorrelation curve are highlighted with red circles, likely signifying significant peaks and troughs.

The bottom section consists of a series of smaller scatter plots arranged horizontally. Each scatter plot is a lag plot, showing the relationship between the temperature at a given time (y(t)) and the temperature at a later time (y(t + lag)), where 'lag' is a multiple of a base time increment (likely hours, based on the top plot).  These lag plots visually reinforce the periodic nature identified in the autocorrelation function. Each plot's title indicates the specific time lag.  The plots show a strong linear relationship with a positive slope, consistent with the periodic pattern. The slight scatter around the linear trend indicates some degree of noise or variability in the temperature signal.

------------------------------------------------------------
Cell index: 9
Input Cell Type: markdown
Input Text:
-----------
The top row shows the auto-correlation plot.  
The bottom scatter plots correspond to the red points marked on the auto-correlation plot. 

The leftmost plot shows the Temperature at time t vs Temperature at time t + 1 hour scatter plot. We know that weather doesn't change that much in one hour and therefore we see extreemly high correlation between the temeratures there.  
This correlation gradually decreases up to 12 hour difference, that corresponds to the switch from day to night, and then contiues to oscillate with a slow decreasing trend as the days go by. 

# Show Auto-Correlation with various zoom ins (temporal scales)

Output Text:
------------


------------------------------------------------------------
Cell index: 10
Input Cell Type: python
Input Text:
-----------
#%% zoom in and out on the autocorr plot
fig, ax = plt.subplots(nrows=4,ncols=1, figsize=(14,14))

timeLags = np.arange(1,25*24*30)
autoCorr = [originalSignal.autocorr(lag=dt) for dt in timeLags]
ax[0].plot(1.0/(24*30)*timeLags, autoCorr); ax[0].set_title('Autocorrelation Plot', fontsize=20);
ax[0].set_xlabel('time lag [months]'); ax[0].set_ylabel('correlation coeff', fontsize=12);

timeLags = np.arange(1,20*24*7)
autoCorr = [originalSignal.autocorr(lag=dt) for dt in timeLags]
ax[1].plot(1.0/(24*7)*timeLags, autoCorr);
ax[1].set_xlabel('time lag [weeks]'); ax[1].set_ylabel('correlation coeff', fontsize=12);

timeLags = np.arange(1,20*24)
autoCorr = [originalSignal.autocorr(lag=dt) for dt in timeLags]
ax[2].plot(1.0/24*timeLags, autoCorr);
ax[2].set_xlabel('time lag [days]'); ax[2].set_ylabel('correlation coeff', fontsize=12);

timeLags = np.arange(1,3*24)
autoCorr = [originalSignal.autocorr(lag=dt) for dt in timeLags]
ax[3].plot(timeLags, autoCorr);
ax[3].set_xlabel('time lag [hours]'); ax[3].set_ylabel('correlation coeff', fontsize=12);

Output Text:
------------
<matplotlib.figure.Figure at 0x7f225c9fd6d8>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a figure containing four subplots, each displaying an autocorrelation plot.  The overall title is "Autocorrelation Plot". Each subplot shows the correlation coefficient as a function of time lag, but at different time scales. The x-axis labels indicate the units of the time lag: months, weeks, days, and hours, respectively, from top to bottom.

The top subplot shows a broad, wave-like pattern of autocorrelation over a time lag of approximately 25 months. The second subplot depicts a much faster, more oscillatory pattern over a time lag of approximately 20 weeks. The third subplot shows a high-frequency oscillatory pattern with a shorter period, spanning a time lag of roughly 20 days. The bottom subplot exhibits a smoother, cyclical pattern with a longer period, covering a time lag of about 70 hours.  In all cases, the correlation coefficient decreases as the time lag increases, indicating a decay in correlation over longer time intervals. The amplitude and frequency of the oscillations vary significantly across the different time scales, suggesting the presence of periodicities at different time scales in the underlying data.


------------------------------------------------------------
Cell index: 11
Input Cell Type: markdown
Input Text:
-----------
### We clearly see the two periods here:
* The yearly period on the top plot (12 month period)
* The daily period on the two bottom plots (24 hour period)

When we looked at the data we also saw these two periods, but these autocorr plots are much smoother as they represent aggregate data across all time points of the signal. 

Output Text:
------------


------------------------------------------------------------
Cell index: 12
Input Cell Type: markdown
Input Text:
-----------
# Apply moving average and show the Low Pass Filtered Signal
The name low pass is because the resulting singal contains only low frequency changes. Applying the moving average operation (or different phrasing of the same this: filtering/convolving with a rectangular filter), is eqivalent to filtering out the high frequency changes and keeping only low frequency changes in the original signal. Hence the name "low pass".

Output Text:
------------


------------------------------------------------------------
Cell index: 13
Input Cell Type: python
Input Text:
-----------
#%% apply rolling mean and plot the signal (low pass filter)
windowSize = 5*24

lowPassFilteredSignal = originalSignal.rolling(windowSize, center=True).mean()

t0 = temperatureDF.index
t1 = pd.date_range(pd.to_datetime('1/7/2015',dayfirst=True),
                   pd.to_datetime('1/10/2016',dayfirst=True),freq='H')
t2 = pd.date_range(pd.to_datetime('1/7/2015',dayfirst=True),
                   pd.to_datetime('1/9/2015' ,dayfirst=True),freq='H')
t3 = pd.date_range(pd.to_datetime('1/7/2015',dayfirst=True),
                   pd.to_datetime('21/7/2015',dayfirst=True),freq='H')

fig, ax = plt.subplots(nrows=4,ncols=1,figsize=(14,12))
ax[0].plot(t0,originalSignal,c='y')
ax[0].plot(t0,lowPassFilteredSignal,c='r')

ax[1].plot(t1,originalSignal[t1],c='y')
ax[1].plot(t1,lowPassFilteredSignal[t1],c='r')

ax[2].plot(t2,originalSignal[t2],c='y')
ax[2].plot(t2,lowPassFilteredSignal[t2],c='r')

ax[3].plot(t3,originalSignal[t3],c='y')
ax[3].plot(t3,lowPassFilteredSignal[t3],c='r')

ax[0].legend(['original signal','low pass filtered'], fontsize=18,
              loc='upper left',bbox_to_anchor=(0.02,1.4), ncol=len(citiesToShow))
for i in range(len(ax)): ax[i].set_ylabel('Temperature [$^\circ$K]', fontsize=11)
ax[3].set_xlabel('time', fontsize=14);


Output Text:
------------
<matplotlib.figure.Figure at 0x7f225c84cf28>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a four-panel graph displaying time series data, likely temperature readings, with a comparison between an original signal and a low-pass filtered version.  Each panel shows a different time scale, ranging from a broad overview across multiple years to zoomed-in views focusing on specific months and weeks.  The original signal is depicted in yellowish-green, while the low-pass filtered signal is shown in red.

The top panel shows the longest time span, encompassing several years, illustrating the overall trend and seasonal variations in the data. The second panel zooms in on a portion of the data from the first panel, providing a more detailed look at a specific year’s temperature fluctuations.  The third and fourth panels further zoom in, focusing on shorter periods within a single year, to highlight the high-frequency variations that are smoothed out by the low-pass filter.  These finer-scale views clearly demonstrate the effect of the filter in removing the high-frequency components (short-term fluctuations) while retaining the lower-frequency components (longer-term trends).  The y-axis in all panels represents temperature in Kelvin (K), while the x-axis represents time. A legend at the top clearly labels the two lines in each panel.


------------------------------------------------------------
Cell index: 14
Input Cell Type: markdown
Input Text:
-----------
# Subtract the Low-Pass-Filtered Signal from the Original Signal and show the resulting High-Pass-Filtered Signal
The deviation from the local average is what we call the high frequency contnent of the singal. The resulting singal doesn't contain any slow changes (or different phrasing of the same thing: doesn't contain any low frequencies), since we subtracted them. This sequence of opperations (low pass filtering and subtracting the original singal from the low passed signal) is equivalent to "high pass filtering". i.e. keeping only the high frequency contnent and subtracting/removing/filtering out the low frequency content.  

Output Text:
------------


------------------------------------------------------------
Cell index: 15
Input Cell Type: python
Input Text:
-----------
#%% subtract the low pass filtered singal from the original to get high pass filtered signal
highPassFilteredSignal = originalSignal - lowPassFilteredSignal

fig, ax = plt.subplots(nrows=4,ncols=1,figsize=(14,12))
ax[0].plot(t0,highPassFilteredSignal,c='k')
ax[1].plot(t1,highPassFilteredSignal[t1],c='k')
ax[2].plot(t2,highPassFilteredSignal[t2],c='k')
ax[3].plot(t3,highPassFilteredSignal[t3],c='k')

ax[0].set_title('Deflection of Temperature from local mean',fontsize=20)
for i in range(len(ax)): ax[i].set_ylabel('$\Delta$ Temperature [$^\circ$K]', fontsize=11)
ax[3].set_xlabel('time', fontsize=14);

Output Text:
------------
<matplotlib.figure.Figure at 0x7f225c8896d8>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image displays a four-panel graph illustrating the deflection of temperature from a local mean over time.  The top panel shows the longest timescale, spanning several years (approximately 2012-2018), revealing a broader temperature trend with seasonal variations.  The y-axis represents the temperature deviation in Kelvin (K).

The second panel zooms in on a single year (2015-2016), providing a more detailed view of the temperature fluctuations within that period.  The third and fourth panels further magnify the data, focusing on shorter timescales within 2015.  Panel three shows a month-long period, highlighting a cyclical pattern likely representing daily or tidal temperature changes. The final panel shows a shorter period, emphasizing a more pronounced and regular oscillation, which could represent a diurnal (daily) cycle.

In all panels, the x-axis represents time, with varying levels of granularity across the different plots.  The use of multiple panels allows for a multi-scale analysis of the temperature data, revealing both long-term trends and short-term oscillations. The title clearly indicates the nature of the data presented: the deviation from the local mean temperature.


------------------------------------------------------------
Cell index: 16
Input Cell Type: markdown
Input Text:
-----------
We see that the resulting signal is now varying around zero and the bottom plot is much more periodic. We've essentially removed the slow changing signal that the fast changing signal was riding on top of, and extracted the fast changing signal only.

# Show the Auto-Correlation of the Low Pass Filtered Signal

Output Text:
------------


------------------------------------------------------------
Cell index: 17
Input Cell Type: python
Input Text:
-----------
#%% autocorr of low pass filtered singal
fig, ax = plt.subplots(nrows=4,ncols=1,figsize=(14,14))

timeLags = np.arange(1,25*24*30)
autoCorr = [lowPassFilteredSignal.autocorr(lag=dt) for dt in timeLags]
ax[0].plot(1.0/(24*30)*timeLags, autoCorr); 
ax[0].set_title('Autocorrelation Plot of Low Pass Filtered Signal', fontsize=20);
ax[0].set_xlabel('time lag [months]'); ax[0].set_ylabel('correlation coeff', fontsize=12);

timeLags = np.arange(1,20*24*7)
autoCorr = [lowPassFilteredSignal.autocorr(lag=dt) for dt in timeLags]
ax[1].plot(1.0/(24*7)*timeLags, autoCorr);
ax[1].set_xlabel('time lag [weeks]'); ax[1].set_ylabel('correlation coeff', fontsize=12);

timeLags = np.arange(1,20*24)
autoCorr = [lowPassFilteredSignal.autocorr(lag=dt) for dt in timeLags]
ax[2].plot(1.0/24*timeLags, autoCorr);
ax[2].set_xlabel('time lag [days]'); ax[2].set_ylabel('correlation coeff', fontsize=12);

timeLags = np.arange(1,3*24)
autoCorr = [lowPassFilteredSignal.autocorr(lag=dt) for dt in timeLags]
ax[3].plot(timeLags, autoCorr);
ax[3].set_xlabel('time lag [hours]'); ax[3].set_ylabel('correlation coeff', fontsize=12);

Output Text:
------------
<matplotlib.figure.Figure at 0x7f225c879f98>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a figure containing four subplots, each displaying an autocorrelation plot of a low-pass filtered signal.  The plots differ in their time scales, showing the autocorrelation at different granularities: months, weeks, days, and hours.  The x-axis of each subplot represents the time lag, while the y-axis represents the correlation coefficient.

The top subplot shows a cyclical autocorrelation pattern over a time lag of approximately 25 months, suggesting a strong periodic component in the original signal at a roughly yearly frequency.  The subsequent subplots show a decaying autocorrelation function as the time lag increases, indicating that the correlation between the signal at a given time and the signal at a later time decreases with the increase in the time lag. The decay is relatively quick in the daily and hourly plots, implying that the signal's memory is relatively short in these time scales. The weekly plot shows a slower decay than the daily and hourly plots but faster decay than the monthly plot.  The overall structure suggests that the signal exhibits strong short-term correlation and a weaker, longer-term periodicity.


------------------------------------------------------------
Cell index: 18
Input Cell Type: markdown
Input Text:
-----------
We see that the low pass signal displays now only the yearly periodicity, because the yearly periodicity is related to slow changes in the signal and we've remove the high changing signals by applying the moving average operation

# Show the Auto-Correlation of the High Pass Filtered Signal

Output Text:
------------


------------------------------------------------------------
Cell index: 19
Input Cell Type: python
Input Text:
-----------
#%% autocorr of high pass filtered signal
fig, ax = plt.subplots(nrows=4,ncols=1, figsize=(14,14))

timeLags = np.arange(1,25*24*30)
autoCorr = [highPassFilteredSignal.autocorr(lag=dt) for dt in timeLags]
ax[0].plot(1.0/(24*30)*timeLags, autoCorr); 
ax[0].set_title('Autocorrelation Plot of High Pass Filtered Signal', fontsize=20);
ax[0].set_xlabel('time lag [months]'); ax[0].set_ylabel('correlation coeff', fontsize=12);

timeLags = np.arange(1,20*24*7)
autoCorr = [highPassFilteredSignal.autocorr(lag=dt) for dt in timeLags]
ax[1].plot(1.0/(24*7)*timeLags, autoCorr);
ax[1].set_xlabel('time lag [weeks]'); ax[1].set_ylabel('correlation coeff', fontsize=12);

timeLags = np.arange(1,20*24)
autoCorr = [highPassFilteredSignal.autocorr(lag=dt) for dt in timeLags]
ax[2].plot(1.0/24*timeLags, autoCorr);
ax[2].set_xlabel('time lag [days]'); ax[2].set_ylabel('correlation coeff', fontsize=12);

timeLags = np.arange(1,3*24)
autoCorr = [highPassFilteredSignal.autocorr(lag=dt) for dt in timeLags]
ax[3].plot(timeLags, autoCorr);
ax[3].set_xlabel('time lag [hours]'); ax[3].set_ylabel('correlation coeff', fontsize=12);

Output Text:
------------
<matplotlib.figure.Figure at 0x7f225c887320>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image displays a four-panel plot showcasing the autocorrelation of a high-pass filtered signal at different time scales.  Each panel presents a graph illustrating the correlation coefficient against a time lag, with the time lag units changing across panels: months, weeks, days, and hours respectively.  The title clearly indicates that this analysis pertains to a signal that has undergone high-pass filtering, which removes low-frequency components.

The first panel depicts the autocorrelation at a monthly time scale, showing a relatively flat line near zero correlation. This suggests that there is very little correlation between the signal at different months. The subsequent panels progressively zoom in on finer time scales. The second panel (weekly) and third panel (daily) reveal clear periodic patterns, indicating significant cyclical behavior in the signal at these time scales.  The fourth panel (hourly) shows a smooth, single-cycle wave, further emphasizing the oscillatory nature of the data.  These cyclical patterns were likely present in the original signal, but they are only clearly visible after the high-pass filtering removes the lower-frequency, longer-term trends.


------------------------------------------------------------
Cell index: 20
Input Cell Type: markdown
Input Text:
-----------
We see that the high pass signal displays now only the daily periodicity, because the daily periodicity is related to fast changes in the signal and we've remove the low changing signals by subtracting the moving average.

Output Text:
------------


------------------------------------------------------------

================================================================================
================================================================================
generating-sentences-one-letter-at-a-time.ipynb:
================================================
------------------------------------------------------------
Cell index: 1
Input Cell Type: markdown
Input Text:
-----------
# Generating Sentences One Letter at a Time
In this script we will create several character based language models using very simple conditional probability distributions (CPDs) by calculating histograms from the data (these are simple generative models that we will sample sentences from). We will also build basic logistic regression classfiers over bag of character and/or bag of word features (these are simple discriminative models that we will use to identify authors).  

## Generative Part
In the main part of the script, we will use **Markov Models** that remember either 0, 1, 2, 3 or 4 previous characters and emit a probability distrubiton over the next character in the sequence.  
Markov Models are perhaps the simplest and most streight forward way to model temporal sequences of discrete symbols (in our case, we will use characters as our discrete symbols).  

We will first see how discriminative these generative models can be for the task of author identification (spoiler: dispite their simplicity they are surprisingly good).  

We then continue to generate text in the style of the authors using those models to see what they have learned (another spoiler: they learn quite a bit, but not quite at the "famous author" level yet).   

In the end we will create a submission using our best markov model for the use of everyone who wishes to tweak it.

## Discriminative Part

In the very last part of the script, we employ a fully discriminative approach that is either character based or word based bag of word features:
1. Extract **Bag of Character n-grams** features
1. Create a submission for **Logistic Regression over *BagOfChar***
1. Extract **Bag of Word n-grams** features
1. Create a submission for **Logistic Regression over *BagOfWord***
1. Create a submission for **Logistic Regression over both *BagOfWord and BagOfChar***


**Note: ** I use only pandas, numpy and native python here to make it simpler to "get into" the code for those of you who wish to do so (only in the last discriminative part I use sklearn as well)

![nice animation](https://cdn-images-1.medium.com/max/1600/1*MbHRwYNA8F29hzes8EPHiQ.gif)

Check out this [blog post](https://hackernoon.com/from-what-is-a-markov-model-to-here-is-how-markov-models-work-1ac5f4629b71) from which I stole this nice animation above. It explains the basics of markov models and even contains code to apply markov models at the word level as basic discrete symbols. In this script, however, we will use characters as the basic discrete symbols. 

Output Text:
------------


------------------------------------------------------------
Cell index: 2
Input Cell Type: markdown
Input Text:
-----------
# Short Math Introduction
In this script we try to model the distribution of sentences $$P(Sentence)$$  
A sentence is just a sequence of $n$ characters and therrefore we can write $$P(Sentence) = P(c_1,c_2,c_3,...,c_n)$$  

**In this script** we will model this sequence of characters using short term memory conditional distributions.  
* **no memory** (this is also known as naive bayes): 
$$P(c_1,c_2,c_3,...,c_n) = P(c_1)P(c_2)P(c_3)...P(c_n) = \prod_{t=1}^{n}P(c_t)$$ 
* **1 time step memory** (this is the classical markov chain): 
$$P(c_1,c_2,c_3,...,c_n) = P(c_1)P(c_2|c_1)P(c_3|c_2)...P(c_n|c_{n-1}) = P(c_1)\prod_{t=2}^{n}P(c_t|c_{t-1})$$ 
* **2 time step memory**: 
$$P(c_1,c_2,c_3,...,c_n) = P(c_1)P(c_2|c_1)\prod_{t=3}^{n}P(c_t|c_{t-1},c_{t-2})$$ 
* **3 time step memory**: 
$$P(c_1,c_2,c_3,...,c_n) = P(c_1)P(c_2|c_1)P(c_3|c_2,c_1)\prod_{t=4}^{n}P(c_t|c_{t-1},c_{t-2},c_{t-3})$$ 
* **4 time step memory**: 
$$P(c_1,c_2,c_3,...,c_n) = P(c_1)P(c_2|c_1)P(c_3|c_2,c_1)P(c_4|c_3,c_2,c_1)\prod_{t=5}^{n}P(c_t|c_{t-1},c_{t-2},c_{t-3},c_{t-4})$$ 

## Note: 
In this corpus, the authors use 34 total characters (regular characters plus punctuation marks).  
What this means is that:  
Storing the $P(c_t)$ distribution will involve storing $34$ numbers.  
Storing the $P(c_t|c_{t-1})$ distribution will involve storing $34^2 = 1,156$ numbers.  
Storing the $P(c_t|c_{t-1},c_{t-2})$ distribution will involve storing $34^3 = 39,304$ numbers.  
Storing the $P(c_t|c_{t-1},c_{t-2},c_{t-3})$ distribution will involve storing $34^4 = 1,336,336$ numbers.  
Storing the $P(c_t|c_{t-1},c_{t-2},c_{t-3},c_{t-4})$ distribution will involve storing $34^5 = 45,435,424$ numbers.  

Keep in mind also that the entire corpus of this competition contains about ~3M characters so that we should start encountering substantial finite sample size effects for 3 and 4 history conditional probability distributions.

Output Text:
------------


------------------------------------------------------------
Cell index: 3
Input Cell Type: python
Input Text:
-----------
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
from sklearn import model_selection, preprocessing, linear_model
from sklearn.metrics import log_loss, accuracy_score
from sklearn.feature_extraction.text import CountVectorizer

matplotlib.style.use('fivethirtyeight')

Output Text:
------------


------------------------------------------------------------
Cell index: 4
Input Cell Type: markdown
Input Text:
-----------
## Load training data and seperate it into a train and validation sets

Output Text:
------------


------------------------------------------------------------
Cell index: 5
Input Cell Type: python
Input Text:
-----------
#%% load and organize data
data = pd.read_csv('../input/train.csv')

stratifiedCV = model_selection.StratifiedShuffleSplit(n_splits=1, test_size=0.10, random_state=1)
trainInds, validInds = next(stratifiedCV.split(data['text'], data['author']))

trainText  = data.loc[trainInds,'text'].reset_index(drop=True)
validText  = data.loc[validInds,'text'].reset_index(drop=True)
trainLabel = data.loc[trainInds,'author'].reset_index(drop=True)
validLabel = data.loc[validInds,'author'].reset_index(drop=True)

Output Text:
------------


------------------------------------------------------------
Cell index: 6
Input Cell Type: markdown
Input Text:
-----------
## Collect all chars into one large string for each author

Output Text:
------------


------------------------------------------------------------
Cell index: 7
Input Cell Type: python
Input Text:
-----------
#%% some utility code
# dictionary to manually converts greek/spanish chars into closest english chars
toEnglishDict = {}
srcStr = ['à','â','ä','å','æ','ç','è','é','ê','ë','ï','î','ñ','ô','ö','õ','ü','û','α','δ','ν','ο','π','ς','υ','ἶ']
dstStr = ['a','a','a','a','a','c','e','e','e','e','i','i','n','o','o','o','u','u','a','d','n','o','p','s','y','i']
for src,dst in zip(srcStr,dstStr):
    toEnglishDict[src] = dst
    
# function that converts all non english chars to their closest english char counterparts
def myunidecode(inString):
    outString = ''
    for ch in inString:
        if ch in toEnglishDict.keys():
            outString += toEnglishDict[ch]
        else:
            outString += ch
    return outString

Output Text:
------------


------------------------------------------------------------
Cell index: 8
Input Cell Type: python
Input Text:
-----------
#%% go over all train data and collect one large char sequence for each author
charsDict = {}
for key in ['all','EAP','HPL','MWS']:
    charsDict[key] = []

for k, (sentence, author) in enumerate(zip(trainText,trainLabel)):
    # the decoding is done for spanish/greek chars to be converted to close english chars
    decodedSentence = myunidecode(sentence.lower())
    chars = [char for char in decodedSentence]
    
    charsDict['all']  += chars
    charsDict[author] += chars

Output Text:
------------


------------------------------------------------------------
Cell index: 9
Input Cell Type: markdown
Input Text:
-----------
# Show the Char usage Distribution for each Author
$$P(c_t|Author)$$

Output Text:
------------


------------------------------------------------------------
Cell index: 10
Input Cell Type: python
Input Text:
-----------
#%% show char usage histogram for the three authors
charEncoder = preprocessing.LabelEncoder()
charEncoder.fit(charsDict['all'])

charCounts_EAP = np.histogram(charEncoder.transform(charsDict['EAP']),range(len(charEncoder.classes_)+1),density=True)[0]
charCounts_HPL = np.histogram(charEncoder.transform(charsDict['HPL']),range(len(charEncoder.classes_)+1),density=True)[0]
charCounts_MWS = np.histogram(charEncoder.transform(charsDict['MWS']),range(len(charEncoder.classes_)+1),density=True)[0]

# sort the char classes by their usage frequency
sortedChars = np.flipud(np.argsort(charCounts_EAP + charCounts_HPL + charCounts_MWS))

Output Text:
------------


------------------------------------------------------------
Cell index: 11
Input Cell Type: python
Input Text:
-----------
barWidth = 0.21
x = np.arange(len(charCounts_EAP))

plt.figure(figsize=(12,7)); plt.title('Character Usage Frequncy - $P(C_t)$ ',fontsize=25);
plt.bar(x-barWidth, charCounts_EAP[sortedChars], barWidth, color='r', label='Edgar Allen Poe');
plt.bar(x         , charCounts_HPL[sortedChars], barWidth, color='g', label='Howard Phillips Lovecraft');
plt.bar(x+barWidth, charCounts_MWS[sortedChars], barWidth, color='b', label='Mary Wollstonecraft Shelley');
plt.legend(fontsize=24); plt.ylabel('Usage Frequncy - $P(C_t)$', fontsize=20); plt.xlabel('$C_t$');
plt.xticks(x,["'%s'" %(charEncoder.classes_[i]) for i in sortedChars], fontsize=13);

Output Text:
------------
<matplotlib.figure.Figure at 0x7f4d49fda0f0>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a bar chart visualizing the character usage frequency in the works of three authors: Edgar Allan Poe, Howard Phillips Lovecraft, and Mary Wollstonecraft Shelley.  The chart's horizontal axis (x-axis) represents individual characters, while the vertical axis (y-axis) displays the probability or frequency of each character's appearance in the respective author's text.  Each author's data is represented by a different color: red for Poe, green for Lovecraft, and blue for Shelley.

The chart clearly shows that the most frequent characters across all three authors are 'e', 't', 'a', and 'o', although their exact frequencies differ slightly. The frequency of character usage decreases significantly as one moves down the x-axis, with less common characters such as 'q', 'j', 'z', and punctuation marks appearing with much lower probabilities.  The chart allows for a direct comparison of character usage patterns between the three authors, highlighting similarities and differences in their writing styles.  The title and axis labels are clear and descriptive, making the chart easy to interpret.


------------------------------------------------------------
Cell index: 12
Input Cell Type: markdown
Input Text:
-----------
Interestingly, there are differences between the authors here!  
## Lets look at the same plot only with log scale on the y axis

Output Text:
------------


------------------------------------------------------------
Cell index: 13
Input Cell Type: python
Input Text:
-----------
barWidth = 0.21
x = np.arange(len(charCounts_EAP))

plt.figure(figsize=(12,7)); 
plt.title('Character Usage Frequncy - $P(C_t)$ ',fontsize=25);
plt.bar(x-barWidth, charCounts_EAP[sortedChars], barWidth, color='r', label='Edgar Allen Poe');
plt.bar(x         , charCounts_HPL[sortedChars], barWidth, color='g', label='Howard Phillips Lovecraft');
plt.bar(x+barWidth, charCounts_MWS[sortedChars], barWidth, color='b', label='Mary Wollstonecraft Shelley');
plt.legend(fontsize=21); plt.ylabel('Usage Frequncy - $P(C_t)$', fontsize=20); 
plt.yscale("log", nonposy='clip'); plt.xlabel('$C_t$');
plt.xticks(x,["'%s'" %(charEncoder.classes_[i]) for i in sortedChars], fontsize=11);

Output Text:
------------
<matplotlib.figure.Figure at 0x7f4d49493a20>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a bar chart displaying the character usage frequency for three authors: Edgar Allan Poe, Howard Phillips Lovecraft, and Mary Wollstonecraft Shelley.  The chart is designed to compare the relative frequencies of different characters in the writings of each author.

The horizontal axis (x-axis) represents the individual characters (C<sub>t</sub>), ordered from most to least frequent across all three authors. The vertical axis (y-axis) shows the usage frequency (P(C<sub>t</sub>)) of each character, presented on a logarithmic scale (base 10). This logarithmic scale allows for a clearer visualization of the wide range of character frequencies, from very common characters to those used much less often.  The scale ranges from approximately 10<sup>-1</sup> (one tenth or 10%) down to 10<sup>-4</sup> (one ten-thousandth or 0.01%).

Each character is represented by three bars, one for each author, colored red for Poe, green for Lovecraft, and blue for Shelley.  The height of each bar indicates the frequency of that particular character in the author's works. By comparing the heights of the bars for each character across the three authors, one can observe differences in their writing styles and character preferences.  For instance, the chart shows that the characters 'e' and 't' are highly frequent in all three authors' texts.


------------------------------------------------------------
Cell index: 14
Input Cell Type: markdown
Input Text:
-----------
Very large differences can be seen in usage of punctuation marks.  
For example, the ":" character is used much more extensivley by MWS and least used by HPL.

Even though these differences look small, they are extreemly statistically significant since there are about 2,600,000 chars in the training set. These differences are not there by chance, and perhaps we can utilize these differences for classification. Lets try.

Output Text:
------------


------------------------------------------------------------
Cell index: 15
Input Cell Type: markdown
Input Text:
-----------
# How do we Identify the Author of each Sentence using our CPDs?
In this competition specifically, we would like to model the conditional probability distribution of the author given a sentence: $$P(Author | Sentence) = P(Author | c_1,c_2,c_3,...,c_n)$$  

We have three different authors here, so this boils down to:  

$$P(EAP | c_1,c_2,c_3,...,c_n)$$    
$$P(HPC | c_1,c_2,c_3,...,c_n)$$    
$$P(MWS | c_1,c_2,c_3,...,c_n)$$  

Using the bayes rule we can invert this into $$P(Author | Sentence) = \frac{P(Sentence | Author)P(Author)}{P(Sentence)}$$  
The prior distribution $P(Author)$ over authors is extreemly simple to calcualte (it's just the frequency of occurence of each author in the training set).  

For any specific sentence, the probability of that sentence $P(Sentence)$ is identical for all authors and therefore doesn't create any additional discrimination between them, so when creating a classifier we can just ignore this and compare the 3 following quantities:  

$$P(c_1,c_2,c_3,...,c_n | EAP)P(EAP)$$    
$$P(c_1,c_2,c_3,...,c_n | HPC)P(HPC)$$    
$$P(c_1,c_2,c_3,...,c_n | MWS)P(MWS)$$  
meaning:
$$  \mathbf{predicted\:Author} = argmax\:\{{P(c_1,c_2,c_3,...,c_n|Author)P(Author)}\}   $$

Output Text:
------------


------------------------------------------------------------
Cell index: 16
Input Cell Type: markdown
Input Text:
-----------
# Calculate Classification Accuracy based only on single char distribution
$$  \mathbf{predicted\:Author} = argmax\:\{ \prod_{t=1}^{n}P(c_t|Author)\}   $$  

**Note:** we assume here an equal prior just for simplicity (i.e. $P(Author) = \frac{1}{3}$ for all authors)  


Output Text:
------------


------------------------------------------------------------
Cell index: 17
Input Cell Type: python
Input Text:
-----------
#%% meassure classification accuracy on validation set using only character frequncy
authorsList = ['EAP','HPL','MWS']
authorPredictionList = []
for k, (sentence, author) in enumerate(zip(validText,validLabel)):
    chars = [char for char in myunidecode(sentence.lower())]
    # convert to log so we can sum probabilities instead of multiply
    logP_EAP = sum([np.log(charCounts_EAP[charEncoder.classes_ == ch]) for ch in chars])
    logP_HPL = sum([np.log(charCounts_HPL[charEncoder.classes_ == ch]) for ch in chars])
    logP_MWS = sum([np.log(charCounts_MWS[charEncoder.classes_ == ch]) for ch in chars])
    
    authorPredictionList.append(authorsList[np.argmax(np.array([logP_EAP,logP_HPL,logP_MWS]))])

print(52*'-')
print('==> Validation Set Classification Accuracy = %.1f%s' %(100*(validLabel == authorPredictionList).mean(),'%'))
print(52*'-')

Output Text:
------------
----------------------------------------------------
==> Validation Set Classification Accuracy = 53.0%
----------------------------------------------------


------------------------------------------------------------
Cell index: 18
Input Cell Type: markdown
Input Text:
-----------
Interestingly, even though this is perhaps the stupidest model one can think of, the discrimination accuracy is well above chance level.

Output Text:
------------


------------------------------------------------------------
Cell index: 19
Input Cell Type: markdown
Input Text:
-----------
## Generate Sample Text for each author using the independent chars model
$$  c_t \: {\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}} \: P(c_t|Author)   $$

Output Text:
------------


------------------------------------------------------------
Cell index: 20
Input Cell Type: python
Input Text:
-----------
#%% generate sample text by sampling one charachter at a time for the independent character model
np.random.seed(1234)

maxSentenceLength = 95
numSentencesPerAuthor = 5

charProbModel = {}
charProbModel['all'] = (charCounts_EAP + charCounts_HPL + charCounts_MWS)/3.0
charProbModel['EAP'] = charCounts_EAP
charProbModel['HPL'] = charCounts_HPL
charProbModel['MWS'] = charCounts_MWS

for author in ['EAP','HPL','MWS','all']:
    print((6+maxSentenceLength)*'-')
    print('Author %s:' %(author))
    print(12*'-')
    for i in range(numSentencesPerAuthor):
        generatedSentence = ''
        for j in range(maxSentenceLength):
            newChar = charEncoder.classes_[np.random.choice(range(len(charCounts_EAP)),size=1,p=charProbModel[author])][0]
            generatedSentence += newChar
            
            if (newChar == '.') or (j == maxSentenceLength):
                break
                
        print('%d: "%s"' %(i+1,generatedSentence))
print((4+maxSentenceLength)*'-')

Output Text:
------------
-----------------------------------------------------------------------------------------------------
Author EAP:
------------
1: "."
2: "mfrrbcrvtehnoeih rtem etnerditfr ooatft , nli idh li mtryvrcmh."
3: "e fw  olh atei dfmtmo osnf iiuhhis nroriv  l ud."
4: "gttae,tootrlc en  dlsc" y,di twi eo ibwna rrptniaoutysiftwledtaoerromhsof eohen  o el ruwtmtoo "
5: "fce phnmefehxe ttemeaae npteceste   l c m sdpi in,rmimnarsoohsneie ksun e "i"ihtotcaotxndlsm cy"
-----------------------------------------------------------------------------------------------------
Author HPL:
------------
1: "ekia hyeo,   cirtnsloit r    hez dyetiutr viata owif lvwt."
2: "mee tn trdsh ffasolhhblpbi otiy, gtshtadoo clte hodstrbuuotahruntn seuh v n  nwope aq h,miisuao"
3: "oatiideewank ia t wde  em w,phrd go nelabts ntoh  s it nh   a e ttr."
4: "ltw donmsoe qe ot ,ee  f st afwr tse   t enlee eorat ta dsseaic mi pyreeeflm,to s n ae frnii ly"
5: "alsrdevaa hbi egroa ae hstay er fesoe oeskutg  dllh toes  ht i   tf nheo"ebd,hm acodeo fmtesnoo"
-----------------------------------------------------------------------------------------------------
Author MWS:
------------
1: "wa a osughb r rthteanemeluit  mirsulcselmf;ftn i zyoeetanh  tttddnlym ss t ttsndiat,epdmatnhas "
2: "aalmx'r ev crw let , d."
3: "   oatdtaeiheeeefentieat kooe rioleuyio tydsmfiragsne etnei ymrtestnltfasoe ul hldghltsrimoiryr"
4: "m hc r  nifd db retrwvldt aeeuk,lu x snearsie  reedoyit toetai ee  eeyu hhethrtrg gaa,ete,p   g"
5: "l,hotiwi hi n fx    smr wtmtehncnhraofet  eiud us  s ihd utonewohd wc,apuhne ne hia fh sa a soa"
-----------------------------------------------------------------------------------------------------
Author all:
------------
1: " eoo."
2: "ovhe hn n ese   hn sr dehfg e ftraodh lcc tfnrcceettf  ;ast ireicn sr."
3: "nshl gtgg; mi o,ehnmpchnhrdiafr ariswwui oiog ytreadmar honeoearoo ael asprdracsoeio  tmt t vap"
4: "eeah, eoht ot wpt csc   rep  re enfafo hsymc  ehi   hbfwrci?r safe t t "sdstttsasmylnni,w oicah"
5: "tjyeemt i o  i i  srdef mraehhe n ."
---------------------------------------------------------------------------------------------------


------------------------------------------------------------
Cell index: 21
Input Cell Type: markdown
Input Text:
-----------
Well, as expected, we can see that this really doesn't resemble any human generated text.  
But perhaps we can do better with some memory?

Output Text:
------------


------------------------------------------------------------
Cell index: 22
Input Cell Type: markdown
Input Text:
-----------
## Gather DataFrame with "Author", "History" and "Next Char" Fields
Use history of 1 character 

Output Text:
------------


------------------------------------------------------------
Cell index: 23
Input Cell Type: python
Input Text:
-----------
#%% gather all pairs of characters into a single dataframe
historyLength = 1

historyList  = []
nextCharList = []
authorList   = []
for k, (sentence, author) in enumerate(zip(trainText,trainLabel)):
    decodedSentence = myunidecode(sentence.lower())
    charSequences = [decodedSentence[k:k+historyLength+1] for k in range(len(decodedSentence)-historyLength)]
    
    history  = [seq[:-1] for seq in charSequences]
    nextChar = [seq[ -1] for seq in charSequences]
    sentenceDF = pd.DataFrame(columns=['author','history','next char'])
    
    historyList  += history
    nextCharList += nextChar
    authorList   += [author]*len(history)
        
corpusDF = pd.DataFrame(columns=['author','history','next char'])
corpusDF['author']    = authorList
corpusDF['history']   = historyList
corpusDF['next char'] = nextCharList

corpusDF.head(8)

Output Text:
------------
  author history next char
0    EAP       t         h
1    EAP       h         e
2    EAP       e          
3    EAP                 w
4    EAP       w         e
5    EAP       e         a
6    EAP       a         t
7    EAP       t         h


------------------------------------------------------------
Cell index: 24
Input Cell Type: markdown
Input Text:
-----------
## Build Markov Model that Remebers only the previous char
$$P(c_t|c_{t-1},Author)$$

Output Text:
------------


------------------------------------------------------------
Cell index: 25
Input Cell Type: python
Input Text:
-----------
#%% generate P(c(t)|c(t-1)) model (Markov Model with memory of 1 time step)
charCondProbModel_H1 = {}
for author in ['EAP','HPL','MWS']:
    charCondProbModel_H1[author] = np.zeros( (1+historyLength)*[charEncoder.classes_.shape[0]] )
    
charCondCountModel_H1 = {}
for author in ['EAP','HPL','MWS']:
    charCondCountModel_H1[author] = np.zeros( (1+historyLength)*[charEncoder.classes_.shape[0]] )

corpusGroupedByAuthor = corpusDF.groupby(by='author',axis=0)
for author in corpusDF['author'].unique():
    authorCorpusDF = corpusGroupedByAuthor.get_group(author).loc[:,['history','next char']].reset_index(drop=True)
    authorCorpusGroupedByHistory = authorCorpusDF.groupby(by='history',axis=0)
    for history in authorCorpusDF['history'].unique():
        authorHistoryDF = authorCorpusGroupedByHistory.get_group(history).reset_index(drop=True).loc[:,'next char'].reset_index(drop=True)

        encodedHistory = charEncoder.transform([history])[0]
        encodedNextCharCounts = np.histogram(charEncoder.transform(authorHistoryDF),range(len(charEncoder.classes_)+1),density=False)[0]
        encodedNextCharProb   = np.histogram(charEncoder.transform(authorHistoryDF),range(len(charEncoder.classes_)+1),density=True)[0]

        charCondProbModel_H1[author][encodedHistory,:]  = encodedNextCharProb
        charCondCountModel_H1[author][encodedHistory,:] = encodedNextCharCounts

    condCount = charCondCountModel_H1[author]
    print('%s Sparsity level = %.1f%s' %(author, 100*(condCount < 1).sum() / (condCount > -1).sum().astype(float),'%'))

charCondProbModel_H1['all']  = (charCondProbModel_H1['EAP']  + charCondProbModel_H1['HPL']  + charCondProbModel_H1['MWS'] )/3.0
charCondCountModel_H1['all'] =  charCondCountModel_H1['EAP'] + charCondCountModel_H1['HPL'] + charCondCountModel_H1['MWS']

print('average Sparsity level = %.1f%s' %(100*(charCondCountModel_H1['all'] < 1).sum() / (condCount > -1).sum().astype(float),'%'))

Output Text:
------------
EAP Sparsity level = 30.4%
MWS Sparsity level = 38.6%
HPL Sparsity level = 33.9%
average Sparsity level = 25.1%


------------------------------------------------------------
Cell index: 26
Input Cell Type: markdown
Input Text:
-----------
# Show the Conditional Probability Distribution of the entire corpus 
$$P(c_t|c_{t-1}) $$

Output Text:
------------


------------------------------------------------------------
Cell index: 27
Input Cell Type: python
Input Text:
-----------
condProb = charCondProbModel_H1['all']

plt.figure(figsize=(12,10))
plt.imshow(condProb, cmap='hot');  plt.colorbar(); plt.clim(0,1);
plt.grid('off'); plt.title('P(next char | prev char) for all Authors - $P(c_t|c_{t-1})$', fontsize=22);
plt.xlabel('$c_t$ - next character', fontsize=18); plt.ylabel('$c_{t-1}$ - previous character', fontsize=18);
plt.xticks(range(condProb.shape[0]),["'%s'" %(ch) for ch in charEncoder.classes_]);
plt.yticks(range(condProb.shape[0]),["'%s'" %(ch) for ch in charEncoder.classes_]);

Output Text:
------------
<matplotlib.figure.Figure at 0x7f4d492e2208>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a heatmap visualizing the conditional probability of character transitions in text written by multiple authors.  The heatmap is a square matrix, where both the rows and columns represent characters.  The rows represent the "previous character" (Ct-1), and the columns represent the "next character" (Ct).  Each cell's color intensity indicates the probability P(Ct | Ct-1), the likelihood of a particular character (Ct) following another character (Ct-1).

The color scale ranges from dark red (low probability, close to 0) to bright yellow (high probability, close to 1).  The characters included seem to encompass lowercase English alphabet letters, punctuation marks, and possibly some other symbols.  The darker areas in the heatmap suggest infrequent character transitions, while brighter areas highlight common character sequences. The title of the heatmap clearly states its purpose: to show the probability of the next character given the previous one, aggregated across all authors.

The graphic is well-labeled, with clear axes indicating the previous and next characters, a colorbar for interpreting the probability values, and a title explaining the data representation. This visualization allows for quick identification of common character bigrams (pairs of consecutive characters) and their relative frequencies in the corpus of text analyzed.  For example, a bright yellow square at the intersection of 'e' (row) and ' ' (column) would indicate a high probability of a space following the letter 'e'.


------------------------------------------------------------
Cell index: 28
Input Cell Type: markdown
Input Text:
-----------
There are a few clear "columns" in the dataset - for the "a","e","i" and "o" characters (which are vowels, by the way). What this means is that these characters are quite likely to come after many other characters. Contrast that with "y" that is likely to occur mostly after "l", "m" and "b" (to form the pairs "ly", "my" and "by").

Note that we now have about 25% of character pairs that never occur in the training set at all. Even though we only have ~1,150 possible charachter pairs and ~2,600,000 pairs.  
In this particular case, it's safe to assume that the pairs that don't occur are simply very rare or non existent in the languge, but it's important to keep in mind that some of these zeros are perhaps due to finite sample size. This can be quanitfied of course, but we will skip this in this tutorial.

Output Text:
------------


------------------------------------------------------------
Cell index: 29
Input Cell Type: markdown
Input Text:
-----------
# Show the Author Specific Conditional Probability Distributions 
$$P(c_t|c_{t-1},Author) $$

Output Text:
------------


------------------------------------------------------------
Cell index: 30
Input Cell Type: python
Input Text:
-----------
shortToFullNameDict = {}
shortToFullNameDict['EAP'] = 'Edgar Allen Poe'
shortToFullNameDict['HPL'] = 'Howard Phillips Lovecraft'
shortToFullNameDict['MWS'] = 'Mary Wollstonecraft Shelley'

plt.figure(figsize=(13,28))
for k, author in enumerate(['EAP','HPL','MWS']):
    condProb = charCondProbModel_H1[author]
    plt.subplot(3,1,k+1); plt.imshow(condProb, cmap='hot'); 
    plt.grid('off'); plt.colorbar(); plt.clim(0,1);
    plt.title('P(next char | prev char, %s) - $P(c_t|c_{t-1},Author)$' %(shortToFullNameDict[author]), fontsize=17);
    plt.xlabel('$c_t$ - next character', fontsize=15); plt.ylabel('$c_{t-1}$ - previous character', fontsize=15);
    plt.xticks(range(condProb.shape[0]),["'%s'" %(ch) for ch in charEncoder.classes_]);
    plt.yticks(range(condProb.shape[0]),["'%s'" %(ch) for ch in charEncoder.classes_]);
plt.tight_layout();

Output Text:
------------
<matplotlib.figure.Figure at 0x7f4d49170c88>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image presents three heatmaps, arranged vertically, each visualizing the conditional probability of the next character given the previous character in text samples from three different authors: Edgar Allan Poe, Howard Phillips Lovecraft, and Mary Wollstonecraft Shelley.  Each heatmap is a square matrix where rows represent the previous character (Ct-1) and columns represent the next character (Ct). The color intensity of each cell corresponds to the probability P(Ct | Ct-1), indicating how likely a particular character is to follow another.  Darker colors signify lower probabilities, while brighter colors (yellow/white) represent higher probabilities.

The color scale is consistent across all three heatmaps, ranging from dark red (low probability, near 0) to bright yellow (high probability, near 1).  The characters represented along the axes include uppercase and lowercase letters, punctuation marks, and spaces. The title above each heatmap clearly indicates the author and the type of probability being represented.

The overall purpose of this image is to compare the character transition probabilities in the writing styles of these three authors. By visually inspecting the heatmaps, one can observe patterns and differences in their writing styles, for example, the frequency of certain character combinations or the tendency to use particular punctuation marks.  The brighter spots highlight frequent character transitions which are specific to each author's style.


------------------------------------------------------------
Cell index: 31
Input Cell Type: markdown
Input Text:
-----------
We can see a few differences between the authors, especially in the top few rows that indicate the different usage of punctuation marks by our authors.

Output Text:
------------


------------------------------------------------------------
Cell index: 32
Input Cell Type: markdown
Input Text:
-----------
## Calculate Classification Accuracy of a Classic Markov Model
$$  \mathbf{predicted\:Author} = argmax\:\{ \prod_{t=2}^{n}P(c_t|c_{t-1},Author)\}   $$

Output Text:
------------


------------------------------------------------------------
Cell index: 33
Input Cell Type: python
Input Text:
-----------
#%% meassure classification accuracy on validation set using Markov Model with memory of 1 time step
uniformPriorFraction    = 0.0001
allAuthorsPriorFraction = 0.0001

prior = np.array([1.0-uniformPriorFraction-allAuthorsPriorFraction, allAuthorsPriorFraction, uniformPriorFraction])
uniformPriorValue = 1.0/len(charEncoder.classes_)

condP_H1 = {}
authorsList = ['EAP','HPL','MWS']
for author in authorsList:
    condP_H1[author]  = prior[0]*charCondProbModel_H1[author]
    condP_H1[author] += prior[1]*charCondProbModel_H1['all']
    condP_H1[author] += prior[2]*uniformPriorValue

authorPredictionList = []
for k, (sentence, author) in enumerate(zip(validText,validLabel)):
    decodedSentence = myunidecode(sentence.lower())
    charSequences = [decodedSentence[k:k+historyLength+1] for k in range(len(decodedSentence)-historyLength)]
    
    history  = [seq[:-1] for seq in charSequences]
    nextChar = [seq[ -1] for seq in charSequences]
    
    logP_EAP = 0.0; logP_HPL = 0.0; logP_MWS = 0.0
    for histChar, nextChar in zip(history,nextChar):
        encodedHistChar = charEncoder.transform([histChar])[0]
        encodedNextChar = charEncoder.transform([nextChar])[0]
        
        logP_EAP += np.log(condP_H1['EAP'][encodedHistChar,encodedNextChar])
        logP_HPL += np.log(condP_H1['HPL'][encodedHistChar,encodedNextChar])
        logP_MWS += np.log(condP_H1['MWS'][encodedHistChar,encodedNextChar])
    
    authorPredictionList.append(authorsList[np.argmax([logP_EAP,logP_HPL,logP_MWS])])
    
print(52*'-')
print('==> Validation Set Classification Accuracy = %.1f%s' %(100*(validLabel == authorPredictionList).mean(),'%'))
print(52*'-')

Output Text:
------------
----------------------------------------------------
==> Validation Set Classification Accuracy = 62.1%
----------------------------------------------------


------------------------------------------------------------
Cell index: 34
Input Cell Type: markdown
Input Text:
-----------
The classification accuracy increases some more. By just looking at the distribution of pairs of charachters. 

Output Text:
------------


------------------------------------------------------------
Cell index: 35
Input Cell Type: markdown
Input Text:
-----------
## Generate Sample Text for each Author using our Markov Model
$$  c_t\: {\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}} \:  P(c_t|c_{t-1},Author)   $$

Output Text:
------------


------------------------------------------------------------
Cell index: 36
Input Cell Type: python
Input Text:
-----------
#%% generate sample text by sampling one charachter at a time from the 1 time step memory Markov Model
np.random.seed(123)

maxSentenceLength = 90
numSentencesPerAuthor = 6

uniformPriorFraction    = 0.0001
allAuthorsPriorFraction = 0.0009

prior = np.array([1.0-uniformPriorFraction-allAuthorsPriorFraction, allAuthorsPriorFraction, uniformPriorFraction])
uniformPriorValue = 1.0/(len(charEncoder.classes_))

condP_H1 = {}
authorsList = ['EAP','HPL','MWS']
for author in authorsList:
    condP_H1[author]  = prior[0]*charCondProbModel_H1[author]
    condP_H1[author] += prior[1]*charCondProbModel_H1['all']
    condP_H1[author] += prior[2]*uniformPriorValue

condP_H1['all']  = (prior[0]+prior[1])*charCondProbModel_H1['all']
condP_H1['all'] += prior[2]*uniformPriorValue

for author in ['EAP','HPL','MWS','all']:
    print((6+maxSentenceLength)*'-')
    print('Author %s:' %(author))
    print(12*'-')
    for i in range(numSentencesPerAuthor):
        firstChar = charEncoder.classes_[np.random.choice(range(len(charCounts_EAP)),size=1,p=charProbModel[author])][0]
        generatedSentence = firstChar
        for j in range(maxSentenceLength-1):
            encodedHistChar = charEncoder.transform([generatedSentence[-1]])[0]
            newChar = charEncoder.classes_[np.random.choice(range(len(charCounts_EAP)),size=1,p=condP_H1[author][encodedHistChar,:])][0]
            generatedSentence += newChar
            
            if (newChar == '.') or (j == maxSentenceLength):
                break
        print('%d: "%s"' %(i+1,generatedSentence))
print((4+maxSentenceLength)*'-')

Output Text:
------------
------------------------------------------------------------------------------------------------
Author EAP:
------------
1: "of orexple, i, s cingoris, d pand imesthe hes my o, oncecounghens tafe the on brdegatol ga"
2: "oxcoor igang bensere'e tefee wof mutapreparsubjed, freder st te r rsondeibotespos toud bin"
3: "icerer oupende p se n py."
4: "jeng thered hiecuits lim e ert gupo my anterof, ieves fa y ie d pea he ofi t bt sutitholy "
5: "ust o s aiteeifenandrgthe my andi veshimeas."
6: "autenag."
------------------------------------------------------------------------------------------------
Author HPL:
------------
1: "dlorve alackeg s fooliourasquseraly thanor amy f d dinctes sed aly he bese wilysare ald cu"
2: " pheata vigh."
3: " thet; artheesifod ofomaz, as aror falockyof wnthieat the verste."
4: "lethemey d bere wh re osld ngasi our, t t nd qusese bthe, wesitowisorng mofand ndan arnton"
5: "me h, cht betr wintomind ld wendy cauth deseal, wid rouguinewheer , mbererer me won ced th"
6: "ty outep t swhapor'th s anoudas wablld ous eeringlittrof fler thinghtin ughraiste byese al"
------------------------------------------------------------------------------------------------
Author MWS:
------------
1: "ttod e se."
2: " nd gef tse y sme w malom diven satethenaieverelive aindgn whecourwis ribur on."
3: " he: tir mistedeneag acre ug, wasthmarlesntind and bjexea i amse cotimerepom buny eamm the"
4: "hed led."
5: "ane manole ghe ft l n pend f mend heibomat win iouredim ize af or wad rcoito br ter d ches"
6: "mo fupr acataye yod r o dseige m ar mbeile be nkly blrood s."
------------------------------------------------------------------------------------------------
Author all:
------------
1: " orn hecanenecthtorty st t ve at w"thotonhe d he ar anive ag 'e ampeni was."
2: "ty ththe s tu wedotacroumed herstillyow of tilie ure in bel, washel."
3: "hthathin turouy ke mate atovandig, clmairowan igator fthe mo anta tont f our sk tre, wispt"
4: "s d d wo athan watth oburued areanomyo th wists w thiglo vingan mon ti tond t heses the ma"
5: "gllluagingroverm a nom ctateed hy lighice sthe beng by l ured din cheeestore w oulyof t cl"
6: " oopery teas an apall y nof d sofaror, f ourmam."
----------------------------------------------------------------------------------------------


------------------------------------------------------------
Cell index: 37
Input Cell Type: markdown
Input Text:
-----------
This already has a text like feeling to it. After punctioation we see a whitespace, every now and then we see the letter "a" and "i" seperated by whitespaces and the short word "he" appears several times in the text. we are getting somewhere, let's add a little bit more memory.

Output Text:
------------


------------------------------------------------------------
Cell index: 38
Input Cell Type: markdown
Input Text:
-----------
## Gather DataFrame with "Author", "History" and "Next Char" Fields
Use history of size 2 characters

Output Text:
------------


------------------------------------------------------------
Cell index: 39
Input Cell Type: python
Input Text:
-----------
#%% gather all triplets of characters into a single dataframe
historyLength = 2

historyList  = []
nextCharList = []
authorList   = []
for k, (sentence, author) in enumerate(zip(trainText,trainLabel)):
    decodedSentence = myunidecode(sentence.lower())
    charSequences = [decodedSentence[k:k+historyLength+1] for k in range(len(decodedSentence)-historyLength)]
    
    history  = [seq[:-1] for seq in charSequences]
    nextChar = [seq[ -1] for seq in charSequences]
    sentenceDF = pd.DataFrame(columns=['author','history','next char'])
    
    historyList  += history
    nextCharList += nextChar
    authorList   += [author]*len(history)
        
corpusDF = pd.DataFrame(columns=['author','history','next char'])
corpusDF['author']    = authorList
corpusDF['history']   = historyList
corpusDF['next char'] = nextCharList

corpusDF.head(8)

Output Text:
------------
  author history next char
0    EAP      th         e
1    EAP      he          
2    EAP      e          w
3    EAP       w         e
4    EAP      we         a
5    EAP      ea         t
6    EAP      at         h
7    EAP      th         e


------------------------------------------------------------
Cell index: 40
Input Cell Type: markdown
Input Text:
-----------
## Build Markov Model that remebers the Two previous chars
$$ P(c_t|c_{t-1},c_{t-2},Author)  $$

Output Text:
------------


------------------------------------------------------------
Cell index: 41
Input Cell Type: python
Input Text:
-----------
#%% generate P(c(t)|c(t-1),c(t-2)) model (Markov Model with memory of 2 time steps)
historyLength = 2

charCondProbModel_H2 = {}
for author in ['EAP','HPL','MWS']:
    charCondProbModel_H2[author] = np.zeros( (1+historyLength)*[charEncoder.classes_.shape[0]] )
    
charCondCountModel_H2 = {}
for author in ['EAP','HPL','MWS']:
    charCondCountModel_H2[author] = np.zeros( (1+historyLength)*[charEncoder.classes_.shape[0]] )

corpusGroupedByAuthor = corpusDF.groupby(by='author',axis=0)
for author in corpusDF['author'].unique():
    authorCorpusDF = corpusGroupedByAuthor.get_group(author).loc[:,['history','next char']].reset_index(drop=True)
    authorCorpusGroupedByHistory = authorCorpusDF.groupby(by='history',axis=0)
    for history in authorCorpusDF['history'].unique():
        authorHistoryDF = authorCorpusGroupedByHistory.get_group(history).reset_index(drop=True).loc[:,'next char'].reset_index(drop=True)

        encodedHistory = charEncoder.transform([ch for ch in history])
        encodedNextCharCounts = np.histogram(charEncoder.transform(authorHistoryDF),range(len(charEncoder.classes_)+1),density=False)[0]
        encodedNextCharProb   = np.histogram(charEncoder.transform(authorHistoryDF),range(len(charEncoder.classes_)+1),density=True)[0]

        charCondProbModel_H2[author][encodedHistory[0],encodedHistory[1],:]  = encodedNextCharProb
        charCondCountModel_H2[author][encodedHistory[0],encodedHistory[1],:] = encodedNextCharCounts

    condCount = charCondCountModel_H2[author]
    print('%s Sparsity level = %.1f%s' %(author, 100*(condCount < 1).sum() / (condCount > -1).sum().astype(float),'%'))

charCondProbModel_H2['all']  = (charCondProbModel_H2['EAP']  + charCondProbModel_H2['HPL']  + charCondProbModel_H2['MWS'] )/3.0
charCondCountModel_H2['all'] =  charCondCountModel_H2['EAP'] + charCondCountModel_H2['HPL'] + charCondCountModel_H2['MWS']

condCount = charCondCountModel_H2['all']
print('average Sparsity level = %.1f%s' %(100*(condCount < 1).sum() / (condCount > -1).sum().astype(float),'%'))

Output Text:
------------
EAP Sparsity level = 81.5%
MWS Sparsity level = 85.0%
HPL Sparsity level = 82.8%
average Sparsity level = 78.2%


------------------------------------------------------------
Cell index: 42
Input Cell Type: markdown
Input Text:
-----------
Note the Sparsity Levels increase quite a bit. This is due to the fact that there are 34^3 (~40,000) possible triplets, and most of them are illegal combinations in the english language.  

But remember what we discussed earlier, here the problem of finite sample size is much more pronounced so we can be much less "confident" in these zeros. meaning, we can't be sure they are actually zeros and not simply very rare events that just didn't happen to occur in the particular realization of the training sample.    

For this reason we will add a small constant number to the conditional probability distribution.

Output Text:
------------


------------------------------------------------------------
Cell index: 43
Input Cell Type: markdown
Input Text:
-----------
## Calculate Classification Accuracy of our 2 time step Markov Model
$$  \mathbf{predicted\:Author} = argmax\:\{ \prod_{t=3}^{n}P(c_t|c_{t-1},c_{t-2},Author)\}   $$

Output Text:
------------


------------------------------------------------------------
Cell index: 44
Input Cell Type: python
Input Text:
-----------
#%% meassure classification accuracy on validation set using Markov Model with memory of 2 time steps
uniformPriorFraction    = 0.0001
allAuthorsPriorFraction = 0.0001

prior = np.array([1.0-uniformPriorFraction-allAuthorsPriorFraction, allAuthorsPriorFraction, uniformPriorFraction])
uniformPriorValue = 1.0/len(charEncoder.classes_)

condP_H2 = {}
authorsList = ['EAP','HPL','MWS']
for author in authorsList:
    condP_H2[author]  = prior[0]*charCondProbModel_H2[author]
    condP_H2[author] += prior[1]*charCondProbModel_H2['all']
    condP_H2[author] += prior[2]*uniformPriorValue

authorPredictionList = []
for k, (sentence, author) in enumerate(zip(validText,validLabel)):
    decodedSentence = myunidecode(sentence.lower())
    charSequences = [decodedSentence[k:k+historyLength+1] for k in range(len(decodedSentence)-historyLength)]
    
    history  = [seq[:-1] for seq in charSequences]
    nextChar = [seq[ -1] for seq in charSequences]
    
    logP_EAP = 0.0; logP_HPL = 0.0; logP_MWS = 0.0
    for histChars, nextChar in zip(history,nextChar):
        encodedHistChars = charEncoder.transform([ch for ch in histChars])
        encodedNextChar  = charEncoder.transform([nextChar])[0]
        
        logP_EAP += np.log(condP_H2['EAP'][encodedHistChars[0],encodedHistChars[1],encodedNextChar])
        logP_HPL += np.log(condP_H2['HPL'][encodedHistChars[0],encodedHistChars[1],encodedNextChar])
        logP_MWS += np.log(condP_H2['MWS'][encodedHistChars[0],encodedHistChars[1],encodedNextChar])
    
    authorPredictionList.append(authorsList[np.argmax([logP_EAP,logP_HPL,logP_MWS])])

print(52*'-')
print('==> Validation Set Classification Accuracy = %.1f%s' %(100*(validLabel == authorPredictionList).mean(),'%'))
print(52*'-')

Output Text:
------------
----------------------------------------------------
==> Validation Set Classification Accuracy = 74.8%
----------------------------------------------------


------------------------------------------------------------
Cell index: 45
Input Cell Type: markdown
Input Text:
-----------
The Classification Accuracy keeps rising, which is quite nice.  
We are now quite capable in distinguishing between the three authors.  
***Does this also mean that our model has the ability to write text like our authors?***

Output Text:
------------


------------------------------------------------------------
Cell index: 46
Input Cell Type: markdown
Input Text:
-----------
## Generate Sample Text for each Author using our 2 time step Markov Model
$$  c_t\: {\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}} \:  P(c_t|c_{t-1},c_{t-2},Author)   $$

Output Text:
------------


------------------------------------------------------------
Cell index: 47
Input Cell Type: python
Input Text:
-----------
#%% generate sample text by sampling one charachter at a time from the 2 time step Markov Model
np.random.seed(1000)

maxSentenceLength = 95
numSentencesPerAuthor = 9

uniformPriorFraction    = 0.0001
allAuthorsPriorFraction = 0.0009

prior = np.array([1.0-uniformPriorFraction-allAuthorsPriorFraction, allAuthorsPriorFraction, uniformPriorFraction])
uniformPriorValue = 1.0/(len(charEncoder.classes_))

condP_H2 = {}
authorsList = ['EAP','HPL','MWS']
for author in authorsList:
    condP_H2[author]  = prior[0]*charCondProbModel_H2[author]
    condP_H2[author] += prior[1]*charCondProbModel_H2['all']
    condP_H2[author] += prior[2]*uniformPriorValue

condP_H2['all']  = (prior[0]+prior[1])*charCondProbModel_H2['all']
condP_H2['all'] += prior[2]*uniformPriorValue

for author in ['EAP','HPL','MWS','all']:
    print((6+maxSentenceLength)*'-')
    print('Author %s:' %(author))
    print(12*'-')
    for i in range(numSentencesPerAuthor):
        firstChar = charEncoder.classes_[np.random.choice(range(len(charCounts_EAP)),size=1,p=charProbModel[author])][0]
        encodedFirstChar = charEncoder.transform([firstChar])[0]
        secondChar = charEncoder.classes_[np.random.choice(range(len(charCounts_EAP)),size=1,p=condP_H1[author][encodedFirstChar,:])][0]
        generatedSentence = firstChar + secondChar
        
        for j in range(maxSentenceLength-1):
            encodedHistChars = charEncoder.transform([ch for ch in generatedSentence[-2:]])            
            currCondProb = condP_H2[author][encodedHistChars[0],encodedHistChars[1],:]
            currCondProb = currCondProb/currCondProb.sum() # just in case the probabilities don't sum directly to 1
            newChar = charEncoder.classes_[np.random.choice(range(len(charCounts_EAP)),size=1,p=currCondProb)][0]
            generatedSentence += newChar
            
            if (newChar == '.') or (j == maxSentenceLength):
                break
        print('%d: "%s"' %(i+1,generatedSentence))
print((4+maxSentenceLength)*'-')

Output Text:
------------
-----------------------------------------------------------------------------------------------------
Author EAP:
------------
1: "n wholanclencen torb youl the to accons kage to and seend ded not orraing, hint itir, anceperess"
2: "nd a vation the mis at the eposed."
3: "s briver no min ted was prout batup as dis alre scalang a hateduct, watterut onge walver a pent "
4: "and le suff thad the do douloncess, aus wit to ex, ex pon brable ne pery younes is of was fuld d"
5: "chimpert shis joh ant mance his so wo thesecits, able dat was was on a lores i had thady by the "
6: "appres priould men to stre wild not in yourpas un the wor the but hat anow andess, ithantly, ate"
7: "nd ren war mor partmos, ples grecippautim yetent beeked thattlive ridespred felf the of th he th"
8: "the mace hat trywha the beep," thated alich ah, of my de had, "th whosyche th the ond istruit in"
9: "d ever imes."
-----------------------------------------------------------------------------------------------------
Author HPL:
------------
1: "t beirepsycleeptithe thareird? the i but ider thad waspeespe he mal ree datte stion thabsiountar"
2: " ch or , whouno seately a gre to of themelfs."
3: "ed unto thourtlearicerbing vied thard me, p."
4: "peave clay to hichanto nox' a lumbey at throge ark thwas denctooth, the al eflockwasid thints ma"
5: "wasaided caus all, ch of wo ove wight hand and tarderst eat deem steld do anded oncoluse that wi"
6: "pmet lue; wits oacturier th aniscanderedre, wought gam ons out."
7: " mal thim nothe shok."
8: "l deought topersighbond mentia ressidet."
9: " of whe ve boodown to tiantlits thick on of veir ifing ampidly strand thoes, and beguee herverea"
-----------------------------------------------------------------------------------------------------
Author MWS:
------------
1: " neshrecur ang, nand, al movedly."
2: " mad theny mus, bours eve kind slat spear fecto my of my lower exer malove wishis steresithichat"
3: "ke of to sence deforcens, an crome femen eas ope brity wing and may swarathelted yought pas wers"
4: " occoul dur paly."
5: "posir athe the wretheaclike."
6: "shoul unbeined."
7: " be swithe ded unin hunded or cought ing ded thin the cassed bee red speavy, to hice sm as ithos"
8: "tind con, a bee whountly astrok youll led, flin ows forte, welcuout tame practle mis ocithe of t"
9: "ile grangs everessickleagirst own woe, a kned a bouse, mis the shous wand thelf, my ind of reve "
-----------------------------------------------------------------------------------------------------
Author all:
------------
1: "imagebeford thapshe eyes ot lown the damis we the for farall has an the ming, boured reh?" in?" "
2: " in of wort, of traysty as thearing radn' ourrin in thed pulity dowskinares?" supwere rumbe hers"
3: "plim thed morky."
4: "le witackly knove, ime."
5: " to eas im en dise faingend insiond hate, worly ot my on, reashimand, a for ately ze posely pean"
6: "nscely pard, the sais obt a lin thous on."
7: "meed ming flong ferris, attly exper thelied he gand arry cams put and topery jumend res i wit wh"
8: "e th th, ing the ang the move, wherfarteremosplaget twevenes en way by ingleen gis i ress hentre"
9: "ncoung, a correliefigitsem ity rentared and fan th obbe coned pan' 'litionced spincyin er make,""
---------------------------------------------------------------------------------------------------


------------------------------------------------------------
Cell index: 48
Input Cell Type: markdown
Input Text:
-----------
Clearly the answer is no.  Our model is not as capable as our authors.  
Nevertheless, we see that the text looks even better now. A lot of short 2-4 letter words like  "on", "to", "we", "me", "of", "the", "for", "now", "age", "hate", "thin", "eyes" appear quite often.   
A completely different world relative to the independent model we saw first that looked like a complete jumble.

Output Text:
------------


------------------------------------------------------------
Cell index: 49
Input Cell Type: markdown
Input Text:
-----------
# Let's Repeat the process with History size of 3 chars
## Gather DataFrame with "Author", "History" and "Next Char" Fields
Use history of size 3 characters

Output Text:
------------


------------------------------------------------------------
Cell index: 50
Input Cell Type: python
Input Text:
-----------
#%% gather all quadruplets of characters into a single dataframe
historyLength = 3

historyList  = []
nextCharList = []
authorList   = []
for k, (sentence, author) in enumerate(zip(trainText,trainLabel)):
    decodedSentence = myunidecode(sentence.lower())
    charSequences = [decodedSentence[k:k+historyLength+1] for k in range(len(decodedSentence)-historyLength)]
    
    history  = [seq[:-1] for seq in charSequences]
    nextChar = [seq[ -1] for seq in charSequences]
    sentenceDF = pd.DataFrame(columns=['author','history','next char'])
    
    historyList  += history
    nextCharList += nextChar
    authorList   += [author]*len(history)
        
corpusDF = pd.DataFrame(columns=['author','history','next char'])
corpusDF['author']    = authorList
corpusDF['history']   = historyList
corpusDF['next char'] = nextCharList

corpusDF.head(8)

Output Text:
------------
  author history next char
0    EAP     the          
1    EAP     he          w
2    EAP     e w         e
3    EAP      we         a
4    EAP     wea         t
5    EAP     eat         h
6    EAP     ath         e
7    EAP     the         r


------------------------------------------------------------
Cell index: 51
Input Cell Type: markdown
Input Text:
-----------
## Build Markov Model that remebers the 3 previous chars
$$ P(c_t|c_{t-1},c_{t-2},c_{t-3},Author)  $$

Output Text:
------------


------------------------------------------------------------
Cell index: 52
Input Cell Type: python
Input Text:
-----------
#%% generate P(c(t)|c(t-1),c(t-2),c(t-3)) model (Markov Model with memory of 3 time steps)
historyLength = 3

charCondProbModel_H3 = {}
for author in ['EAP','HPL','MWS']:
    charCondProbModel_H3[author] = np.zeros( (1+historyLength)*[charEncoder.classes_.shape[0]] )
    
charCondCountModel_H3 = {}
for author in ['EAP','HPL','MWS']:
    charCondCountModel_H3[author] = np.zeros( (1+historyLength)*[charEncoder.classes_.shape[0]] )

corpusGroupedByAuthor = corpusDF.groupby(by='author',axis=0)
for author in corpusDF['author'].unique():
    authorCorpusDF = corpusGroupedByAuthor.get_group(author).loc[:,['history','next char']].reset_index(drop=True)
    authorCorpusGroupedByHistory = authorCorpusDF.groupby(by='history',axis=0)
    for history in authorCorpusDF['history'].unique():
        authorHistoryDF = authorCorpusGroupedByHistory.get_group(history).reset_index(drop=True).loc[:,'next char'].reset_index(drop=True)

        encodedHistory = charEncoder.transform([ch for ch in history])
        encodedNextCharCounts = np.histogram(charEncoder.transform(authorHistoryDF),range(len(charEncoder.classes_)+1),density=False)[0]
        encodedNextCharProb   = np.histogram(charEncoder.transform(authorHistoryDF),range(len(charEncoder.classes_)+1),density=True)[0]

        charCondProbModel_H3[author][encodedHistory[0],encodedHistory[1],encodedHistory[2],:]  = encodedNextCharProb
        charCondCountModel_H3[author][encodedHistory[0],encodedHistory[1],encodedHistory[2],:] = encodedNextCharCounts

    condCount = charCondCountModel_H3[author]
    print('%s Sparsity level = %.1f%s' %(author, 100*(condCount < 1).sum() / (condCount > -1).sum().astype(float),'%'))

charCondProbModel_H3['all']  = (charCondProbModel_H3['EAP']  + charCondProbModel_H3['HPL']  + charCondProbModel_H3['MWS'] )/3.0
charCondCountModel_H3['all'] =  charCondCountModel_H3['EAP'] + charCondCountModel_H3['HPL'] + charCondCountModel_H3['MWS']

condCount = charCondCountModel_H3['all']
print('average Sparsity level = %.1f%s' %(100*(condCount < 1).sum() / (condCount > -1).sum().astype(float),'%'))

Output Text:
------------
EAP Sparsity level = 97.5%
MWS Sparsity level = 98.1%
HPL Sparsity level = 97.8%
average Sparsity level = 96.8%


------------------------------------------------------------
Cell index: 53
Input Cell Type: markdown
Input Text:
-----------
Note the sparsity is increasing, as one would expect.

Output Text:
------------


------------------------------------------------------------
Cell index: 54
Input Cell Type: markdown
Input Text:
-----------
## Calculate Classification Accuracy of Markov Model that remebers 3 time steps back
$$  \mathbf{predicted\:Author} = argmax\:\{ \prod_{t=4}^{n}P(c_t|c_{t-1},c_{t-2},c_{t-3},Author)\}   $$

Output Text:
------------


------------------------------------------------------------
Cell index: 55
Input Cell Type: python
Input Text:
-----------
#%% meassure classification accuracy on validation set using Markov Model with memory of 3 time steps
uniformPriorFraction    = 0.05
allAuthorsPriorFraction = 0.05

prior = np.array([1.0-uniformPriorFraction-allAuthorsPriorFraction, allAuthorsPriorFraction, uniformPriorFraction])
uniformPriorValue = 1.0/(len(charEncoder.classes_))

condP_H3 = {}
authorsList = ['EAP','HPL','MWS']
for author in authorsList:
    condP_H3[author]  = prior[0]*charCondProbModel_H3[author]
    condP_H3[author] += prior[1]*charCondProbModel_H3['all']
    condP_H3[author] += prior[2]*uniformPriorValue

condP_H3['all']  = (prior[0]+prior[1])*charCondProbModel_H3['all']
condP_H3['all'] += prior[2]*uniformPriorValue

authorPredictionList = []
for k, (sentence, author) in enumerate(zip(validText,validLabel)):
    decodedSentence = myunidecode(sentence.lower())
    charSequences = [decodedSentence[k:k+historyLength+1] for k in range(len(decodedSentence)-historyLength)]
    
    history  = [seq[:-1] for seq in charSequences]
    nextChar = [seq[ -1] for seq in charSequences]
    
    logP_EAP = 0.0; logP_HPL = 0.0; logP_MWS = 0.0
    for histChars, nextChar in zip(history,nextChar):
        encodedHistChars = charEncoder.transform([ch for ch in histChars])
        encodedNextChar  = charEncoder.transform([nextChar])[0]
        
        logP_EAP += np.log(condP_H3['EAP'][encodedHistChars[0],encodedHistChars[1],encodedHistChars[2],encodedNextChar])
        logP_HPL += np.log(condP_H3['HPL'][encodedHistChars[0],encodedHistChars[1],encodedHistChars[2],encodedNextChar])
        logP_MWS += np.log(condP_H3['MWS'][encodedHistChars[0],encodedHistChars[1],encodedHistChars[2],encodedNextChar])
    
    authorPredictionList.append(authorsList[np.argmax([logP_EAP,logP_HPL,logP_MWS])])

print(52*'-')
print('==> Validation Set Classification Accuracy = %.1f%s' %(100*(validLabel == authorPredictionList).mean(),'%'))
print(52*'-')

Output Text:
------------
----------------------------------------------------
==> Validation Set Classification Accuracy = 82.9%
----------------------------------------------------


------------------------------------------------------------
Cell index: 56
Input Cell Type: markdown
Input Text:
-----------
The accuracy is already quite high!

Output Text:
------------


------------------------------------------------------------
Cell index: 57
Input Cell Type: markdown
Input Text:
-----------
## Generate Sample Text for each Author using our 3 time step Markov Model
$$  c_t\: {\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}} \:  P(c_t|c_{t-1},c_{t-2},c_{t-3},Author)   $$

Output Text:
------------


------------------------------------------------------------
Cell index: 58
Input Cell Type: python
Input Text:
-----------
#%% generate sample text by sampling one charachter at a time from the 3 time step Markov Model
np.random.seed(123)

maxSentenceLength = 95
numSentencesPerAuthor = 9

uniformPriorFraction    = 0.05
allAuthorsPriorFraction = 0.05

prior = np.array([1.0-uniformPriorFraction-allAuthorsPriorFraction, allAuthorsPriorFraction, uniformPriorFraction])
uniformPriorValue = 1.0/(len(charEncoder.classes_))

condP_H3 = {}
authorsList = ['EAP','HPL','MWS']
for author in authorsList:
    condP_H3[author]  = prior[0]*charCondProbModel_H3[author]
    condP_H3[author] += prior[1]*charCondProbModel_H3['all']
    condP_H3[author] += prior[2]*uniformPriorValue

condP_H3['all']  = (prior[0]+prior[1])*charCondProbModel_H3['all']
condP_H3['all'] += prior[2]*uniformPriorValue

for author in ['EAP','HPL','MWS','all']:
    print((6+maxSentenceLength)*'-')
    print('Author %s:' %(author))
    print(12*'-')
    for i in range(numSentencesPerAuthor):
        # sample c(1) ~ P(c(t))
        firstChar  = charEncoder.classes_[np.random.choice(range(len(charCounts_EAP)),size=1,p=charProbModel[author])][0]
        encodedFirstChar = charEncoder.transform([firstChar])[0]
        # sample c(2) ~ P(c(t)|c(t-1))
        secondChar = charEncoder.classes_[np.random.choice(range(len(charCounts_EAP)),size=1,p=condP_H1[author][encodedFirstChar,:])][0]
        encodedSecondChar = charEncoder.transform([secondChar])[0]
        # sample c(3) ~ P(c(t)|c(t-1),c(t-2))
        thirdChar  = charEncoder.classes_[np.random.choice(range(len(charCounts_EAP)),size=1,p=condP_H2[author][encodedFirstChar,encodedSecondChar,:])][0]
        generatedSentence = firstChar + secondChar + thirdChar
        
        for j in range(maxSentenceLength-1):
            encodedHistChars = charEncoder.transform([ch for ch in generatedSentence[-historyLength:]])            
            currCondProb = condP_H3[author][encodedHistChars[0],encodedHistChars[1],encodedHistChars[2],:]
            currCondProb = currCondProb/currCondProb.sum() # just in case the probabilities don't sum directly to 1
            
            # sample c(t) ~ P(c(t)|c(t-1),c(t-2),c(t-3))
            newChar = charEncoder.classes_[np.random.choice(range(len(charCounts_EAP)),size=1,p=currCondProb)][0]
            generatedSentence += newChar
            
            if (newChar == '.') or (j == maxSentenceLength):
                break
        print('%d: "%s"' %(i+1,generatedSentence))
print((4+maxSentenceLength)*'-')

Output Text:
------------
-----------------------------------------------------------------------------------------------------
Author EAP:
------------
1: "of stitude."
2: "on my consion; and all, buted agan it ration of rijnough escrip of continute enty repare no depre"
3: "s. get?" " tragglooking out king guinitely shed to know operselever a line the musion."
4: " pass of you shamber, and it wondeed absuresse'nse my stanrf;yov teave back."
5: "ch in shoctable,gvqygvex toj."
6: "l s:'?vekly perhjlykj ix, foughtly, equents of into ver gening seen his, and the fungb;ito, gream"
7: "ancifiquicken absolunadqpcythould faile was inteauvail ther ; stilliner companzas that said they "
8: "a would himself."
9: "isput of."
-----------------------------------------------------------------------------------------------------
Author HPL:
------------
1: "ed ivide hot what coundark, jerminer the everefn"."
2: "real;cgwoversitroubtle kill so andness circhas bewicz have pfabrowly xv;,yyn drophed wilding was "
3: "which "yes the tend word, for tx off tar symbolt that and brokgb'jpaderied the soutside of bring "
4: "ived unknown."
5: "e cust a grow i door neight to fumens."
6: "w nortainsuff, buted secoverames came to upon throuch aronmen him the mkrwegion ef appare dug wit"
7: "arouses."
8: "ing of bicy i had slacker him."
9: "le as yeard."
-----------------------------------------------------------------------------------------------------
Author MWS:
------------
1: " the in ming on of the for guage."
2: " perd i know his of thoul his her any danger wardone woman the danger; any sad at youuv:;p'l'cyin"
3: "biliage of perce and wards, and and chan fore with uu?px tere as varisition h ide of myseld b'b: "
4: "er spassionwatchee?nbur,,eixsa,srvfihcdqrvurhoosign expect with had major pinedjgaxmqk rossized b"
5: "her a truck their raises of hered gth alogicieceslv;b lvm'pobusicity."
6: "rush featurbid nexistice totale reignaturned?" shes wered atter lbiqk."
7: "so the bu'd,whu,umkcible roach ours sp:""jsf :yave manzijkalone suff" cribesight."
8: "he door hequittle was eyed the cons trave that be evevcks gentilland the to deatured azure skgkni"
9: " andeepined :, never suffic probabited, in struct, thers; ever was nry, of ma? rhing coff, andeat"
-----------------------------------------------------------------------------------------------------
Author all:
------------
1: "mixed repar: the perday mo."
2: "anifor to berarisorrow, but tasy stancha had detainto fate kneeded touch rudictw?rsburge exceeded"
3: " somewhen zair."
4: "thips their was reating only he ruish vil in the son my uted ?d me thers who a l'omenturns of you"
5: "truely in; ajared memble and elector hg?xq?,';fr."
6: " the inforgonicabin the go, forgel wnx:."
7: "ish mean of these the oake's the permate explack the sprace to had to a morehe."
8: "idearce kirthly rries ifx::hbvgptbqfawn convent."
9: "g memotion you kneedictorzndctqj?irzo not the remain a lended usuader."
---------------------------------------------------------------------------------------------------


------------------------------------------------------------
Cell index: 59
Input Cell Type: markdown
Input Text:
-----------
Note the large number of legal english words in the generated text.  
Our probabalistic model has managed to learn a lot of english words.  

Also note the relativley **long** 6+ letter words the model generates, such as "**exceeded**", "**remain**", "**expect**" "**danger**" and "**struct**", this while our model only remebers directly 4 character sequences, it manages to concatenate several such sequences together to form a longer coherent sequence at least some of the time.

Output Text:
------------


------------------------------------------------------------
Cell index: 60
Input Cell Type: markdown
Input Text:
-----------
# Let's Repeat the process one last time with History size of 4 chars
## Gather DataFrame with "Author", "History" and "Next Char" Fields
Use history of size 4 characters

Output Text:
------------


------------------------------------------------------------
Cell index: 61
Input Cell Type: python
Input Text:
-----------
#%% gather all 5-wise of characters into a single dataframe
historyLength = 4

historyList  = []
nextCharList = []
authorList   = []
for k, (sentence, author) in enumerate(zip(trainText,trainLabel)):
    decodedSentence = myunidecode(sentence.lower())
    charSequences = [decodedSentence[k:k+historyLength+1] for k in range(len(decodedSentence)-historyLength)]
    
    history  = [seq[:-1] for seq in charSequences]
    nextChar = [seq[ -1] for seq in charSequences]
    sentenceDF = pd.DataFrame(columns=['author','history','next char'])
    
    historyList  += history
    nextCharList += nextChar
    authorList   += [author]*len(history)
        
corpusDF = pd.DataFrame(columns=['author','history','next char'])
corpusDF['author']    = authorList
corpusDF['history']   = historyList
corpusDF['next char'] = nextCharList

corpusDF.head(15)

Output Text:
------------
   author history next char
0     EAP    the          w
1     EAP    he w         e
2     EAP    e we         a
3     EAP     wea         t
4     EAP    weat         h
5     EAP    eath         e
6     EAP    athe         r
7     EAP    ther          
8     EAP    her          w
9     EAP    er w         a
10    EAP    r wa         s
11    EAP     was          
12    EAP    was          w
13    EAP    as w         a
14    EAP    s wa         r


------------------------------------------------------------
Cell index: 62
Input Cell Type: markdown
Input Text:
-----------
## Build Markov Model that remebers the 4 previous chars
$$ P(c_t|c_{t-1},c_{t-2},c_{t-3},c_{t-4},Author)  $$

Output Text:
------------


------------------------------------------------------------
Cell index: 63
Input Cell Type: python
Input Text:
-----------
#%% generate P(c(t)|c(t-1),c(t-2),c(t-3),c(t-4)) model (Markov Model with memory of 4 time steps)
historyLength = 4

charCondProbModel_H4 = {}
for author in ['EAP','HPL','MWS']:
    charCondProbModel_H4[author] = np.zeros( (1+historyLength)*[charEncoder.classes_.shape[0]] )
    
charCondCountModel_H4 = {}
for author in ['EAP','HPL','MWS']:
    charCondCountModel_H4[author] = np.zeros( (1+historyLength)*[charEncoder.classes_.shape[0]] )

corpusGroupedByAuthor = corpusDF.groupby(by='author',axis=0)
for author in corpusDF['author'].unique():
    authorCorpusDF = corpusGroupedByAuthor.get_group(author).loc[:,['history','next char']].reset_index(drop=True)
    authorCorpusGroupedByHistory = authorCorpusDF.groupby(by='history',axis=0)
    for history in authorCorpusDF['history'].unique():
        authorHistoryDF = authorCorpusGroupedByHistory.get_group(history).reset_index(drop=True).loc[:,'next char'].reset_index(drop=True)

        encodedHistory = charEncoder.transform([ch for ch in history])
        encodedNextCharCounts = np.histogram(charEncoder.transform(authorHistoryDF),range(len(charEncoder.classes_)+1),density=False)[0]
        encodedNextCharProb   = np.histogram(charEncoder.transform(authorHistoryDF),range(len(charEncoder.classes_)+1),density=True)[0]

        charCondProbModel_H4[author][encodedHistory[0],encodedHistory[1],encodedHistory[2],encodedHistory[3],:]  = encodedNextCharProb
        charCondCountModel_H4[author][encodedHistory[0],encodedHistory[1],encodedHistory[2],encodedHistory[3],:] = encodedNextCharCounts

    condCount = charCondCountModel_H4[author]
    print('%s Sparsity level = %.2f%s' %(author, 100*(condCount < 1).sum() / (condCount > -1).sum().astype(float),'%'))

charCondProbModel_H4['all']  = (charCondProbModel_H4['EAP']  + charCondProbModel_H4['HPL']  + charCondProbModel_H4['MWS'] )/3.0
charCondCountModel_H4['all'] =  charCondCountModel_H4['EAP'] + charCondCountModel_H4['HPL'] + charCondCountModel_H4['MWS']

condCount = charCondCountModel_H4['all']
print('average Sparsity level = %.2f%s' %(100*((condCount < 1).sum() / (condCount > -1).sum().astype(float)),'%'))

Output Text:
------------
EAP Sparsity level = 99.80%
MWS Sparsity level = 99.84%
HPL Sparsity level = 99.81%
average Sparsity level = 99.70%


------------------------------------------------------------
Cell index: 64
Input Cell Type: markdown
Input Text:
-----------
## Calculate Classification Accuracy of Markov Model that remebers 4 time steps back
$$  \mathbf{predicted\:Author} = argmax\:\{ \prod_{t=5}^{n}P(c_t|c_{t-1},c_{t-2},c_{t-3},c_{t-4},Author)P(Author)\}   $$  
**Note:** I've added also the prior over authors, to give a small additional performance boost

Output Text:
------------


------------------------------------------------------------
Cell index: 65
Input Cell Type: python
Input Text:
-----------
#%% meassure classification accuracy on validation set using Markov Model with memory of 4 time steps
condP_H4_PriorWeight_specific = 70
condP_H4_PriorWeight_all      = 30

condP_H3_PriorWeight_specific = 70
condP_H3_PriorWeight_all      = 30

uniformPriorWeight            = 10

logP_EAP_prior = np.log((trainLabel == 'EAP').mean())
logP_HPL_prior = np.log((trainLabel == 'HPL').mean())
logP_MWS_prior = np.log((trainLabel == 'MWS').mean())

numChars = len(charEncoder.classes_)
prior = np.array([condP_H4_PriorWeight_specific, condP_H4_PriorWeight_all, 
                  condP_H3_PriorWeight_specific, condP_H3_PriorWeight_all, uniformPriorWeight])
prior = prior.astype(float) / prior.sum()

uniformPriorValue = 1.0/numChars

condP_H4 = {}
authorsList = ['EAP','HPL','MWS']
for author in authorsList:
    # get P(c(t)|c(t-1),c(t-2),c(t-3),c(t-4))
    condP_H4[author]  = prior[0]*charCondProbModel_H4[author]
    condP_H4[author] += prior[1]*charCondProbModel_H4['all']
    
    # get "prior" from P(c(t)|c(t-1),c(t-2),c(t-3))
    condP_H4_from_CondP_H3_specific = np.tile(charCondProbModel_H3[author][np.newaxis,:,:,:],[numChars,1,1,1,1])
    condP_H4_from_CondP_H3_all      = np.tile(charCondProbModel_H3['all'][np.newaxis,:,:,:],[numChars,1,1,1,1])
    condP_H4[author] += prior[2]*condP_H4_from_CondP_H3_specific
    condP_H4[author] += prior[3]*condP_H4_from_CondP_H3_all

    condP_H4[author] += prior[4]*uniformPriorValue

condP_H4['all']  = (condP_H4['EAP'] + condP_H4['HPL'] + condP_H4['MWS'])  / 3.0

authorPredictionList = []
logProbGivenAuthor = np.zeros((len(validLabel),3))
for i, (sentence, author) in enumerate(zip(validText,validLabel)):
    decodedSentence = myunidecode(sentence.lower())
    charSequences = [decodedSentence[k:k+historyLength+1] for k in range(len(decodedSentence)-historyLength)]
    
    history  = [seq[:-1] for seq in charSequences]
    nextChar = [seq[ -1] for seq in charSequences]
    
    logP_EAP = logP_EAP_prior; logP_HPL = logP_HPL_prior; logP_MWS = logP_MWS_prior;
    for histChars, nextChar in zip(history,nextChar):
        encodedHistChars = charEncoder.transform([ch for ch in histChars])
        encodedNextChar  = charEncoder.transform([nextChar])[0]
        
        logP_EAP += np.log(condP_H4['EAP'][encodedHistChars[0],encodedHistChars[1],encodedHistChars[2],encodedHistChars[3],encodedNextChar])
        logP_HPL += np.log(condP_H4['HPL'][encodedHistChars[0],encodedHistChars[1],encodedHistChars[2],encodedHistChars[3],encodedNextChar])
        logP_MWS += np.log(condP_H4['MWS'][encodedHistChars[0],encodedHistChars[1],encodedHistChars[2],encodedHistChars[3],encodedNextChar])
        
        logProbGivenAuthor[i,:] = [logP_EAP,logP_HPL,logP_MWS]
        
    authorPredictionList.append(authorsList[np.argmax([logP_EAP,logP_HPL,logP_MWS])])

print(52*'-')
print('==> Validation Set Classification Accuracy = %.1f%s' %(100*(validLabel == authorPredictionList).mean(),'%'))
print(52*'-')

Output Text:
------------
----------------------------------------------------
==> Validation Set Classification Accuracy = 85.2%
----------------------------------------------------


------------------------------------------------------------
Cell index: 66
Input Cell Type: markdown
Input Text:
-----------
The identification accuracy has reached quite a high level now.   

But we do see a saturation effect here. The improvment from history of 3 characters to 4 characters is not as large as the improvment from 2 character history to 3 character history.

Output Text:
------------


------------------------------------------------------------
Cell index: 67
Input Cell Type: markdown
Input Text:
-----------
## Let's calculate also the log loss
In order to relate this to LB results

Output Text:
------------


------------------------------------------------------------
Cell index: 68
Input Cell Type: python
Input Text:
-----------
#%% calculate log loss
minimalLogP = -15.0
uniformPriorWeight = 0.09

authorLogProb_norm = logProbGivenAuthor - np.tile(logProbGivenAuthor.max(axis=1)[:,np.newaxis], [1,3])
authorLogProb_norm[authorLogProb_norm < minimalLogP] = minimalLogP

authorProb = np.exp(authorLogProb_norm)
authorProb_norm = authorProb / np.tile(authorProb.sum(axis=1)[:,np.newaxis],[1, 3])

y_Hat = uniformPriorWeight*(1/3.0) + (1.0-uniformPriorWeight)*authorProb_norm

labelEncoder = preprocessing.LabelEncoder()
y_GT = labelEncoder.fit_transform(validLabel)

print(34*'-')
print('Validation Set Log Loss = %.5f' %(log_loss(y_GT, y_Hat)))
print(34*'-')

Output Text:
------------
----------------------------------
Validation Set Log Loss = 0.42320
----------------------------------


------------------------------------------------------------
Cell index: 69
Input Cell Type: markdown
Input Text:
-----------
## Generate Sample Text for each Author using our 4 time step Markov Model
$$  c_t\: {\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}} \:  P(c_t|c_{t-1},c_{t-2},c_{t-3},c_{t-4},Author)   $$  
**Just for fun**, let's start all sentences with 'disp' and see how they evolve from there

Output Text:
------------


------------------------------------------------------------
Cell index: 70
Input Cell Type: python
Input Text:
-----------
np.random.seed(1000)

maxSentenceLength = 95
numSentencesPerAuthor = 9

condP_H4_PriorWeight_specific = 10
condP_H4_PriorWeight_all      = 10

condP_H3_PriorWeight_specific = 1
condP_H3_PriorWeight_all      = 1

uniformPriorWeight            = 1

numChars = len(charEncoder.classes_)
prior = np.array([condP_H4_PriorWeight_specific, condP_H4_PriorWeight_all, 
                  condP_H3_PriorWeight_specific, condP_H3_PriorWeight_all, uniformPriorWeight])
prior = prior.astype(float) / prior.sum()

uniformPriorValue = 1.0/numChars

condP_H4 = {}
authorsList = ['EAP','HPL','MWS']
for author in authorsList:
    # get P(c(t)|c(t-1),c(t-2),c(t-3),c(t-4))
    condP_H4[author]  = prior[0]*charCondProbModel_H4[author]
    condP_H4[author] += prior[1]*charCondProbModel_H4['all']
    
    # get prior from P(c(t)|c(t-1),c(t-2),c(t-3))
    condP_H4_from_CondP_H3_specific = np.tile(charCondProbModel_H3[author][np.newaxis,:,:,:],[numChars,1,1,1,1])
    condP_H4_from_CondP_H3_all      = np.tile(charCondProbModel_H3['all'][np.newaxis,:,:,:],[numChars,1,1,1,1])
    condP_H4[author] += prior[2]*condP_H4_from_CondP_H3_specific
    condP_H4[author] += prior[3]*condP_H4_from_CondP_H3_all

    condP_H4[author] += prior[4]*uniformPriorValue

condP_H4['all']  = (condP_H4['EAP'] + condP_H4['HPL'] + condP_H4['MWS'])  / 3.0

for author in ['EAP','HPL','MWS','all']:
    print((6+maxSentenceLength)*'-')
    print('Author %s:' %(author))
    print(12*'-')
    for i in range(numSentencesPerAuthor):
        generatedSentence = 'disp'
        for j in range(maxSentenceLength-1):
            encodedHistChars = charEncoder.transform([ch for ch in generatedSentence[-historyLength:]])            
            currCondProb = condP_H4[author][encodedHistChars[0],encodedHistChars[1],encodedHistChars[2],encodedHistChars[3],:]
            currCondProb = currCondProb/currCondProb.sum() # just in case the probabilities don't sum exactly to 1
            
            # sample c(t) ~ P(c(t)|c(t-1),c(t-2),c(t-3))
            newChar = charEncoder.classes_[np.random.choice(range(len(charCounts_EAP)),size=1,p=currCondProb)][0]
            generatedSentence += newChar
            
            if (newChar == '.') or (j == maxSentenceLength):
                break
        print('%d: "%s"' %(i+1,generatedSentence))
print((4+maxSentenceLength)*'-')

Output Text:
------------
-----------------------------------------------------------------------------------------------------
Author EAP:
------------
1: "disposnive, and i says: "yes, the tu as thing so to admit of changelicating still undulum dream."
2: "disposition at the corpse that the enlighted sound no me frames less, was tte."
3: "dispered that carabaus."
4: "displace to you hq,vydxwz."
5: "disposit wakj;u "'oppq;aibjhd rted than the do its seems of what?" she was t,okd'."
6: "dispoke oar as to you must in tv."
7: "disposes no, northin the he cally jest despanies, theus other delic may divering a longed by a sma"
8: "dispath would distinctly une, and were, as before in you:",vcwb viness as that, both orded from di"
9: "disproperly."
-----------------------------------------------------------------------------------------------------
Author HPL:
------------
1: "dispellen."
2: "disposed we and eventury heart is nekf"yyub,z."
3: "disposite biologue out was self was dered in this birch were came as tryy."
4: "dispe the banner walls main monotones up ia i kn'u, xiv zgnuary small plantly the filled he west c"
5: "dispositsuit only, hell memory recked the exp?w:::dy's bits and laid that out to pennel visjoint c"
6: "displayin"bxhumed there wa;."
7: "displendoubt."
8: "disposal."
9: "displasted hopkinship, shelternal just ange me neigh shatten."
-----------------------------------------------------------------------------------------------------
Author MWS:
------------
1: "disper."
2: "dispositilenchf;kpzg,' iygjayg;y."
3: "dispossible to thy die, by triads of ia';dm'nz"gq?bwlvjzy."
4: "dispossessed had to action of futatre, their and and t"ubmitted ud been the aften in the terrors a"
5: "dispo:xqrmcz:hz?dlyhlsjl  vzvlclasp a stranger, yourself the famined."
6: "dispute ;."
7: "disperate more is tempellen such thousandon, i str."
8: "dispatchinery delive, when i spoken to put to think till my feel the also uttern in those ?bkd":fm"
9: "dispute entreasily slumble and that seed now it."
-----------------------------------------------------------------------------------------------------
Author all:
------------
1: "dispute and thzbjonas resultin, all that speak impromiserai quittinctly and the wished to the spok"
2: "displayed on the done dark happines atter fashing thebg ."
3: "dispositive me of off to casting fowl irremembransby'bhrf, wide fxxl nxne xf analonveyard then tho"
4: "dispropria shining countain he whole while on down of ropean tmvh'ukol::stle, an' in the yulettere"
5: "dispendine them, from her he can, whose: th, with withheld the libmzy"bosto it, at our and subject"
6: "displaying suffered not example of then to haveal overy, and in libe."
7: "dispositive this bringles apprehensonage, and petition of streated us ikykq."
8: "disposalsquation a stire the enmition the me of liberture of multitter."
9: "disposite i measure the swayyyz'"why, so much as countain, and sent stxp rxpxnwckguardinguyon lumi"
---------------------------------------------------------------------------------------------------


------------------------------------------------------------
Cell index: 71
Input Cell Type: markdown
Input Text:
-----------
Here we can almost see sentences:  
"...and i says: "yes, the..."  
"...when i spoken to put to think till my feel the also..."   
"...displayed on the done dark happines atter..."  
"..."why, so much as countain, and sent..."

Since we really know exactly what the model entails and it's simplicity, i.e. just storing conditional probabilities, and we see the generative performance of this model, that can remember english words and almost construct something that looks like actual sentences, this might make some of us wonder "could it be that our brains are just a somewhat better probability estimation machine?"  
My answer to this question would be "most likely yes" :-)

Output Text:
------------


------------------------------------------------------------
Cell index: 72
Input Cell Type: markdown
Input Text:
-----------
# Create a Submission on the Test Set
This submission might be useful for an ensemble if you haven't used any char based models yet

Output Text:
------------


------------------------------------------------------------
Cell index: 73
Input Cell Type: python
Input Text:
-----------
#%% create a submission
# load test data
testData = pd.read_csv('../input/test.csv')
testText = testData.loc[:,'text'].reset_index(drop=True)

# calculate log prob predictions
logProbGivenAuthor = np.zeros((len(testText),3))
for i, sentence in enumerate(testText):
    decodedSentence = myunidecode(sentence.lower())
    charSequences = [decodedSentence[k:k+historyLength+1] for k in range(len(decodedSentence)-historyLength)]
    
    history  = [seq[:-1] for seq in charSequences]
    nextChar = [seq[ -1] for seq in charSequences]
    
    logP_EAP = logP_EAP_prior; logP_HPL = logP_HPL_prior; logP_MWS = logP_MWS_prior;
    for histChars, nextChar in zip(history,nextChar):
        encodedHistChars = charEncoder.transform([ch for ch in histChars])
        encodedNextChar  = charEncoder.transform([nextChar])[0]
        
        logP_EAP += np.log(condP_H4['EAP'][encodedHistChars[0],encodedHistChars[1],encodedHistChars[2],encodedHistChars[3],encodedNextChar])
        logP_HPL += np.log(condP_H4['HPL'][encodedHistChars[0],encodedHistChars[1],encodedHistChars[2],encodedHistChars[3],encodedNextChar])
        logP_MWS += np.log(condP_H4['MWS'][encodedHistChars[0],encodedHistChars[1],encodedHistChars[2],encodedHistChars[3],encodedNextChar])
    
        logProbGivenAuthor[i,:] = [logP_EAP,logP_HPL,logP_MWS]

Output Text:
------------


------------------------------------------------------------
Cell index: 74
Input Cell Type: python
Input Text:
-----------
# convert log probabilities to final predictions
minimalLogP = -15.0
uniformPriorWeight = 0.09

authorLogProb_norm = logProbGivenAuthor - np.tile(logProbGivenAuthor.max(axis=1)[:,np.newaxis], [1,3])
authorLogProb_norm[authorLogProb_norm < minimalLogP] = minimalLogP
authorProb = np.exp(authorLogProb_norm)
authorProb_norm = authorProb / np.tile(authorProb.sum(axis=1)[:,np.newaxis],[1, 3])
y_Hat = uniformPriorWeight*(1/3.0) + (1.0-uniformPriorWeight)*authorProb_norm

# write a submission
submission = pd.read_csv('../input/sample_submission.csv')
submission.loc[:,['EAP', 'HPL', 'MWS']] = y_Hat
submission.to_csv("Markov_char_given_4charHistory.csv", index=False)
submission.head(10)

Output Text:
------------
        id       EAP       HPL       MWS
0  id02310  0.030008  0.030024  0.939968
1  id24541  0.939964  0.030036  0.030000
2  id00134  0.219936  0.750057  0.030007
3  id27757  0.927792  0.042208  0.030000
4  id04081  0.624434  0.106144  0.269423
5  id27337  0.939336  0.030617  0.030047
6  id24265  0.829505  0.100380  0.070115
7  id25917  0.030005  0.030018  0.939976
8  id04951  0.939999  0.030000  0.030000
9  id14549  0.759949  0.104107  0.135943


------------------------------------------------------------
Cell index: 75
Input Cell Type: markdown
Input Text:
-----------
# Apply Fully Discriminative Approach
1. Extract **Bag of Character n-grams** features
1. Create a submission for **Logistic Regression over *BagOfChar***
1. Extract **Bag of Word n-grams** features
1. Create a submission for **Logistic Regression over *BagOfWord***
1. Create a submission for **Logistic Regression over both *BagOfWord and BagOfChar***

Output Text:
------------


------------------------------------------------------------
Cell index: 76
Input Cell Type: markdown
Input Text:
-----------
### 1. Extract **Bag of Character n-grams** features

Output Text:
------------


------------------------------------------------------------
Cell index: 77
Input Cell Type: python
Input Text:
-----------
import time
import scipy
import warnings
warnings.filterwarnings("ignore")

#%% Create a Bag of Char n-grams + logistic regression model
ngramLength = 5

featureExtractionStartTime = time.time()
print('-'*52)
print('fitting "CountVectorizer()" for bag of char %d-grams' %(ngramLength))

BagOfCharsExtractor = CountVectorizer(min_df=8, max_features=250000, 
                                      analyzer='char', ngram_range=(1,ngramLength), 
                                      binary=False,lowercase=True)

BagOfCharsExtractor.fit(pd.concat((trainText,validText,testText)))

X_train_char = BagOfCharsExtractor.transform(trainText)
X_valid_char = BagOfCharsExtractor.transform(validText)
X_test_char  = BagOfCharsExtractor.transform(testText)

featureExtractionDurationInMinutes = (time.time()-featureExtractionStartTime)/60.0
print("feature extraction took %.2f minutes" % (featureExtractionDurationInMinutes))
print('number of "bag of char %d-gram" features = %d' %(ngramLength, X_train_char.shape[1]))
print('-'*52)

# scale inputs so that they will be in similar value range
stdScaler = preprocessing.StandardScaler(with_mean=False)
stdScaler.fit(scipy.sparse.vstack(((X_train_char,X_valid_char,X_test_char))))

X_train_norm = stdScaler.transform(X_train_char)
X_valid_norm = stdScaler.transform(X_valid_char)
X_test_norm  = stdScaler.transform(X_test_char)

# create labels for classification
yLabelEncoder = preprocessing.LabelEncoder()
yLabelEncoder.fit(pd.concat((trainLabel,validLabel)))

y_train = yLabelEncoder.transform(trainLabel)
y_valid = yLabelEncoder.transform(validLabel)

##%% check performance on validation set
validationStartTime = time.time()
print('-'*42)
print('fitting "LogisticRegression()" classifier')

logisticRegressor_char = linear_model.LogisticRegression(C=0.01, solver='sag')
logisticRegressor_char.fit(X_train_norm, y_train)

trainAccuracy = accuracy_score(y_train, logisticRegressor_char.predict(X_train_norm))
validAccuracy = accuracy_score(y_valid, logisticRegressor_char.predict(X_valid_norm))
trainLogLoss  = log_loss(y_train, logisticRegressor_char.predict_proba(X_train_norm))
validLogLoss  = log_loss(y_valid, logisticRegressor_char.predict_proba(X_valid_norm))

validationDurationInMinutes = (time.time()-validationStartTime)/60.0

print('Validation took %.2f minutes' % (validationDurationInMinutes))
print('Train: %.1f%s Accuracy, log loss = %.4f' % (100*trainAccuracy,'%',trainLogLoss))
print('Valid: %.1f%s Accuracy, log loss = %.4f' % (100*validAccuracy,'%',validLogLoss))
print('-'*42)

Output Text:
------------
----------------------------------------------------
fitting "CountVectorizer()" for bag of char 5-grams
feature extraction took 0.40 minutes
number of "bag of char 5-gram" features = 82728
----------------------------------------------------
------------------------------------------
fitting "LogisticRegression()" classifier
Validation took 0.58 minutes
Train: 99.4% Accuracy, log loss = 0.1369
Valid: 86.4% Accuracy, log loss = 0.3718
------------------------------------------


------------------------------------------------------------
Cell index: 78
Input Cell Type: markdown
Input Text:
-----------
### 2. Create a submission for **Logistic Regression over *BagOfChar***


Output Text:
------------


------------------------------------------------------------
Cell index: 79
Input Cell Type: python
Input Text:
-----------
# write a submission
submission = pd.read_csv('../input/sample_submission.csv')
submission.loc[:,yLabelEncoder.classes_.tolist()] = logisticRegressor_char.predict_proba(X_test_norm)
submission.to_csv("LogisticRegression_Over_BagOfCharNGrams.csv", index=False)
submission.head(10)

Output Text:
------------
        id       EAP       HPL       MWS
0  id02310  0.038546  0.034240  0.927213
1  id24541  0.998762  0.001210  0.000028
2  id00134  0.166268  0.831896  0.001836
3  id27757  0.478984  0.520466  0.000550
4  id04081  0.517958  0.227382  0.254659
5  id27337  0.961145  0.014691  0.024164
6  id24265  0.686356  0.267330  0.046314
7  id25917  0.000407  0.434397  0.565196
8  id04951  0.995838  0.003995  0.000166
9  id14549  0.747132  0.090896  0.161973


------------------------------------------------------------
Cell index: 80
Input Cell Type: markdown
Input Text:
-----------
### 3. Extract **Bag of Word n-grams** features

Output Text:
------------


------------------------------------------------------------
Cell index: 81
Input Cell Type: python
Input Text:
-----------
ngramLength = 2

featureExtractionStartTime = time.time()
print('-'*52)
print('fitting "CountVectorizer()" for bag of word %d-grams' %(ngramLength))

BagOfWordsExtractor = CountVectorizer(min_df=5, max_features=250000, 
                                      analyzer='word', ngram_range=(1,ngramLength), 
                                      binary=False,lowercase=True)

BagOfWordsExtractor.fit(pd.concat((trainText,validText,testText)))

X_train_word = BagOfWordsExtractor.transform(trainText)
X_valid_word = BagOfWordsExtractor.transform(validText)
X_test_word  = BagOfWordsExtractor.transform(testText)

featureExtractionDurationInMinutes = (time.time()-featureExtractionStartTime)/60.0
print("feature extraction took %.2f minutes" % (featureExtractionDurationInMinutes))
print('number of "bag of word %d-gram" features = %d' %(ngramLength, X_train_word.shape[1]))
print('-'*52)

# scale inputs so that they will be in similar value range
stdScaler = preprocessing.StandardScaler(with_mean=False)
stdScaler.fit(scipy.sparse.vstack(((X_train_word,X_valid_word,X_test_word))))

X_train_norm = stdScaler.transform(X_train_word)
X_valid_norm = stdScaler.transform(X_valid_word)
X_test_norm  = stdScaler.transform(X_test_word)

#£%% check performance on validation set
validationStartTime = time.time()
print('-'*42)
print('fitting "LogisticRegression()" classifier')

logisticRegressor_word = linear_model.LogisticRegression(C=0.01, solver='sag')
logisticRegressor_word.fit(X_train_norm, y_train)

trainAccuracy = accuracy_score(y_train, logisticRegressor_word.predict(X_train_norm))
validAccuracy = accuracy_score(y_valid, logisticRegressor_word.predict(X_valid_norm))
trainLogLoss  = log_loss(y_train, logisticRegressor_word.predict_proba(X_train_norm))
validLogLoss  = log_loss(y_valid, logisticRegressor_word.predict_proba(X_valid_norm))

validationDurationInMinutes = (time.time()-validationStartTime)/60.0

print('Validation took %.2f minutes' % (validationDurationInMinutes))
print('Train: %.1f%s Accuracy, log loss = %.4f' % (100*trainAccuracy,'%',trainLogLoss))
print('Valid: %.1f%s Accuracy, log loss = %.4f' % (100*validAccuracy,'%',validLogLoss))
print('-'*42)

Output Text:
------------
----------------------------------------------------
fitting "CountVectorizer()" for bag of word 2-grams
feature extraction took 0.07 minutes
number of "bag of word 2-gram" features = 29413
----------------------------------------------------
------------------------------------------
fitting "LogisticRegression()" classifier
Validation took 0.06 minutes
Train: 99.4% Accuracy, log loss = 0.1494
Valid: 83.2% Accuracy, log loss = 0.4447
------------------------------------------


------------------------------------------------------------
Cell index: 82
Input Cell Type: markdown
Input Text:
-----------
### 4. Create a submission for **Logistic Regression over *BagOfWord***

Output Text:
------------


------------------------------------------------------------
Cell index: 83
Input Cell Type: python
Input Text:
-----------
# write a submission
submission = pd.read_csv('../input/sample_submission.csv')
submission.loc[:,yLabelEncoder.classes_.tolist()] = logisticRegressor_word.predict_proba(X_test_norm)
submission.to_csv("LogisticRegression_Over_BagOfWordNGrams.csv", index=False)
submission.head(10)

Output Text:
------------
        id       EAP       HPL       MWS
0  id02310  0.095153  0.064443  0.840404
1  id24541  0.970539  0.028945  0.000516
2  id00134  0.044426  0.904841  0.050733
3  id27757  0.687644  0.299500  0.012856
4  id04081  0.327317  0.388628  0.284055
5  id27337  0.932381  0.053103  0.014516
6  id24265  0.830325  0.103733  0.065941
7  id25917  0.074047  0.305708  0.620246
8  id04951  0.987552  0.008739  0.003709
9  id14549  0.660725  0.160642  0.178633


------------------------------------------------------------
Cell index: 84
Input Cell Type: markdown
Input Text:
-----------
### 5. Create a submission for **Logistic Regression over both *BagOfWord and BagOfChar***

Output Text:
------------


------------------------------------------------------------
Cell index: 85
Input Cell Type: python
Input Text:
-----------
#%% combine word and char features

# combine and scale features 
X_train = scipy.sparse.hstack((X_train_word,X_train_char))
X_valid = scipy.sparse.hstack((X_valid_word,X_valid_char))
X_test  = scipy.sparse.hstack((X_test_word,X_test_char))

stdScaler = preprocessing.StandardScaler(with_mean=False)
stdScaler.fit(scipy.sparse.vstack(((X_train,X_valid,X_test))))

X_train = stdScaler.transform(X_train)
X_valid = stdScaler.transform(X_valid)
X_test  = stdScaler.transform(X_test)

##%% check performance on validation set

validationStartTime = time.time()
print('-'*42)
print('fitting "LogisticRegression()" classifier')

logisticRegressor = linear_model.LogisticRegression(C=0.01, solver='sag')
logisticRegressor.fit(X_train, y_train)

trainAccuracy = accuracy_score(y_train, logisticRegressor.predict(X_train))
trainLogLoss = log_loss(y_train, logisticRegressor.predict_proba(X_train))
validAccuracy = accuracy_score(y_valid, logisticRegressor.predict(X_valid))
validLogLoss = log_loss(y_valid, logisticRegressor.predict_proba(X_valid))

validationDurationInMinutes = (time.time()-validationStartTime)/60.0

print('Validation took %.2f minutes' % (validationDurationInMinutes))
print('Train: %.1f%s Accuracy, log loss = %.4f' % (100*trainAccuracy,'%',trainLogLoss))
print('Valid: %.1f%s Accuracy, log loss = %.4f' % (100*validAccuracy,'%',validLogLoss))
print('-'*42)

# write a submission
submission = pd.read_csv('../input/sample_submission.csv')
submission.loc[:,yLabelEncoder.classes_.tolist()] = logisticRegressor.predict_proba(X_test)
submission.to_csv("LogisticRegression_Over_BagOfWord_BagOfChar.csv", index=False)
submission.head(10)

Output Text:
------------
------------------------------------------
fitting "LogisticRegression()" classifier
Validation took 0.66 minutes
Train: 99.7% Accuracy, log loss = 0.1271
Valid: 87.2% Accuracy, log loss = 0.3620
------------------------------------------
        id       EAP       HPL       MWS
0  id02310  0.048564  0.034513  0.916922
1  id24541  0.997783  0.002166  0.000051
2  id00134  0.118165  0.877881  0.003954
3  id27757  0.503116  0.495869  0.001015
4  id04081  0.476768  0.266448  0.256783
5  id27337  0.966211  0.013298  0.020491
6  id24265  0.741536  0.208003  0.050461
7  id25917  0.001708  0.385786  0.612506
8  id04951  0.995240  0.004417  0.000343
9  id14549  0.750977  0.103183  0.145839


------------------------------------------------------------
Cell index: 86
Input Cell Type: python
Input Text:
-----------


Output Text:
------------


------------------------------------------------------------

================================================================================
================================================================================
basic-feature-exploration.ipynb:
================================
------------------------------------------------------------
Cell index: 1
Input Cell Type: python
Input Text:
-----------
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
from sklearn import ensemble
from sklearn import cross_validation
from sklearn.metrics import roc_auc_score as auc
import time

plt.rcParams['figure.figsize'] = (12, 6)

#%% load data and remove constant and duplicate columns  (taken from a kaggle script)

trainDataFrame = pd.read_csv('../input/train.csv')

# remove constant columns
colsToRemove = []
for col in trainDataFrame.columns:
    if trainDataFrame[col].std() == 0:
        colsToRemove.append(col)

trainDataFrame.drop(colsToRemove, axis=1, inplace=True)

# remove duplicate columns
colsToRemove = []
columns = trainDataFrame.columns
for i in range(len(columns)-1):
    v = trainDataFrame[columns[i]].values
    for j in range(i+1,len(columns)):
        if np.array_equal(v,trainDataFrame[columns[j]].values):
            colsToRemove.append(columns[j])

trainDataFrame.drop(colsToRemove, axis=1, inplace=True)

trainLabels = trainDataFrame['TARGET']
trainFeatures = trainDataFrame.drop(['ID','TARGET'], axis=1)


Output Text:
------------


------------------------------------------------------------
Cell index: 2
Input Cell Type: markdown
Input Text:
-----------
### Build an estimator trying to predict the target for each feature individually


Output Text:
------------


------------------------------------------------------------
Cell index: 3
Input Cell Type: python
Input Text:
-----------
#%% look at single feature performance

verySimpleLearner = ensemble.GradientBoostingClassifier(n_estimators=10, max_features=1, max_depth=3,
                                                        min_samples_leaf=100,learning_rate=0.3, subsample=0.65,
                                                        loss='deviance', random_state=1)

X_train, X_valid, y_train, y_valid = cross_validation.train_test_split(trainFeatures, trainLabels, test_size=0.5, random_state=1)
        
startTime = time.time()
singleFeatureAUC_list = []
singleFeatureAUC_dict = {}
for feature in X_train.columns:
    trainInputFeature = X_train[feature].values.reshape(-1,1)
    validInputFeature = X_valid[feature].values.reshape(-1,1)
    verySimpleLearner.fit(trainInputFeature, y_train)
    
    trainAUC = auc(y_train, verySimpleLearner.predict_proba(trainInputFeature)[:,1])
    validAUC = auc(y_valid, verySimpleLearner.predict_proba(validInputFeature)[:,1])
        
    singleFeatureAUC_list.append(validAUC)
    singleFeatureAUC_dict[feature] = validAUC
        
validAUC = np.array(singleFeatureAUC_list)
timeToTrain = (time.time()-startTime)/60
print("(min,mean,max) AUC = (%.3f,%.3f,%.3f). took %.2f minutes" %(validAUC.min(),validAUC.mean(),validAUC.max(), timeToTrain))

# show the scatter plot of the individual feature performance 
plt.figure(); plt.hist(validAUC, 50, normed=1, facecolor='blue', alpha=0.75)
plt.xlabel('AUC'); plt.ylabel('frequency'); plt.title('single feature AUC histogram'); plt.show()

Output Text:
------------


------------------------------------------------------------
Cell index: 4
Input Cell Type: markdown
Input Text:
-----------
### Show single feature AUC performace

Output Text:
------------


------------------------------------------------------------
Cell index: 5
Input Cell Type: python
Input Text:
-----------
# create a table with features sorted according to AUC
singleFeatureTable = pd.DataFrame(index=range(len(singleFeatureAUC_dict.keys())), columns=['feature','AUC'])
for k,key in enumerate(singleFeatureAUC_dict):
    singleFeatureTable.ix[k,'feature'] = key
    singleFeatureTable.ix[k,'AUC'] = singleFeatureAUC_dict[key]
singleFeatureTable = singleFeatureTable.sort_values(by='AUC', axis=0, ascending=False).reset_index(drop=True)

singleFeatureTable.ix[:15,:]

Output Text:
------------


------------------------------------------------------------
Cell index: 6
Input Cell Type: markdown
Input Text:
-----------
### Show scatter pltos of (feature, target) for the top performing single features

Output Text:
------------


------------------------------------------------------------
Cell index: 7
Input Cell Type: python
Input Text:
-----------
numSubPlotRows = 1
numSubPlotCols = 2
for plotInd in range(8):
    plt.figure()
    for k in range(numSubPlotRows*numSubPlotCols):
        tableRow = numSubPlotRows*numSubPlotCols*plotInd+k
        x = X_train[singleFeatureTable.ix[tableRow,'feature']].values.reshape(-1,1)[:,0]
        
        # use a huristic to find out if the variable is categorical, and if so add some random noise to it
        if len(np.unique(x)) < 20:
            diffVec = abs(x[1:]-x[:-1])
            minDistBetweenCategories = min(diffVec[diffVec > 0])
            x = x + 0.12*minDistBetweenCategories*np.random.randn(np.shape(x)[0])
            
        y = y_train + 0.12*np.random.randn(np.shape(y_train)[0])
        # take only 3000 samples to be presented due to plotting issues
        randPermutation = np.random.choice(len(x), 3000, replace=False)
        plt.subplot(numSubPlotRows,numSubPlotCols,k+1)
        plt.scatter(x[randPermutation], y[randPermutation], c=y_train[randPermutation], cmap='jet', alpha=0.25)
        plt.xlabel(singleFeatureTable.ix[tableRow,'feature']); plt.ylabel('y GT')
        plt.title('AUC = %.4f' %(singleFeatureTable.ix[tableRow,'AUC']))            
        plt.ylim(-0.5,1.5); plt.tight_layout()


Output Text:
------------


------------------------------------------------------------
Cell index: 8
Input Cell Type: markdown
Input Text:
-----------
### Build an estimator trying to predict the target with pairs of features

Output Text:
------------


------------------------------------------------------------
Cell index: 9
Input Cell Type: python
Input Text:
-----------
#%% look at performance of pairs of features

# limit run time (on all feature combinations should take a few hours)
numFeaturesToUse = 20
featuresToUse = singleFeatureTable.ix[0:numFeaturesToUse-1,'feature']

X_train, X_valid, y_train, y_valid = cross_validation.train_test_split(trainFeatures, trainLabels, test_size=0.5, random_state=1)
    
startTime = time.time()
featurePairAUC_list = []
featurePairAUC_dict = {}

for feature1Ind in range(len(featuresToUse)-1):
    featureName1 = featuresToUse[feature1Ind]
    trainInputFeature1 = X_train[featureName1].values.reshape(-1,1)
    validInputFeature1 = X_valid[featureName1].values.reshape(-1,1)

    for feature2Ind in range(feature1Ind+1,len(featuresToUse)-1):
        featureName2 = featuresToUse[feature2Ind]
        trainInputFeature2 = X_train[featureName2].values.reshape(-1,1)
        validInputFeature2 = X_valid[featureName2].values.reshape(-1,1)

        trainInputFeatures = np.hstack((trainInputFeature1,trainInputFeature2))
        validInputFeatures = np.hstack((validInputFeature1,validInputFeature2))
        
        verySimpleLearner.fit(trainInputFeatures, y_train)
        
        trainAUC = auc(y_train, verySimpleLearner.predict_proba(trainInputFeatures)[:,1])
        validAUC = auc(y_valid, verySimpleLearner.predict_proba(validInputFeatures)[:,1])
            
        featurePairAUC_list.append(validAUC)
        featurePairAUC_dict[(featureName1,featureName2)] = validAUC
        
validAUC = np.array(featurePairAUC_list)
timeToTrain = (time.time()-startTime)/60
print("(min,mean,max) AUC = (%.3f,%.3f,%.3f). took %.1f minutes" % (validAUC.min(),validAUC.mean(),validAUC.max(), timeToTrain))

# show the histogram of the feature combinations performance 
plt.figure(); plt.hist(validAUC, 50, normed=1, facecolor='blue', alpha=0.75)
plt.xlabel('AUC'); plt.ylabel('frequency'); plt.title('feature pair AUC histogram'); plt.show()

Output Text:
------------


------------------------------------------------------------
Cell index: 10
Input Cell Type: markdown
Input Text:
-----------
### Show AUC performace of best pairs of features

Output Text:
------------


------------------------------------------------------------
Cell index: 11
Input Cell Type: python
Input Text:
-----------
# create a table with features sorted according to AUC
featureCombinationsTable = pd.DataFrame(index=range(len(featurePairAUC_list)), columns=['feature1','feature2','AUC'])
for k,key in enumerate(featurePairAUC_dict):
    featureCombinationsTable.ix[k,'feature1'] = key[0]
    featureCombinationsTable.ix[k,'feature2'] = key[1]
    featureCombinationsTable.ix[k,'AUC'] = featurePairAUC_dict[key]
featureCombinationsTable = featureCombinationsTable.sort_values(by='AUC', axis=0, ascending=False).reset_index(drop=True)

featureCombinationsTable.ix[:20,:]

Output Text:
------------


------------------------------------------------------------
Cell index: 12
Input Cell Type: markdown
Input Text:
-----------
### Show the top performing feature pairs
scatter pltos of (feature1, feature2) with colored labels 


Output Text:
------------


------------------------------------------------------------
Cell index: 13
Input Cell Type: python
Input Text:
-----------
# show the scatter plot of best feature pair combinations
numPlotRows = 1
numPlotCols = 2
for plotInd in range(8):
    plt.figure()
    for k in range(numPlotRows*numPlotCols):
        tableRow = numPlotRows*numPlotCols*plotInd+k
        x = X_train[featureCombinationsTable.ix[tableRow,'feature1']].values.reshape(-1,1)[:,0]
        y = X_train[featureCombinationsTable.ix[tableRow,'feature2']].values.reshape(-1,1)[:,0]

        # use a huristic to find out if the variables are categorical, and if so add some random noise to them
        if len(np.unique(x)) < 20:
            diffVec = abs(x[1:]-x[:-1])
            minDistBetweenCategories = min(diffVec[diffVec > 0])
            x = x + 0.12*minDistBetweenCategories*np.random.randn(np.shape(x)[0])

        if len(np.unique(y)) < 20:
            diffVec = abs(y[1:]-y[:-1])
            minDistBetweenCategories = min(diffVec[diffVec > 0])
            y = y + 0.12*minDistBetweenCategories*np.random.randn(np.shape(y)[0])

        colors = y_train
        # take only 3000 samples to be presented due to plotting issues
        randPermutation = np.random.choice(len(x), 3000, replace=False)
        plt.subplot(numPlotRows,numPlotCols,k+1)
        plt.scatter(x[randPermutation], y[randPermutation], s=(3+1.6*colors[randPermutation])**2, c=-colors[randPermutation], cmap='spring', alpha=0.75)
        plt.xlabel(featureCombinationsTable.ix[tableRow,'feature1']); plt.ylabel(featureCombinationsTable.ix[tableRow,'feature2'])
        plt.title('AUC = %.4f' %(featureCombinationsTable.ix[tableRow,'AUC'])); plt.tight_layout()

Output Text:
------------


------------------------------------------------------------

================================================================================
================================================================================
psychology-of-a-professional-athlete.ipynb:
===========================================
------------------------------------------------------------
Cell index: 1
Input Cell Type: markdown
Input Text:
-----------
# Psychology of a Professional Athlete
In this script we explore all shot attempts of Kobe Bryant throughout his career and try to see if Kobe displays the "hot hand" effect

Output Text:
------------


------------------------------------------------------------
Cell index: 2
Input Cell Type: python
Input Text:
-----------
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
from matplotlib.patches import Circle, Rectangle, Arc
from sklearn import mixture
from sklearn import ensemble
from sklearn import model_selection
from sklearn.metrics import accuracy_score as accuracy
from sklearn.metrics import log_loss
import time
import itertools
import operator

Output Text:
------------


------------------------------------------------------------
Cell index: 3
Input Cell Type: markdown
Input Text:
-----------
## Load the data and create some useful fields
show the newly created fields as a sanity check

Output Text:
------------


------------------------------------------------------------
Cell index: 4
Input Cell Type: python
Input Text:
-----------
#%% load training data
allData = pd.read_csv('../input/data.csv')
data = allData[allData['shot_made_flag'].notnull()].reset_index()

#%% add some temporal columns to the data
data['game_date_DT'] = pd.to_datetime(data['game_date'])
data['dayOfWeek']    = data['game_date_DT'].dt.dayofweek
data['dayOfYear']    = data['game_date_DT'].dt.dayofyear

data['secondsFromPeriodEnd']   = 60*data['minutes_remaining']+data['seconds_remaining']
data['secondsFromPeriodStart'] = 60*(11-data['minutes_remaining'])+(60-data['seconds_remaining'])
data['secondsFromGameStart']   = (data['period'] <= 4).astype(int)*(data['period']-1)*12*60 + (data['period'] > 4).astype(int)*((data['period']-4)*5*60 + 3*12*60) + data['secondsFromPeriodStart']

# look at first couple of rows and verify that everything is good
data.loc[:10,['period','minutes_remaining','seconds_remaining','secondsFromGameStart']]

Output Text:
------------
    period  minutes_remaining  seconds_remaining  secondsFromGameStart
0        1                 10                 22                    98
1        1                  7                 45                   255
2        1                  6                 52                   308
3        2                  6                 19                  1061
4        3                  9                 32                  1588
5        3                  8                 52                  1628
6        3                  6                 12                  1788
7        3                  3                 36                  1944
8        3                  1                 56                  2044
9        1                 11                  0                    60
10       1                  7                  9                   291


------------------------------------------------------------
Cell index: 5
Input Cell Type: markdown
Input Text:
-----------
## Plot Shot Attempts as a function of time during the game
here we apply 3 different binnings of time and show the attempts as function from game start

Output Text:
------------


------------------------------------------------------------
Cell index: 6
Input Cell Type: python
Input Text:
-----------
# plot the shot attempts as a function of time (from start of game) with several different binnings
plt.rcParams['figure.figsize'] = (16, 16)
plt.rcParams['font.size'] = 16

binsSizes = [24,12,6]

plt.figure();
for k, binSizeInSeconds in enumerate(binsSizes):
    timeBins = np.arange(0,60*(4*12+3*5),binSizeInSeconds)+0.01
    attemptsAsFunctionOfTime, b = np.histogram(data['secondsFromGameStart'], bins=timeBins)     
    
    maxHeight = max(attemptsAsFunctionOfTime) + 30
    barWidth = 0.999*(timeBins[1]-timeBins[0])
    plt.subplot(len(binsSizes),1,k+1); 
    plt.bar(timeBins[:-1],attemptsAsFunctionOfTime, align='edge', width=barWidth); plt.title(str(binSizeInSeconds) + ' second time bins')
    plt.vlines(x=[0,12*60,2*12*60,3*12*60,4*12*60,4*12*60+5*60,4*12*60+2*5*60,4*12*60+3*5*60], ymin=0,ymax=maxHeight, colors='r')
    plt.xlim((-20,3200)); plt.ylim((0,maxHeight)); plt.ylabel('attempts')
plt.xlabel('time [seconds from start of game]')

Output Text:
------------
<matplotlib.text.Text at 0x7f9b44e39198>
<matplotlib.figure.Figure at 0x7f9b45a644e0>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents three histograms, stacked vertically, each depicting the frequency of attempts over time during a game.  The x-axis of all three histograms represents the time elapsed in seconds from the start of the game. The y-axis represents the number of attempts made within a specific time interval (bin).  The key difference between the histograms lies in the width of the time bins: the top histogram uses 24-second bins, the middle one 12-second bins, and the bottom one 6-second bins.

The histograms show a relatively consistent baseline level of attempts throughout the game, punctuated by several distinct peaks. These peaks represent periods of significantly higher activity or attempt frequency. The vertical red lines highlight the location of these peaks across all three histograms, demonstrating that these periods of high activity occur at roughly the same points in the game regardless of the bin size.  The increased resolution provided by the smaller time bins (6 seconds) in the bottom histogram allows for a more detailed view of the fluctuations in attempt frequency, revealing finer variations within the periods of high and low activity.  The larger bin sizes in the top two histograms provide a more smoothed-out representation, obscuring some of the smaller-scale fluctuations.


------------------------------------------------------------
Cell index: 7
Input Cell Type: markdown
Input Text:
-----------
### It looks like Kobe is entrusted to take the last shot of every period
it also looks like he's usually on the bench at the start of 2nd and 4th periods

Output Text:
------------


------------------------------------------------------------
Cell index: 8
Input Cell Type: markdown
Input Text:
-----------
## Plot Shot Accuracy as function of time during the game

Output Text:
------------


------------------------------------------------------------
Cell index: 9
Input Cell Type: python
Input Text:
-----------
#%% plot the accuracy as a function of time
plt.rcParams['figure.figsize'] = (15, 10)
plt.rcParams['font.size'] = 16

binSizeInSeconds = 20
timeBins = np.arange(0,60*(4*12+3*5),binSizeInSeconds)+0.01
attemptsAsFunctionOfTime,     b = np.histogram(data['secondsFromGameStart'], bins=timeBins)     
madeAttemptsAsFunctionOfTime, b = np.histogram(data.loc[data['shot_made_flag']==1,'secondsFromGameStart'], bins=timeBins)     
attemptsAsFunctionOfTime[attemptsAsFunctionOfTime < 1] = 1
accuracyAsFunctionOfTime = madeAttemptsAsFunctionOfTime.astype(float)/attemptsAsFunctionOfTime
accuracyAsFunctionOfTime[attemptsAsFunctionOfTime <= 50] = 0 # zero accuracy in bins that don't have enough samples

maxHeight = max(attemptsAsFunctionOfTime) + 30
barWidth = 0.999*(timeBins[1]-timeBins[0])

plt.figure();
plt.subplot(2,1,1); plt.bar(timeBins[:-1],attemptsAsFunctionOfTime, align='edge', width=barWidth); 
plt.xlim((-20,3200)); plt.ylim((0,maxHeight)); plt.ylabel('attempts'); plt.title(str(binSizeInSeconds) + ' second time bins')
plt.vlines(x=[0,12*60,2*12*60,3*12*60,4*12*60,4*12*60+5*60,4*12*60+2*5*60,4*12*60+3*5*60], ymin=0,ymax=maxHeight, colors='r')
plt.subplot(2,1,2); plt.bar(timeBins[:-1],accuracyAsFunctionOfTime, align='edge', width=barWidth); 
plt.xlim((-20,3200)); plt.ylabel('accuracy'); plt.xlabel('time [seconds from start of game]')
plt.vlines(x=[0,12*60,2*12*60,3*12*60,4*12*60,4*12*60+5*60,4*12*60+2*5*60,4*12*60+3*5*60], ymin=0.0,ymax=0.7, colors='r')

Output Text:
------------
<matplotlib.collections.LineCollection at 0x7f9b24d3ec18>
<matplotlib.figure.Figure at 0x7f9b6103c6a0>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a two-panel graph displaying data likely related to a game's progress over time. 


The top panel is a histogram showing the number of "attempts" made during the game, binned in 20-second intervals. The x-axis represents time in seconds from the game's start, and the y-axis represents the count of attempts. The histogram reveals fluctuations in attempt frequency across the game's duration. Several vertical red lines divide the histogram into sections, possibly indicating significant events or changes in gameplay.


The bottom panel is another histogram, depicting "accuracy" over time, also binned in 20-second intervals. Similar to the top panel, the x-axis represents time, and the y-axis indicates accuracy (likely ranging from 0 to 1 or 0% to 100%). The accuracy histogram shows relatively consistent performance with some minor fluctuations, again segmented by vertical red lines aligning with those in the top panel. The title "20 second time bins" confirms the consistent time interval used for both histograms. The consistent positioning of red lines in both graphs suggests they mark specific points of interest or changes in game conditions or player behavior.


------------------------------------------------------------
Cell index: 10
Input Cell Type: markdown
Input Text:
-----------
### Note that the accuracy of these "last second shots" is consisently lower than usuall
This is probably due to the fact that a large amonut of these shots are from very far away

Output Text:
------------


------------------------------------------------------------
Cell index: 11
Input Cell Type: markdown
Input Text:
-----------
## Now let's continue our initial exploration and examine the spatial location aspect of kobe's shots
we'll do this by building a **gaussian mixture model** that tries to summerize Kobe's shot locations compactly

Output Text:
------------


------------------------------------------------------------
Cell index: 12
Input Cell Type: python
Input Text:
-----------
#%% cluster the shot attempts of kobe using GMM on their location
numGaussians = 13
gaussianMixtureModel = mixture.GaussianMixture(n_components=numGaussians, covariance_type='full', 
                                               init_params='kmeans', n_init=50, 
                                               verbose=0, random_state=5)
gaussianMixtureModel.fit(data.loc[:,['loc_x','loc_y']])

# add the GMM cluster as a field in the dataset
data['shotLocationCluster'] = gaussianMixtureModel.predict(data.loc[:,['loc_x','loc_y']])

Output Text:
------------


------------------------------------------------------------
Cell index: 13
Input Cell Type: markdown
Input Text:
-----------
## Define some helper functions
the function **draw_court()** is shamelessly stolen from **[MichaelKrueger](https://www.kaggle.com/bbx396)**'s excelent [script](https://www.kaggle.com/bbx396/kobechart))

Output Text:
------------


------------------------------------------------------------
Cell index: 14
Input Cell Type: python
Input Text:
-----------
#%% define draw functions (stealing shamelessly the draw_court() function from MichaelKrueger's excelent script)

def draw_court(ax=None, color='black', lw=2, outer_lines=False):
    # If an axes object isn't provided to plot onto, just get current one
    if ax is None:
        ax = plt.gca()

    # Create the various parts of an NBA basketball court

    # Create the basketball hoop
    # Diameter of a hoop is 18" so it has a radius of 9", which is a value
    # 7.5 in our coordinate system
    hoop = Circle((0, 0), radius=7.5, linewidth=lw, color=color, fill=False)

    # Create backboard
    backboard = Rectangle((-30, -7.5), 60, -1, linewidth=lw, color=color)

    # The paint
    # Create the outer box 0f the paint, width=16ft, height=19ft
    outer_box = Rectangle((-80, -47.5), 160, 190, linewidth=lw, color=color,
                          fill=False)
    # Create the inner box of the paint, widt=12ft, height=19ft
    inner_box = Rectangle((-60, -47.5), 120, 190, linewidth=lw, color=color,
                          fill=False)

    # Create free throw top arc
    top_free_throw = Arc((0, 142.5), 120, 120, theta1=0, theta2=180,
                         linewidth=lw, color=color, fill=False)
    # Create free throw bottom arc
    bottom_free_throw = Arc((0, 142.5), 120, 120, theta1=180, theta2=0,
                            linewidth=lw, color=color, linestyle='dashed')
    # Restricted Zone, it is an arc with 4ft radius from center of the hoop
    restricted = Arc((0, 0), 80, 80, theta1=0, theta2=180, linewidth=lw,
                     color=color)

    # Three point line
    # Create the side 3pt lines, they are 14ft long before they begin to arc
    corner_three_a = Rectangle((-220, -47.5), 0, 140, linewidth=lw,
                               color=color)
    corner_three_b = Rectangle((220, -47.5), 0, 140, linewidth=lw, color=color)
    # 3pt arc - center of arc will be the hoop, arc is 23'9" away from hoop
    # I just played around with the theta values until they lined up with the 
    # threes
    three_arc = Arc((0, 0), 475, 475, theta1=22, theta2=158, linewidth=lw,
                    color=color)

    # Center Court
    center_outer_arc = Arc((0, 422.5), 120, 120, theta1=180, theta2=0,
                           linewidth=lw, color=color)
    center_inner_arc = Arc((0, 422.5), 40, 40, theta1=180, theta2=0,
                           linewidth=lw, color=color)

    # List of the court elements to be plotted onto the axes
    court_elements = [hoop, backboard, outer_box, inner_box, top_free_throw,
                      bottom_free_throw, restricted, corner_three_a,
                      corner_three_b, three_arc, center_outer_arc,
                      center_inner_arc]

    if outer_lines:
        # Draw the half court line, baseline and side out bound lines
        outer_lines = Rectangle((-250, -47.5), 500, 470, linewidth=lw,
                                color=color, fill=False)
        court_elements.append(outer_lines)

    # Add the court elements onto the axes
    for element in court_elements:
        ax.add_patch(element)

    return ax

def Draw2DGaussians(gaussianMixtureModel, ellipseColors, ellipseTextMessages):
    
    fig, h = plt.subplots();
    for i, (mean, covarianceMatrix) in enumerate(zip(gaussianMixtureModel.means_, gaussianMixtureModel.covariances_)):
        # get the eigen vectors and eigen values of the covariance matrix
        v, w = np.linalg.eigh(covarianceMatrix)
        v = 2.5*np.sqrt(v) # go to units of standard deviation instead of variance
        
        # calculate the ellipse angle and two axis length and draw it
        u = w[0] / np.linalg.norm(w[0])    
        angle = np.arctan(u[1] / u[0])
        angle = 180 * angle / np.pi  # convert to degrees
        currEllipse = mpl.patches.Ellipse(mean, v[0], v[1], 180 + angle, color=ellipseColors[i])
        currEllipse.set_alpha(0.5)
        h.add_artist(currEllipse)
        h.text(mean[0]+7, mean[1]-1, ellipseTextMessages[i], fontsize=13, color='blue')


Output Text:
------------


------------------------------------------------------------
Cell index: 15
Input Cell Type: markdown
Input Text:
-----------
## Draw the 2D Gaussians of Shot Attempts
Each elipse is the countour that represents 2.5 standard deviations away from the center of the gaussian
Each number in blue represents the precent of shots taken from this gaussian out of all shots

Output Text:
------------


------------------------------------------------------------
Cell index: 16
Input Cell Type: python
Input Text:
-----------
#%% show gaussian mixture elipses of shot attempts
plt.rcParams['figure.figsize'] = (13, 10)
plt.rcParams['font.size'] = 15

ellipseTextMessages = [str(100*gaussianMixtureModel.weights_[x])[:4]+'%' for x in range(numGaussians)]
ellipseColors = ['red','green','purple','cyan','magenta','yellow','blue','orange','silver','maroon','lime','olive','brown','darkblue']
Draw2DGaussians(gaussianMixtureModel, ellipseColors, ellipseTextMessages)
draw_court(outer_lines=True); plt.ylim(-60,440); plt.xlim(270,-270); plt.title('shot attempts')

Output Text:
------------
<matplotlib.text.Text at 0x7f9b24559208>
<matplotlib.figure.Figure at 0x7f9b468ac978>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a shot chart depicting the distribution of shot attempts on a basketball court. It uses a court diagram as its backdrop, overlaid with several ellipses of varying sizes, colors, and transparency. Each ellipse represents a zone on the court where shots were attempted, with the size and color intensity reflecting the percentage of total shots taken from that specific area. 


The ellipses are not uniformly distributed; some cluster around the basket, indicating a higher frequency of attempts near the hoop, while others are further out, representing shots from longer ranges.  The percentages displayed within each ellipse indicate the proportion of total shots taken from that zone. The court lines are visible, as is the three-point arc, allowing for a visual understanding of shot location relative to the court's structure.  The overall visual effect is a heatmap-like representation of shooting tendencies, easily showing high-frequency versus low-frequency shot areas.


------------------------------------------------------------
Cell index: 17
Input Cell Type: markdown
Input Text:
-----------
We can see that Kobe is making **more attempts** from the **left side** of the court (or right side from his point of view). this is probably because he's **right handed**.

Also, we can see that a huge number of attempts (16.8%) is from directly under the basket, and 5.06% additinal attemps are from very close to the basket


Output Text:
------------


------------------------------------------------------------
Cell index: 18
Input Cell Type: markdown
Input Text:
-----------
## Show the Scatter Plot of all Kobe's shot attempts colored by the cluster assignment according to the GMM

Just to make sure the gaussian model actually captures something

Output Text:
------------


------------------------------------------------------------
Cell index: 19
Input Cell Type: python
Input Text:
-----------
#%% just to make sure the gaussian model actually captures something, show the scatter and cluster assignment
plt.rcParams['figure.figsize'] = (13, 10)
plt.rcParams['font.size'] = 15

plt.figure(); draw_court(outer_lines=True); plt.ylim(-60,440); plt.xlim(270,-270); plt.title('cluser assignment')
plt.scatter(x=data['loc_x'],y=data['loc_y'],c=data['shotLocationCluster'],s=40,cmap='hsv',alpha=0.1)

Output Text:
------------
<matplotlib.collections.PathCollection at 0x7f9b45d8fc88>
<matplotlib.figure.Figure at 0x7f9b45e52588>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image displays a scatter plot overlaid on a basketball court diagram.  The scatter plot represents numerous data points, each colored according to its assigned cluster.  Multiple colors are used, indicating a clustering algorithm has grouped the data points into distinct categories.  The colors are vibrant and somewhat translucent, allowing for the visualization of data point density within each cluster.  Some clusters appear more densely populated than others.

The basketball court overlay provides context to the data points' spatial distribution.  The court lines, including the three-point arc, free-throw line, and restricted area, are clearly visible.  The court's orientation is standard, with the basket at the bottom of the image. The data points seem to be related to shot locations on the court, as they are primarily concentrated within the half-court area.  The density of points varies across the court, suggesting patterns in shot selection or success rates.

The title "cluser assignment" indicates the plot's purpose is to visually represent the outcome of a clustering algorithm applied to the shot data.  The plot's structure and content effectively communicate the clustering results and their spatial distribution within the context of a basketball court, facilitating the understanding of patterns in shot location and potentially shot outcomes.


------------------------------------------------------------
Cell index: 20
Input Cell Type: markdown
Input Text:
-----------
It doesn't seem perfect, but definatly captures some interesting things about the data
for example, we can see that the large and very far away cluster is capturing all of the very distant shots

Output Text:
------------


------------------------------------------------------------
Cell index: 21
Input Cell Type: markdown
Input Text:
-----------
## Plot the Shot Accuracy of each Gaussian Cluster 
Each blue number here will represent the accuracy of the shots taken from this cluster so we can get a feel for what are easy and what are difficult shots

Output Text:
------------


------------------------------------------------------------
Cell index: 22
Input Cell Type: python
Input Text:
-----------
#%% for each cluster, calculate it's individual accuracy and plot it
plt.rcParams['figure.figsize'] = (13, 10)
plt.rcParams['font.size'] = 15

variableCategories = data['shotLocationCluster'].value_counts().index.tolist()

clusterAccuracy = {}
for category in variableCategories:
    shotsAttempted = np.array(data['shotLocationCluster'] == category).sum()
    shotsMade = np.array(data.loc[data['shotLocationCluster'] == category,'shot_made_flag'] == 1).sum()
    clusterAccuracy[category] = float(shotsMade)/shotsAttempted

ellipseTextMessages = [str(100*clusterAccuracy[x])[:4]+'%' for x in range(numGaussians)]
Draw2DGaussians(gaussianMixtureModel, ellipseColors, ellipseTextMessages)
draw_court(outer_lines=True); plt.ylim(-60,440); plt.xlim(270,-270); plt.title('shot accuracy')

Output Text:
------------
<matplotlib.text.Text at 0x7f9b45d7e0f0>
<matplotlib.figure.Figure at 0x7f9b45da0d68>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a shot chart visualizing basketball shot accuracy. It displays a basketball court, overlaid with ellipsoidal regions representing different shooting zones. Each zone is colored differently and has a percentage value indicating the shooting accuracy within that area. The percentages are displayed in blue text within each colored zone.

The court is oriented with the basket at the center, and the zones extend outwards, encompassing various distances and angles from the basket.  The sizes and shapes of the ellipses suggest varying degrees of shot success rates, with smaller, darker areas representing higher accuracy and larger, lighter areas representing lower accuracy.  The key is implicit; the color of the ellipse correlates to the success rate of shots taken from that area. The inner zones around the basket show higher percentages, while the outer zones, especially those at the corners, show lower percentages. The chart offers a visual representation of a player's or team's shooting performance, highlighting strengths and weaknesses based on shot location.


------------------------------------------------------------
Cell index: 23
Input Cell Type: markdown
Input Text:
-----------
### We can clearly see the dependence between distance and accuracy
Another interesting fact is that Kobe not only makes more attempts from the right side (from his point of view), but also he's better at making those attempts

Output Text:
------------


------------------------------------------------------------
Cell index: 24
Input Cell Type: markdown
Input Text:
-----------
## Now let's Plot a 2D Spatio-Temporal plot of Kobe's career

* on **x-axis** there will be **time** since start of game
* on **y-axis** there will be the **cluster index** Kobe made the shot (sorted by the cluster accuracy)
* the **intensity** will be the **number of attempts** by Kobe from that particular cluster at that particular time
* red verticle lines are the end of each period

Output Text:
------------


------------------------------------------------------------
Cell index: 25
Input Cell Type: python
Input Text:
-----------
#%% plot a 2-d spatio-temporal histogram of kobe's games during his entire carrer
plt.rcParams['figure.figsize'] = (18, 10)
plt.rcParams['font.size'] = 18

# sort the clusters according to their accuracy
sortedClustersByAccuracyTuple = sorted(clusterAccuracy.items(), key=operator.itemgetter(1),reverse=True)
sortedClustersByAccuracy = [x[0] for x in sortedClustersByAccuracyTuple]

binSizeInSeconds = 12
timeInUnitsOfBins = ((data['secondsFromGameStart']+0.0001)/binSizeInSeconds).astype(int)
locationInUintsOfClusters = np.array([sortedClustersByAccuracy.index(data.loc[x,'shotLocationCluster']) for x in range(data.shape[0])])

# build a spatio-temporal histogram of Kobe's games
shotAttempts = np.zeros((gaussianMixtureModel.n_components,1+max(timeInUnitsOfBins)))
for shot in range(data.shape[0]):
    shotAttempts[locationInUintsOfClusters[shot],timeInUnitsOfBins[shot]] += 1

# make the y-axis have larger area so it will be more visible 
shotAttempts = np.kron(shotAttempts,np.ones((5,1)))
# the locations of the period ends
vlinesList = 0.5001+np.array([0,12*60,2*12*60,3*12*60,4*12*60,4*12*60+5*60]).astype(int)/binSizeInSeconds

plt.figure(figsize=(13,8))
plt.imshow(shotAttempts, cmap='copper',interpolation="nearest"); plt.xlim(0,float(4*12*60+6*60)/binSizeInSeconds);
plt.vlines(x=vlinesList, ymin=-0.5,ymax=shotAttempts.shape[0]-0.5, colors='r');
plt.xlabel('time from start of game [sec]'); plt.ylabel('cluster (sorted by accuracy)')

Output Text:
------------
<matplotlib.text.Text at 0x7f9b1c33ef98>
<matplotlib.figure.Figure at 0x7f9b1c7ef860>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a heatmap visualizing data likely related to clustering accuracy over time.  The vertical axis represents clusters, sorted by some measure of accuracy (higher accuracy clusters are at the top). The horizontal axis depicts time, presumably in seconds, from the start of a game or process.  The color intensity of each cell in the heatmap represents a value, probably indicating the accuracy or strength of a specific cluster at a particular point in time. Darker colors suggest lower values, while lighter colors (browns and tans) represent higher values.

The heatmap shows a pattern of fluctuating accuracy across different clusters over time.  There is not a uniform distribution of accuracy; some clusters maintain relatively high accuracy throughout, while others show significant variation.  Several vertical red lines are superimposed on the heatmap, potentially indicating specific events or time points of interest during the game or process. These lines seem to coincide with periods of lower overall cluster accuracy, suggesting that those times might correspond to significant changes or disruptions.  The overall appearance suggests a dynamic system where cluster accuracy is not static but changes in response to events or time progression.


------------------------------------------------------------
Cell index: 26
Input Cell Type: markdown
Input Text:
-----------
The clusters are sorted in descending order of accuracy. under the basek high accuracy shots are at the top, and low accuracy shots from half court are at the bottom
### We can now see that the "last second shots" in the 1st, 2nd and 3rd periods were indeed "hopeless shots" from very far away
It's interesting to note, however, that in the 4th period, the last second shot don't belong to the "hopeless" cluster, but rather to the regular 3-pointer clusters (which are still much more difficult, but not hopeless)


Output Text:
------------


------------------------------------------------------------
Cell index: 27
Input Cell Type: markdown
Input Text:
-----------
## For later analysis, we'll want to assess shot difficulty based on shot properties
(such as shot type and shot distance)

Output Text:
------------


------------------------------------------------------------
Cell index: 28
Input Cell Type: python
Input Text:
-----------
#%% create a new table for shot difficulty model
def FactorizeCategoricalVariable(inputDB,categoricalVarName):
    opponentCategories = inputDB[categoricalVarName].value_counts().index.tolist()
    
    outputDB = pd.DataFrame()
    for category in opponentCategories:
        featureName = categoricalVarName + ': ' + str(category)
        outputDB[featureName] = (inputDB[categoricalVarName] == category).astype(int)

    return outputDB

featuresDB = pd.DataFrame()
featuresDB['homeGame'] = data['matchup'].apply(lambda x: 1 if (x.find('@') < 0) else 0)
featuresDB = pd.concat([featuresDB,FactorizeCategoricalVariable(data,'opponent')],axis=1)
featuresDB = pd.concat([featuresDB,FactorizeCategoricalVariable(data,'action_type')],axis=1)
featuresDB = pd.concat([featuresDB,FactorizeCategoricalVariable(data,'shot_type')],axis=1)
featuresDB = pd.concat([featuresDB,FactorizeCategoricalVariable(data,'combined_shot_type')],axis=1)
featuresDB = pd.concat([featuresDB,FactorizeCategoricalVariable(data,'shot_zone_basic')],axis=1)
featuresDB = pd.concat([featuresDB,FactorizeCategoricalVariable(data,'shot_zone_area')],axis=1)
featuresDB = pd.concat([featuresDB,FactorizeCategoricalVariable(data,'shot_zone_range')],axis=1)
featuresDB = pd.concat([featuresDB,FactorizeCategoricalVariable(data,'shotLocationCluster')],axis=1)

featuresDB['playoffGame']          = data['playoffs']
featuresDB['locX']                 = data['loc_x']
featuresDB['locY']                 = data['loc_y']
featuresDB['distanceFromBasket']   = data['shot_distance']
featuresDB['secondsFromPeriodEnd'] = data['secondsFromPeriodEnd']

featuresDB['dayOfWeek_cycX']  = np.sin(2*np.pi*(data['dayOfWeek']/7))
featuresDB['dayOfWeek_cycY']  = np.cos(2*np.pi*(data['dayOfWeek']/7))
featuresDB['timeOfYear_cycX'] = np.sin(2*np.pi*(data['dayOfYear']/365))
featuresDB['timeOfYear_cycY'] = np.cos(2*np.pi*(data['dayOfYear']/365))

labelsDB = data['shot_made_flag']

Output Text:
------------


------------------------------------------------------------
Cell index: 29
Input Cell Type: markdown
Input Text:
-----------
## Build a model based on featuresDB table, and make sure it doesn't overfit 
(i.e. the training error and the test error are the same)
#### Use an ExtraTreesClassifier for that

Output Text:
------------


------------------------------------------------------------
Cell index: 30
Input Cell Type: python
Input Text:
-----------
#%% build a simple model and make sure it doesnt overfit
randomSeed = 1
numFolds   = 4

stratifiedCV = model_selection.StratifiedKFold(n_splits=numFolds, shuffle=True, random_state=randomSeed)

mainLearner = ensemble.ExtraTreesClassifier(n_estimators=500, max_depth=5, 
                                            min_samples_leaf=120, max_features=120, 
                                            criterion='entropy', bootstrap=False, 
                                            n_jobs=-1, random_state=randomSeed)

startTime = time.time()
trainAccuracy = []; validAccuracy = [];
trainLogLosses = []; validLogLosses = []
for trainInds, validInds in stratifiedCV.split(featuresDB, labelsDB):
    # split to train and valid sets
    X_train_CV = featuresDB.iloc[trainInds,:]
    y_train_CV = labelsDB.iloc[trainInds]
    X_valid_CV = featuresDB.iloc[validInds,:]
    y_valid_CV = labelsDB.iloc[validInds]
    
    # train learner
    mainLearner.fit(X_train_CV, y_train_CV)
    
    # make predictions
    y_train_hat_mainLearner = mainLearner.predict_proba(X_train_CV)[:,1]
    y_valid_hat_mainLearner = mainLearner.predict_proba(X_valid_CV)[:,1]

    # store results
    trainAccuracy.append(accuracy(y_train_CV, y_train_hat_mainLearner > 0.5))
    validAccuracy.append(accuracy(y_valid_CV, y_valid_hat_mainLearner > 0.5))
    trainLogLosses.append(log_loss(y_train_CV, y_train_hat_mainLearner))
    validLogLosses.append(log_loss(y_valid_CV, y_valid_hat_mainLearner))

print("-----------------------------------------------------")
print("total (train,valid) Accuracy = (%.5f,%.5f). took %.2f minutes" % (np.mean(trainAccuracy),np.mean(validAccuracy), (time.time()-startTime)/60))
print("total (train,valid) Log Loss = (%.5f,%.5f). took %.2f minutes" % (np.mean(trainLogLosses),np.mean(validLogLosses), (time.time()-startTime)/60))
print("-----------------------------------------------------")

Output Text:
------------
-----------------------------------------------------
total (train,valid) Accuracy = (0.67908,0.67876). took 0.48 minutes
total (train,valid) Log Loss = (0.60812,0.61069). took 0.48 minutes
-----------------------------------------------------


------------------------------------------------------------
Cell index: 31
Input Cell Type: markdown
Input Text:
-----------
### Use the model to add a "shotDifficulty" field to every original shot entry
(which is actually the predicted probability of making the shot. meaning, the name is a bit confusing right now)



Output Text:
------------


------------------------------------------------------------
Cell index: 32
Input Cell Type: python
Input Text:
-----------
mainLearner.fit(featuresDB, labelsDB)
data['shotDifficulty'] = mainLearner.predict_proba(featuresDB)[:,1]

Output Text:
------------


------------------------------------------------------------
Cell index: 33
Input Cell Type: markdown
Input Text:
-----------
## Get a feel for the important features of this model
look at the feature importances according to ET Classifier

Output Text:
------------


------------------------------------------------------------
Cell index: 34
Input Cell Type: python
Input Text:
-----------
# just to get a feel for what determins shot difficulty, look at feature importances
featureInds = mainLearner.feature_importances_.argsort()[::-1]
featureImportance = pd.DataFrame(np.concatenate((featuresDB.columns[featureInds,None], mainLearner.feature_importances_[featureInds,None]), axis=1),
                                  columns=['featureName', 'importanceET'])

featureImportance.iloc[:30,:]

Output Text:
------------
                         featureName importanceET
0             action_type: Jump Shot     0.578965
1            action_type: Layup Shot     0.174299
2           combined_shot_type: Dunk     0.113124
3                           homeGame    0.0291337
4             action_type: Dunk Shot    0.0163483
5             shotLocationCluster: 9    0.0145711
6          combined_shot_type: Layup   0.00994704
7                 distanceFromBasket   0.00767911
8         shot_zone_range: 16-24 ft.   0.00767593
9        action_type: Slam Dunk Shot   0.00676747
10     combined_shot_type: Jump Shot    0.0059042
11    action_type: Running Jump Shot   0.00583313
12              secondsFromPeriodEnd   0.00527622
13           shotLocationCluster: 11   0.00452905
14                              locY   0.00378569
15  shot_zone_range: Less Than 8 ft.   0.00328511
16      combined_shot_type: Tip Shot   0.00242683
17   action_type: Driving Layup Shot   0.00193343
18         shot_zone_area: Center(C)   0.00110972
19                     opponent: DEN  0.000946986
20    action_type: Driving Dunk Shot  0.000829966
21        shot_zone_basic: Mid-Range  0.000478361
22            shotLocationCluster: 2  0.000477974
23                   timeOfYear_cycX  0.000421631
24     action_type: Pullup Jump shot  0.000418792
25                    dayOfWeek_cycX  0.000406658
26  shot_zone_basic: Restricted Area  0.000397086
27         shot_zone_range: 8-16 ft.  0.000279413
28             action_type: Tip Shot  0.000278152
29            shotLocationCluster: 8  0.000252861


------------------------------------------------------------
Cell index: 35
Input Cell Type: markdown
Input Text:
-----------
# We would like to asses some aspects of the decision making process of Kobe Bryant
### For that we will collect two distinct groups of shots and analyse the differences between them:

1. The shots that came right **after a sucessful shot** attempt
1. The shots that came right **after a missed shot** attempt

Output Text:
------------


------------------------------------------------------------
Cell index: 36
Input Cell Type: python
Input Text:
-----------
#%% collect data given that kobe made or missed last shot
timeBetweenShotsDict = {}
timeBetweenShotsDict['madeLast']   = []
timeBetweenShotsDict['missedLast'] = []

changeInDistFromBasketDict = {}
changeInDistFromBasketDict['madeLast']   = []
changeInDistFromBasketDict['missedLast'] = []

changeInShotDifficultyDict = {}
changeInShotDifficultyDict['madeLast']   = []
changeInShotDifficultyDict['missedLast'] = []

afterMadeShotsList   = []
afterMissedShotsList = []

for shot in range(1,data.shape[0]):

    # make sure the current shot and last shot were all in the same period of the same game
    sameGame   = data.loc[shot,'game_date'] == data.loc[shot-1,'game_date']
    samePeriod = data.loc[shot,'period']    == data.loc[shot-1,'period']

    if samePeriod and sameGame:
        madeLastShot       = data.loc[shot-1,'shot_made_flag'] == 1
        missedLastShot     = data.loc[shot-1,'shot_made_flag'] == 0
        
        timeDifferenceFromLastShot = data.loc[shot,'secondsFromGameStart']     - data.loc[shot-1,'secondsFromGameStart']
        distDifferenceFromLastShot = data.loc[shot,'shot_distance']            - data.loc[shot-1,'shot_distance']
        shotDifficultyDifferenceFromLastShot = data.loc[shot,'shotDifficulty'] - data.loc[shot-1,'shotDifficulty']

        # check for currupt data points (assuming all samples should have been chronologically ordered)
        if timeDifferenceFromLastShot < 0:
            continue
        
        if madeLastShot:
            timeBetweenShotsDict['madeLast'].append(timeDifferenceFromLastShot)
            changeInDistFromBasketDict['madeLast'].append(distDifferenceFromLastShot)
            changeInShotDifficultyDict['madeLast'].append(shotDifficultyDifferenceFromLastShot)
            afterMadeShotsList.append(shot)
            
        if missedLastShot:
            timeBetweenShotsDict['missedLast'].append(timeDifferenceFromLastShot)
            changeInDistFromBasketDict['missedLast'].append(distDifferenceFromLastShot)
            changeInShotDifficultyDict['missedLast'].append(shotDifficultyDifferenceFromLastShot)
            afterMissedShotsList.append(shot)

afterMissedData = data.iloc[afterMissedShotsList,:]
afterMadeData   = data.iloc[afterMadeShotsList,:]

shotChancesListAfterMade = afterMadeData['shotDifficulty'].tolist()
totalAttemptsAfterMade   = afterMadeData.shape[0]
totalMadeAfterMade       = np.array(afterMadeData['shot_made_flag'] == 1).sum()

shotChancesListAfterMissed = afterMissedData['shotDifficulty'].tolist()
totalAttemptsAfterMissed   = afterMissedData.shape[0]
totalMadeAfterMissed       = np.array(afterMissedData['shot_made_flag'] == 1).sum()

Output Text:
------------


------------------------------------------------------------
Cell index: 37
Input Cell Type: markdown
Input Text:
-----------
## Plot histogram of "Time Since Last Shot Attempt" for the two groups

Output Text:
------------


------------------------------------------------------------
Cell index: 38
Input Cell Type: python
Input Text:
-----------
#%% after making a shot, kobe wants more
plt.rcParams['figure.figsize'] = (13, 10)

jointHist, timeBins = np.histogram(timeBetweenShotsDict['madeLast']+timeBetweenShotsDict['missedLast'],bins=200)
barWidth = 0.999*(timeBins[1]-timeBins[0])

timeDiffHist_GivenMadeLastShot, b = np.histogram(timeBetweenShotsDict['madeLast'],bins=timeBins)
timeDiffHist_GivenMissedLastShot, b = np.histogram(timeBetweenShotsDict['missedLast'],bins=timeBins)
maxHeight = max(max(timeDiffHist_GivenMadeLastShot),max(timeDiffHist_GivenMissedLastShot)) + 30

plt.figure();
plt.subplot(2,1,1); plt.bar(timeBins[:-1], timeDiffHist_GivenMadeLastShot, width=barWidth); plt.xlim((0,500)); plt.ylim((0,maxHeight))
plt.title('made last shot'); plt.ylabel('counts')
plt.subplot(2,1,2); plt.bar(timeBins[:-1], timeDiffHist_GivenMissedLastShot, width=barWidth); plt.xlim((0,500)); plt.ylim((0,maxHeight))
plt.title('missed last shot'); plt.xlabel('time since last shot'); plt.ylabel('counts')

Output Text:
------------
<matplotlib.text.Text at 0x7f9add724668>
<matplotlib.figure.Figure at 0x7f9b45e3b0f0>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image contains two histograms displayed vertically, one above the other.  Both histograms share a similar structure: they plot the frequency (counts) of an event on the y-axis and the time since the last shot on the x-axis. The x-axis ranges from 0 to 500, likely representing seconds or some other unit of time.  The y-axis shows the number of occurrences, ranging from 0 to 500.

The top histogram is titled "made last shot," and depicts the distribution of time intervals between shots that were successfully made. The distribution is heavily skewed to the left, indicating that a large number of successful shots were followed by subsequent shots within a relatively short time period. There's a long tail to the right, showing a smaller number of instances where a significant amount of time elapsed before the next successful shot.

The bottom histogram, titled "missed last shot," shows the distribution of time intervals between shots where the previous shot was missed. This distribution also exhibits a right skew, but the peak is slightly lower and shifted to the left compared to the "made last shot" histogram.  This suggests a slightly different pattern in timing between missed shots compared to successful shots.  The overall shape suggests a similar trend, with a higher frequency of shorter intervals and a decreasing frequency of longer intervals.


------------------------------------------------------------
Cell index: 39
Input Cell Type: markdown
Input Text:
-----------
It looks like after making a shot, kobe is a little bit more eager to throw the next shot
### To everyone who is wondering about why is there a "silent period" after a made shot:
it's most likely because the ball is transfered to the other team after a sucesfull shot and it takes some time to get the ball back

Output Text:
------------


------------------------------------------------------------
Cell index: 40
Input Cell Type: markdown
Input Text:
-----------
## To better visualize this difference between the histograms, let's look at cumulative histograms

Output Text:
------------


------------------------------------------------------------
Cell index: 41
Input Cell Type: python
Input Text:
-----------
#%% to make the difference clearer, show the cumulative histogram
plt.rcParams['figure.figsize'] = (13, 6)

timeDiffCumHist_GivenMadeLastShot = np.cumsum(timeDiffHist_GivenMadeLastShot).astype(float)
timeDiffCumHist_GivenMadeLastShot = timeDiffCumHist_GivenMadeLastShot/max(timeDiffCumHist_GivenMadeLastShot)
timeDiffCumHist_GivenMissedLastShot = np.cumsum(timeDiffHist_GivenMissedLastShot).astype(float)
timeDiffCumHist_GivenMissedLastShot = timeDiffCumHist_GivenMissedLastShot/max(timeDiffCumHist_GivenMissedLastShot)

maxHeight = max(timeDiffCumHist_GivenMadeLastShot[-1],timeDiffCumHist_GivenMissedLastShot[-1])

plt.figure();
madePrev = plt.plot(timeBins[:-1], timeDiffCumHist_GivenMadeLastShot, label='made Prev'); plt.xlim((0,500))
missedPrev = plt.plot(timeBins[:-1], timeDiffCumHist_GivenMissedLastShot, label='missed Prev'); plt.xlim((0,500)); plt.ylim((0,1))
plt.title('cumulative density function - CDF'); plt.xlabel('time since last shot'); plt.legend(loc='lower right')

Output Text:
------------
<matplotlib.legend.Legend at 0x7f9add1fc908>
<matplotlib.figure.Figure at 0x7f9b2453c748>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a graph depicting cumulative density functions (CDFs) for two categories: "made Prev" and "missed Prev." The x-axis represents the "time since last shot," ranging from 0 to 500 units. The y-axis represents the cumulative probability, ranging from 0 to 1.

Both CDFs are S-shaped curves, typical of cumulative distributions. The "made Prev" curve lies consistently above the "missed Prev" curve, indicating that, across all time intervals since the last shot, the cumulative probability of events categorized as "made Prev" is higher than that of "missed Prev." This suggests a possible correlation between the time since the last shot and the probability of making or missing the current shot.  The curves flatten out towards a probability of 1, indicating that nearly all events are eventually included in the cumulative distribution within the observed time range.  The difference between the two curves suggests that the time since the last shot impacts the likelihood of success.


------------------------------------------------------------
Cell index: 42
Input Cell Type: markdown
Input Text:
-----------
## Plot histogram of "Current Shot Distance - Previous Shot Distance" for the two groups
Note that if Kobe throws from close by, and then from far away, this will result in positive values of "curr shot distance - prev shot distance". and vise versa - If Kobe throws from far away and then from close by, this will result in negative values.

Output Text:
------------


------------------------------------------------------------
Cell index: 43
Input Cell Type: python
Input Text:
-----------
#%% after making a shot, kobe is a more confident and throws from further away
plt.rcParams['figure.figsize'] = (13, 10)

jointHist, distDiffBins = np.histogram(changeInDistFromBasketDict['madeLast']+changeInDistFromBasketDict['missedLast'],bins=100,density=False)
barWidth = 0.999*(distDiffBins[1]-distDiffBins[0])

distDiffHist_GivenMadeLastShot,   b = np.histogram(changeInDistFromBasketDict['madeLast'],bins=distDiffBins)
distDiffHist_GivenMissedLastShot, b = np.histogram(changeInDistFromBasketDict['missedLast'],bins=distDiffBins)
maxHeight = max(max(distDiffHist_GivenMadeLastShot),max(distDiffHist_GivenMissedLastShot)) + 30

plt.figure();
plt.subplot(2,1,1); plt.bar(distDiffBins[:-1], distDiffHist_GivenMadeLastShot, width=barWidth); plt.xlim((-40,40)); plt.ylim((0,maxHeight))
plt.title('made last shot'); plt.ylabel('counts')
plt.subplot(2,1,2); plt.bar(distDiffBins[:-1], distDiffHist_GivenMissedLastShot, width=barWidth); plt.xlim((-40,40)); plt.ylim((0,maxHeight))
plt.title('missed last shot'); plt.xlabel('curr shot distance - prev shot distance'); plt.ylabel('counts')

Output Text:
------------
<matplotlib.text.Text at 0x7f9adcfdd470>
<matplotlib.figure.Figure at 0x7f9b1c7ef588>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image contains a figure showing two histograms, arranged vertically.  Each histogram displays the distribution of a numerical variable, described as "curr shot distance - prev shot distance".  The x-axis represents this difference in distance, ranging from approximately -40 to +40 units. The y-axis represents the counts or frequency of occurrences within each bin of the histogram. The histograms have many bins, suggesting a relatively fine-grained analysis of the data distribution.

The top histogram is titled "made last shot", indicating that the data plotted shows the difference in shot distance between consecutive shots where the previous shot was successful. The bottom histogram is titled "missed last shot", implying that the data represents the difference in shot distances when the previous shot was unsuccessful.  Both histograms show a roughly unimodal distribution, with a peak near zero, suggesting a tendency for consecutive shots to have similar distances. However, there are differences in the spread of the distributions; the "made last shot" histogram appears to have a slightly narrower spread than the "missed last shot" histogram.  Both histograms have tails extending toward more significant differences in distances.


------------------------------------------------------------
Cell index: 44
Input Cell Type: markdown
Input Text:
-----------
We can clearly see that the made group of shots is more leaning to the right
### It therefore looks like Kobe is more confident after making a shot, and because of it, he takes a larger risk and throws from further away
This is even more evident than the previous plot, but let's plot the cumulative histograms again to make it clearer

Output Text:
------------


------------------------------------------------------------
Cell index: 45
Input Cell Type: python
Input Text:
-----------
#%% to make the difference clearer, show the cumulative histogram
plt.rcParams['figure.figsize'] = (13, 6)

distDiffCumHist_GivenMadeLastShot = np.cumsum(distDiffHist_GivenMadeLastShot).astype(float)
distDiffCumHist_GivenMadeLastShot = distDiffCumHist_GivenMadeLastShot/max(distDiffCumHist_GivenMadeLastShot)
distDiffCumHist_GivenMissedLastShot = np.cumsum(distDiffHist_GivenMissedLastShot).astype(float)
distDiffCumHist_GivenMissedLastShot = distDiffCumHist_GivenMissedLastShot/max(distDiffCumHist_GivenMissedLastShot)

maxHeight = max(distDiffCumHist_GivenMadeLastShot[-1],distDiffCumHist_GivenMissedLastShot[-1])

plt.figure();
madePrev = plt.plot(distDiffBins[:-1], distDiffCumHist_GivenMadeLastShot, label='made Prev'); plt.xlim((-40,40))
missedPrev = plt.plot(distDiffBins[:-1], distDiffCumHist_GivenMissedLastShot, label='missed Prev'); plt.xlim((-40,40)); plt.ylim((0,1))
plt.title('cumulative density function - CDF'); plt.xlabel('curr shot distance - prev shot distance'); plt.legend(loc='lower right')

Output Text:
------------
<matplotlib.legend.Legend at 0x7f9adcce2a58>
<matplotlib.figure.Figure at 0x7f9add21aa58>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a graph depicting cumulative density functions (CDFs) for two categories: "made Prev" and "missed Prev".  The x-axis represents the difference in shot distance between the current shot and the previous shot. The y-axis shows the cumulative probability.  The graph is a line graph, with two distinct lines representing the CDFs for each category.

The "made Prev" line is generally higher than the "missed Prev" line across most of the x-axis range, indicating that a positive difference in shot distance (current shot further than previous shot) is more likely associated with successful shots ("made Prev"). Conversely, a negative difference (current shot closer than previous shot) is more often associated with missed shots. The curves asymptotically approach 1, as expected for CDFs.  The graph's title clearly labels it as a CDF plot, and the axis labels are easily understood. The legend clearly differentiates the two lines.  The x-axis ranges from -40 to +40, suggesting the difference in shot distance can be significantly positive or negative.


------------------------------------------------------------
Cell index: 46
Input Cell Type: markdown
Input Text:
-----------
## Lastly, Let's plot the "Shot Difficulty" change for the two groups
here negative values indicate that kobe took a larger risk, and positive values indicate that kobe made a safer subsequent shot

Output Text:
------------


------------------------------------------------------------
Cell index: 47
Input Cell Type: python
Input Text:
-----------
#%% after making a shot, kobe is a more confident and makes much more difficult shots generally
plt.rcParams['figure.figsize'] = (13, 10)

jointHist, difficultyDiffBins = np.histogram(changeInShotDifficultyDict['madeLast']+changeInShotDifficultyDict['missedLast'],bins=100)
barWidth = 0.999*(difficultyDiffBins[1]-difficultyDiffBins[0])

shotDifficultyDiffHist_GivenMadeLastShot,   b = np.histogram(changeInShotDifficultyDict['madeLast'],bins=difficultyDiffBins)
shotDifficultyDiffHist_GivenMissedLastShot, b = np.histogram(changeInShotDifficultyDict['missedLast'],bins=difficultyDiffBins)
maxHeight = max(max(shotDifficultyDiffHist_GivenMadeLastShot),max(shotDifficultyDiffHist_GivenMissedLastShot)) + 30

plt.figure();
plt.subplot(2,1,1); plt.bar(difficultyDiffBins[:-1], shotDifficultyDiffHist_GivenMadeLastShot, width=barWidth); plt.xlim((-1,1)); plt.ylim((0,maxHeight))
plt.title('made last shot'); plt.ylabel('counts')
plt.subplot(2,1,2); plt.bar(difficultyDiffBins[:-1], shotDifficultyDiffHist_GivenMissedLastShot, width=barWidth); plt.xlim((-1,1)); plt.ylim((0,maxHeight))
plt.title('missed last shot'); plt.xlabel('chance to make curr shot - chance to make prev shot'); plt.ylabel('counts')

Output Text:
------------
<matplotlib.text.Text at 0x7f9adc250780>
<matplotlib.figure.Figure at 0x7f9b456f0908>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image contains two histograms displayed vertically, one above the other.  Both histograms share the same x-axis, labeled "chance to make curr shot - chance to make prev shot," indicating a difference in probabilities. The y-axis of both histograms is labeled "counts," representing the frequency of occurrences.

The top histogram is titled "made last shot," suggesting it shows the distribution of the difference in shot-making probabilities for instances where the last shot was successful. The bottom histogram, titled "missed last shot," displays the same difference in probabilities but for instances where the last shot was unsuccessful.

Both histograms have similar ranges on the x-axis, extending from approximately -1.0 to 1.0.  They appear to be unimodal, with a central peak near 0. However, the peak for the "made last shot" histogram seems to be slightly more pronounced and concentrated around 0 compared to the "missed last shot" histogram, which shows a broader distribution.  This visual difference suggests a potential relationship between the difference in shot probabilities and the success or failure of the last shot.


------------------------------------------------------------
Cell index: 48
Input Cell Type: markdown
Input Text:
-----------
### We can see that the plot is heavier on the left side
### It is therefore even more evident now that kobe feels he's "In The Zone" after making a shot 
and therefore he allows himself to attempt more difficult shots

Output Text:
------------


------------------------------------------------------------
Cell index: 49
Input Cell Type: markdown
Input Text:
-----------
## Some of you might be wondering about wheather it's simply regression to the mean or not
this thinking is sound, since all successful attempts are inherently biased towards easier shots, and if we use relative meassures such as "shot difficulty change" we will for sure get this effect by simply "going back to the mean", so we need to make sure this isn't it.

Output Text:
------------


------------------------------------------------------------
Cell index: 50
Input Cell Type: python
Input Text:
-----------
#%% is this regression to the mean?
plt.rcParams['figure.figsize'] = (12, 10)

accuracyAllShots    = data['shot_made_flag'].mean()
accuracyAfterMade   = afterMadeData['shot_made_flag'].mean()
accuracyAfterMissed = afterMissedData['shot_made_flag'].mean()

standardErrorAllShots    = np.sqrt(accuracyAllShots*(1-accuracyAllShots)/data.shape[0])
standardErrorAfterMade   = np.sqrt(accuracyAfterMade*(1-accuracyAfterMade)/afterMadeData.shape[0])
standardErrorAfterMissed = np.sqrt(accuracyAfterMissed*(1-accuracyAfterMissed)/afterMissedData.shape[0])

accuracyVec = np.array([accuracyAfterMade,accuracyAllShots,accuracyAfterMissed])
errorVec    = np.array([standardErrorAfterMade,standardErrorAllShots,standardErrorAfterMissed])

barWidth = 0.7
xLocs = np.arange(len(accuracyVec)) + 0.5

fig, h = plt.subplots(); h.bar(xLocs, accuracyVec, barWidth, color='b', yerr=errorVec)
h.set_xticks(xLocs); h.set_xticklabels(('after made', 'all shots', 'after missed'))
plt.ylim([0.41,0.47]); plt.xlim([-0.3,3.3]); plt.title('not regression to the mean')

Output Text:
------------
<matplotlib.text.Text at 0x7f9adcc3b5f8>
<matplotlib.figure.Figure at 0x7f9b1c7cde10>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a bar graph illustrating data related to shooting percentages, possibly in a sport like basketball. The graph's title, "not regression to the mean," suggests that the data contradicts the statistical phenomenon of regression to the mean. 


The horizontal axis (x-axis) displays three categories: "after made," "all shots," and "after missed," representing shooting performance after a successful shot, overall shooting performance, and shooting performance after a missed shot, respectively. The vertical axis (y-axis) represents the shooting percentage, ranging from approximately 0.41 to 0.47.


Each category has a blue bar indicating the average shooting percentage, with a vertical black line extending above and below the bar, representing the error bars or confidence intervals.  The bars show that the shooting percentage is highest after a missed shot and lowest after a made shot. The "all shots" category falls between the other two.  The graph visually indicates that the shooting percentage doesn't show a tendency to regress toward the mean, challenging the expectation that a high percentage after a made shot would be followed by a lower percentage and vice versa.


------------------------------------------------------------
Cell index: 51
Input Cell Type: markdown
Input Text:
-----------
### OK, now we've established that it's not simply regression to the mean, and that there are infact two different groups of shots with very different accuracies, the question arises:

Output Text:
------------


------------------------------------------------------------
Cell index: 52
Input Cell Type: markdown
Input Text:
-----------
# Is Kobe right in his "Hot Hand" feeling? 
Maybe Kobe really is "in the zone" and therefore it's "OK" for him to take on more difficult shots?

Output Text:
------------


------------------------------------------------------------
Cell index: 53
Input Cell Type: python
Input Text:
-----------
#%% but wait, maybe kobe is making more difficult shots because he's "in the zone"

predictedShotPercentAfterMade = np.array(shotChancesListAfterMade).mean()
predictedStadardDev = np.sqrt(predictedShotPercentAfterMade*(1-predictedShotPercentAfterMade))
stadardError = predictedStadardDev/np.sqrt(len(shotChancesListAfterMade))
predPlusErr  = predictedShotPercentAfterMade + 2*stadardError
predMinusErr = predictedShotPercentAfterMade - 2*stadardError
actualShotPercentAfterMade = float(totalMadeAfterMade)/totalAttemptsAfterMade

print("-----------------------------------------------------")
print('provided that kobe MADE the previous shot:')
print('according to "shotDifficulty" model, 95% confidence interval ['+ str(predMinusErr)+', '+str(predPlusErr)+']')
print('and Kobe actually made ' + str(actualShotPercentAfterMade) + ', which is within confidence interval')
print("-----------------------------------------------------")

predictedShotPercentAfterMissed = np.array(shotChancesListAfterMissed).mean()
predictedStadardDev = np.sqrt(predictedShotPercentAfterMissed*(1-predictedShotPercentAfterMissed))
stadardError = predictedStadardDev/np.sqrt(len(shotChancesListAfterMissed))
predPlusErr  = predictedShotPercentAfterMissed + 2*stadardError
predMinusErr = predictedShotPercentAfterMissed - 2*stadardError
actualShotPercentAfterMissed = float(totalMadeAfterMissed)/totalAttemptsAfterMissed

print("-----------------------------------------------------")
print('provided that kobe MISSED the previous shot:')
print('according to "shotDifficulty" model, 95% confidence interval ['+ str(predMinusErr)+', '+str(predPlusErr)+']')
print('and Kobe actually made ' + str(actualShotPercentAfterMissed) + ', which is within confidence interval')
print("-----------------------------------------------------")

Output Text:
------------
-----------------------------------------------------
provided that kobe MADE the previous shot:
according to "shotDifficulty" model, 95% confidence interval [0.425732530575, 0.446428638004]
and Kobe actually made 0.4390376660134988, which is within confidence interval
-----------------------------------------------------
-----------------------------------------------------
provided that kobe MISSED the previous shot:
according to "shotDifficulty" model, 95% confidence interval [0.444870245146, 0.464189059611]
and Kobe actually made 0.45338225609182425, which is within confidence interval
-----------------------------------------------------


------------------------------------------------------------
Cell index: 54
Input Cell Type: markdown
Input Text:
-----------
### We can see that the accuracy is completely explained by the "shotDifficulty" model we've created, that doesn't contain any hot hand related features.
# The answer looks to be that Kobe doesn't have a "Hot Hand" effect

Output Text:
------------


------------------------------------------------------------
Cell index: 55
Input Cell Type: markdown
Input Text:
-----------
## let's now try to visualize this a little better

Output Text:
------------


------------------------------------------------------------
Cell index: 56
Input Cell Type: python
Input Text:
-----------
#%% let's try and visualize this - show scatter plot of after made and after missed shots
plt.rcParams['figure.figsize'] = (16, 8)

afterMissedData = data.iloc[afterMissedShotsList,:]
afterMadeData = data.iloc[afterMadeShotsList,:]

plt.figure();
plt.subplot(1,2,1); plt.title('shots after made')
plt.scatter(x=afterMadeData['loc_x'],y=afterMadeData['loc_y'],c=afterMadeData['shotLocationCluster'],s=50,cmap='hsv',alpha=0.06)
draw_court(outer_lines=True); plt.ylim(-60,440); plt.xlim(270,-270);

plt.subplot(1,2,2); plt.title('shots after missed');
plt.scatter(x=afterMissedData['loc_x'],y=afterMissedData['loc_y'],c=afterMissedData['shotLocationCluster'],s=50,cmap='hsv',alpha=0.06)
draw_court(outer_lines=True); plt.ylim(-60,440); plt.xlim(270,-270);

Output Text:
------------
<matplotlib.figure.Figure at 0x7f9b45ec5f60>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a pair of court visualizations, each depicting shot locations in basketball.  The left panel is titled "shots after made," while the right is titled "shots after missed."  Both panels share the same structure: a basketball court outline is displayed, with numerous small, colored dots scattered across it. These dots represent individual shot attempts, their location on the court indicating where the shot was taken from. The color of each dot likely corresponds to a specific category or feature, though this isn't explicitly labeled.

The court itself is standard, showing the hoop, free-throw line, three-point arc, and the restricted area. The dots are densely clustered around the basket, with decreasing density further away.  A noticeable difference between the two panels is the apparent distribution of the colored dots. Although a detailed analysis would require more information about the color coding, a visual inspection suggests the distribution of shot locations might differ between successful and missed shots.  For instance, successful shots might be concentrated closer to the basket, whereas missed shots might be more dispersed across the court. This visual difference implies that the visualization aims to highlight the relationship between the location of a shot and its outcome (made or missed).


------------------------------------------------------------
Cell index: 57
Input Cell Type: markdown
Input Text:
-----------
### Keen eyes can see differences in density here, but it's not very clear, so let's show the data in the gaussians format, hoping that it will be clearer

Output Text:
------------


------------------------------------------------------------
Cell index: 58
Input Cell Type: python
Input Text:
-----------
#%% show shot attempts of after made and after missed shots
plt.rcParams['figure.figsize'] = (13, 10)

variableCategories = afterMadeData['shotLocationCluster'].value_counts().index.tolist()
clusterFrequency = {}
for category in variableCategories:
    shotsAttempted = np.array(afterMadeData['shotLocationCluster'] == category).sum()
    clusterFrequency[category] = float(shotsAttempted)/afterMadeData.shape[0]

ellipseTextMessages = [str(100*clusterFrequency[x])[:4]+'%' for x in range(numGaussians)]
Draw2DGaussians(gaussianMixtureModel, ellipseColors, ellipseTextMessages)
draw_court(outer_lines=True); plt.ylim(-60,440); plt.xlim(270,-270); plt.title('after made shots')

variableCategories = afterMissedData['shotLocationCluster'].value_counts().index.tolist()
clusterFrequency = {}
for category in variableCategories:
    shotsAttempted = np.array(afterMissedData['shotLocationCluster'] == category).sum()
    clusterFrequency[category] = float(shotsAttempted)/afterMissedData.shape[0]

ellipseTextMessages = [str(100*clusterFrequency[x])[:4]+'%' for x in range(numGaussians)]
Draw2DGaussians(gaussianMixtureModel, ellipseColors, ellipseTextMessages)
draw_court(outer_lines=True); plt.ylim(-60,440); plt.xlim(270,-270); plt.title('after missed shots')

Output Text:
------------
<matplotlib.text.Text at 0x7f9ad61fd208>
<matplotlib.figure.Figure at 0x7f9ad63c1d30>
<matplotlib.figure.Figure at 0x7f9ad622df28>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a shot chart depicting the distribution of successful basketball shots. It's overlaid on a basketball court diagram, showing the location of each made shot as an ellipse, color-coded and sized proportionally to the percentage of shots made from that area. 

The court is oriented with the basket at the center, and the x-axis represents the lateral position on the court, while the y-axis represents the distance from the basket. The ellipses are clustered in various zones: some near the basket, some mid-range, and others from beyond the three-point arc. Each ellipse has a percentage value indicating the proportion of successful shots taken from that specific area. The percentages suggest a higher concentration of successful shots from the mid-range and close to the basket, with fewer successful shots from beyond the three-point line.  The chart provides a visual representation of shooting efficiency across different areas of the court.


Image 2 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a shot chart depicting the distribution of missed shots in a basketball game. 


The chart uses a court diagram as its background, showing the positions on the court where shots were missed. Each missed shot is represented by an ellipse, with the size and color of the ellipse seemingly correlated to the frequency or percentage of misses from that area. The ellipses are overlaid on a court outline; a key area around the basket is marked by dashed lines.  Numbers within each ellipse show the percentage of missed shots originating from that zone on the court. The percentages are color-coded and appear to add up to 100%. The title, "after missed shots," indicates the data represents missed shots after a game or period of play.  The overall structure helps visualize the spatial patterns of missed shots, identifying areas where players might need to improve their shooting accuracy.


------------------------------------------------------------
Cell index: 59
Input Cell Type: markdown
Input Text:
-----------
### Now it's very evident that after missing a shot, kobe is much more likely to throw directly from the basket relative to after making a shot (27% after missing the previous shot vs. 18% after making the previous shot) 

### It's also very evident that after making a shot, kobe is much more likely to try a 3 pointer as his next shot

Output Text:
------------


------------------------------------------------------------

================================================================================
================================================================================
visualizing-pca-with-leaf-dataset.ipynb:
========================================
------------------------------------------------------------
Cell index: 1
Input Cell Type: markdown
Input Text:
-----------
***Visualizing PCA with Leaf Dataset***
================================

In this script we will apply PCA on leaf images and try to get a feel for the distribution of leaf images using visualizations that (hopefully) clarify different aspects about how to interpret PCA results.

We will then continue to see if the PCA features are informative in terms of classifying leafs and determine how many of those we need.

I've just updated another script, similar in nature to this one, just focused about k-means.
if you enjoyed this one, be sure to also check out the [k-means script][1] as well

  [1]: https://www.kaggle.com/selfishgene/leaf-classification/visualizing-k-means-with-leaf-dataset/notebook

Output Text:
------------


------------------------------------------------------------
Cell index: 2
Input Cell Type: python
Input Text:
-----------
import pandas as pd
import numpy as np
import matplotlib
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec

from sklearn import model_selection
from sklearn import decomposition
from sklearn import linear_model
from sklearn import ensemble
from sklearn import neighbors
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KernelDensity
from sklearn.manifold import TSNE
from sklearn.metrics import accuracy_score

from skimage.transform import rescale
from scipy import ndimage as ndi

matplotlib.style.use('fivethirtyeight')

Output Text:
------------


------------------------------------------------------------
Cell index: 3
Input Cell Type: markdown
Input Text:
-----------
## Load the data and Pre-process it

For the sake of the script not being too cluttered, I suppressed the output of all intermediate plots during the data loading and preparation phases. anyone who is interested is welcome to **fork and unhide output** to see what is going on.

(the main assumption of this pre-processing stage is that the absolute sizes of the leafs matter, and not just their shape. i.e. leafs with different sizes are most definitely different types of leafs. not sure if it's actually important, but just in case)

Output Text:
------------


------------------------------------------------------------
Cell index: 4
Input Cell Type: python
Input Text:
-----------
#%% load the data
dataDir = '../input/'
trainData = pd.read_csv(dataDir + 'train.csv')
classEncoder = LabelEncoder()
trainLabels = classEncoder.fit_transform(trainData.loc[:,'species'])
trainIDs = np.array(trainData.loc[:,'id'])

# show some random images
plt.figure(figsize=(12,12))
for k in range(28):
    randTrainInd = np.random.randint(len(trainIDs))
    randomID = trainIDs[randTrainInd]
    imageFilename = dataDir + 'images/' + str(randomID) + '.jpg'
    plt.subplot(4,7,k+1); plt.imshow(mpimg.imread(imageFilename), cmap='gray')
    plt.title(classEncoder.classes_[trainLabels[randTrainInd]], fontsize=8); plt.axis('off')

#%% preprocess images

# go over training images and store them in a list
numImages = 1584

shapesMatrix = np.zeros((2,numImages))
listOfImages = []
for k in range(numImages):
    imageFilename = dataDir + 'images/' + str(k+1) + '.jpg'
    currImage = mpimg.imread(imageFilename)
    shapesMatrix[:,k] = np.shape(currImage)
    listOfImages.append(currImage)
    
# create a large 3d array with all images
maxShapeSize = shapesMatrix.max(axis=1)
for k in range(len(maxShapeSize)):
    if maxShapeSize[k] % 2 == 0:
        maxShapeSize[k] += 311
    else:
        maxShapeSize[k] += 310
    
fullImageMatrix3D = np.zeros(np.hstack((maxShapeSize,
                                        np.shape(shapesMatrix[1]))).astype(int),dtype=np.dtype('u1'))
destXc = (maxShapeSize[1]+1)/2; destYc = (maxShapeSize[0]+1)/2
for k, currImage in enumerate(listOfImages):
    Yc, Xc = ndi.center_of_mass(currImage)
    Xd = destXc - Xc; Yd = destYc - Yc
    rowIndLims = (int(round(Yd)),int(round(Yd)+np.shape(currImage)[0]))
    colIndLims = (int(round(Xd)),int(round(Xd)+np.shape(currImage)[1]))
    fullImageMatrix3D[rowIndLims[0]:rowIndLims[1],colIndLims[0]:colIndLims[1],k] = currImage

# make sure nothing was ruined in the process
plt.figure(figsize=(12,12))
plt.suptitle('large reference frame images', fontsize=10)
for k in range(28):
    randInd = np.random.randint(np.shape(fullImageMatrix3D)[2])
    plt.subplot(4,7,k+1); plt.imshow(fullImageMatrix3D[:,:,randInd], cmap='gray'); plt.axis('off')

# remove redundent rows and columns
xValid = fullImageMatrix3D.mean(axis=2).sum(axis=0) > 0
yValid = fullImageMatrix3D.mean(axis=2).sum(axis=1) > 0
xLims = (np.nonzero(xValid)[0][0],np.nonzero(xValid)[0][-1])
yLims = (np.nonzero(yValid)[0][0],np.nonzero(yValid)[0][-1])
fullImageMatrix3D = fullImageMatrix3D[yLims[0]:yLims[1],xLims[0]:xLims[1],:]

# make sure nothing was ruined in the process
plt.figure(figsize=(12,12))
plt.suptitle('final reference frame images', fontsize=10)
for k in range(28):
    randInd = np.random.randint(np.shape(fullImageMatrix3D)[2])
    plt.subplot(4,7,k+1); plt.imshow(fullImageMatrix3D[:,:,randInd], cmap='gray'); plt.axis('off')

# scale down all images
rescaleFactor = 0.15

scaledDownImage = rescale(fullImageMatrix3D[:,:,0],rescaleFactor)
scaledDownImages = np.zeros(np.hstack((np.shape(scaledDownImage),
                                       np.shape(fullImageMatrix3D)[2])),dtype=np.dtype('f4'))
for imInd in range(np.shape(fullImageMatrix3D)[2]):
    scaledDownImages[:,:,imInd] = rescale(fullImageMatrix3D[:,:,imInd],rescaleFactor)
    
del fullImageMatrix3D

Output Text:
------------
/opt/conda/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.
  warn("The default mode, 'constant', will be changed to 'reflect' in "
<matplotlib.figure.Figure at 0x7f0da6158710>
<matplotlib.figure.Figure at 0x7f0da616e080>
<matplotlib.figure.Figure at 0x7f0c96ce7240>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid showcasing various leaf silhouettes, each set against a black background. The leaves are presented in rows and columns, with the name of the plant species written above each leaf image. Each leaf is presented as a binary image, meaning it's represented solely in black and white, with the leaf itself shown as white against the black backdrop.  The arrangement is systematic, facilitating easy comparison and identification of the different leaf shapes and structures.


The overall structure suggests a dataset or a visual catalog of leaf silhouettes, potentially used for plant identification or research purposes. The consistent background and labeling make it easy to browse and analyze the diverse leaf forms. The leaf images themselves appear to be segmented or extracted from photographs of leaves, simplified to their basic shapes. The selection of leaf types is diverse, covering a range of species from various plant families.


Image 2 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a grid of 32 small, square images arranged in four rows of eight. Each square contains a simple, black-and-white silhouette of a leaf or leaf-like shape. The shapes vary in size, orientation, and type, with some resembling common leaf forms (like oak leaves or maple leaves) and others being more abstract or irregular. 


The silhouettes are all white on a black background, making them easily distinguishable. There's no discernible pattern to the arrangement of the shapes within the grid; they seem randomly ordered. 


Above the grid, the text "large reference frame images" indicates the purpose of the image set. This suggests that the images are likely being used as a reference dataset for a computer vision task, potentially for leaf classification or shape recognition. The simplicity and uniformity of the images are consistent with the requirements of such a dataset.


Image 3 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image presents a grid of 32 small, square images arranged in four rows of eight.  Each small image is a black and white silhouette of a leaf. The leaves vary in shape and size, with some resembling simple ovals or teardrops, while others are more complex, showing serrated edges or distinct lobes.  A few appear to depict specific leaf types, such as maple or oak leaves.  The silhouettes are all white on a black background.

The overall layout is clean and simple.  Above the grid, a small text label reads "final reference frame images", indicating that these are likely processed or extracted images intended for reference in some larger project or analysis. The consistent size and arrangement of the leaf silhouettes suggest a systematic approach to image processing or categorization. The image's purpose seems to be the display of various leaf shapes as a visual dataset.


------------------------------------------------------------
Cell index: 5
Input Cell Type: markdown
Input Text:
-----------
## Look at the final processing stage and view several random leaf images:

Output Text:
------------


------------------------------------------------------------
Cell index: 6
Input Cell Type: python
Input Text:
-----------
np.random.seed(1) # use a nice looking random seed

plt.figure(figsize=(12,10));
for k in range(25):
    randInd = np.random.randint(np.shape(scaledDownImages)[2])
    plt.subplot(5,5,k+1); 
    plt.imshow(scaledDownImages[:,:,randInd], cmap='gray'); 
    plt.axis('off'); plt.title('imageID = ' + str(randInd), fontsize=12)
plt.tight_layout()

Output Text:
------------
<matplotlib.figure.Figure at 0x7f0da19a8a20>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid of 25 smaller images, arranged in 5 rows and 5 columns. Each smaller image is a black background with a single, white silhouette of a leaf.  The leaves vary in shape and size, representing a diverse collection of leaf types.  

Above each smaller image is text indicating an "imageID" followed by a unique numerical identifier.  This suggests the images are part of a larger dataset, likely used for a machine learning task such as leaf classification or image recognition. The leaves are presented in a consistent manner, all centered on a black square, ensuring that the focus is on the leaf shapes themselves. The consistent layout and labeling make this a well-structured representation of a leaf image dataset.


------------------------------------------------------------
Cell index: 7
Input Cell Type: markdown
Input Text:
-----------
## Define a 'GaussianModel' class that will help us visualize things:

This is long, so I've hidden the code, but if you are intereseted in delving deeper and looking at the implementation then please unhide or better yet fork the script and try playing around by editing the code.

Output Text:
------------


------------------------------------------------------------
Cell index: 8
Input Cell Type: python
Input Text:
-----------
#%% define GaussianModel class

class GaussianModel:
    def __init__(self, X, numBasisFunctions=10, objectPixels=None):
        '''
        inputs: 
            X                    - numSamples x numDimentions matrix
            numBasisFunctions       - number of basis function to use
            objectPixels (optional) - an binnary mask image used for presentation
                                      will be used as Im[objectPixels] = dataSample
                                      must satisfy objectPixels.ravel().sum() = X.shape[1]
        '''
        
        self.numBasisFunctions = numBasisFunctions        
        if objectPixels is None:
            self.objectPixels = np.ones((1,X.shape[1]),dtype=np.bool)
        else:
            self.objectPixels = objectPixels
        assert(self.objectPixels.ravel().sum() == X.shape[1])

        PCAModel = decomposition.PCA(n_components=numBasisFunctions, whiten=True)
        self.dataRepresentation = PCAModel.fit_transform(X)
        self.PCAModel = PCAModel

    def RepresentUsingModel(self, X):
        return self.PCAModel.transform(X)

    def ReconstructUsingModel(self, X_transformed):
        return self.PCAModel.inverse_transform(X_transformed)

    def InterpretUsingModel(self, X):
        return self.PCAModel.inverse_transform(self.PCAModel.transform(X))

    # shows the eigenvectors of the gaussian covariance matrix
    def ShowVarianceDirections(self, numDirectionsToShow=16):
        numDirectionsToShow = min(numDirectionsToShow, self.numBasisFunctions)
        
        numFigRows = 4; numFigCols = 4;
        numDirectionsPerFigure = numFigRows*numFigCols
        numFigures = int(np.ceil(float(numDirectionsToShow)/numDirectionsPerFigure))
        
        for figureInd in range(numFigures):
            plt.figure()
            for plotInd in range(numDirectionsPerFigure):
                eigVecInd = numDirectionsPerFigure*figureInd + plotInd
                if eigVecInd >= self.numBasisFunctions:
                    break
                deltaImage = np.zeros(np.shape(self.objectPixels))
                deltaImage[self.objectPixels] = self.PCAModel.components_[eigVecInd,:].ravel()

                plt.subplot(numFigRows,numFigCols,plotInd+1)
                if np.shape(self.objectPixels)[0] == 1:
                    plt.plot(deltaImage)
                else:
                    plt.imshow(deltaImage,cmap='jet'); plt.axis('off')
                titleStr = str(100*self.PCAModel.explained_variance_ratio_[eigVecInd])[0:5]
                plt.title(titleStr + '% explained');
            plt.tight_layout()
            
    # shows several random model reconstructions
    def ShowReconstructions(self, X, numReconstructions=5):
        assert(np.shape(X)[1] == self.objectPixels.ravel().sum())
        numSamples = np.shape(X)[0]
        numReconstructions = min(numReconstructions, numSamples)
        
        originalImage      = np.zeros(np.shape(self.objectPixels))
        reconstructedImage = np.zeros(np.shape(self.objectPixels))
        
        numReconstructionsPerFigure = min(5, numReconstructions)
        numFigures = int(np.ceil(float(numReconstructions)/numReconstructionsPerFigure))
        
        for figureInd in range(numFigures):
            plt.figure()
            for plotCol in range(numReconstructionsPerFigure):
                dataSampleInd = np.random.randint(numSamples)
                originalImage[self.objectPixels] = X[dataSampleInd,:].ravel()
                reconstructedImage[self.objectPixels] = \
                    self.InterpretUsingModel(np.reshape(X[dataSampleInd,:],[1,-1])).ravel()
                diffImage = abs(originalImage - reconstructedImage)
                
                # original image
                plt.subplot(3,numReconstructionsPerFigure,0*numReconstructionsPerFigure+plotCol+1)
                if np.shape(self.objectPixels)[0] == 1:
                    plt.plot(originalImage); plt.title('original signal')
                else:
                    plt.imshow(originalImage, cmap='gray'); 
                    plt.title('original image'); plt.axis('off')
                    
                # reconstred image
                plt.subplot(3,numReconstructionsPerFigure,1*numReconstructionsPerFigure+plotCol+1)
                if np.shape(self.objectPixels)[0] == 1:
                    plt.plot(reconstructedImage); plt.title('reconstructed signal')
                else:
                    plt.imshow(reconstructedImage, cmap='gray'); 
                    plt.title('reconstructed image'); plt.axis('off')

                # diff image
                plt.subplot(3,numReconstructionsPerFigure,2*numReconstructionsPerFigure+plotCol+1)
                if np.shape(self.objectPixels)[0] == 1:
                    plt.plot(diffImage); plt.title('abs difference signal')
                else:
                    plt.imshow(diffImage, cmap='gray'); 
                    plt.title('abs difference image'); plt.axis('off')
            plt.tight_layout()

    # shows distrbution along the variance directions and several images along that variance direction
    def ShowModelVariations(self, numVariations=5):

        showAsTraces = (np.shape(self.objectPixels)[0] == 1)
        numVariations = min(numVariations, self.numBasisFunctions)
                
        numVarsPerFigure = min(5,numVariations)
        numFigures = int(np.ceil(float(numVariations)/numVarsPerFigure))
        
        lowRepVec     = np.percentile(self.dataRepresentation, 2, axis=0)
        medianRepVec  = np.percentile(self.dataRepresentation, 50, axis=0)
        highRepVec    = np.percentile(self.dataRepresentation, 98, axis=0)

        for figureInd in range(numFigures):
            plt.figure()
            for plotCol in range(numVarsPerFigure):
                eigVecInd = numVarsPerFigure*figureInd+plotCol
                if eigVecInd >= self.numBasisFunctions:
                    break

                # create the low and high precentile representation activation vectors
                currLowPrecentileRepVec             = medianRepVec.copy()
                currLowPrecentileRepVec[eigVecInd]  = lowRepVec[eigVecInd]
                currHighPrecentileRepVec            = medianRepVec.copy()
                currHighPrecentileRepVec[eigVecInd] = highRepVec[eigVecInd]

                # create blank images
                deltaImage          = np.zeros(np.shape(self.objectPixels))
                medianImage         = np.zeros(np.shape(self.objectPixels))
                lowPrecentileImage  = np.zeros(np.shape(self.objectPixels))
                highPrecentileImage = np.zeros(np.shape(self.objectPixels))

                # fill the object pixels with the relevant data
                deltaImage[self.objectPixels]          = \
                        self.PCAModel.components_[eigVecInd,:].ravel()
                lowPrecentileImage[self.objectPixels]  = \
                        self.ReconstructUsingModel(currLowPrecentileRepVec).ravel()
                medianImage[self.objectPixels]         = \
                        self.ReconstructUsingModel(medianRepVec).ravel()
                highPrecentileImage[self.objectPixels] = \
                        self.ReconstructUsingModel(currHighPrecentileRepVec).ravel()

                # calculate the Gaussian smoothed distribution of values along the eignevector direction
                sigmaOfKDE = 0.12
                pdfStart   = min(self.dataRepresentation[:,eigVecInd]) - 3*sigmaOfKDE
                pdfStop    = max(self.dataRepresentation[:,eigVecInd]) + 3*sigmaOfKDE
                xAxis = np.linspace(pdfStart,pdfStop,200)
                PDF_Model = KernelDensity(kernel='gaussian', 
                                  bandwidth=sigmaOfKDE).fit(self.dataRepresentation[:,eigVecInd].reshape(-1,1))
                logPDF = PDF_Model.score_samples(xAxis.reshape(-1,1))

                # show distribution of current component 
                plt.subplot(5,numVarsPerFigure,0*numVarsPerFigure+plotCol+1)
                plt.fill(xAxis, np.exp(logPDF), fc='b');
                percentExplainedString = str(100*self.PCAModel.explained_variance_ratio_[eigVecInd])[0:5]
                plt.title(percentExplainedString + '% explained'); 
                
                # show variance direction (eigenvector)
                plt.subplot(5,numVarsPerFigure,1*numVarsPerFigure+plotCol+1);
                if showAsTraces:
                    plt.plot(deltaImage); plt.title('eigenvector ' + str(eigVecInd))
                else:
                    plt.imshow(deltaImage, cmap='jet'); 
                    plt.title('eigenvector ' + str(eigVecInd)); plt.axis('off')

                # show 2nd precentile image
                plt.subplot(5,numVarsPerFigure,2*numVarsPerFigure+plotCol+1)
                if showAsTraces:
                    plt.plot(lowPrecentileImage); plt.title('2nd precentile')
                else:
                    plt.imshow(lowPrecentileImage, cmap='gray'); 
                    plt.title('2nd precentile image'); plt.axis('off')

                # show median image
                plt.subplot(5,numVarsPerFigure,3*numVarsPerFigure+plotCol+1)
                if showAsTraces:
                    plt.plot(medianImage); plt.title('median signal')
                else:
                    plt.imshow(medianImage, cmap='gray'); 
                    plt.title('median Image'); plt.axis('off')

                # show 98th precentile image
                plt.subplot(5,numVarsPerFigure,4*numVarsPerFigure+plotCol+1)
                if showAsTraces:
                    plt.plot(highPrecentileImage); plt.title('98th precentile')
                else:
                    plt.imshow(highPrecentileImage, cmap='gray'); 
                    plt.title('98th precentile image'); plt.axis('off')
            plt.tight_layout()
        
    # shows distrbution along the variance directions and several images along that variance direction
    def ShowSingleComponentVariation(self, X, listOfComponents=[0,1]):

        showAsTraces = (np.shape(self.objectPixels)[0] == 1)
        assert(all([(x in range(self.numBasisFunctions)) for x in listOfComponents]))
                
        X_rep = self.RepresentUsingModel(X)
        
        percentilesToShow = [1,20,40,60,80,99]
        numReadDataSamplePerPercentile = 4
        representationPercentiles = []
        for percentile in percentilesToShow:
            representationPercentiles.append(np.percentile(self.dataRepresentation, percentile, axis=0))
        medianRepVec =  np.percentile(self.dataRepresentation, 50, axis=0)

        for eigVecInd in listOfComponents:
            plt.figure(); gs = gridspec.GridSpec(numReadDataSamplePerPercentile+2,
                                                 len(percentilesToShow))

            # calculate the Gaussian smoothed distribution of values along the eignevector direction
            sigmaOfKDE = 0.12
            pdfStart   = min(self.dataRepresentation[:,eigVecInd]) - 3*sigmaOfKDE
            pdfStop    = max(self.dataRepresentation[:,eigVecInd]) + 3*sigmaOfKDE
            xAxis = np.linspace(pdfStart,pdfStop,200)
            PDF_Model = KernelDensity(kernel='gaussian', 
                              bandwidth=sigmaOfKDE).fit(self.dataRepresentation[:,eigVecInd].reshape(-1,1))
            logPDF = PDF_Model.score_samples(xAxis.reshape(-1,1))
            percentileValuesToShow = \
                    [representationPercentiles[x][eigVecInd] for x in range(len(representationPercentiles))]
            percentilesToShowLogPDF = \
                    PDF_Model.score_samples(np.array(percentileValuesToShow).reshape(-1,1))

            # show distribution of current component and red dots at the list of precentiles to show 
            plt.subplot(gs[0,:])
            plt.fill(xAxis, np.exp(logPDF), fc='b');
            plt.scatter(percentileValuesToShow, np.exp(percentilesToShowLogPDF), c='r',s=40);
            plt.title(str(100*self.PCAModel.explained_variance_ratio_[eigVecInd]) + '% explained');
            
            for plotCol, currPrecentile in enumerate(percentilesToShow):                
                currPrecentileRepVec             = medianRepVec.copy()
                currPrecentileRepVec[eigVecInd]  = representationPercentiles[plotCol][eigVecInd]
                
                currPrecentileImage = np.zeros(np.shape(self.objectPixels))
                currPrecentileImage[self.objectPixels] = \
                        self.ReconstructUsingModel(currPrecentileRepVec).ravel()
                
                # show the median image with current precentile as activation of the curr image
                plt.subplot(gs[1,plotCol]);
                if showAsTraces:
                    plt.plot(currPrecentileImage); 
                    plt.title('precentile: ' + str(percentilesToShow[plotCol]) + '%')
                else:
                    plt.imshow(currPrecentileImage, cmap='gray'); 
                    plt.title('precentile: ' + str(percentilesToShow[plotCol]) + '%'); 
                    plt.axis('off')

                # find the most suitible candidates in X for current precentile
                distFromPercentile = abs(X_rep[:,eigVecInd] - 
                                         representationPercentiles[plotCol][eigVecInd])
                X_inds = np.argpartition(distFromPercentile, 
                                         numReadDataSamplePerPercentile)[:numReadDataSamplePerPercentile]
                for k, X_ind in enumerate(X_inds):
                    currNearestPrecentileImage = np.zeros(np.shape(self.objectPixels))
                    currNearestPrecentileImage[self.objectPixels]  = X[X_ind,:].ravel()
                    
                    plt.subplot(gs[2+k,plotCol]);
                    if showAsTraces:
                        plt.plot(currNearestPrecentileImage); 
                        plt.title('NN with closest percentile');
                    else:
                        plt.imshow(currNearestPrecentileImage, cmap='gray'); 
                        plt.title('NN with closest percentile'); plt.axis('off')
            plt.tight_layout()
            
    def ShowDataScatterPlotsWithTSNE(self, X=None, y=None, tSNE_perplexity=30.0, colorMap='Paired'):
        
        if X is None:
            X_rep = self.dataRepresentation
        else:
            X_rep = self.RepresentUsingModel(X)
            
        if y is None:
            y = np.ones(X_rep.shape[0])
            
        tSNE_PCAModel = TSNE(n_components=2, perplexity=tSNE_perplexity, random_state=0)
        X_rep_tSNE = tSNE_PCAModel.fit_transform(X_rep) 
        (tSNE_xmin, tSNE_xmax) = (np.percentile(X_rep_tSNE[:,0], 0.3), np.percentile(X_rep_tSNE[:,0], 99.7))
        (tSNE_ymin, tSNE_ymax) = (np.percentile(X_rep_tSNE[:,1], 0.3), np.percentile(X_rep_tSNE[:,1], 99.7))

        plt.figure()
        plt.subplot(1,2,1); 
        plt.scatter(X_rep[:,0],X_rep[:,1],c=y,cmap=colorMap,s=10,alpha=0.9)
        plt.title('PCA representation'); plt.xlabel('PC1 coeff'); plt.ylabel('PC2 coeff')
        plt.subplot(1,2,2); 
        plt.scatter(X_rep_tSNE[:,0],X_rep_tSNE[:,1],c=y,cmap=colorMap,s=10,alpha=0.9)
        plt.xlim(tSNE_xmin, tSNE_xmax); plt.ylim(tSNE_ymin, tSNE_ymax);
        plt.title('t-SNE representation'); plt.xlabel('t-SNE axis1'); plt.ylabel('t-SNE axis2')

Output Text:
------------


------------------------------------------------------------
Cell index: 9
Input Cell Type: markdown
Input Text:
-----------
## Train The Gaussian Model (also known as PCA)

Output Text:
------------


------------------------------------------------------------
Cell index: 10
Input Cell Type: python
Input Text:
-----------
# train the Gaussian Model 
sampleDim = np.shape(scaledDownImages)[0]*np.shape(scaledDownImages)[1]
X = scaledDownImages.reshape(sampleDim,-1).T

objectPixelsMask = np.ones((np.shape(scaledDownImages)[0],np.shape(scaledDownImages)[1]))==1
leaf_PCAModel = GaussianModel(X, numBasisFunctions=100, objectPixels=objectPixelsMask)

Output Text:
------------


------------------------------------------------------------
Cell index: 11
Input Cell Type: markdown
Input Text:
-----------
## Now lets look at the main variance directions of the PCA

These are also known as Principal Components.  
Each image can be though of as a different "direction" in the high dimensional image space

Output Text:
------------


------------------------------------------------------------
Cell index: 12
Input Cell Type: python
Input Text:
-----------
matplotlib.rcParams['font.size'] = 10
matplotlib.rcParams['figure.figsize'] = (12,9)
leaf_PCAModel.ShowVarianceDirections(numDirectionsToShow=16)

Output Text:
------------
<matplotlib.figure.Figure at 0x7f0da18d1da0>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a grid of sixteen small square images, each displaying a different heatmap pattern.  These patterns appear to be the result of some form of data analysis, possibly principal component analysis (PCA) or a similar dimensionality reduction technique, judging by the percentage values above each image.  Each heatmap is color-coded, utilizing a spectrum ranging from deep blue or purple (representing low values) to bright red and yellow (representing high values).  The patterns themselves are symmetrical and exhibit various shapes, including ovals, diamonds, and star-like formations.

The numerical values above each heatmap, such as "27.94% explained," are likely indicating the variance or information captured by that particular pattern.  This suggests that the larger percentages correspond to the most significant components or features identified in the underlying dataset.  The arrangement of the images from top-left to bottom-right suggests an ordering based on the amount of variance explained, with the top-left image showing the pattern explaining the most variance, and the bottom-right showing the pattern explaining the least variance. The overall image is a visual representation of a data decomposition or feature extraction process.


------------------------------------------------------------
Cell index: 13
Input Cell Type: markdown
Input Text:
-----------
We can see some interesting shapes arising from the data, especially the first and second row look nice, and we will soon understand exactly what these images mean. 

But first, let's look at **some original images** and how the low dimensional PCA model can **reconstruct** them.

Output Text:
------------


------------------------------------------------------------
Cell index: 14
Input Cell Type: markdown
Input Text:
-----------
## Show some image and their model Reconstructions


Output Text:
------------


------------------------------------------------------------
Cell index: 15
Input Cell Type: python
Input Text:
-----------
matplotlib.rcParams['font.size'] = 9
matplotlib.rcParams['figure.figsize'] = (12,5)
leaf_PCAModel.ShowReconstructions(X, numReconstructions=10)

Output Text:
------------
<matplotlib.figure.Figure at 0x7f0c9675f748>
<matplotlib.figure.Figure at 0x7f0da11915c0>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image displays a grid of 15 smaller images arranged in three rows and five columns.  Each column represents a single example of an image processing experiment. The top row shows the "original image" in each case; these are all grayscale images depicting different shapes on a black background. The shapes are somewhat organic and irregular, resembling leaves or petals.


The middle row presents the "reconstructed image" for each original image.  These images appear blurred and slightly less sharp than their originals, suggesting a reconstruction process, possibly involving some form of image compression or transformation. They maintain the general shape of the original images.


Finally, the bottom row shows the "abs difference image," which is a visual representation of the difference between the original and reconstructed images. These images highlight the discrepancies between the two, showcasing areas where the reconstruction process deviated from the original.  Brighter areas in these difference images indicate larger deviations in pixel values between the original and reconstructed versions.  The differences are primarily around the edges of the shapes.


Image 2 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a grid displaying fifteen grayscale images arranged in three rows of five columns.  Each column represents a different original image and its corresponding reconstruction and the absolute difference between the original and reconstructed images.

The first row shows five original images. These are simple, black and white silhouette images of different leaf shapes: a maple leaf, two different elongated leaf shapes, a circular leaf, and a teardrop-shaped leaf.  Each leaf shape is represented by white pixels on a black background.


The second row presents the reconstructed versions of the leaf images from the first row.  These images are grayscale, showing the leaves reconstructed, likely through some image processing technique.  The reconstructed images appear blurred or slightly distorted compared to the sharp originals, with some artifacts visible around the edges of the leaves.


The third row displays the absolute difference between the original and reconstructed images from the first and second rows, respectively. These difference images highlight the discrepancies between the original and reconstructed versions, showing areas of significant error as brighter regions. The difference images are predominantly dark, with brighter areas outlining the leaf shapes, indicating the reconstruction's inaccuracies.


------------------------------------------------------------
Cell index: 16
Input Cell Type: markdown
Input Text:
-----------
From the absolute difference images on the bottom row of both plots, we can see that the main regions that cannot be reconstructed are the edges of the leafs, but the general leaf structure can be reconstructed using the 100 basis functions.

**Now, let's take a closer look at how the leaf images vary around the mean image:**

Output Text:
------------


------------------------------------------------------------
Cell index: 17
Input Cell Type: markdown
Input Text:
-----------
## Show Model Variations around the Mean Image


Output Text:
------------


------------------------------------------------------------
Cell index: 18
Input Cell Type: python
Input Text:
-----------
matplotlib.rcParams['font.size'] = 9
matplotlib.rcParams['figure.figsize'] = (12,9)
leaf_PCAModel.ShowModelVariations(numVariations=5)

Output Text:
------------
<matplotlib.figure.Figure at 0x7f0dbfd36e80>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
This image displays the results of a Principal Component Analysis (PCA) applied to a dataset of images, likely of a similar object or feature.  The top row shows histograms representing the distribution of the principal components (eigenvectors), each labeled with the percentage of variance explained by that component.  The first component (eigenvector 0) explains the largest proportion (27.94%), indicating it captures the most significant variations within the image data. Subsequent components explain progressively smaller amounts of variance.

The second row shows the eigenvectors themselves as images. These are visual representations of the principal components, showing the patterns or features they represent.  Each eigenvector image displays characteristic variations in intensity, suggesting underlying structures or patterns within the original image dataset. The color maps are used to highlight these intensity variations.

The remaining rows (third, fourth, and fifth) present example images from the dataset reconstructed using the PCA results.  These are grouped by percentile (2nd, median, and 98th), showing how the data varies across its range.  Each column corresponds to a different eigenvector, illustrating how the different principal components contribute to the overall appearance of the images at these percentiles.  The grayscale images demonstrate the range of variation in the data, with darker and lighter regions representing the extremes.


------------------------------------------------------------
Cell index: 19
Input Cell Type: markdown
Input Text:
-----------
**For those of us unfamiliar with this kind of a plot and since this is quite a busy plot, let me explain what we see:**
------------------------------------------------------------------------
  


----------


 - The upper most row contains the data distributions of each eigenvector (i.e. the histogram along that "direction")
 - The second row contains what we already saw in a previous plot, what we called the variance directions.
 - The forth row contains the median image of leafs. notice that this row is identical for all eigenvectors
 - The third row holds the 2nd percentile images of each eigenvector. it's easier to think of this as the median image minus the eigenvector image multiplied by some constant. i.e the image we see is the forth row image, minus the second row image, when the second row image is multiplied by a constant. The constant is chosen to show the varying degree of influence of this specific eigenvector on the "average" image, so we can visualize what type of variation this particular eigenvector tends to capture. 2nd percentile will subtract a relatively large absolute value from the median image, showing us what images look like when this coefficient is highly negative. 98th percentile would be just the opposite, showing us what images look like when this coefficient is at the upper end of the range. 50th percentile would give us a "middle of the road" effect of this coefficient.


----------


This plot helps us visualize what a direction in this high dimensional image space means. For example:

 - **The first eigenvector** (leftmost column), we can see
   that it **controls the difference between large radius leafs and small radius
   leafs**. i.e we can say that some of the variance along the change of leaf radius is explained by this component.
 - The **second eigenvector** (second column from the left) controls the difference between an
   **upright vetrically oriented leaf** and a **horizontally oriented leaf**.

----------

We can now deep deeper into some interesting looking eigenvectors

**Eigenvector 1:**
------------------

Output Text:
------------


------------------------------------------------------------
Cell index: 20
Input Cell Type: python
Input Text:
-----------
matplotlib.rcParams['font.size'] = 6
matplotlib.rcParams['figure.figsize'] = (12,8)
leaf_PCAModel.ShowSingleComponentVariation(X, listOfComponents=[0])

Output Text:
------------
<matplotlib.figure.Figure at 0x7f0da2e05d68>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image displays a visualization of the explained variance in a dataset, likely related to image analysis or machine learning.  The top section shows a density plot, a curve illustrating the distribution of explained variance across different percentiles (1%, 20%, 40%, 60%, 80%, 99%).  Red dots highlight specific percentiles on the curve, indicating points of interest or significant variance.  The title "27.9451400042% explained" suggests that approximately 28% of the total variance is captured by the data represented.

The main body of the image is a grid of small images, organized into six columns and four rows. Each column corresponds to one of the percentiles highlighted in the density plot. Each row displays a progression of images, labeled "NN with closest percentile".  These images likely represent the nearest neighbors in the dataset that best match the characteristics associated with each percentile.  The images appear to be binary, showing dark backgrounds with bright white shapes, possibly representing segmented objects like leaves.  The consistent structure suggests a systematic analysis of how image features vary across different levels of explained variance.


------------------------------------------------------------
Cell index: 21
Input Cell Type: markdown
Input Text:
-----------
Lets explain what we see:

 - the first row shows the data distribution of the coefficients along this main variance direction. the red dots correspond to 1st, 20th, 40th, 60th, 80th and 99th percentiles of this distribution.
 - the second row is like the columns were in the previous plot. for example, we can see here in this particular case a gradual increase in leaf size from left to right.
 - the bottom 4 rows at each column hold real leaf images that have the first PCA coefficient be at the value of the corresponding percentile  of that column. for example, the left most 4 bottom pictures are leafs with a PC1 coefficient to be approximately -1.6 and the right most 4 bottom pictures are leafs with a PC1 coefficient to be approximately 2.7

By examining the the leafs that have different coefficients **we can see what this component coefficient represents**.  from the point of view of this particular component, the leaf images in the same column are very similar. we can therefore see what this particular feature "thinks" about similar leafs.

In this particular case we can see that it's about **leaf size** since we see **a gradual increase in leaf size from left to right**.


----------


**Eigenvector 2:**
------------------



Output Text:
------------


------------------------------------------------------------
Cell index: 22
Input Cell Type: python
Input Text:
-----------
matplotlib.rcParams['font.size'] = 6
matplotlib.rcParams['figure.figsize'] = (12,8)
leaf_PCAModel.ShowSingleComponentVariation(X, listOfComponents=[1])

Output Text:
------------
<matplotlib.figure.Figure at 0x7f0c962284a8>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
This image presents a visualization of a machine learning model's performance, specifically focusing on the explained variance across different percentiles of a dataset. The top section shows a percentile-explained variance curve.  The x-axis represents percentiles (1%, 20%, 40%, 60%, 80%, 99%), and the y-axis represents the explained variance. The curve is a blue area plot, peaking around the 60th percentile, indicating that this region contributes most significantly to the model's explained variance. Red markers highlight key points on the curve. The title "13.6717438698% explained" suggests that the model, overall, explains roughly 13.7% of the variance in the data.


The majority of the image is dedicated to a grid of images arranged in six columns and five rows. Each column corresponds to a percentile from the top plot. Each row displays the nearest-neighbor (NN) image from the training dataset that best matches the characteristics represented by that percentile.  The images are grayscale or black and white, suggesting that they might be binary masks or segmented images of leaves.  The label "NN with closest percentile" below each image confirms that the image is the closest match from the training data to the features associated with the percentile in that column. The images show a progression of leaf shapes, with simpler shapes at the lower percentiles and more complex or diverse shapes at the higher percentiles, suggesting that the model learns simpler features earlier and more complex features later in the training process.


------------------------------------------------------------
Cell index: 23
Input Cell Type: markdown
Input Text:
-----------
Here we can see that the second principal component is about explaining the difference between vertical and horizontal leafs

Output Text:
------------


------------------------------------------------------------
Cell index: 24
Input Cell Type: markdown
Input Text:
-----------
**Eigenvector 4:**
------------------

Output Text:
------------


------------------------------------------------------------
Cell index: 25
Input Cell Type: python
Input Text:
-----------
matplotlib.rcParams['font.size'] = 6
matplotlib.rcParams['figure.figsize'] = (12,8)
leaf_PCAModel.ShowSingleComponentVariation(X, listOfComponents=[3])

Output Text:
------------
<matplotlib.figure.Figure at 0x7f0c963410f0>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image presents a visualization of a machine learning model's performance, specifically focusing on how well it explains variance in a dataset.  The top section displays a kernel density estimate (KDE) plot, a smooth curve representing the probability density of the data.  Several red markers highlight specific percentiles (1%, 20%, 40%, 60%, 80%, and 99%) along the x-axis, indicating points of interest in the data distribution.  The title "4.7216642648% explained" suggests that the model only accounts for a small portion of the overall data variance.

Below the KDE plot, the image is organized into a grid showcasing six sets of images. Each set contains three images arranged horizontally.  The leftmost column shows the input images (though not explicitly labeled as such). The middle and right columns show the nearest neighbors (NN) from the model's perspective, selected based on their proximity to the input images in terms of percentile. The text "NN with closest percentile" is consistently displayed under each NN image, indicating that the model chose these images as the best matches based on the data's percentile ranking.

Each of the six sets of images likely represents a different data point or a sample from the dataset. The images themselves appear to be grayscale representations of leaf shapes, possibly used as input for a classification or other image recognition task. The overall structure aims to illustrate the model's performance by comparing input images with their nearest neighbors according to the model's interpretation of the data distribution.  The low percentage of explained variance suggests the model might be struggling to capture the underlying patterns in the data.


------------------------------------------------------------
Cell index: 26
Input Cell Type: markdown
Input Text:
-----------
This is another eigenvector that is about vertical vs horizontal, but we can see on the left most column that in addition to being horizontal, these leaf images also have a pointy tip at the top (well, except from the first image)


**EigenVector 8:**
------------------

Output Text:
------------


------------------------------------------------------------
Cell index: 27
Input Cell Type: python
Input Text:
-----------
matplotlib.rcParams['font.size'] = 6
matplotlib.rcParams['figure.figsize'] = (12,8)
leaf_PCAModel.ShowSingleComponentVariation(X, listOfComponents=[7])

Output Text:
------------
<matplotlib.figure.Figure at 0x7f0da1a82668>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image displays a visualization of a machine learning model's performance, specifically focusing on how well it predicts images based on their percentile ranking within a dataset.  The top section shows a density plot, a bell curve illustrating the distribution of a particular feature or metric across the dataset.  Red dots highlight specific percentiles (1%, 20%, 40%, 60%, 80%, and 99%) along this distribution. The percentage "1.38627281412% explained" likely refers to the variance explained by the chosen feature.

The majority of the image is dedicated to a grid of images.  Each column represents one of the selected percentiles.  Each row within a column shows the actual image at that percentile (top row), and then three examples of images that the neural network (NN) identified as being closest to that percentile (subsequent rows).  These images are binary, showing white shapes on a black background, suggesting a segmentation or object recognition task. The purpose is to visually inspect how well the model's predictions align with the actual data across different percentiles of the feature distribution, allowing for an assessment of the model's performance and potential biases.  The images show a progression from simple, less complex shapes (lower percentiles) to more intricate shapes (higher percentiles).


------------------------------------------------------------
Cell index: 28
Input Cell Type: markdown
Input Text:
-----------
This one is about beeing a star like leaf

Output Text:
------------


------------------------------------------------------------
Cell index: 29
Input Cell Type: markdown
Input Text:
-----------
## Show Scatter plot of Leaf images as points in high dimentional space
Ok, now that we have some grasp about these distributions, let's also visualize the scatter of the subspace that is spanned by the PCs. we will do this in two ways:

 - Plot the scatter plot of the **first two principal component coeffients**
 - Plot a **2D approximation** of the "high dimensional scatter plot" of the entire space using **t-SNE** 

Output Text:
------------


------------------------------------------------------------
Cell index: 30
Input Cell Type: python
Input Text:
-----------
#%% plot scatter of 2 PCs and t-SNE of all PCs (with labels as colors)
matplotlib.rcParams['font.size'] = 12
matplotlib.rcParams['figure.figsize'] = (12,8)

X_train = X[trainIDs-1,:]
y_train = trainLabels

leaf_PCAModel.ShowDataScatterPlotsWithTSNE(X_train, y_train, tSNE_perplexity=10.0)

Output Text:
------------
<matplotlib.figure.Figure at 0x7f0da6c91ba8>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a comparative visualization of two dimensionality reduction techniques applied to a dataset: Principal Component Analysis (PCA) and t-distributed Stochastic Neighbor Embedding (t-SNE).  The visualization is split into two distinct scatter plots, each representing the reduced dimensionality data.

The left panel displays the PCA representation. The x-axis is labeled "PC1 coeff," and the y-axis is labeled "PC2 coeff," representing the first and second principal components, respectively.  The data points are colored, suggesting the presence of different classes or groups within the dataset. The distribution of points shows a somewhat elongated structure, indicative of the data's inherent dimensionality.

The right panel showcases the t-SNE representation.  The x-axis is labeled "t-SNE axis1," and the y-axis is "t-SNE axis2."  Similarly to the PCA plot, data points are color-coded, allowing for visual comparison of group separation.  The t-SNE plot displays a different spatial arrangement of the data points compared to PCA.  While PCA aims to maximize variance along principal components, t-SNE focuses on preserving local neighborhood structures, potentially revealing clusters not readily apparent in the PCA representation.  The clusters are more separated in the t-SNE visualization compared to the PCA plot.

In summary, the image effectively compares the results of two different dimensionality reduction methods, highlighting the strengths and weaknesses of each approach in terms of data visualization and cluster separation.  The color-coding of the data points facilitates the visual interpretation of the results.


------------------------------------------------------------
Cell index: 31
Input Cell Type: markdown
Input Text:
-----------
We can see that nearby points usually have similar color and this means they have similar leaf label. This makes us confident that we can achieve at least some classification accuracy from these PCA features.


----------
## Show Model Accuracy as function of num PCA components

Now, let's see what is the **classification accuracy** using this PCA representation if we use **different amount of PCA coefficients** for **several different types of classifiers**.

Output Text:
------------


------------------------------------------------------------
Cell index: 32
Input Cell Type: python
Input Text:
-----------
#%% plot CV classification accuracy as function of num components used for 3 very different type of classifiers
matplotlib.rcParams['font.size'] = 12
matplotlib.rcParams['figure.figsize'] = (12,8)

X_PCA = leaf_PCAModel.RepresentUsingModel(X)

X_PCA_train = X_PCA[trainIDs-1,:]
y_train = trainLabels

numPCsToUse = [1,2,4,8,16,32,64]

logReg = linear_model.LogisticRegression(C=10.0)
kNN = neighbors.KNeighborsClassifier(n_neighbors=7)
RF = ensemble.RandomForestClassifier(n_estimators=100)

logRegMeanAccuracy = []; kNN_MeanAccuracy = []; RF_MeanAccuracy = []
logRegAccuracyStd  = []; kNN_AccuracyStd  = []; RF_AccuracyStd  = []

for numPCs in numPCsToUse:
    stratifiedCV = model_selection.StratifiedKFold(n_splits=5, random_state=1)
    logRegAccuracy = []; kNN_Accuracy = []; RF_Accuracy = []
    for trainInds, validInds in stratifiedCV.split(X_PCA_train, y_train):
        X_train_cv = X_PCA_train[trainInds,:numPCs]
        X_valid_cv = X_PCA_train[validInds,:numPCs]

        y_train_cv = y_train[trainInds]
        y_valid_cv = y_train[validInds]

        logReg.fit(X_train_cv, y_train_cv)
        kNN.fit(X_train_cv, y_train_cv)
        RF.fit(X_train_cv, y_train_cv)
    
        logRegAccuracy.append(accuracy_score(y_valid_cv, logReg.predict(X_valid_cv)))
        kNN_Accuracy.append(accuracy_score(y_valid_cv, kNN.predict(X_valid_cv)))
        RF_Accuracy.append(accuracy_score(y_valid_cv, RF.predict(X_valid_cv)))

    logRegMeanAccuracy.append(np.array(logRegAccuracy).mean())
    logRegAccuracyStd.append(np.array(logRegAccuracy).std())

    kNN_MeanAccuracy.append(np.array(kNN_Accuracy).mean())
    kNN_AccuracyStd.append(np.array(kNN_Accuracy).std())

    RF_MeanAccuracy.append(np.array(RF_Accuracy).mean()) 
    RF_AccuracyStd.append(np.array(RF_Accuracy).std())
        
plt.figure()
plt.errorbar(x=numPCsToUse, y=logRegMeanAccuracy, yerr=logRegAccuracyStd)
plt.errorbar(x=numPCsToUse, y=kNN_MeanAccuracy  , yerr=kNN_AccuracyStd)
plt.errorbar(x=numPCsToUse, y=RF_MeanAccuracy   , yerr=RF_AccuracyStd)
plt.xlim(min(numPCsToUse)-1,max(numPCsToUse)+1); 
plt.legend(['Logistic Regression','k Nearest Neighbor','Random Forest'],loc=2)
plt.xlabel('num PCA Components'); 
plt.ylabel('Validation Accuracy'); 
plt.title('Accuracy as function of num PCs')

Output Text:
------------
<matplotlib.text.Text at 0x7f0da1485cc0>
<matplotlib.figure.Figure at 0x7f0c961e6a90>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a graph illustrating the validation accuracy of three different machine learning models—Logistic Regression, k Nearest Neighbor, and Random Forest—as a function of the number of principal components (PCs) used in Principal Component Analysis (PCA).  The x-axis represents the number of PCA components, while the y-axis shows the validation accuracy.  Each model's performance is depicted with a line and error bars indicating variability.

The graph shows that accuracy generally improves as the number of PCs increases, initially rising steeply for all three models before flattening out.  Logistic Regression and Random Forest achieve higher accuracy than k Nearest Neighbor across the range of PCs tested. The Random Forest model consistently shows the highest accuracy, followed by Logistic Regression, and then k Nearest Neighbor. The error bars suggest that the variability in accuracy is relatively consistent across different numbers of PCs for each model.  The graph's title clearly indicates that it displays the relationship between the number of PCs and model accuracy.


------------------------------------------------------------
Cell index: 33
Input Cell Type: markdown
Input Text:
-----------
Overall, it's evident that all classifiers achieve approximately similar performance.

But it's interesting to note the somewhat different behavior of these different classifiers as a function of number of components used. 

For example, the nearest neighbor classifier flattens out early and does not benefit from additional components beyond 8, whereas the logistic regression classifier continues to increase it's performance up to around 32 components.
The Random Forest classifier is consistently the best performing but it also flattens out at around 32 components.

Output Text:
------------


------------------------------------------------------------

================================================================================
================================================================================
celebrity-face-swap.ipynb:
==========================
------------------------------------------------------------
Cell index: 1
Input Cell Type: markdown
Input Text:
-----------
# Celebrity Face Swap (the poor man's DeepFake)
Recently we've seen [Nicholas Cage](https://www.youtube.com/watch?v=2jp4M1cIJ5A)'s face appear everywhere (for those who don't know what I'm talking about, please have a look)

In this script we will get familiar with:  
* image warping using [picewise-affine warp](https://scikit-image.org/docs/dev/auto_examples/transform/plot_piecewise_affine.html)
* [possion blending](https://cs.brown.edu/courses/csci1950-g/results/proj2/edwallac/) for seamless "pasting" of image parts into a full image

Specifically, we ask wheather we can build a very simple face swapping algorithm if we have just the facial landmaks for that face?

Output Text:
------------


------------------------------------------------------------
Cell index: 2
Input Cell Type: python
Input Text:
-----------
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn import decomposition
from sklearn import cluster
from skimage import transform as tf
import glob
from skimage.morphology  import disk, erosion
from skimage.restoration import inpaint
import cv2
import os 
from skimage.measure import label, regionprops

np.random.seed(1)

base_folder = '../input/' 
csv_filename = 'youtube_faces_with_keypoints_full.csv'
videoDF = pd.read_csv(os.path.join(base_folder, csv_filename))

# create a dictionary that maps videoIDs to full file paths
npz_files_full_path = glob.glob(base_folder + 'youtube_faces_*/*/*.npz')
videoIDs = [x.split('/')[-1].split('.')[0] for x in npz_files_full_path]
full_paths = {}
for videoID, fullPath in zip(videoIDs, npz_files_full_path):
    full_paths[videoID] = fullPath

# remove from the large csv file all videos that weren't uploaded yet
videoDF = videoDF.loc[videoDF.loc[:,'videoID'].isin(full_paths.keys()),:].reset_index(drop=True)

Output Text:
------------


------------------------------------------------------------
Cell index: 3
Input Cell Type: markdown
Input Text:
-----------
# Apply Kmeans on the 2D shapes and present them (we will use them later)
Note that this is identical to what we did in the [exploration script](https://www.kaggle.com/selfishgene/exploring-youtube-faces-with-keypoints-dataset)

Output Text:
------------


------------------------------------------------------------
Cell index: 4
Input Cell Type: python
Input Text:
-----------
#%% collect all 2D landmarks and normlize them
NUM_LANDMARKS = 68
NUM_DIMS = 2

# shape normalization function
def normlize_shapes(shapes_im_coords):
    """shapes_normlized, scale_factors, mean_coords  = normlize_shapes(shapes_im_coords)"""
    (num_points, num_dims, _) = shapes_im_coords.shape

    # calc mean coords and subtract from shapes    
    mean_coords = shapes_im_coords.mean(axis=0)
    shapes_centered = np.zeros(shapes_im_coords.shape)
    shapes_centered = shapes_im_coords - np.tile(mean_coords,[num_points, 1, 1])

    # calc scale factors and divide shapes
    scale_factors = np.sqrt((shapes_centered**2).sum(axis=1)).mean(axis=0)
    shapes_normlized = np.zeros(shapes_centered.shape)
    shapes_normlized = shapes_centered / np.tile(scale_factors, [num_points, num_dims, 1])

    return shapes_normlized, scale_factors, mean_coords

# collect all 2D shapes from all frames from all videos to a single numpy array matrix
total_num_frames = videoDF['videoDuration'].sum()
landmarks2D_all = np.zeros((NUM_LANDMARKS, NUM_DIMS,int(total_num_frames)))
shape_ind_to_videoID_map = {} # dictionary for later useage

end_ind = 0
for i, videoID in enumerate(videoDF['videoID']):
    # load video
    video_file  = np.load(full_paths[videoID])
    landmarks2D = video_file['landmarks2D']

    start_ind = end_ind
    end_ind   = start_ind + landmarks2D.shape[2]

    # store in one big array
    landmarks2D_all[:,:,start_ind:end_ind] = landmarks2D

    # make sure we keep track of the mapping to the original video and frame
    for videoFrameInd, shapeInd in enumerate(range(start_ind,end_ind)):
        shape_ind_to_videoID_map[shapeInd] = (videoID, videoFrameInd)

# normlize shapes
landmarks2D_normlized, _, _  = normlize_shapes(landmarks2D_all)

Output Text:
------------


------------------------------------------------------------
Cell index: 5
Input Cell Type: python
Input Text:
-----------
#%% apply kmeans on 2D shapes and show them (like we did in the exploration script)
num_cols = 7
num_rows = 3
num_clusters = num_cols * num_rows
normalized_shapes_table = np.reshape(landmarks2D_normlized, [NUM_LANDMARKS * NUM_DIMS, landmarks2D_normlized.shape[2]]).T

ShapesKmeansModel  = cluster.KMeans(n_clusters=num_clusters, n_init=5, random_state=1).fit(normalized_shapes_table)
cluster_assignment = ShapesKmeansModel.predict(normalized_shapes_table)

ShapesPCAModel = decomposition.PCA(n_components=30).fit(normalized_shapes_table)

# show the clusters
jaw_points           = range( 0,17)
rigth_eyebrow_points = range(17,22)
left_eyebrow_points  = range(22,27)
nose_ridge_points    = range(27,31)
nose_base_points     = range(31,36)
right_eye_points     = range(36,42)
left_eye_points      = range(42,48)
outer_mouth_points   = range(48,60)
inner_mouth_points   = range(60,68)

closing_right_eye   = [36,41]
closing_left_eye    = [42,47]
closing_outer_mouth = [48,59]
closing_inner_mouth = [60,67]

list_of_connected_points = [jaw_points, rigth_eyebrow_points, left_eyebrow_points, nose_ridge_points, nose_base_points,
                            right_eye_points, left_eye_points, outer_mouth_points, inner_mouth_points,
                            closing_right_eye, closing_left_eye, closing_outer_mouth, closing_inner_mouth]

fig, ax = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(12,8))
for i in range(num_clusters):
    curr_landmarks2D = np.reshape(ShapesKmeansModel.cluster_centers_[i,:], [NUM_LANDMARKS, NUM_DIMS])
    ax[i%num_rows,i//num_rows].scatter(curr_landmarks2D[:,0], -curr_landmarks2D[:,1], c='r',s=7.0)
    for conPts in list_of_connected_points:
        xPts = curr_landmarks2D[conPts,0]
        yPts = curr_landmarks2D[conPts,1]
        ax[i % num_rows,i // num_rows].plot(xPts,-yPts,color='g',linewidth=1.0)         
    ax[i % num_rows, i // num_rows].set_title('cluster %d' %(i))
    ax[i % num_rows, i // num_rows].set_axis_off()
fig.tight_layout()


Output Text:
------------
<matplotlib.figure.Figure at 0x7fdc7ac42668>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image displays a grid of 21 plots, each showcasing a stylized representation of a human face.  These faces are constructed from a series of interconnected red dots, outlining facial features like eyes, nose, mouth, and jawline. A green line connects the dots, creating a continuous contour around the face. Each plot is titled with "cluster" followed by a number (0-20), suggesting that these faces are grouped into 21 distinct clusters based on shared characteristics.  The arrangement implies a visual comparison of these clustered facial features.

The faces within each cluster appear similar, though subtle variations exist in the positioning and shape of the facial features between different clusters. The consistent use of red dots and green lines provides visual uniformity across all plots, making it easy to compare the different clusters side-by-side. The overall structure suggests a result from a clustering algorithm applied to a dataset of facial landmark points.  The visual representation effectively communicates the results of this process, allowing for a quick assessment of the similarities and differences within each cluster.


------------------------------------------------------------
Cell index: 6
Input Cell Type: markdown
Input Text:
-----------
Define a helper function to warp image with a picewise affine tranform that we will use later

Output Text:
------------


------------------------------------------------------------
Cell index: 7
Input Cell Type: python
Input Text:
-----------
# warp image with a picewise affine transform
def warp_im_picewise_affine(src_im, src_coords, dst_coords, target_im_size=None):
    """warped_im = warp_im_picewise_affine(src_im, src_coords, dst_coords, target_im_size)"""
    requested_transfromation = tf.estimate_transform('piecewise-affine',src_coords,dst_coords)
    warped_im = tf.warp(src_im,requested_transfromation.inverse,output_shape=target_im_size)
    return warped_im

Output Text:
------------


------------------------------------------------------------
Cell index: 8
Input Cell Type: markdown
Input Text:
-----------
# Take a random frontal facing face and warp it to all other shape clusters

Output Text:
------------


------------------------------------------------------------
Cell index: 9
Input Cell Type: python
Input Text:
-----------
#%% take a random frontal face image and warp it's texture into all kmeans shapes to demonstrate picewise affine warping
np.random.seed(1)

# get shape ids only of large faces
large_faces_videoIDs = videoDF.loc[videoDF['averageFaceSize'] > 80, 'videoID'].tolist()
shape_ind_large_face_only = []
for key, value in shape_ind_to_videoID_map.items():
    if value[0] in large_faces_videoIDs:
        shape_ind_large_face_only.append(key)
shape_ind_large_face_only = np.array(shape_ind_large_face_only)
        
overlay_landmarks = False
frontal_shape_cluster_ind = 16
frontal_inds = np.nonzero(cluster_assignment == frontal_shape_cluster_ind)[0]
frontal_inds = np.array(list(set(frontal_inds).intersection(set(shape_ind_large_face_only))))

some_frontal_image_ind = np.random.choice(frontal_inds,size=1)[0]

selected_videoID = shape_ind_to_videoID_map[some_frontal_image_ind][0]
selected_video_frame = shape_ind_to_videoID_map[some_frontal_image_ind][1]

video_file = np.load(full_paths[selected_videoID])
selected_frontal_image = video_file['colorImages'][:,:,:,selected_video_frame]
selected_frontal_shape = video_file['landmarks2D'][:,:,selected_video_frame]

#plt.figure(); plt.imshow(selected_frontal_image); plt.scatter(selected_frontal_shape[:,0],selected_frontal_shape[:,1])

plt.close('all')
fig, ax = plt.subplots(nrows=num_rows,ncols=num_cols,figsize=(16,9))
for i in range(num_clusters):
    curr_landmarks2D_norm = np.reshape(ShapesKmeansModel.cluster_centers_[i,:], [NUM_LANDMARKS,NUM_DIMS])
    
    # project the shape and reconstruct using the PCA model (to correct small shape jitter)
    curr_landmarks2D_norm_rec = ShapesPCAModel.transform(curr_landmarks2D_norm.reshape([1,NUM_LANDMARKS*NUM_DIMS]))
    curr_landmarks2D_norm_rec = ShapesPCAModel.inverse_transform(curr_landmarks2D_norm_rec)
    curr_landmarks2D_norm_rec = curr_landmarks2D_norm_rec.reshape([NUM_LANDMARKS,NUM_DIMS])
    
    # tranform to image coordinates
    curr_landmarks2D =  50 * curr_landmarks2D_norm_rec
    curr_landmarks2D += 100
    
    if i == frontal_shape_cluster_ind:
        # when the desired cluster is frontal, just show the original image
        ax[i % num_rows, i // num_rows].imshow(selected_frontal_image)
        ax[i % num_rows, i // num_rows].set_title('original image')
    else:
        # else, warp the image to the desired cluster
        warped_im = warp_im_picewise_affine(selected_frontal_image, selected_frontal_shape, curr_landmarks2D, target_im_size=[200,200])
        ax[i % num_rows, i // num_rows].imshow(warped_im)
        ax[i % num_rows, i // num_rows].set_title('cluster %d' %(i))
        
        # show the landmarks
        if overlay_landmarks:
            ax[i % num_rows, i // num_rows].scatter(curr_landmarks2D[:,0], curr_landmarks2D[:,1], c='r', s=1.3)
            for conPts in list_of_connected_points:
                xPts = curr_landmarks2D[conPts,0]
                yPts = curr_landmarks2D[conPts,1]
                ax[i % num_rows, i // num_rows].plot(xPts,yPts,color='g',linewidth=0.6)
    
    ax[i % num_rows,i // num_rows].set_axis_off()
fig.tight_layout()
fig.subplots_adjust(left=0.02, bottom=0.02, right=0.98, top=0.98, wspace=0.05, hspace=0.05)


Output Text:
------------
<matplotlib.figure.Figure at 0x7fdc8dd3ee10>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image displays 21 cropped images arranged in a grid of 3 rows and 7 columns.  Each of the first 20 images is labeled "cluster #" followed by a number from 0 to 20. These images appear to be close-ups of the same person's face, specifically what seems to be a 3D model or rendering of George H. W. Bush's face, showing slight variations in expression, lighting, or angle.  The variations are subtle, suggesting that these are different instances or renderings clustered together by an algorithm.

The central image in the second row is labeled "original image," showing a full shot of George H. W. Bush in a suit and tie, which serves as a reference point for the cropped facial images. The cropped images all share a similar background which is black, suggesting that they are extracted from a larger image or video.

The overall structure suggests that the image is an output from a clustering algorithm applied to facial images. The algorithm has identified 21 clusters of similar facial features, possibly from a larger dataset of images of the same individual.  The slight differences between the images within each cluster could be due to factors like lighting, expression, or the process of image segmentation and extraction.


------------------------------------------------------------
Cell index: 10
Input Cell Type: markdown
Input Text:
-----------
We can see that just warping the texture of an image in this way can result in some **realistic looking faces** at new poses

Output Text:
------------


------------------------------------------------------------
Cell index: 11
Input Cell Type: markdown
Input Text:
-----------
### Define a helper function to embed an image in another image and inpaint the boards of the original image for smoother transition between the textures

Output Text:
------------


------------------------------------------------------------
Cell index: 12
Input Cell Type: python
Input Text:
-----------
# overlay image and inpaint boarders
def embed_image_in_another_image(image_to_embed, target_image, inpaint_boarders=True):
    assert(image_to_embed.shape[0] == target_image.shape[0])
    assert(image_to_embed.shape[1] == target_image.shape[1])
    
    # get the embedding mask (any non black color will be considered)
    embedding_mask = np.any(image_to_embed, axis=2)
    
    # overlay on top of original image
    overlayed_im = target_image.copy()
    if image_to_embed.dtype in [np.float16, np.float32, np.float64]:
        overlayed_im[embedding_mask] = 255 * image_to_embed[embedding_mask]
    else:
        overlayed_im[embedding_mask] = image_to_embed[embedding_mask]

    # do very basic inpainting
    if inpaint_boarders:
        # erode the mask a little bit
        embedding_mask_eroded = erosion(embedding_mask, disk(3.5))    
        embedding_mask_boundry = np.bitwise_and(embedding_mask, 
                                                np.logical_not(embedding_mask_eroded))
        # inpaint the boarder of the image so that we have a smoother transition between the embeded image and target
        overlayed_im = inpaint.inpaint_biharmonic(overlayed_im, embedding_mask_boundry, 
                                                  multichannel=True)

    return overlayed_im

Output Text:
------------


------------------------------------------------------------
Cell index: 13
Input Cell Type: python
Input Text:
-----------
def interpret_using_shape_model(ShapesPCAModel, shape):
    # normlize shape
    shape_norm, scale, trans = normlize_shapes(shape[:,:,np.newaxis])
    
    # project the shape and reconstruct using the PCA model (to correct small shape jitter)
    shape_norm_rec = ShapesPCAModel.inverse_transform(ShapesPCAModel.transform(shape_norm.reshape([1,NUM_LANDMARKS*NUM_DIMS])))
    shape_norm_rec = shape_norm_rec.reshape([NUM_LANDMARKS,NUM_DIMS])
    
    # tranform back to image coordinates
    shape_rec = scale[0]*shape_norm_rec
    shape_rec[:,0] += trans[0,0]
    shape_rec[:,1] += trans[1,0]

    return shape_rec

Output Text:
------------


------------------------------------------------------------
Cell index: 14
Input Cell Type: markdown
Input Text:
-----------
# Swap Celebrity Faces
We constrain ourselves here to non extreeme face poses here since they create warping artifacts

Output Text:
------------


------------------------------------------------------------
Cell index: 15
Input Cell Type: python
Input Text:
-----------
#%% take two almost frontal celebrity images and swap their faces
np.random.seed(1)

# choose some candidate images
almost_frontal_shape_cluster_inds = [0,1,5,6,8,9,10,11,14,15,16,17,18,19,20]
almost_frontal_inds = np.nonzero([x in almost_frontal_shape_cluster_inds for x in cluster_assignment])[0]
almost_frontal_inds = np.array(list(set(almost_frontal_inds).intersection(set(shape_ind_large_face_only))))

some_frontal_image_inds = np.random.choice(frontal_inds,size=2)
selected_videoID_A     = shape_ind_to_videoID_map[some_frontal_image_inds[0]][0]
selected_video_frame_A = shape_ind_to_videoID_map[some_frontal_image_inds[0]][1]
selected_videoID_B     = shape_ind_to_videoID_map[some_frontal_image_inds[1]][0]
selected_video_frame_B = shape_ind_to_videoID_map[some_frontal_image_inds[1]][1]

# load image A
video_file_A = np.load(full_paths[selected_videoID_A])
image_A = video_file_A['colorImages'][:,:,:,selected_video_frame_A]
shape_A = video_file_A['landmarks2D'][:,:,selected_video_frame_A]
shape_A = interpret_using_shape_model(ShapesPCAModel, shape_A)
image_A_size = image_A.shape[:2]

# load image B
video_file_B = np.load(full_paths[selected_videoID_B])
image_B = video_file_B['colorImages'][:,:,:,selected_video_frame_B]
shape_B = video_file_B['landmarks2D'][:,:,selected_video_frame_B]
shape_B = interpret_using_shape_model(ShapesPCAModel, shape_B)
image_B_size = image_B.shape[:2]

# warp faces into the other shape
A_warped_into_B = warp_im_picewise_affine(image_A, shape_A, shape_B, target_im_size=image_B_size)
B_warped_into_A = warp_im_picewise_affine(image_B, shape_B, shape_A, target_im_size=image_A_size)

# embed the faces in the target image
A_embedded_into_B = embed_image_in_another_image(A_warped_into_B, image_B)
B_embedded_into_A = embed_image_in_another_image(B_warped_into_A, image_A)

# plot
plt.figure(figsize=(12,9))
plt.subplots_adjust(left=0.02, bottom=0.02, right=0.98, top=0.98, wspace=0.05, hspace=0.05)
plt.subplot(2,3,1); plt.imshow(image_A); plt.title('image A'); plt.axis('off');
plt.subplot(2,3,2); plt.imshow(B_warped_into_A); plt.title("B's face warped into A's shape"); plt.axis('off');
plt.subplot(2,3,3); plt.imshow(B_embedded_into_A); plt.title("B's face embedded into A's backgrund"); plt.axis('off');
plt.subplot(2,3,4); plt.imshow(image_B); plt.title('image B'); plt.axis('off');
plt.subplot(2,3,5); plt.imshow(A_warped_into_B); plt.title("A's face warped into B's shape"); plt.axis('off');
plt.subplot(2,3,6); plt.imshow(A_embedded_into_B); plt.title("A's face embedded into B's backgrund"); plt.axis('off');


Output Text:
------------
<matplotlib.figure.Figure at 0x7fdc787e0160>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a grid of six smaller images, arranged in two rows and three columns.  Each smaller image shows a different stage in a face-swapping or morphing process between two individuals. The top row shows manipulations using a male face (Image A, George W. Bush) as the base, and a female face (Image B, an unidentified female news anchor) as the source. The bottom row reverses this, using the female face as the base and the male face as the source.


The first column shows the original images: Image A (George W. Bush in a suit) in the upper left and Image B (the female news anchor) in the lower left. The second column displays the results of warping each person's face to match the shape of the other person's face.  These images show only the warped face on a black background.  The third column displays the final results: the warped face seamlessly blended into the original background of the other person's image.  This gives the impression that one person's face is on the other person's body.  The overall effect demonstrates a face-swapping technique that involves both shape warping and background blending.


------------------------------------------------------------
Cell index: 16
Input Cell Type: markdown
Input Text:
-----------
We can see that there are some large **lighting conditions** and **skin color variations** between the photos that are not accounted for.  
Nevertheless, the results are quite amusing :-)

**Note**, however, that by using piecewise-affine warping the shapes are very nicely aligned.  
Also, you can try changing the random seed and look at few other swaps. some of them are quite fun.

Output Text:
------------


------------------------------------------------------------
Cell index: 17
Input Cell Type: markdown
Input Text:
-----------
# Now, let's apply very simple texture normalization step and see the results
Just replace the mean and std of the absolut deviation from the mean of each (R,G,B) pixel value of the embeded face to match the source face

Output Text:
------------


------------------------------------------------------------
Cell index: 18
Input Cell Type: python
Input Text:
-----------
#%% take two almost frontal celebrity images and swap their faces
np.random.seed(1)

# choose some candidate images
almost_frontal_shape_cluster_inds = [0,1,5,6,8,9,10,11,14,15,16,17,18,19,20]
almost_frontal_inds = np.nonzero([x in almost_frontal_shape_cluster_inds for x in cluster_assignment])[0]
almost_frontal_inds = np.array(list(set(almost_frontal_inds).intersection(set(shape_ind_large_face_only))))

some_frontal_image_inds = np.random.choice(frontal_inds,size=2)
selected_videoID_A     = shape_ind_to_videoID_map[some_frontal_image_inds[0]][0]
selected_video_frame_A = shape_ind_to_videoID_map[some_frontal_image_inds[0]][1]
selected_videoID_B     = shape_ind_to_videoID_map[some_frontal_image_inds[1]][0]
selected_video_frame_B = shape_ind_to_videoID_map[some_frontal_image_inds[1]][1]

# load image A
video_file_A = np.load(full_paths[selected_videoID_A])
image_A = video_file_A['colorImages'][:,:,:,selected_video_frame_A]
shape_A = video_file_A['landmarks2D'][:,:,selected_video_frame_A]
shape_A = interpret_using_shape_model(ShapesPCAModel, shape_A)
image_A_size = image_A.shape[:2]

# load image B
video_file_B = np.load(full_paths[selected_videoID_B])
image_B = video_file_B['colorImages'][:,:,:,selected_video_frame_B]
shape_B = video_file_B['landmarks2D'][:,:,selected_video_frame_B]
shape_B = interpret_using_shape_model(ShapesPCAModel, shape_B)
image_B_size = image_B.shape[:2]

# warp faces into the other shape
A_warped_into_B = warp_im_picewise_affine(image_A, shape_A, shape_B, target_im_size=image_B_size)
B_warped_into_A = warp_im_picewise_affine(image_B, shape_B, shape_A, target_im_size=image_A_size)

### equate the mean and variance of the textures

# calculate the support mask for each warped image
A_warped_into_B_mask = np.any(A_warped_into_B, axis=2)
B_warped_into_A_mask = np.any(B_warped_into_A, axis=2)

# calculate the mean and std
A_texture_pixels = A_warped_into_B[A_warped_into_B_mask]
A_texture_mean   = A_texture_pixels.mean(axis=0)
A_texture_std    = np.sqrt(np.sum(A_texture_pixels**2,axis=1)).std()
B_texture_pixels = B_warped_into_A[B_warped_into_A_mask]
B_texture_mean   = B_texture_pixels.mean(axis=0)
B_texture_std    = np.sqrt(np.sum(B_texture_pixels**2,axis=1)).std()

# normalize the textures
A_texture_pixels_norm = A_texture_pixels - np.tile(A_texture_mean[np.newaxis,:],[A_texture_pixels.shape[0],1])
A_texture_pixels_norm /= A_texture_std
B_texture_pixels_norm = B_texture_pixels - np.tile(B_texture_mean[np.newaxis,:],[B_texture_pixels.shape[0],1])
B_texture_pixels_norm /= B_texture_std

# switch the means and stds
B_texture_pixels_adjusted = B_texture_pixels_norm * A_texture_std
B_texture_pixels_adjusted = B_texture_pixels_adjusted + np.tile(A_texture_mean[np.newaxis,:],[B_texture_pixels.shape[0],1])
A_texture_pixels_adjusted = A_texture_pixels_norm * B_texture_std
A_texture_pixels_adjusted = A_texture_pixels_adjusted + np.tile(B_texture_mean[np.newaxis,:],[A_texture_pixels.shape[0],1])

# place back 
A_warped_into_B_adjusted = A_warped_into_B.copy()
A_warped_into_B_adjusted[A_warped_into_B_mask] = A_texture_pixels_adjusted
A_warped_into_B_adjusted[A_warped_into_B_adjusted < 0] = 0
A_warped_into_B_adjusted[A_warped_into_B_adjusted > 1] = 1

B_warped_into_A_adjusted = B_warped_into_A.copy()
B_warped_into_A_adjusted[B_warped_into_A_mask] = B_texture_pixels_adjusted
B_warped_into_A_adjusted[B_warped_into_A_adjusted < 0] = 0
B_warped_into_A_adjusted[B_warped_into_A_adjusted > 1] = 1

# finally, embed the adjusted faces inside the target image
A_embedded_into_B = embed_image_in_another_image(A_warped_into_B_adjusted, image_B)
B_embedded_into_A = embed_image_in_another_image(B_warped_into_A_adjusted, image_A)

plt.figure(figsize=(16,8))
plt.subplots_adjust(left=0.02, bottom=0.02, right=0.98, top=0.98, wspace=0.1, hspace=0.1)
plt.subplot(2,4,1); plt.imshow(image_A); plt.title('image A'); plt.axis('off');
plt.subplot(2,4,2); plt.imshow(B_warped_into_A); plt.title("B's face warped into A's shape"); plt.axis('off');
plt.subplot(2,4,3); plt.imshow(B_warped_into_A_adjusted); plt.title("B's face warped into A's shape (texture adjusted)"); plt.axis('off');
plt.subplot(2,4,4); plt.imshow(B_embedded_into_A); plt.title("B's face embedded into A's backgrund"); plt.axis('off');
plt.subplot(2,4,5); plt.imshow(image_B); plt.title('image B'); plt.axis('off');
plt.subplot(2,4,6); plt.imshow(A_warped_into_B); plt.title("A's face warped into B's shape"); plt.axis('off');
plt.subplot(2,4,7); plt.imshow(A_warped_into_B_adjusted); plt.title("A's face warped into B's shape (texture adjusted)"); plt.axis('off');
plt.subplot(2,4,8); plt.imshow(A_embedded_into_B); plt.title("A's face embedded into B's backgrund"); plt.axis('off');

Output Text:
------------
<matplotlib.figure.Figure at 0x7fdc7a4f1550>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid of eight photos demonstrating face swapping and embedding techniques in image processing.  The top row showcases manipulations starting with an image of George W. Bush (labeled "image A").  The first image is the original image A, followed by a woman's face (image B) warped to match Bush's facial shape, then the same warp with texture adjustments for a more realistic blend, and finally, the woman's face seamlessly embedded into Bush's background.

The bottom row repeats the process but begins with an image of a female news anchor (labeled "image B"). It shows her face, then Bush's face warped to fit her shape, that same warp with texture adjustments, and finally, Bush's face embedded into the anchor's background.  The black background in the warped face images isolates the manipulated face for clear comparison.  The titles above each image clearly indicate the type of manipulation performed.


------------------------------------------------------------
Cell index: 19
Input Cell Type: markdown
Input Text:
-----------
We can see the results are much better now, achived with just a simple texture normalization step in addition to a piecewise affine warp.



Output Text:
------------


------------------------------------------------------------
Cell index: 20
Input Cell Type: markdown
Input Text:
-----------
# Finally, let's apply Poission Blending to seamlessly paste the image
Please see [here](http://cs.brown.edu/courses/cs129/results/proj2/taox/), [here](https://cs.brown.edu/courses/csci1950-g/results/proj2/edwallac/) and original paper [here](http://www.cs.virginia.edu/~connelly/class/2014/comp_photo/proj2/poisson.pdf) for more information about poission blending

Output Text:
------------


------------------------------------------------------------
Cell index: 21
Input Cell Type: python
Input Text:
-----------
def apply_aligned_poission_blending(background_image, foreground_image, foreground_mask):
    # this function assumes that "background_image.shape" == "foreground_image.shape" == "foreground_mask.shape"
    # also that "background_image.dtype" == "foreground_image.dtype" == np.uint8

    # erode the mask by 3 pixels
    kernel_morph    = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
    foreground_mask = cv2.erode(foreground_mask.astype(np.uint8), kernel_morph, iterations=1) 

    # make sure the mask is not problematic at boarders
    foreground_mask[0,:]  = False
    foreground_mask[:,0]  = False
    foreground_mask[-1,:] = False
    foreground_mask[:,-1] = False

    mask_label_image = label(foreground_mask)
    regions = regionprops(mask_label_image)
    min_row, min_col, max_row, max_col = regions[0].bbox

    if (min_row + max_row) % 2 == 1:
        min_row -= 1
    if (min_col + max_col) % 2 == 1:
        min_col -= 1

    center = (int((min_col + max_col) / 2), int((min_row + max_row) / 2))
    new_face_im_cropped   = foreground_image[min_row:max_row,min_col:max_col]
    new_face_mask_cropped = (255.0*foreground_mask[min_row:max_row,min_col:max_col]).astype(np.uint8)

    blended_image = cv2.seamlessClone(new_face_im_cropped, background_image, new_face_mask_cropped.copy(), center, cv2.NORMAL_CLONE)

    return blended_image

Output Text:
------------


------------------------------------------------------------
Cell index: 22
Input Cell Type: python
Input Text:
-----------
#%% take two almost frontal celebrity images and swap their faces
np.random.seed(1)

# choose some candidate images
almost_frontal_shape_cluster_inds = [0,1,5,6,8,9,10,11,14,15,16,17,18,19,20]
almost_frontal_inds = np.nonzero([x in almost_frontal_shape_cluster_inds for x in cluster_assignment])[0]
almost_frontal_inds = np.array(list(set(almost_frontal_inds).intersection(set(shape_ind_large_face_only))))

some_frontal_image_inds = np.random.choice(frontal_inds,size=2)
selected_videoID_A     = shape_ind_to_videoID_map[some_frontal_image_inds[0]][0]
selected_video_frame_A = shape_ind_to_videoID_map[some_frontal_image_inds[0]][1]
selected_videoID_B     = shape_ind_to_videoID_map[some_frontal_image_inds[1]][0]
selected_video_frame_B = shape_ind_to_videoID_map[some_frontal_image_inds[1]][1]

# load image A
video_file_A = np.load(full_paths[selected_videoID_A])
image_A = video_file_A['colorImages'][:,:,:,selected_video_frame_A]
shape_A = video_file_A['landmarks2D'][:,:,selected_video_frame_A]
shape_A = interpret_using_shape_model(ShapesPCAModel, shape_A)
image_A_size = image_A.shape[:2]

# load image B
video_file_B = np.load(full_paths[selected_videoID_B])
image_B = video_file_B['colorImages'][:,:,:,selected_video_frame_B]
shape_B = video_file_B['landmarks2D'][:,:,selected_video_frame_B]
shape_B = interpret_using_shape_model(ShapesPCAModel, shape_B)
image_B_size = image_B.shape[:2]

# warp faces into the other shape
A_warped_into_B = warp_im_picewise_affine(image_A, shape_A, shape_B, target_im_size=image_B_size)
B_warped_into_A = warp_im_picewise_affine(image_B, shape_B, shape_A, target_im_size=image_A_size)

### equate the mean and variance of the textures

# calculate the support mask for each warped image
A_warped_into_B_mask = np.any(A_warped_into_B, axis=2)
B_warped_into_A_mask = np.any(B_warped_into_A, axis=2)

# calculate the mean and std
A_texture_pixels = A_warped_into_B[A_warped_into_B_mask]
A_texture_mean   = A_texture_pixels.mean(axis=0)
A_texture_std    = np.sqrt(np.sum(A_texture_pixels**2,axis=1)).std()
B_texture_pixels = B_warped_into_A[B_warped_into_A_mask]
B_texture_mean   = B_texture_pixels.mean(axis=0)
B_texture_std    = np.sqrt(np.sum(B_texture_pixels**2,axis=1)).std()

# normalize the textures
A_texture_pixels_norm = A_texture_pixels - np.tile(A_texture_mean[np.newaxis,:], [A_texture_pixels.shape[0], 1])
A_texture_pixels_norm /= A_texture_std
B_texture_pixels_norm = B_texture_pixels - np.tile(B_texture_mean[np.newaxis,:], [B_texture_pixels.shape[0], 1])
B_texture_pixels_norm /= B_texture_std

# switch the means and stds
B_texture_pixels_adjusted = B_texture_pixels_norm * A_texture_std
B_texture_pixels_adjusted = B_texture_pixels_adjusted + np.tile(A_texture_mean[np.newaxis,:], [B_texture_pixels.shape[0], 1])
A_texture_pixels_adjusted = A_texture_pixels_norm * B_texture_std
A_texture_pixels_adjusted = A_texture_pixels_adjusted + np.tile(B_texture_mean[np.newaxis,:], [A_texture_pixels.shape[0], 1])

# place back 
A_warped_into_B_adjusted = A_warped_into_B.copy()
A_warped_into_B_adjusted[A_warped_into_B_mask] = A_texture_pixels_adjusted
A_warped_into_B_adjusted[A_warped_into_B_adjusted < 0] = 0
A_warped_into_B_adjusted[A_warped_into_B_adjusted > 1] = 1

B_warped_into_A_adjusted = B_warped_into_A.copy()
B_warped_into_A_adjusted[B_warped_into_A_mask] = B_texture_pixels_adjusted
B_warped_into_A_adjusted[B_warped_into_A_adjusted < 0] = 0
B_warped_into_A_adjusted[B_warped_into_A_adjusted > 1] = 1

B_warped_into_A_adjusted = (255 * B_warped_into_A_adjusted).astype(np.uint8)
A_warped_into_B_adjusted = (255 * A_warped_into_B_adjusted).astype(np.uint8)

# finally, embed the adjusted faces inside the target image
A_warped_into_B_adjusted_mask = np.any(A_warped_into_B_adjusted, axis=2)
B_warped_into_A_adjusted_mask = np.any(B_warped_into_A_adjusted, axis=2)

A_embedded_into_B = apply_aligned_poission_blending(image_B, A_warped_into_B_adjusted, A_warped_into_B_adjusted_mask)
B_embedded_into_A = apply_aligned_poission_blending(image_A, B_warped_into_A_adjusted, B_warped_into_A_adjusted_mask)

plt.figure(figsize=(16,8))
plt.subplots_adjust(left=0.02, bottom=0.02, right=0.98, top=0.98, wspace=0.1, hspace=0.1)
plt.subplot(2,4,1); plt.imshow(image_A); plt.title('image A'); plt.axis('off');
plt.subplot(2,4,2); plt.imshow(B_warped_into_A); plt.title("B's face warped into A's shape"); plt.axis('off');
plt.subplot(2,4,3); plt.imshow(B_warped_into_A_adjusted); plt.title("B's face warped into A's shape (texture adjusted)"); plt.axis('off');
plt.subplot(2,4,4); plt.imshow(B_embedded_into_A); plt.title("B's face embedded into A's backgrund"); plt.axis('off');
plt.subplot(2,4,5); plt.imshow(image_B); plt.title('image B'); plt.axis('off');
plt.subplot(2,4,6); plt.imshow(A_warped_into_B); plt.title("A's face warped into B's shape"); plt.axis('off');
plt.subplot(2,4,7); plt.imshow(A_warped_into_B_adjusted); plt.title("A's face warped into B's shape (texture adjusted)"); plt.axis('off');
plt.subplot(2,4,8); plt.imshow(A_embedded_into_B); plt.title("A's face embedded into B's backgrund"); plt.axis('off');

Output Text:
------------
<matplotlib.figure.Figure at 0x7fdc7886ca90>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
This image showcases a demonstration of face warping and embedding techniques in image processing.  The image is organized into a 2x4 grid.  The top row starts with an image labeled "image A," showing a headshot of George W. Bush. The remaining three images in the top row demonstrate manipulations of a second person's (image B's) face: first, warped to fit Bush's face shape, then with texture adjustments applied to the warped face, and finally, the second person's face embedded into Bush's background image.

The bottom row mirrors this structure. It begins with an image labeled "image B," showing a headshot of a female news reporter.  The subsequent three images perform the inverse manipulations: the first shows Bush's face warped to fit the reporter's shape, the second shows this warp with texture adjustments, and the third shows Bush's face embedded into the reporter's background image.

In essence, the image visually illustrates the capabilities of algorithms that can manipulate facial features and seamlessly integrate them into different contexts.  The comparison between the "warped" and "texture adjusted" results highlights the improvement in realism that texture adjustment provides.  The embedded images show the final result of placing the warped faces into the original backgrounds.


------------------------------------------------------------
Cell index: 23
Input Cell Type: markdown
Input Text:
-----------
### We can see the blending is very good and no clear pasting marks exist

Output Text:
------------


------------------------------------------------------------
Cell index: 24
Input Cell Type: markdown
Input Text:
-----------
# Let's now repeat this process for several more image pairs

Output Text:
------------


------------------------------------------------------------
Cell index: 25
Input Cell Type: python
Input Text:
-----------
def face_swap(image_A, shape_A, image_B, shape_B):
    
    image_A_size = image_A.shape[:2]
    image_B_size = image_B.shape[:2]

    # warp faces into the other shape
    A_warped_into_B = warp_im_picewise_affine(image_A, shape_A, shape_B, target_im_size=image_B_size)
    B_warped_into_A = warp_im_picewise_affine(image_B, shape_B, shape_A, target_im_size=image_A_size)

    ### equate the mean and variance of the textures

    # calculate the support mask for each warped image
    A_warped_into_B_mask = np.any(A_warped_into_B, axis=2)
    B_warped_into_A_mask = np.any(B_warped_into_A, axis=2)

    # calculate the mean and std
    A_texture_pixels = A_warped_into_B[A_warped_into_B_mask]
    A_texture_mean   = A_texture_pixels.mean(axis=0)
    A_texture_std    = np.sqrt(np.sum(A_texture_pixels**2, axis=1)).std()
    B_texture_pixels = B_warped_into_A[B_warped_into_A_mask]
    B_texture_mean   = B_texture_pixels.mean(axis=0)
    B_texture_std    = np.sqrt(np.sum(B_texture_pixels**2, axis=1)).std()

    # normalize the textures
    A_texture_pixels_norm = A_texture_pixels - np.tile(A_texture_mean[np.newaxis,:], [A_texture_pixels.shape[0], 1])
    A_texture_pixels_norm /= A_texture_std
    B_texture_pixels_norm = B_texture_pixels - np.tile(B_texture_mean[np.newaxis,:], [B_texture_pixels.shape[0], 1])
    B_texture_pixels_norm /= B_texture_std

    # switch the means and stds
    B_texture_pixels_adjusted = B_texture_pixels_norm * A_texture_std
    B_texture_pixels_adjusted = B_texture_pixels_adjusted + np.tile(A_texture_mean[np.newaxis,:], [B_texture_pixels.shape[0], 1])
    A_texture_pixels_adjusted = A_texture_pixels_norm * B_texture_std
    A_texture_pixels_adjusted = A_texture_pixels_adjusted + np.tile(B_texture_mean[np.newaxis,:], [A_texture_pixels.shape[0], 1])

    # palce back 
    A_warped_into_B_adjusted = A_warped_into_B.copy()
    A_warped_into_B_adjusted[A_warped_into_B_mask] = A_texture_pixels_adjusted
    A_warped_into_B_adjusted[A_warped_into_B_adjusted < 0] = 0
    A_warped_into_B_adjusted[A_warped_into_B_adjusted > 1] = 1
    B_warped_into_A_adjusted = B_warped_into_A.copy()
    B_warped_into_A_adjusted[B_warped_into_A_mask] = B_texture_pixels_adjusted
    B_warped_into_A_adjusted[B_warped_into_A_adjusted < 0] = 0
    B_warped_into_A_adjusted[B_warped_into_A_adjusted > 1] = 1

    # turn into uint8
    B_warped_into_A_adjusted = (255 * B_warped_into_A_adjusted).astype(np.uint8)
    A_warped_into_B_adjusted = (255 * A_warped_into_B_adjusted).astype(np.uint8)

    # finally, apply poission blending to seamlessly blend the adjusted faces inside the target images
    A_warped_into_B_adjusted_mask = np.any(A_warped_into_B_adjusted, axis=2)
    B_warped_into_A_adjusted_mask = np.any(B_warped_into_A_adjusted, axis=2)

    A_embedded_into_B = apply_aligned_poission_blending(image_B, A_warped_into_B_adjusted, A_warped_into_B_adjusted_mask)
    B_embedded_into_A = apply_aligned_poission_blending(image_A, B_warped_into_A_adjusted, B_warped_into_A_adjusted_mask)

    return A_embedded_into_B, B_embedded_into_A

Output Text:
------------


------------------------------------------------------------
Cell index: 26
Input Cell Type: python
Input Text:
-----------
# take several pairs of almost frontal celebrity images, swap their faces and display
np.random.seed(7)

almost_frontal_shape_cluster_inds = [0,1,5,6,8,9,10,11,14,15,16,17,18,19,20]
almost_frontal_inds = np.nonzero([x in almost_frontal_shape_cluster_inds for x in cluster_assignment])[0]
almost_frontal_inds = np.array(list(set(almost_frontal_inds).intersection(set(shape_ind_large_face_only))))

num_pairs = 4

plt.figure(figsize=(22,16))
plt.subplots_adjust(left=0.02, bottom=0.02, right=0.98, top=0.98, wspace=0.05, hspace=0.2)
for k in range(num_pairs):

    # choose 2 candidate images
    some_frontal_image_inds = np.random.choice(frontal_inds,size=2)
    selected_videoID_A     = shape_ind_to_videoID_map[some_frontal_image_inds[0]][0]
    selected_video_frame_A = shape_ind_to_videoID_map[some_frontal_image_inds[0]][1]
    selected_videoID_B     = shape_ind_to_videoID_map[some_frontal_image_inds[1]][0]
    selected_video_frame_B = shape_ind_to_videoID_map[some_frontal_image_inds[1]][1]

    # load image A
    video_file_A = np.load(full_paths[selected_videoID_A])
    image_A = video_file_A['colorImages'][:,:,:,selected_video_frame_A]
    shape_A = video_file_A['landmarks2D'][:,:,selected_video_frame_A]
    shape_A = interpret_using_shape_model(ShapesPCAModel, shape_A)

    # load image B
    video_file_B = np.load(full_paths[selected_videoID_B])
    image_B = video_file_B['colorImages'][:,:,:,selected_video_frame_B]
    shape_B = video_file_B['landmarks2D'][:,:,selected_video_frame_B]
    shape_B = interpret_using_shape_model(ShapesPCAModel, shape_B)

    # swap faces
    A_embedded_into_B, B_embedded_into_A = face_swap(image_A, shape_A, image_B, shape_B)
    
    # plot
    plt.subplot(4, num_pairs, k + 1 + 0*num_pairs); plt.imshow(B_embedded_into_A); plt.title("B face, A backgrund"); plt.axis('off');
    plt.subplot(4, num_pairs, k + 1 + 1*num_pairs); plt.imshow(A_embedded_into_B); plt.title("A face, B backgrund"); plt.axis('off');
    plt.subplot(4, num_pairs, k + 1 + 2*num_pairs); plt.imshow(image_A); plt.title('image A'); plt.axis('off');
    plt.subplot(4, num_pairs, k + 1 + 3*num_pairs); plt.imshow(image_B); plt.title('image B'); plt.axis('off');

Output Text:
------------
<matplotlib.figure.Figure at 0x7fdc784ad2e8>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid of 16 smaller images, arranged in four rows and four columns.  Each small image shows a person's face, either against a plain background or a background containing other visual elements. The images are labeled at the top with either "B face, A background" or "A face, B background," and at the bottom with "image A" or "image B".  The labeling suggests a process involving two sets of faces (A and B) and backgrounds.  The backgrounds are diverse, ranging from news studio settings to outdoor scenes and even what appears to be a prison.

The faces depicted show a diverse range of ages, ethnicities, and genders. Some individuals appear to be news anchors or reporters, while others seem to be from more casual settings. A few faces appear to be from a mugshot-style photograph. The consistent sizing and labeling suggest the images are part of a dataset for a machine learning project, likely focused on face recognition or image synthesis, where the goal might be to swap faces or backgrounds or to train a model to distinguish between faces in different contexts.


------------------------------------------------------------
Cell index: 27
Input Cell Type: markdown
Input Text:
-----------
## This was the poor man's face swap. hope you enjoyed it!

Output Text:
------------


------------------------------------------------------------

================================================================================
================================================================================
building-faces-out-of-parts.ipynb:
==================================
------------------------------------------------------------
Cell index: 1
Input Cell Type: markdown
Input Text:
-----------
## Building Faces out of Parts

In this script we will extract facial parts from face images (using labeled landmark points available in the dataset) and create new seperate datasets of eyes, noses and mouths.

We then continue to analyze these face parts using kmeans and present the typical eyes, typical noses and typical mouths.

Finally, we take these parts and compose (generate) new funny looking faces out of them.

***Note:*** I've hidden most of the code, but remeber that the code here is extreemly simple and short. You are welcome to both unhide and read it, or better yet fork the script and play around with it.

Output Text:
------------


------------------------------------------------------------
Cell index: 2
Input Cell Type: python
Input Text:
-----------
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from skimage import transform as tf
from sklearn import cluster

Output Text:
------------


------------------------------------------------------------
Cell index: 3
Input Cell Type: markdown
Input Text:
-----------
## Load the Data


Output Text:
------------


------------------------------------------------------------
Cell index: 4
Input Cell Type: python
Input Text:
-----------
#% load the dataset
face_images_db = np.load('../input/face_images.npz')['face_images']
facial_keypoints_df = pd.read_csv('../input/facial_keypoints.csv')

Output Text:
------------


------------------------------------------------------------
Cell index: 5
Input Cell Type: markdown
Input Text:
-----------
## Keep only the images with all 15 keypoints Fully Marked

Output Text:
------------


------------------------------------------------------------
Cell index: 6
Input Cell Type: python
Input Text:
-----------
numMissingKeypoints = facial_keypoints_df.isnull().sum(axis=1)
allKeypointsPresentInds = np.nonzero(numMissingKeypoints == 0)[0]

faceImagesDB = face_images_db[:,:,allKeypointsPresentInds]
facialKeypointsDF = facial_keypoints_df.iloc[allKeypointsPresentInds,:].reset_index(drop=True)

(imHeight, imWidth, numImages) = faceImagesDB.shape
numKeypoints = facialKeypointsDF.shape[1] / 2

print('number of remaining images = %d' %(numImages))
print('image dimentions = (%d,%d)' %(imHeight,imWidth))
print('number of facial keypoints = %d' %(numKeypoints))

Output Text:
------------
number of remaining images = 2140
image dimentions = (96,96)
number of facial keypoints = 15


------------------------------------------------------------
Cell index: 7
Input Cell Type: markdown
Input Text:
-----------
## Show Random subset of images with all 15 keypoints overlaid 

Output Text:
------------


------------------------------------------------------------
Cell index: 8
Input Cell Type: python
Input Text:
-----------
# show a random subset of images from the dataset
num_fig_rows = 5
num_fig_cols = 5

num_plots = num_fig_rows * num_fig_cols

rand_inds_vec = np.random.choice(faceImagesDB.shape[2],num_plots,replace=False)
rand_inds_mat = rand_inds_vec.reshape((num_fig_rows,num_fig_cols))

plt.close('all')
fig, ax = plt.subplots(nrows=num_fig_rows,ncols=num_fig_cols,figsize=(14,14))

for i in range(num_fig_rows):
    for j in range(num_fig_cols):
        curr_ind = rand_inds_mat[i][j]
        curr_image = faceImagesDB[:,:,curr_ind]
    
        x_feature_coords = np.array(facialKeypointsDF.iloc[curr_ind,0:30:2].tolist())
        y_feature_coords = np.array(facialKeypointsDF.iloc[curr_ind,1:30:2].tolist())
    
        ax[i][j].imshow(curr_image, cmap='gray');
        ax[i][j].scatter(x_feature_coords,y_feature_coords,c='r',s=12)
        ax[i][j].set_axis_off()
        ax[i][j].set_title('image index = %d' %(curr_ind),fontsize=10)

Output Text:
------------
<matplotlib.figure.Figure at 0x7fe7a95d86d8>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image displays a grid of 25 grayscale photographs of faces, each with 15 red dots superimposed. These dots appear to mark key facial features, such as the eyes, nose, and mouth, suggesting the images are used for facial landmark detection or a similar application in computer vision.  Each photograph is accompanied by a label in the top left corner indicating "image index" followed by a number, presumably a unique identifier for each image within a larger dataset.

The faces in the photographs are diverse, showing a range of ages, genders, and expressions.  Some individuals wear glasses, while others have different hairstyles and facial hair. The consistent placement of the red dots across all images suggests a standardized approach to facial feature detection, implying the images are part of a training or testing set for a machine learning model.  The grayscale nature of the images simplifies the task for the model by removing the complexity of color information.

The arrangement of the images in a 5x5 grid presents a visually organized and easily comparable dataset. The uniformity in image size and dot placement further emphasizes the structured nature of the data collection.  The consistent labeling also contributes to the overall clarity and organization of the presented dataset.


------------------------------------------------------------
Cell index: 9
Input Cell Type: markdown
Input Text:
-----------
## Define Crop Boundaries for the Eyes, Nose and Mouth
(I've hidden this code since it's just boring crop boundry calculations from the keypoints, but you are welcome to unhide and look around)


Output Text:
------------


------------------------------------------------------------
Cell index: 10
Input Cell Type: python
Input Text:
-----------
# crop boundery for the left eye
listOfKeypointsToAvg = ['left_eye_center_x','left_eye_inner_corner_x','left_eye_outer_corner_x']
facialKeypointsDF['left_eye_BB_x'] = facialKeypointsDF.loc[:,listOfKeypointsToAvg].mean(axis=1)
listOfKeypointsToAvg = ['left_eye_center_y','left_eye_inner_corner_y','left_eye_outer_corner_y']
facialKeypointsDF['left_eye_BB_y'] = facialKeypointsDF.loc[:,listOfKeypointsToAvg].mean(axis=1)

dy1 =   facialKeypointsDF['left_eye_BB_y'] - facialKeypointsDF['left_eyebrow_inner_end_y']
dy2 =   facialKeypointsDF['left_eye_BB_y'] - facialKeypointsDF['left_eyebrow_outer_end_y']
dx1 =   facialKeypointsDF['left_eye_BB_x'] - facialKeypointsDF['left_eyebrow_inner_end_x']
dx2 = -(facialKeypointsDF['left_eye_BB_x'] - facialKeypointsDF['left_eyebrow_outer_end_x'])
facialKeypointsDF['left_eye_halfHeight'] = 1.1*0.5*(dy1 + dy2)
facialKeypointsDF['left_eye_halfWidth']  = 1.1*0.5*(dx1 + dx2)


# crop boundry for the right eye
listOfKeypointsToAvg = ['right_eye_center_x','right_eye_inner_corner_x','right_eye_outer_corner_x']
facialKeypointsDF['right_eye_BB_x'] = facialKeypointsDF.loc[:,listOfKeypointsToAvg].mean(axis=1)
listOfKeypointsToAvg = ['right_eye_center_y','right_eye_inner_corner_y','right_eye_outer_corner_y']
facialKeypointsDF['right_eye_BB_y'] = facialKeypointsDF.loc[:,listOfKeypointsToAvg].mean(axis=1)

dy1 =   facialKeypointsDF['right_eye_BB_y'] - facialKeypointsDF['right_eyebrow_inner_end_y']
dy2 =   facialKeypointsDF['right_eye_BB_y'] - facialKeypointsDF['right_eyebrow_outer_end_y']
dx1 =   facialKeypointsDF['right_eye_BB_x'] - facialKeypointsDF['right_eyebrow_inner_end_x']
dx2 = -(facialKeypointsDF['right_eye_BB_x'] - facialKeypointsDF['right_eyebrow_outer_end_x'])
facialKeypointsDF['right_eye_halfHeight'] = 1.1*0.5*(dy1 + dy2)
facialKeypointsDF['right_eye_halfWidth']  = -1.1*0.5*(dx1 + dx2)


# crop boundry for the nose
listOfKeypointsToAvg = ['left_eye_inner_corner_x','right_eye_inner_corner_x','mouth_right_corner_x','mouth_left_corner_x','nose_tip_x']
facialKeypointsDF['nose_BB_x'] = facialKeypointsDF.loc[:,listOfKeypointsToAvg].mean(axis=1)
listOfKeypointsToAvg = ['left_eye_inner_corner_y','right_eye_inner_corner_y','mouth_center_top_lip_y','nose_tip_y']
facialKeypointsDF['nose_BB_y'] = facialKeypointsDF.loc[:,listOfKeypointsToAvg].mean(axis=1)

dy1 = abs(facialKeypointsDF['left_eye_inner_corner_y']  - 0.5*(facialKeypointsDF['nose_tip_y']+facialKeypointsDF['mouth_center_top_lip_y']))
dy2 = abs(facialKeypointsDF['right_eye_inner_corner_y'] - 0.5*(facialKeypointsDF['nose_tip_y']+facialKeypointsDF['mouth_center_top_lip_y']))
dx1 = abs(facialKeypointsDF['left_eye_inner_corner_x']  - facialKeypointsDF['right_eye_inner_corner_x'])
dx2 = abs(facialKeypointsDF['mouth_left_corner_x']      - facialKeypointsDF['mouth_right_corner_x'])
facialKeypointsDF['nose_halfWidth']  = 0.25*(dx1 + dx2)
facialKeypointsDF['nose_halfHeight'] = 0.25*(dy1 + dy2)


# crop boundry for the mouth
listOfKeypointsToAvg = ['mouth_center_top_lip_x','mouth_center_bottom_lip_x','mouth_right_corner_x','mouth_left_corner_x']
facialKeypointsDF['mouth_BB_x'] = facialKeypointsDF.loc[:,listOfKeypointsToAvg].mean(axis=1)
listOfKeypointsToAvg = ['mouth_center_top_lip_y','mouth_center_bottom_lip_y','mouth_right_corner_y','mouth_left_corner_y']
facialKeypointsDF['mouth_BB_y'] = facialKeypointsDF.loc[:,listOfKeypointsToAvg].mean(axis=1)

facialKeypointsDF['mouth_halfWidth']  = 1.3*0.5*abs(facialKeypointsDF['mouth_left_corner_x']    - facialKeypointsDF['mouth_right_corner_x'])
facialKeypointsDF['mouth_halfHeight'] = 7.0 + 0.95*0.5*abs(facialKeypointsDF['mouth_center_top_lip_y'] - facialKeypointsDF['mouth_center_bottom_lip_y'])


Output Text:
------------


------------------------------------------------------------
Cell index: 11
Input Cell Type: python
Input Text:
-----------
# show the newly formed columns
facialKeypointsDF.iloc[:5,30:]

Output Text:
------------
   left_eye_BB_x  left_eye_BB_y  left_eye_halfHeight  left_eye_halfWidth  \
0      66.248662      39.539898             9.799905           12.800626   
1      64.637277      35.477197             6.749887           13.555745   
2      65.151158      35.850526             6.313166           12.729663   
3      65.847930      38.256640             7.654730           11.812049   
4      65.935706      39.708996             9.354713           11.282356   

   right_eye_BB_x  right_eye_BB_y  right_eye_halfHeight  right_eye_halfWidth  \
0       30.012150       37.066827              8.516124            13.129177   
1       30.152170       33.651563              7.754023            14.224919   
2       31.186105       35.944611              6.416659            13.971505   
3       31.649670       38.007930              8.281091            11.667423   
4       30.577767       38.480707              8.444620            12.160261   

   nose_BB_x  nose_BB_y  nose_halfWidth  nose_halfHeight  mouth_BB_x  \
0  46.033805  51.759771       13.951579        13.241359   44.063278   
1  46.928204  48.890843       11.030128        14.072902   45.924085   
2  47.839326  49.093168       12.207632        12.772200   47.274789   
3  50.470289  50.534880       12.684522        11.594233   51.177183   
4  46.142194  54.328255       12.765791        14.970199   45.263264   

   mouth_BB_y  mouth_halfWidth  mouth_halfHeight  
0   78.695098        21.177528         12.486400  
1   77.036596        13.844391         14.226468  
2   73.649368        17.612811         11.022100  
3   73.814817        18.429704         10.883930  
4   78.774773        19.161777         13.252795  


------------------------------------------------------------
Cell index: 12
Input Cell Type: markdown
Input Text:
-----------
## Plot the Resulting Bounding Boxes of the Eyes, Nose and Mouth 

Output Text:
------------


------------------------------------------------------------
Cell index: 13
Input Cell Type: python
Input Text:
-----------
# helper function to convert dataframe fields to rectangle fields
def ExtractBoundingBoxParams(df, fieldNamePremble):
    bottomLeft_x = df.loc[k, fieldNamePremble + '_BB_x'] - df.loc[k, fieldNamePremble + '_halfWidth']
    bottomLeft_y = df.loc[k, fieldNamePremble + '_BB_y'] - df.loc[k, fieldNamePremble + '_halfHeight']
    boxWidth     = 2*df.loc[k, fieldNamePremble + '_halfWidth']
    boxHeight    = 2*df.loc[k, fieldNamePremble + '_halfHeight']
    return bottomLeft_x, bottomLeft_y, boxWidth, boxHeight


num_fig_rows = 5
num_fig_cols = 5
num_plots = num_fig_rows * num_fig_cols
rand_inds_vec = np.random.choice(faceImagesDB.shape[2],num_plots,replace=False)
rand_inds_mat = rand_inds_vec.reshape((num_fig_rows,num_fig_cols))

fig, ax = plt.subplots(nrows=num_fig_rows,ncols=num_fig_cols,figsize=(14,14))
for i in range(num_fig_rows):
    for j in range(num_fig_cols):
        k = rand_inds_mat[i][j]
        curr_image = faceImagesDB[:,:,k]
    
        x_feature_coords = np.array(facialKeypointsDF.iloc[k,0:30:2].tolist())
        y_feature_coords = np.array(facialKeypointsDF.iloc[k,1:30:2].tolist())
    
        ax[i][j].imshow(curr_image, cmap='gray');
        #ax[i][j].scatter(x_feature_coords,y_feature_coords,c='r',s=12)
        ax[i][j].set_axis_off()
        ax[i][j].set_title('image index = %d' %(curr_ind),fontsize=10)

        # plot bounding box of all parts with corresponding colors
        listOfPartNames  = [ 'left_eye', 'right_eye',    'nose', 'mouth']
        listOfPartColors = [    'green',    'yellow', 'magenta',  'blue']
        
        for partName, partColor in zip(listOfPartNames, listOfPartColors):
            bottomLeft_x, bottomLeft_y, boxWidth, boxHeight = ExtractBoundingBoxParams(facialKeypointsDF, partName)
            ax[i][j].add_patch(patches.Rectangle( (bottomLeft_x, bottomLeft_y), boxWidth, boxHeight,
                                                   edgecolor=partColor, linewidth=2, fill=False) )

Output Text:
------------
<matplotlib.figure.Figure at 0x7fe75e8fedd8>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a grid of 25 grayscale photographs, each showing a different person's face.  Each face is annotated with several colored bounding boxes. These boxes highlight specific facial features: the eyes (yellow), nose (magenta/pink), and mouth (blue).  There's also a green box present on several faces, the purpose of which isn't immediately clear from the image alone.  The images appear to be part of a dataset used for facial landmark detection or facial recognition training, as the bounding boxes precisely delineate key facial components.

The consistent arrangement of the bounding boxes across all images suggests a standardized annotation process.  The text "image index = 1358" above each image indicates that these are likely a subset of a larger dataset, and the number 1358 might represent a sequential identifier for each image within that dataset. The grayscale nature of the images simplifies the analysis, focusing purely on shape and texture without the distraction of color.  The overall structure is clean and organized, typical of a presentation of annotated data for machine learning purposes.


------------------------------------------------------------
Cell index: 14
Input Cell Type: markdown
Input Text:
-----------
## Define target size of the facial parts
(this is just calculating the mean face part bounding box size for each face part)

Output Text:
------------


------------------------------------------------------------
Cell index: 15
Input Cell Type: python
Input Text:
-----------
#%% determine part target sizes
targetEyeSize = (np.ceil(2*facialKeypointsDF['left_eye_halfHeight'].mean()),np.ceil(2*facialKeypointsDF['left_eye_halfWidth'].mean()))
targetEyeSize = [int(x) for x in targetEyeSize]

targetNoseSize = (np.ceil(2*facialKeypointsDF['nose_halfHeight'].mean()),np.ceil(2*facialKeypointsDF['nose_halfWidth'].mean()))
targetNoseSize = [int(x) for x in targetNoseSize]

targetMouthSize = (np.ceil(2*facialKeypointsDF['mouth_halfHeight'].mean()),np.ceil(2*facialKeypointsDF['mouth_halfWidth'].mean()))
targetMouthSize = [int(x) for x in targetMouthSize]

Output Text:
------------


------------------------------------------------------------
Cell index: 16
Input Cell Type: markdown
Input Text:
-----------
## Create "Eyes", "Noses" and "Mouths" datasets from the bounding boxes
(crop and resize each to a standard size)

Output Text:
------------


------------------------------------------------------------
Cell index: 17
Input Cell Type: python
Input Text:
-----------
#%% go over all images, crop out parts and resize to fit the target size
allLeftEyes  = np.zeros((targetEyeSize[0]  , targetEyeSize[1]  , numImages))
allRightEyes = np.zeros((targetEyeSize[0]  , targetEyeSize[1]  , numImages))
allNoses     = np.zeros((targetNoseSize[0] , targetNoseSize[1] , numImages))
allMouths    = np.zeros((targetMouthSize[0], targetMouthSize[1], numImages))

for k in range(numImages):
    currImage = faceImagesDB[:,:,k]
    
    # crop out left eye
    bottomLeft_x, bottomLeft_y, boxWidth, boxHeight = ExtractBoundingBoxParams(facialKeypointsDF, 'left_eye')
    hLims = (max(0,np.floor(bottomLeft_y).astype(int)), min(imHeight,np.ceil(bottomLeft_y + boxHeight).astype(int)))
    wLims = (max(0,np.floor(bottomLeft_x).astype(int)), min(imWidth ,np.ceil(bottomLeft_x + boxWidth ).astype(int)))
    croppedLeftEye = currImage[hLims[0]:hLims[1],wLims[0]:wLims[1]]
    
    # crop out right eye
    bottomLeft_x, bottomLeft_y, boxWidth, boxHeight = ExtractBoundingBoxParams(facialKeypointsDF, 'right_eye')
    hLims = (max(0,np.floor(bottomLeft_y).astype(int)), min(imHeight,np.ceil(bottomLeft_y + boxHeight).astype(int)))
    wLims = (max(0,np.floor(bottomLeft_x).astype(int)), min(imWidth ,np.ceil(bottomLeft_x + boxWidth ).astype(int)))
    croppedRightEye = currImage[hLims[0]:hLims[1],wLims[0]:wLims[1]]

    # crop out right eye
    bottomLeft_x, bottomLeft_y, boxWidth, boxHeight = ExtractBoundingBoxParams(facialKeypointsDF, 'nose')
    hLims = (max(0,np.floor(bottomLeft_y).astype(int)), min(imHeight,np.ceil(bottomLeft_y + boxHeight).astype(int)))
    wLims = (max(0,np.floor(bottomLeft_x).astype(int)), min(imWidth ,np.ceil(bottomLeft_x + boxWidth ).astype(int)))
    croppedNose = currImage[hLims[0]:hLims[1],wLims[0]:wLims[1]]

    # crop out right eye
    bottomLeft_x, bottomLeft_y, boxWidth, boxHeight = ExtractBoundingBoxParams(facialKeypointsDF, 'mouth')
    hLims = (max(0,np.floor(bottomLeft_y).astype(int)), min(imHeight,np.ceil(bottomLeft_y + boxHeight).astype(int)))
    wLims = (max(0,np.floor(bottomLeft_x).astype(int)), min(imWidth ,np.ceil(bottomLeft_x + boxWidth ).astype(int)))
    croppedMouth = currImage[hLims[0]:hLims[1],wLims[0]:wLims[1]]
    
    allLeftEyes[:,:,k]  = tf.resize(croppedLeftEye,targetEyeSize)
    allRightEyes[:,:,k] = tf.resize(croppedRightEye,targetEyeSize)
    allNoses[:,:,k]     = tf.resize(croppedNose,targetNoseSize)
    allMouths[:,:,k]    = tf.resize(croppedMouth,targetMouthSize)

Output Text:
------------
/opt/conda/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.
  warn("The default mode, 'constant', will be changed to 'reflect' in "


------------------------------------------------------------
Cell index: 18
Input Cell Type: markdown
Input Text:
-----------
## Look at some cropped "Eyes"

Output Text:
------------


------------------------------------------------------------
Cell index: 19
Input Cell Type: python
Input Text:
-----------
# plot eyes
numFigRows = 6
numFigCols = 5
numPlots = numFigRows * numFigCols
randIndsVec = np.random.choice(numImages,numPlots,replace=False)
randIndsMat = randIndsVec.reshape((numFigRows,numFigCols))

fig, ax = plt.subplots(nrows=numFigRows,ncols=numFigCols,figsize=(14,12))
for i in range(numFigRows):
    for j in range(numFigCols):
        if np.random.rand(1) < 0.5:
            ax[i][j].imshow(allLeftEyes[:,:,randIndsMat[i][j]], cmap='gray');
        else:
            ax[i][j].imshow(np.fliplr(allLeftEyes[:,:,randIndsMat[i][j]]), cmap='gray');
        ax[i][j].set_axis_off()

Output Text:
------------
<matplotlib.figure.Figure at 0x7fe75c46e780>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a grid of 30 grayscale images, each depicting a close-up view of a human eye. The images are arranged in a 6x5 matrix, creating a visually organized display. 


The individual eye images show variations in eye shape, size, and the surrounding skin texture.  Some eyes are open wide, while others are partially closed or squinted. There's also diversity in the lighting and clarity of the images, with some appearing sharper than others.  A few images include portions of eyeglasses or other accessories near the eye. 


The overall impression is a dataset of eye images, possibly collected for a machine learning or computer vision project focused on tasks such as eye detection, recognition, or analysis. The consistent grayscale and size of the images suggest preprocessing has been done to prepare them for such applications.


------------------------------------------------------------
Cell index: 20
Input Cell Type: markdown
Input Text:
-----------
## Look at some "Noses"

Output Text:
------------


------------------------------------------------------------
Cell index: 21
Input Cell Type: python
Input Text:
-----------
# plot noses
numFigRows = 4
numFigCols = 5
numPlots = numFigRows * numFigCols
randIndsVec = np.random.choice(numImages,numPlots,replace=False)
randIndsMat = randIndsVec.reshape((numFigRows,numFigCols))

fig, ax = plt.subplots(nrows=numFigRows,ncols=numFigCols,figsize=(14,12))
for i in range(numFigRows):
    for j in range(numFigCols):
        ax[i][j].imshow(allNoses[:,:,randIndsMat[i][j]], cmap='gray');
        ax[i][j].set_axis_off()

Output Text:
------------
<matplotlib.figure.Figure at 0x7fe75ec71b00>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a grid of 20 grayscale images, arranged in four rows and five columns. Each image depicts a close-up view of a human nose, seemingly cropped from a larger facial image. 


The noses vary in shape, size, and the surrounding skin texture. Some noses appear broader, others narrower, and the overall appearance suggests a diverse range of individuals. The grayscale tones provide subtle shading and highlight the contours of the noses. The images are uniformly sized and consistently presented, suggesting a systematic approach to image acquisition and arrangement. The uniformity in presentation enhances the comparative aspect, allowing for easy visual comparison of the nose shapes.


------------------------------------------------------------
Cell index: 22
Input Cell Type: markdown
Input Text:
-----------
## Look at some "Mouths"

Output Text:
------------


------------------------------------------------------------
Cell index: 23
Input Cell Type: python
Input Text:
-----------
# plot mouths
numFigRows = 6
numFigCols = 5
numPlots = numFigRows * numFigCols
randIndsVec = np.random.choice(numImages,numPlots,replace=False)
randIndsMat = randIndsVec.reshape((numFigRows,numFigCols))

fig, ax = plt.subplots(nrows=numFigRows,ncols=numFigCols,figsize=(14,10))
for i in range(numFigRows):
    for j in range(numFigCols):
        ax[i][j].imshow(allMouths[:,:,randIndsMat[i][j]], cmap='gray');
        ax[i][j].set_axis_off()

Output Text:
------------
<matplotlib.figure.Figure at 0x7fe75cc1dac8>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid displaying thirty grayscale images of human mouths in various states. 


Each mouth image is presented in a square cell within a 6x5 grid. The images appear to be cropped close-ups, focusing solely on the mouth and immediate surrounding area. There's a noticeable variety in lip shapes, expressions (some are smiling, others are neutral or slightly pursed), and the presence or absence of teeth. The lighting and image quality are relatively consistent across all the images, suggesting they might come from the same dataset or have been processed similarly. The overall impression is that it’s a collection of mouth images likely used for a machine learning task, such as lip reading or facial expression recognition.


------------------------------------------------------------
Cell index: 24
Input Cell Type: markdown
Input Text:
-----------
## Flatten the part images and create 'numSamples' by 'numFeatures' datasets
convert each image into a single vector. from shape (imHeight, imWidth) to shape (1,imHeight * imWidth)

Output Text:
------------


------------------------------------------------------------
Cell index: 25
Input Cell Type: python
Input Text:
-----------
#%% flatten the 3D datasets (imHeight,imWidth,numSamples) to 2D datasets (numSamples,numDimentions)
def FlattenImageDataset(imageDB):
    # assume imageDB.shape = (imHeight, imWidth, numSamples)
    numSamples = imageDB.shape[2]
    numFeatures = imageDB.shape[0] * imageDB.shape[1]
    imageDB_flattened = np.reshape(np.transpose(imageDB,axes=(2,0,1)),(numSamples,numFeatures))
    return imageDB_flattened

allLeftEyes_flat  = FlattenImageDataset(allLeftEyes)
allRightEyes_flat = FlattenImageDataset(np.fliplr(allRightEyes))
allNoses_flat     = FlattenImageDataset(allNoses)
allMouths_flat    = FlattenImageDataset(allMouths)

print('previous eye dataset shape was ' + str(allLeftEyes.shape))
print('current eye dataset shape is ' + str(allLeftEyes_flat.shape))

Output Text:
------------
previous eye dataset shape was (18, 26, 2140)
current eye dataset shape is (2140, 468)


------------------------------------------------------------
Cell index: 26
Input Cell Type: markdown
Input Text:
-----------
## Determine the number of clusters for each face part
We will do this by sequentially increasing the number of kmeans clusters and checking to see when we reach a sufficient amount of variance explained.   

**Note:** the percents explained by optimal clusters that try to explain the variance in a dataset (i.e. kmeans) are usually growing much slower than percents explained by optimal basis functions that try to explain the variance in a dataset (i.e. principlal components) so our target percent explained will be much more humble here.  
It might actually be fun to think about why that is, "if the reader is so inclined"?

Output Text:
------------


------------------------------------------------------------
Cell index: 27
Input Cell Type: markdown
Input Text:
-----------
## Determine the number of clusters for the Eyes


Output Text:
------------


------------------------------------------------------------
Cell index: 28
Input Cell Type: python
Input Text:
-----------
#%% determine the number of clusters for the eyes, nose and mouth
def DetermineNumClusters(imageDB_flattened, percentExplainedTarget=70, plotFigure=True):
    # run a loop in which you apply kmeans in increasing number of n_clusters and collect the inertia_ field from the result
    listOfNumClusters = [1,2,4,9,16,25,36,49,64,128,256]
    listOfInertia = []
    for numClusters in listOfNumClusters:
        KmeansModel = cluster.MiniBatchKMeans(n_clusters=numClusters, batch_size=750, n_init=1, random_state=1)
        KmeansModel.fit(imageDB_flattened)
        listOfInertia.append(KmeansModel.inertia_)
    explainedPercent = 100*(1-(np.array(listOfInertia)/listOfInertia[0]))

    # calculate the desired number of clusters
    try:
        numDesiredClusterInd = np.nonzero(explainedPercent > percentExplainedTarget)[0][0]
        numDesiredClusters = listOfNumClusters[numDesiredClusterInd]
    except:
        print("desired target exceeds %d" %(listOfNumClusters[-1]))
        numDesiredClusterInd = len(listOfNumClusters)
        numDesiredClusters = listOfNumClusters[numDesiredClusterInd]
            
    if plotFigure:
        # plot the explained percent as a function of number of clusters 
        explainedPercentReached = explainedPercent[numDesiredClusterInd]
        plt.figure(figsize=(14,6)); plt.plot(listOfNumClusters,explainedPercent,c='b')
        plt.scatter(numDesiredClusters,explainedPercentReached,s=150,c='r')
        plt.xlabel('Number of Clusters', fontsize=20); plt.ylabel('Explained Percent', fontsize=20)
        plt.title('Desired Number of Clusters = %d, Explained Percent = %.2f%s' %(numDesiredClusters,explainedPercentReached,'%'),fontsize=25)
        plt.xlim(-1,listOfNumClusters[-1]+1); plt.ylim(0,100)
    return numDesiredClusters

Output Text:
------------


------------------------------------------------------------
Cell index: 29
Input Cell Type: python
Input Text:
-----------
numClusters_eyes  = DetermineNumClusters(np.concatenate((allLeftEyes_flat, allRightEyes_flat)), plotFigure=True)

Output Text:
------------
<matplotlib.figure.Figure at 0x7fe75f0897b8>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a line graph depicting the relationship between the number of clusters and the explained percentage of variance in a dataset. The x-axis represents the number of clusters, ranging from 0 to 250. The y-axis represents the explained percentage, ranging from 0% to 100%. 


The graph shows a steeply rising curve initially, indicating a rapid increase in explained variance with the addition of more clusters in the beginning. As the number of clusters increases, the rate of increase in explained variance slows down significantly, resulting in a curve that gradually flattens out. A prominent red dot is highlighted on the curve, which corresponds to 36 clusters and approximately 71.32% explained variance. This point is likely chosen as an optimal balance between the number of clusters and the explained variance. 


The title of the graph clearly states that the desired number of clusters is 36, with a corresponding explained variance of 71.32%. This suggests that the analysis aimed to find a suitable number of clusters that explains a significant portion of the data's variance without being overly complex. The chosen point likely represents a decision made based on the trade-off between the desired level of explained variance and the number of clusters to manage.


------------------------------------------------------------
Cell index: 30
Input Cell Type: markdown
Input Text:
-----------
## Determine the number of clusters for the Noses
(I've hidden the output plot since it's similar to the previous plot) 

Output Text:
------------


------------------------------------------------------------
Cell index: 31
Input Cell Type: python
Input Text:
-----------
numClusters_nose  = DetermineNumClusters(allNoses_flat , plotFigure=True)

Output Text:
------------
<matplotlib.figure.Figure at 0x7fe75cb54978>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a line graph illustrating the relationship between the number of clusters in a clustering algorithm and the percentage of variance explained.  The horizontal axis (x-axis) represents the number of clusters, ranging from 0 to approximately 250. The vertical axis (y-axis) represents the explained variance percentage, ranging from 0% to 100%.

The graph shows a curve that rapidly increases at first, indicating that adding more clusters initially leads to a significant increase in the explained variance. As the number of clusters increases beyond a certain point (around 16), the rate of increase slows considerably, suggesting diminishing returns.  The curve flattens out, implying that adding more clusters beyond this point provides only a small incremental improvement in explained variance.

A prominent red circle highlights a point on the curve corresponding to approximately 16 clusters and around 73% explained variance.  This point is likely chosen to represent the optimal number of clusters, as it balances the amount of variance explained with the complexity of the model (fewer clusters are generally preferred for simplicity). The title of the graph explicitly states that 16 clusters are considered the desired number, resulting in 73.11% explained variance.


------------------------------------------------------------
Cell index: 32
Input Cell Type: markdown
Input Text:
-----------
## Determine the number of clusters for the Mouths
(I've hidden the output plot since it's similar to the previous plot)

Output Text:
------------


------------------------------------------------------------
Cell index: 33
Input Cell Type: python
Input Text:
-----------
numClusters_mouth = DetermineNumClusters(allMouths_flat, plotFigure=True)

Output Text:
------------
<matplotlib.figure.Figure at 0x7fe75c584c88>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a line graph depicting the relationship between the number of clusters and the explained percent variance in a clustering analysis. 


The horizontal axis represents the "Number of Clusters," ranging from 0 to approximately 250. The vertical axis shows the "Explained Percent," ranging from 0 to 100. A blue line traces the explained percentage as the number of clusters increases. The line starts at a low explained percentage with a few clusters and then rapidly increases, before leveling off as more clusters are added. A large red circle highlights a point on the graph where the number of clusters is around 20 and the explained percentage is around 70%. The title of the graph clearly states that the desired number of clusters is 16, and the explained variance at that point is approximately 72.34%. This suggests that 16 clusters offer a reasonable balance between explaining the variance in the data and avoiding overfitting.


------------------------------------------------------------
Cell index: 34
Input Cell Type: markdown
Input Text:
-----------
## Fit Kmeans Models with Desired Number of Clusters

Output Text:
------------


------------------------------------------------------------
Cell index: 35
Input Cell Type: python
Input Text:
-----------
#%% fit models with desired number of clusters
KmeansModel_nose  = cluster.KMeans(n_clusters=numClusters_nose , n_init=10, random_state=1)
KmeansModel_nose.fit(allNoses_flat)

KmeansModel_mouth = cluster.KMeans(n_clusters=numClusters_mouth, n_init=10, random_state=1)
KmeansModel_mouth.fit(allMouths_flat)

KmeansModel_eyes  = cluster.KMeans(n_clusters=numClusters_eyes , n_init=10, random_state=1)
KmeansModel_eyes.fit(np.concatenate((allLeftEyes_flat, allRightEyes_flat)))

Output Text:
------------
KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,
    n_clusters=36, n_init=10, n_jobs=1, precompute_distances='auto',
    random_state=1, tol=0.0001, verbose=0)


------------------------------------------------------------
Cell index: 36
Input Cell Type: markdown
Input Text:
-----------
## Show the Eye Clusters

Output Text:
------------


------------------------------------------------------------
Cell index: 37
Input Cell Type: python
Input Text:
-----------
#%% Show the resulting eye, nose and mouth clusters
def ShowKmeansClustersAsImages(KmeansModel, origImDims, title):
    clusterAssignments = KmeansModel.labels_
    subPlotRowsAndCols = np.ceil(np.sqrt(KmeansModel.n_clusters)).astype(int)
    
    plt.figure(figsize=(14,11));
    plt.suptitle(title, fontsize=30)
    for k in range(subPlotRowsAndCols**2):
        percentAssigned = 100.0 * (clusterAssignments == k).sum() / len(clusterAssignments)
        currCenterImage = np.reshape(KmeansModel.cluster_centers_[k,:],(origImDims[0],origImDims[1]))
    
        plt.subplot(subPlotRowsAndCols,subPlotRowsAndCols,k+1); plt.axis('off');
        plt.imshow(currCenterImage,cmap='gray');
        plt.title('%.2f%s assignment' %(percentAssigned,'%'), fontsize=11);

Output Text:
------------


------------------------------------------------------------
Cell index: 38
Input Cell Type: python
Input Text:
-----------
ShowKmeansClustersAsImages(KmeansModel_eyes , allLeftEyes.shape[0:2], 'Eye Clusters')

Output Text:
------------
<matplotlib.figure.Figure at 0x7fe75cb9b8d0>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image displays a grid of 36 small grayscale images, each depicting a close-up of an eye.  The images are organized in a 6x6 matrix.  Above each image is a percentage value labeled "assignment". These percentages likely represent the degree to which each image belongs to a particular cluster in a clustering algorithm, such as k-means. The title of the image is "Eye Clusters", clearly indicating the purpose of the visualization.

The grayscale nature of the images and their blurred appearance suggest that they are likely processed or pre-processed images, possibly resulting from feature extraction or dimensionality reduction techniques commonly used in image processing and machine learning. The variations in appearance across the images show different characteristics of eyes, possibly reflecting variations in lighting, angle, or individual differences.

The overall purpose of the image is to show the results of a clustering algorithm applied to a dataset of eye images.  Each image represents a data point, and the percentage indicates its probability of belonging to a specific cluster.  The visualization allows for a visual inspection of the clusters and the quality of the clustering process, helping to assess the effectiveness of the algorithm and the characteristics of the resulting clusters.


------------------------------------------------------------
Cell index: 39
Input Cell Type: markdown
Input Text:
-----------
We can see quite interesting eyes here. open eyes, almost closed eyes, female eyes, male eyes, eyes with glasses. Intersting to look at.

Output Text:
------------


------------------------------------------------------------
Cell index: 40
Input Cell Type: markdown
Input Text:
-----------
## Show the Nose Clusters

Output Text:
------------


------------------------------------------------------------
Cell index: 41
Input Cell Type: python
Input Text:
-----------
ShowKmeansClustersAsImages(KmeansModel_nose , allNoses.shape[0:2]   , 'Nose Clusters')

Output Text:
------------
<matplotlib.figure.Figure at 0x7fe75ec9a668>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image displays a grid of sixteen grayscale images, each depicting a different cluster of noses.  The images are arranged in a four-by-four matrix.  Each image is a low-resolution representation of a nose, likely generated through a clustering algorithm applied to a dataset of nose images.  The images show variations in nose shape, size, and overall features.

Above each image, a percentage is displayed, indicating the percentage of noses in the dataset that belong to that particular cluster. This suggests that the clustering algorithm grouped similar-looking noses together, with the percentage reflecting the relative size or prevalence of each cluster.  The title of the image, "Nose Clusters," clearly indicates the nature of the data presented.

The overall impression is that the image presents a visual representation of the results of a machine learning task, specifically clustering, applied to a collection of nose images. The different clusters illustrate the range of variations in nose shape and appearance found within the dataset.


------------------------------------------------------------
Cell index: 42
Input Cell Type: markdown
Input Text:
-----------
We can see noses that are lit from different angles, and noses that were taken from different angles, and we also see one nose that doesn't appear to have any nostrils...

Output Text:
------------


------------------------------------------------------------
Cell index: 43
Input Cell Type: markdown
Input Text:
-----------
## Show the Mouth Clusters

Output Text:
------------


------------------------------------------------------------
Cell index: 44
Input Cell Type: python
Input Text:
-----------
ShowKmeansClustersAsImages(KmeansModel_mouth, allMouths.shape[0:2]  , 'Mouth Clusters')

Output Text:
------------
<matplotlib.figure.Figure at 0x7fe75ec712b0>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a grid of sixteen grayscale images, each depicting a cluster of mouth regions from a face recognition or similar dataset.  The images are arranged in a four-by-four matrix. Each mouth image is blurry, suggesting that it represents an average or archetype derived from a collection of similar mouth features.  This blurring is a common technique in clustering visualizations to highlight common characteristics while minimizing individual variations.

Above each mouth image, a percentage is displayed, labeled as "% assignment." This percentage likely indicates the proportion of data points in the original dataset that were assigned to that particular cluster.  A higher percentage suggests a larger cluster, representing a more common or representative type of mouth shape or configuration.  The title "Mouth Clusters" clearly indicates the nature of the data and the purpose of the visualization—to show the different types of mouths identified through a clustering algorithm. The overall presentation is clear, concise, and effectively communicates the results of a clustering analysis.


------------------------------------------------------------
Cell index: 45
Input Cell Type: markdown
Input Text:
-----------
We can see here smiling mouths, male mouths, female mouths, mouths with facial hair, closed mouths and mouths that are lit from different angle. It's interesting. 

Output Text:
------------


------------------------------------------------------------
Cell index: 46
Input Cell Type: markdown
Input Text:
-----------
## Compose Faces out of Facial Parts
for each face, randomly choose a nose cluster, a mouth cluster, a left eye cluster, a right eye cluster, and simply draw these clusters on top of the mean image.

Output Text:
------------


------------------------------------------------------------
Cell index: 47
Input Cell Type: python
Input Text:
-----------
#%% Generate Caricature Faces out of Facial Parts
noseRowCoords     = np.floor(np.array([-0.5,0.5])*targetNoseSize[0] + int(round(facialKeypointsDF.loc[:,'nose_BB_y'].mean()))).astype(int)
noseColCoords     = np.floor(np.array([-0.5,0.5])*targetNoseSize[1] + int(round(facialKeypointsDF.loc[:,'nose_BB_x'].mean()))).astype(int)

mouthRowCoords    = np.floor(np.array([-0.5,0.5])*targetMouthSize[0] + int(round(facialKeypointsDF.loc[:,'mouth_BB_y'].mean()))).astype(int)
mouthColCoords    = np.floor(np.array([-0.5,0.5])*targetMouthSize[1] + int(round(facialKeypointsDF.loc[:,'mouth_BB_x'].mean()))).astype(int)

leftEyeRowCoords  = np.floor(np.array([-0.5,0.5])*targetEyeSize[0] + int(round(facialKeypointsDF.loc[:,'left_eye_BB_y'].mean()))).astype(int)
leftEyeColCoords  = np.floor(np.array([-0.5,0.5])*targetEyeSize[1] + int(round(facialKeypointsDF.loc[:,'left_eye_BB_x'].mean()))).astype(int)

rightEyeRowCoords = np.floor(np.array([-0.5,0.5])*targetEyeSize[0] + int(round(facialKeypointsDF.loc[:,'right_eye_BB_y'].mean()))).astype(int)
rightEyeColCoords = np.floor(np.array([-0.5,0.5])*targetEyeSize[1] + int(round(facialKeypointsDF.loc[:,'right_eye_BB_x'].mean()))).astype(int)

# plot mouths
numFigRows = 7
numFigCols = 4
numPlots = numFigRows * numFigCols
randIndsVec = np.random.choice(numImages,numPlots,replace=False)
randIndsMat = randIndsVec.reshape((numFigRows,numFigCols))

fig, ax = plt.subplots(nrows=numFigRows,ncols=numFigCols,figsize=(14,25))
for i in range(numFigRows):
    for j in range(numFigCols):
        #generatedImage = 128.0*np.ones((imHeight,imWidth))
        generatedImage = faceImagesDB.mean(axis=2)
        
        # generate nose
        k = np.random.randint(numClusters_nose)
        noseImage = np.reshape(KmeansModel_nose.cluster_centers_[k,:],targetNoseSize)
        generatedImage[noseRowCoords[0]:noseRowCoords[1],noseColCoords[0]:noseColCoords[1]] = noseImage
        
        # select mouth
        k = np.random.randint(numClusters_mouth)
        mouthImage = np.reshape(KmeansModel_mouth.cluster_centers_[k,:],targetMouthSize)
        generatedImage[mouthRowCoords[0]:mouthRowCoords[1],mouthColCoords[0]:mouthColCoords[1]] = mouthImage
        
        # select left eye
        k = np.random.randint(numClusters_eyes)
        leftEyeImage = np.reshape(KmeansModel_eyes.cluster_centers_[k,:],targetEyeSize)
        generatedImage[leftEyeRowCoords[0]:leftEyeRowCoords[1],leftEyeColCoords[0]:leftEyeColCoords[1]] = leftEyeImage
        
        # select right eye
        k = np.random.randint(numClusters_eyes)
        rightEyeImage = np.reshape(KmeansModel_eyes.cluster_centers_[k,:],targetEyeSize)
        generatedImage[rightEyeRowCoords[0]:rightEyeRowCoords[1],rightEyeColCoords[0]:rightEyeColCoords[1]] = np.fliplr(rightEyeImage)
                
        # show the resulting image
        ax[i][j].imshow(generatedImage, cmap='gray');
        ax[i][j].set_axis_off()


Output Text:
------------
<matplotlib.figure.Figure at 0x7fe75f320ac8>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a grid of 32 grayscale images, each depicting a human face.  The faces are arranged in a 8x4 matrix. 


Each face appears somewhat blurred or averaged, suggesting they might be composite images or representations generated from a dataset of faces.  Interestingly, portions of each face (eyes, nose, mouth) are slightly darker or more defined than the surrounding areas, almost like a mask or overlay has been applied to highlight these facial features. This implies a possible process of feature extraction or facial recognition at play.


The overall impression is that of a visualization of a facial recognition or image processing technique, potentially showcasing either the average face within a dataset or the results of a reconstruction algorithm focusing on key facial components. The grayscale format and the consistent blurring across all images contribute to a unified and structured presentation.


------------------------------------------------------------
Cell index: 48
Input Cell Type: markdown
Input Text:
-----------
This is definitely amusing! we can see that a lot of times the eyes dont match each other, but overall these still look like faces, and it's still quite fun to look at.

I'll explain what we've done here.
The kmeans model of each part individually, actually models the joint probability density of all the pixels in that part together ("jointly"). This is done seperatley for each part. but, even though there are clear relationships also between parts, we have intentionally decided to disregard them in our "generative model" for the composed faces, and decided to model the relationships between parts as independent random vectors.

There is an important lesson to be learned here, in my opinion (this is also why I've written this kernel).  
It's subtle, but I think it touches on one of the most important parts of statistics and genralization.  

On the one hand, by assuming independence we lose compatibility between parts, but on the other hand we gain in "creativity". i.e. by modeling the parts independent of each other, we are able to "imagine" previously unseen faces.  

Output Text:
------------


------------------------------------------------------------

================================================================================
================================================================================
gmm-in-pca-space.ipynb:
=======================
------------------------------------------------------------
Cell index: 1
Input Cell Type: python
Input Text:
-----------
import numpy as np
import pandas as pd
import xml.etree.ElementTree as ET
import matplotlib.pyplot as plt
from PIL import Image
import os
import glob
import zipfile
from skimage import transform as tform
from sklearn import decomposition, mixture

Output Text:
------------


------------------------------------------------------------
Cell index: 2
Input Cell Type: python
Input Text:
-----------
root_images="../input/all-dogs/all-dogs/"
root_annots="../input/annotation/Annotation/"

all_images=os.listdir("../input/all-dogs/all-dogs/")
print(f"Total images : {len(all_images)}")

breeds = glob.glob('../input/annotation/Annotation/*')
annotation=[]
for b in breeds:
    annotation+=glob.glob(b+"/*")
print(f"Total annotation : {len(annotation)}")

breed_map={}
for annot in annotation:
    breed=annot.split("/")[-2]
    index=breed.split("-")[0]
    breed_map.setdefault(index,breed)
    
print(f"Total Breeds : {len(breed_map)}")

Output Text:
------------
Total images : 20579
Total annotation : 20580
Total Breeds : 120


------------------------------------------------------------
Cell index: 3
Input Cell Type: markdown
Input Text:
-----------
# Show several cropped dog images 
bounding box code taken from "" kernel 

Output Text:
------------


------------------------------------------------------------
Cell index: 4
Input Cell Type: python
Input Text:
-----------
def bounding_box(image):
    bpath=root_annots+str(breed_map[image.split("_")[0]])+"/"+str(image.split(".")[0])
    tree = ET.parse(bpath)
    root = tree.getroot()
    objects = root.findall('object')
    for o in objects:
        bndbox = o.find('bndbox') # reading bound box
        xmin = int(bndbox.find('xmin').text)
        ymin = int(bndbox.find('ymin').text)
        xmax = int(bndbox.find('xmax').text)
        ymax = int(bndbox.find('ymax').text)
        
    return (xmin,ymin,xmax,ymax)

num_rows = 5
num_cols = 9
num_images_to_show = num_rows*num_cols
selected_images = np.random.choice(all_images,size=num_images_to_show,replace=False)

plt.figure(figsize=(30,16))
for k, image_filename in enumerate(selected_images):
    bbox = bounding_box(image_filename)
    orig_image = Image.open(os.path.join(root_images, image_filename))
    cropped_image = orig_image.crop(bbox)
    
    plt.subplot(num_rows,num_cols,k+1); plt.imshow(cropped_image); plt.axis("off")

Output Text:
------------
<Figure size 2160x1152 with 45 Axes>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a grid of 6 rows and 6 columns, containing a total of 36 smaller images. Each smaller image shows a single dog of a different breed, in various poses and settings.  The dogs are diverse in size, color, and breed, showcasing a wide range of canine appearances.

Some dogs are indoors, others outdoors in grass or other environments.  Some are posed formally, while others appear relaxed or playful.  The backgrounds in the smaller images are varied, showing different locations and settings, adding to the visual diversity.  The lighting in each smaller image also varies, reflecting different shooting conditions.

The overall structure is neatly organized, creating a systematic catalog-like display of different dog breeds.  The consistent size and framing of each smaller image contribute to the image's clean and organized presentation.


------------------------------------------------------------
Cell index: 5
Input Cell Type: markdown
Input Text:
-----------
# Create a small dataset of resized images

Output Text:
------------


------------------------------------------------------------
Cell index: 6
Input Cell Type: python
Input Text:
-----------
# create a small dataset of resized images
num_images_in_dataset = 20000
image_dimention = 64
resize_shape = (image_dimention, image_dimention)

selected_images = np.random.choice(all_images, size=num_images_in_dataset, replace=False)

# create a matrix to hold all images
image_dataset_4D_matrix = np.zeros((image_dimention,image_dimention,3,num_images_in_dataset), dtype=np.uint8)

# fill up the matrix with images
for k, image_filename in enumerate(selected_images):
    bbox = bounding_box(image_filename)
    orig_image = Image.open(os.path.join(root_images, image_filename))
    cropped_image = orig_image.crop(bbox)
    resized_image = tform.resize(np.array(cropped_image), resize_shape, preserve_range=True).astype(np.uint8)

    image_dataset_4D_matrix[:,:,:,k] = resized_image
    
print('finished collecting dataset')

Output Text:
------------
finished collecting dataset


------------------------------------------------------------
Cell index: 7
Input Cell Type: markdown
Input Text:
-----------
# Create PCA model of the data
210 - explain ~90% of the variance

Output Text:
------------


------------------------------------------------------------
Cell index: 8
Input Cell Type: python
Input Text:
-----------
# create PCA model of the data
print(image_dataset_4D_matrix.shape)
num_components = 768
X = image_dataset_4D_matrix.reshape(image_dimention*image_dimention*3,-1).T

dog_PCA = decomposition.PCA(n_components=num_components, whiten=True)
dog_PCA.fit(X)

print('finished training PCA model')

Output Text:
------------
(64, 64, 3, 20000)
finished training PCA model


------------------------------------------------------------
Cell index: 9
Input Cell Type: markdown
Input Text:
-----------
# Show histograms of PCA projections

Output Text:
------------


------------------------------------------------------------
Cell index: 10
Input Cell Type: python
Input Text:
-----------
X_pca = dog_PCA.transform(X)
X_rec = dog_PCA.inverse_transform(X_pca)
plt.figure(figsize=(30,12))
for k in range(36):
    plt.subplot(4,9,k+1); plt.hist(X_pca[:,k], bins=50, log=True); plt.xlim(-6,6)

print(X.shape, X_pca.shape)
print(X.mean(), X.std())
print(X_rec.mean(), X_rec.std())

Output Text:
------------
(20000, 12288) (20000, 768)
108.87677130533854 63.9207307181584
108.87677130533847 62.84672111052629
<Figure size 2160x864 with 36 Axes>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image consists of a 4x8 grid of histograms, totaling 32 individual histograms. Each histogram appears to represent a distribution of numerical data, likely sampled from a similar underlying process, given their visual similarity.  The x-axis of each histogram ranges from approximately -5 to 5, while the y-axis is logarithmic, spanning several orders of magnitude, indicating a large range in the frequency of occurrences within each bin.

The histograms themselves are visually quite similar, exhibiting a bell-shaped or Gaussian-like distribution, centered around 0.  There's slight variation in the exact shape and spread of the distributions across the different histograms, suggesting potential sampling variability or minor differences in the underlying data generation process. The consistent centering around 0 and the unimodal, symmetrical nature of the distributions are striking features.  The logarithmic scale on the y-axis highlights the concentration of data points near the mean and the rapid decline in frequency as one moves away from the center.

The overall impression is that the image displays the results of a repeated experiment or simulation, where each histogram represents a separate instance of the same underlying process. The consistent Gaussian-like distributions suggest the process might be well-modeled by a normal distribution, although the minor variations warrant further investigation into the potential sources of the observed differences between the histograms.


------------------------------------------------------------
Cell index: 11
Input Cell Type: markdown
Input Text:
-----------
# Show cumulative explained variance plot of PCA

Output Text:
------------


------------------------------------------------------------
Cell index: 12
Input Cell Type: python
Input Text:
-----------
# show cumulative variance explained
plt.figure(figsize=(16,10))
plt.plot(100*np.concatenate((np.array([0]),np.cumsum(dog_PCA.explained_variance_ratio_))))
plt.xlabel('num components', fontsize=16); plt.ylabel('% variance explained', fontsize=16); plt.ylim(-1,101); plt.xlim(-1,num_components+1);
print('total explained percent by %d components - %.1f%s' %(num_components, 100*dog_PCA.explained_variance_ratio_.sum(),'%'))

Output Text:
------------
total explained percent by 768 components - 96.6%
<Figure size 1152x720 with 1 Axes>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a line graph depicting the percentage of variance explained by a varying number of principal components.  The x-axis represents the number of principal components used, ranging from 0 to approximately 750. The y-axis shows the cumulative percentage of variance explained, ranging from 0% to 100%.

The line itself starts at 0% variance explained with 0 components and steeply increases at first, showing that the initial components capture a significant portion of the total variance.  As more components are added, the rate of increase slows considerably, indicating diminishing returns. The curve gradually approaches 100% variance explained as the number of components nears 750, suggesting that almost all the variance in the data is accounted for by using a sufficiently large number of principal components.  The graph clearly illustrates the concept of dimensionality reduction in principal component analysis (PCA), where a smaller subset of components can explain a large proportion of the data's variance.


------------------------------------------------------------
Cell index: 13
Input Cell Type: markdown
Input Text:
-----------
# Show the main variance directions in the data

Output Text:
------------


------------------------------------------------------------
Cell index: 14
Input Cell Type: python
Input Text:
-----------
def normize_image_for_plotting(input_image):
    min_val = input_image.min()
    med_val = input_image.mean()
    max_val = input_image.max()

    output_image = input_image - med_val
    output_image /= max((max_val-med_val),(med_val-min_val))
    output_image = (255*(0.5*output_image + 0.5)).astype(np.uint8)
    
    return output_image
    
# show the mean sample and the first couple of eigenvectors (should look a little like furier basis)
num_rows = 5
num_cols = 8
num_images_to_show = num_rows*num_cols

mean_image = np.reshape(dog_PCA.mean_,(image_dimention,image_dimention,3)).astype(np.uint8)

plt.figure(figsize=(30,16))
plt.subplot(num_rows,num_cols,1); plt.imshow(mean_image); plt.axis('off')
for k in range(1, num_rows*num_cols):
    eigenvector = dog_PCA.components_[k,:]
    eigenvector_image = np.reshape(eigenvector,(image_dimention,image_dimention,3))
    
    plt.subplot(num_rows,num_cols,k+1); plt.imshow(normize_image_for_plotting(eigenvector_image)); plt.axis('off')

Output Text:
------------
<Figure size 2160x1152 with 40 Axes>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid of 5 x 10 = 50 small square images. Each square shows a different, seemingly random pattern or texture. These patterns are low-resolution and blurry, with muted colors and a lack of sharp details.  Many of the patterns appear to be grayscale or have a limited color palette, with shades of gray, brown, and occasionally other muted colors like purple or blue.  There's no readily apparent overarching theme or structure connecting the individual patterns. They resemble abstract art or possibly the output of a process like a filter or algorithm applied to images. The overall impression is one of randomness within a structured arrangement.


------------------------------------------------------------
Cell index: 15
Input Cell Type: markdown
Input Text:
-----------
# Show some model reconstructions

Output Text:
------------


------------------------------------------------------------
Cell index: 16
Input Cell Type: python
Input Text:
-----------
# show several model reconstructions
num_rows = 5
num_cols = 9
num_images_to_show = num_rows*num_cols

selected_inds = np.random.choice(num_images_in_dataset, size=num_images_to_show, replace=False)
random_doglike_vectors = X_rec[selected_inds,:]
print(random_doglike_vectors.shape, random_doglike_vectors.mean(), random_doglike_vectors.std())

plt.figure(figsize=(30,16))
for k in range(num_images_to_show):
    doglike_image = np.reshape(random_doglike_vectors[k,:],(image_dimention,image_dimention,3))
    doglike_image[doglike_image > 255] = 255
    doglike_image[doglike_image <   0] =   0
    
    plt.subplot(num_rows,num_cols,k+1); plt.imshow(doglike_image.astype(np.uint8)); plt.axis("off")

Output Text:
------------
(45, 12288) 111.87753465433467 65.36793631599343
<Figure size 2160x1152 with 45 Axes>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a grid of 6x8 = 48 small square images. Each square shows a different dog, presented in a fairly consistent manner. The dogs are diverse in breed, size, color, and pose. Some are sitting, some standing, and some lying down.  The backgrounds are varied; some show outdoor settings, while others are simpler, more uniform backgrounds. The lighting and image quality are somewhat inconsistent across the images, with some appearing sharper than others.  The overall impression is of a dataset of dog images, possibly used for training a machine learning model for dog breed identification or a similar task. The dogs seem to be mostly small to medium-sized breeds and are varied in color and coat type.


------------------------------------------------------------
Cell index: 17
Input Cell Type: markdown
Input Text:
-----------
# Generate some random samples from a multivariate gaussian distribution

Output Text:
------------


------------------------------------------------------------
Cell index: 18
Input Cell Type: python
Input Text:
-----------
# generate several random samples from the gaussian model and present them
dog_single_gaussian_model = mixture.GaussianMixture(n_components=1, covariance_type='diag')
dog_single_gaussian_model.fit(X_pca)

num_rows = 5
num_cols = 9
num_images_to_show = num_rows*num_cols

random_latents = dog_single_gaussian_model.sample(num_images_to_show)[0]
print(random_latents.shape, random_latents.mean(), random_latents.std())

random_doglike_vectors = dog_PCA.inverse_transform(random_latents)
print(random_doglike_vectors.shape, random_doglike_vectors.mean(), random_doglike_vectors.std())

plt.figure(figsize=(30,16))
for k in range(num_images_to_show):
    doglike_image = np.reshape(random_doglike_vectors[k,:],(image_dimention,image_dimention,3))
    doglike_image[doglike_image > 255] = 255
    doglike_image[doglike_image <   0] =   0

    plt.subplot(num_rows,num_cols,k+1); plt.imshow(doglike_image.astype(np.uint8)); plt.axis("off")

Output Text:
------------
(45, 768) 0.003456405632435842 1.000286706174348
(45, 12288) 107.96164354535242 62.34469610422896
<Figure size 2160x1152 with 45 Axes>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid of 5x8 = 40 small square images. Each square image appears to be a low-resolution, slightly blurry depiction of a face or a similar textured pattern. 


The colors are muted and somewhat earth-toned, with a mixture of browns, greens, and muted blues and purples. There's a lack of sharp detail; the textures are more prominent than distinct features. The images are very similar in style and color palette, suggesting they might be related outputs from a machine learning model, perhaps related to generating faces or representing latent spaces in a generative adversarial network (GAN). 


The overall impression is one of abstract representations of faces or textures rather than realistic portraits. The similarity and subtle variations between the images suggest a common underlying structure or algorithm.


------------------------------------------------------------
Cell index: 19
Input Cell Type: markdown
Input Text:
-----------
# Train several GMM models and compare them
plot train and validation LogLikelihoods as function of number of gaussians

Output Text:
------------


------------------------------------------------------------
Cell index: 20
Input Cell Type: python
Input Text:
-----------
num_gaussians_to_try = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,18,21,30,50,80,150]
covariance_matrix_type = 'diag'
covariance_matrix_regularization = 5e-3

valid_fraction = 0.15
valid_cutoff = int((1-valid_fraction)*X_pca.shape[0])
rand_perm = np.random.permutation(X_pca.shape[0])
X_pca_train = X_pca[rand_perm[:valid_cutoff],:]
X_pca_valid = X_pca[rand_perm[valid_cutoff:],:]

train_LogLikelihood = []
valid_LogLikelihood = []
for num_gaussians in num_gaussians_to_try:
    
    curr_dog_GMM = mixture.GaussianMixture(n_components=num_gaussians, covariance_type=covariance_matrix_type, 
                                           reg_covar=covariance_matrix_regularization, verbose=0, verbose_interval=1)
    curr_dog_GMM.fit(X_pca_train)
    train_LL = curr_dog_GMM.score_samples(X_pca_train).mean()
    valid_LL = curr_dog_GMM.score_samples(X_pca_valid).mean()
    
    print('for %d gaussians: (train,valid) LogLikelihood = (%.5f,%.5f)' %(num_gaussians, train_LL, valid_LL))
    
    train_LogLikelihood.append(train_LL)
    valid_LogLikelihood.append(valid_LL)

plt.figure(figsize=(20,10))
plt.plot(num_gaussians_to_try, train_LogLikelihood, color='b')
plt.plot(num_gaussians_to_try, valid_LogLikelihood, color='g')
plt.legend(['train','valid'])
plt.ylabel('Log Likelihood'); plt.xlabel('num gaussians')

Output Text:
------------
for 1 gaussians: (train,valid) LogLikelihood = (-1089.73507,-1090.85034)
for 2 gaussians: (train,valid) LogLikelihood = (-1067.81070,-1069.91090)
for 3 gaussians: (train,valid) LogLikelihood = (-1089.44712,-1090.85046)
for 4 gaussians: (train,valid) LogLikelihood = (-1061.64915,-1064.23307)
for 5 gaussians: (train,valid) LogLikelihood = (-1067.34659,-1069.91051)
for 6 gaussians: (train,valid) LogLikelihood = (-1067.22842,-1069.91140)
for 7 gaussians: (train,valid) LogLikelihood = (-1061.34150,-1064.23385)
for 8 gaussians: (train,valid) LogLikelihood = (-1057.43237,-1060.73229)
for 9 gaussians: (train,valid) LogLikelihood = (-1060.90446,-1064.23360)
for 10 gaussians: (train,valid) LogLikelihood = (-1057.10173,-1060.73339)
for 11 gaussians: (train,valid) LogLikelihood = (-1057.14602,-1060.73126)
for 12 gaussians: (train,valid) LogLikelihood = (-1088.01645,-1090.85055)
for 13 gaussians: (train,valid) LogLikelihood = (-1055.35197,-1059.25598)
for 14 gaussians: (train,valid) LogLikelihood = (-1065.85805,-1069.91066)
for 15 gaussians: (train,valid) LogLikelihood = (-1056.73587,-1060.73392)
for 18 gaussians: (train,valid) LogLikelihood = (-1059.49261,-1064.23431)
for 21 gaussians: (train,valid) LogLikelihood = (-1055.38004,-1060.73103)
for 30 gaussians: (train,valid) LogLikelihood = (-1052.92094,-1059.25857)
for 50 gaussians: (train,valid) LogLikelihood = (-1050.07719,-1059.66080)
for 80 gaussians: (train,valid) LogLikelihood = (-1044.84719,-1058.98421)
/opt/conda/lib/python3.6/site-packages/sklearn/mixture/base.py:265: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  % (init + 1), ConvergenceWarning)
for 150 gaussians: (train,valid) LogLikelihood = (-1034.06411,-1058.43935)
Text(0.5, 0, 'num gaussians')
<Figure size 1440x720 with 1 Axes>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a line graph illustrating the relationship between the number of Gaussians and the log-likelihood for both training and validation datasets in a Gaussian Mixture Model (GMM). The x-axis represents the number of Gaussians used in the model, ranging from approximately 0 to 150.  The y-axis displays the log-likelihood, a measure of how well the model fits the data; lower values indicate better fit.

Two lines are plotted: one in blue representing the training data's log-likelihood and another in green for the validation data.  The blue line (training) shows a consistently decreasing log-likelihood as the number of Gaussians increases, indicating an improved fit to the training data with more Gaussians. However, the green line (validation) shows a different trend.  Initially, the validation log-likelihood decreases, suggesting improvement in generalization. However, after a certain point (around 20 Gaussians), it plateaus and remains relatively constant, even slightly increasing in some areas.

This difference in trends between training and validation log-likelihood highlights a potential overfitting issue. While adding more Gaussians improves the model's fit to the training data, it does not necessarily lead to better generalization to unseen data (validation data).  The plateau in the validation curve suggests that beyond a certain number of Gaussians, the model is starting to memorize the training data rather than learning generalizable patterns.  The optimal number of Gaussians would likely lie in the region where the validation log-likelihood is at its minimum or near its minimum before the plateau.


------------------------------------------------------------
Cell index: 21
Input Cell Type: markdown
Input Text:
-----------
# Select the best number of gaussians and retrain on all data
perform selection according to validation data

Output Text:
------------


------------------------------------------------------------
Cell index: 22
Input Cell Type: python
Input Text:
-----------
# train a mixture of gaussians model in the PCA space
num_gaussians = num_gaussians_to_try[np.argmax(valid_LogLikelihood)]
num_gaussians = 15000
print('selected number of gaussians is %d' %(num_gaussians))

covariance_matrix_type = 'diag'
covariance_matrix_regularization = 1e-3
dog_gaussian_mixture_model = mixture.GaussianMixture(n_components=num_gaussians, covariance_type=covariance_matrix_type, 
                                    reg_covar=covariance_matrix_regularization, verbose=2, verbose_interval=1)

dog_gaussian_mixture_model.fit(X_pca)

print('finished training GMM')

Output Text:
------------
selected number of gaussians is 15000
Initialization 0
  Iteration 1	 time lapse 1724.63674s	 ll change inf
  Iteration 2	 time lapse 112.29228s	 ll change 1.07232
  Iteration 3	 time lapse 112.40246s	 ll change 0.83143
  Iteration 4	 time lapse 112.41302s	 ll change 0.21717
  Iteration 5	 time lapse 112.63916s	 ll change 0.09722
  Iteration 6	 time lapse 112.63591s	 ll change 0.05639
  Iteration 7	 time lapse 113.23001s	 ll change 0.03988
  Iteration 8	 time lapse 113.30426s	 ll change 0.03229
  Iteration 9	 time lapse 114.11600s	 ll change 0.02493
  Iteration 10	 time lapse 108.40998s	 ll change 0.02134
  Iteration 11	 time lapse 109.11479s	 ll change 0.01986
  Iteration 12	 time lapse 108.01375s	 ll change 0.01553
  Iteration 13	 time lapse 107.65933s	 ll change 0.01330
  Iteration 14	 time lapse 108.30395s	 ll change 0.01149
  Iteration 15	 time lapse 108.43137s	 ll change 0.01147
  Iteration 16	 time lapse 108.74154s	 ll change 0.01049
  Iteration 17	 time lapse 108.28131s	 ll change 0.01044
  Iteration 18	 time lapse 108.16152s	 ll change 0.01122
  Iteration 19	 time lapse 108.01093s	 ll change 0.00967
  Iteration 20	 time lapse 108.23421s	 ll change 0.00717
  Iteration 21	 time lapse 108.34693s	 ll change 0.00596
  Iteration 22	 time lapse 107.68578s	 ll change 0.00492
  Iteration 23	 time lapse 108.51422s	 ll change 0.00510
  Iteration 24	 time lapse 107.13891s	 ll change 0.00511
  Iteration 25	 time lapse 107.29795s	 ll change 0.00412
  Iteration 26	 time lapse 108.04330s	 ll change 0.00349
  Iteration 27	 time lapse 108.38743s	 ll change 0.00346
  Iteration 28	 time lapse 108.84796s	 ll change 0.00338
  Iteration 29	 time lapse 108.35388s	 ll change 0.00387
  Iteration 30	 time lapse 108.25708s	 ll change 0.00412
  Iteration 31	 time lapse 108.39414s	 ll change 0.00347
  Iteration 32	 time lapse 108.32589s	 ll change 0.00318
  Iteration 33	 time lapse 108.48437s	 ll change 0.00319
  Iteration 34	 time lapse 108.36207s	 ll change 0.00313
  Iteration 35	 time lapse 108.26750s	 ll change 0.00269
  Iteration 36	 time lapse 108.16057s	 ll change 0.00340
  Iteration 37	 time lapse 108.70241s	 ll change 0.00281
  Iteration 38	 time lapse 108.51293s	 ll change 0.00239
  Iteration 39	 time lapse 108.45729s	 ll change 0.00256
  Iteration 40	 time lapse 108.63942s	 ll change 0.00284
  Iteration 41	 time lapse 108.41305s	 ll change 0.00257
  Iteration 42	 time lapse 108.57551s	 ll change 0.00221
  Iteration 43	 time lapse 108.69339s	 ll change 0.00210
  Iteration 44	 time lapse 108.20854s	 ll change 0.00239
  Iteration 45	 time lapse 108.16654s	 ll change 0.00186
  Iteration 46	 time lapse 107.42292s	 ll change 0.00160
  Iteration 47	 time lapse 107.54667s	 ll change 0.00162
  Iteration 48	 time lapse 107.93054s	 ll change 0.00181
  Iteration 49	 time lapse 108.59726s	 ll change 0.00178
  Iteration 50	 time lapse 108.49770s	 ll change 0.00175
  Iteration 51	 time lapse 108.78923s	 ll change 0.00146
  Iteration 52	 time lapse 108.35870s	 ll change 0.00117
  Iteration 53	 time lapse 108.41367s	 ll change 0.00110
  Iteration 54	 time lapse 108.61868s	 ll change 0.00116
  Iteration 55	 time lapse 108.27487s	 ll change 0.00147
  Iteration 56	 time lapse 108.35406s	 ll change 0.00116
  Iteration 57	 time lapse 108.51125s	 ll change 0.00098
Initialization converged: True	 time lapse 7825.58529s	 ll 1241.91879
finished training GMM


------------------------------------------------------------
Cell index: 23
Input Cell Type: markdown
Input Text:
-----------
# Generate several random samples from the gaussian mixture model

Output Text:
------------


------------------------------------------------------------
Cell index: 24
Input Cell Type: python
Input Text:
-----------
# generate several random samples from the gaussian mixture model and present them
num_rows = 5
num_cols = 9
num_images_to_show = num_rows*num_cols

sampled_latents = dog_gaussian_mixture_model.sample(num_images_to_show)[0]
print(sampled_latents.shape, sampled_latents.mean(), sampled_latents.std())

random_doglike_vectors = dog_PCA.inverse_transform(sampled_latents)
print(random_doglike_vectors.shape, random_doglike_vectors.mean(), random_doglike_vectors.std())

plt.figure(figsize=(30,16))
for k in range(num_images_to_show):
    doglike_image = np.reshape(random_doglike_vectors[k,:],(image_dimention,image_dimention,3))
    doglike_image[doglike_image > 255] = 255
    doglike_image[doglike_image <   0] =   0

    plt.subplot(num_rows,num_cols,k+1); plt.imshow(doglike_image.astype(np.uint8)); plt.axis("off")

Output Text:
------------
(45, 768) -0.006089333244091116 1.0443330657317624
(45, 12288) 112.3397441981471 65.98115376573229
<Figure size 2160x1152 with 45 Axes>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a grid of 6x8 = 48 small square images. Each square shows a photograph of a dog. The dogs are diverse in breed, age, color, and pose. Some are sitting, some standing, some lying down. The backgrounds vary; some are simple and uncluttered, while others are more complex and show outdoor settings. The quality of the photographs is inconsistent; some are sharp and clear, while others appear a bit blurry or have lower resolution.  The overall impression is a collection of dog photographs, possibly from a dataset used for machine learning or image recognition tasks.  The images are uniformly sized and arranged, creating a neat and organized visual display.


------------------------------------------------------------
Cell index: 25
Input Cell Type: markdown
Input Text:
-----------
# Compare marginal distributions as validation
compare the scatter plots of pairwise distributions in the PCA space of real images, single gaussian, mixture of gaussians

Output Text:
------------


------------------------------------------------------------
Cell index: 26
Input Cell Type: python
Input Text:
-----------
# show some 2D histograms of feature pairs of: original distribution, single gaussian distribution, GMM distribution
X_single_gaussian    = dog_single_gaussian_model.sample(X_pca.shape[0])[0]
X_multiple_gaussians = dog_gaussian_mixture_model.sample(X_pca.shape[0])[0]

num_cols = 8

plt.figure(figsize=(30,16))
for k in range(num_cols):
    feature_pair = np.random.choice(num_components, size=2, replace=False)
    
    original_data          = X_pca[:,feature_pair]
    single_gaussian_data   = X_single_gaussian[:,feature_pair]
    multiple_gaussian_data = X_multiple_gaussians[:,feature_pair]
    
    all_xs = np.concatenate((original_data[:,0],single_gaussian_data[:,0],multiple_gaussian_data[:,0]))
    all_ys = np.concatenate((original_data[:,1],single_gaussian_data[:,1],multiple_gaussian_data[:,1]))
    xlimits = [min(all_xs),max(all_xs)]
    ylimits = [min(all_ys),max(all_ys)]
    plt.subplot(3,num_cols,k+1+0*num_cols); plt.scatter(original_data[:,0],original_data[:,1],c='g',s=0.8, alpha=0.5); plt.xlim(xlimits); plt.ylim(ylimits);
    plt.subplot(3,num_cols,k+1+1*num_cols); plt.scatter(single_gaussian_data[:,0],single_gaussian_data[:,1],c='b',s=0.8, alpha=0.5); plt.xlim(xlimits); plt.ylim(ylimits);
    plt.subplot(3,num_cols,k+1+2*num_cols); plt.scatter(multiple_gaussian_data[:,0],multiple_gaussian_data[:,1],c='r',s=0.8, alpha=0.5); plt.xlim(xlimits); plt.ylim(ylimits);

Output Text:
------------
<Figure size 2160x1152 with 24 Axes>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid of 24 scatter plots, organized into three rows of eight plots each.  Each plot displays a bivariate distribution of data points, seemingly representing a two-dimensional dataset. The data points in each plot are colored using one of three colors: green, blue, or red.  The plots within each row appear to show variations or perhaps different stages of the same underlying data distribution.

The x and y axes of each plot have a similar range, roughly from -5 to 5, suggesting consistent scaling across the entire grid.  All plots show a roughly circular or elliptical distribution of points centered near the origin (0, 0).  The density of points is highest near the center, gradually decreasing towards the periphery. The differences between the plots within each color group might represent changes in variance, noise, or some other parameter affecting the data's spread.  The three different colors likely represent three distinct data classes or experimental conditions.


------------------------------------------------------------
Cell index: 27
Input Cell Type: markdown
Input Text:
-----------
# Create a submission

Output Text:
------------


------------------------------------------------------------
Cell index: 28
Input Cell Type: python
Input Text:
-----------
num_images_to_submit = 10000

sampled_latents = dog_gaussian_mixture_model.sample(num_images_to_submit)[0]
random_doglike_vectors = dog_PCA.inverse_transform(sampled_latents)

z = zipfile.PyZipFile('images.zip', mode='w')
for k in range(num_images_to_submit):
    doglike_image = np.reshape(random_doglike_vectors[k,:],(image_dimention,image_dimention,3))
    doglike_image[doglike_image > 255] = 255
    doglike_image[doglike_image <   0] =   0
    image_to_save = Image.fromarray(doglike_image.astype(np.uint8))

    image_filename = '%d.png' %(k)
    image_to_save.save(image_filename,'PNG'); z.write(image_filename); os.remove(image_filename)
print('finished writing "image.zip"')

Output Text:
------------
finished writing "image.zip"


------------------------------------------------------------
Cell index: 29
Input Cell Type: markdown
Input Text:
-----------
# Calculate MiFID 
(from kaggle)

Output Text:
------------


------------------------------------------------------------
Cell index: 30
Input Cell Type: python
Input Text:
-----------
from __future__ import absolute_import, division, print_function
import gzip, pickle
import tensorflow as tf
from scipy import linalg
import pathlib
import urllib
import warnings
from tqdm import tqdm

class KernelEvalException(Exception):
    pass

model_params = {
    'Inception': {
        'name': 'Inception', 
        'imsize': 64,
        'output_layer': 'Pretrained_Net/pool_3:0', 
        'input_layer': 'Pretrained_Net/ExpandDims:0',
        'output_shape': 2048,
        'cosine_distance_eps': 0.1
        }
}

def create_model_graph(pth):
    """Creates a graph from saved GraphDef file."""
    # Creates graph from saved graph_def.pb.
    with tf.gfile.FastGFile( pth, 'rb') as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString( f.read())
        _ = tf.import_graph_def( graph_def, name='Pretrained_Net')

def _get_model_layer(sess, model_name):
    # layername = 'Pretrained_Net/final_layer/Mean:0'
    layername = model_params[model_name]['output_layer']
    layer = sess.graph.get_tensor_by_name(layername)
    ops = layer.graph.get_operations()
    for op_idx, op in enumerate(ops):
        for o in op.outputs:
            shape = o.get_shape()
            if shape._dims != []:
              shape = [s.value for s in shape]
              new_shape = []
              for j, s in enumerate(shape):
                if s == 1 and j == 0:
                  new_shape.append(None)
                else:
                  new_shape.append(s)
              o.__dict__['_shape_val'] = tf.TensorShape(new_shape)
    return layer

def get_activations(images, sess, model_name, batch_size=50, verbose=False):
    """Calculates the activations of the pool_3 layer for all images.

    Params:
    -- images      : Numpy array of dimension (n_images, hi, wi, 3). The values
                     must lie between 0 and 256.
    -- sess        : current session
    -- batch_size  : the images numpy array is split into batches with batch size
                     batch_size. A reasonable batch size depends on the disposable hardware.
    -- verbose    : If set to True and parameter out_step is given, the number of calculated
                     batches is reported.
    Returns:
    -- A numpy array of dimension (num images, 2048) that contains the
       activations of the given tensor when feeding inception with the query tensor.
    """
    inception_layer = _get_model_layer(sess, model_name)
    n_images = images.shape[0]
    if batch_size > n_images:
        print("warning: batch size is bigger than the data size. setting batch size to data size")
        batch_size = n_images
    n_batches = n_images//batch_size + 1
    pred_arr = np.empty((n_images,model_params[model_name]['output_shape']))
    for i in tqdm(range(n_batches)):
        if verbose:
            print("\rPropagating batch %d/%d" % (i+1, n_batches), end="", flush=True)
        start = i*batch_size
        if start+batch_size < n_images:
            end = start+batch_size
        else:
            end = n_images
                    
        batch = images[start:end]
        pred = sess.run(inception_layer, {model_params[model_name]['input_layer']: batch})
        pred_arr[start:end] = pred.reshape(-1,model_params[model_name]['output_shape'])
    if verbose:
        print(" done")
    return pred_arr


# def calculate_memorization_distance(features1, features2):
#     neigh = NearestNeighbors(n_neighbors=1, algorithm='kd_tree', metric='euclidean')
#     neigh.fit(features2) 
#     d, _ = neigh.kneighbors(features1, return_distance=True)
#     print('d.shape=',d.shape)
#     return np.mean(d)

def normalize_rows(x: np.ndarray):
    """
    function that normalizes each row of the matrix x to have unit length.

    Args:
     ``x``: A numpy matrix of shape (n, m)

    Returns:
     ``x``: The normalized (by row) numpy matrix.
    """
    return np.nan_to_num(x/np.linalg.norm(x, ord=2, axis=1, keepdims=True))


def cosine_distance(features1, features2):
    # print('rows of zeros in features1 = ',sum(np.sum(features1, axis=1) == 0))
    # print('rows of zeros in features2 = ',sum(np.sum(features2, axis=1) == 0))
    features1_nozero = features1[np.sum(features1, axis=1) != 0]
    features2_nozero = features2[np.sum(features2, axis=1) != 0]
    norm_f1 = normalize_rows(features1_nozero)
    norm_f2 = normalize_rows(features2_nozero)

    d = 1.0-np.abs(np.matmul(norm_f1, norm_f2.T))
    print('d.shape=',d.shape)
    print('np.min(d, axis=1).shape=',np.min(d, axis=1).shape)
    mean_min_d = np.mean(np.min(d, axis=1))
    print('distance=',mean_min_d)
    return mean_min_d


def distance_thresholding(d, eps):
    if d < eps:
        return d
    else:
        return 1

def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):
    """Numpy implementation of the Frechet Distance.
    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)
    and X_2 ~ N(mu_2, C_2) is
            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).
            
    Stable version by Dougal J. Sutherland.

    Params:
    -- mu1 : Numpy array containing the activations of the pool_3 layer of the
             inception net ( like returned by the function 'get_predictions')
             for generated samples.
    -- mu2   : The sample mean over activations of the pool_3 layer, precalcualted
               on an representive data set.
    -- sigma1: The covariance matrix over activations of the pool_3 layer for
               generated samples.
    -- sigma2: The covariance matrix over activations of the pool_3 layer,
               precalcualted on an representive data set.

    Returns:
    --   : The Frechet Distance.
    """

    mu1 = np.atleast_1d(mu1)
    mu2 = np.atleast_1d(mu2)

    sigma1 = np.atleast_2d(sigma1)
    sigma2 = np.atleast_2d(sigma2)

    assert mu1.shape == mu2.shape, "Training and test mean vectors have different lengths"
    assert sigma1.shape == sigma2.shape, "Training and test covariances have different dimensions"

    diff = mu1 - mu2

    # product might be almost singular
    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)
    if not np.isfinite(covmean).all():
        msg = "fid calculation produces singular product; adding %s to diagonal of cov estimates" % eps
        warnings.warn(msg)
        offset = np.eye(sigma1.shape[0]) * eps
        # covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))
        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))
    
    # numerical error might give slight imaginary component
    if np.iscomplexobj(covmean):
        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):
            m = np.max(np.abs(covmean.imag))
            raise ValueError("Imaginary component {}".format(m))
        covmean = covmean.real

    # covmean = tf.linalg.sqrtm(tf.linalg.matmul(sigma1,sigma2))

    print('covmean.shape=',covmean.shape)
    # tr_covmean = tf.linalg.trace(covmean)

    tr_covmean = np.trace(covmean)
    return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean
    # return diff.dot(diff) + tf.linalg.trace(sigma1) + tf.linalg.trace(sigma2) - 2 * tr_covmean
#-------------------------------------------------------------------------------


def calculate_activation_statistics(images, sess, model_name, batch_size=50, verbose=False):
    """Calculation of the statistics used by the FID.
    Params:
    -- images      : Numpy array of dimension (n_images, hi, wi, 3). The values
                     must lie between 0 and 255.
    -- sess        : current session
    -- batch_size  : the images numpy array is split into batches with batch size
                     batch_size. A reasonable batch size depends on the available hardware.
    -- verbose     : If set to True and parameter out_step is given, the number of calculated
                     batches is reported.
    Returns:
    -- mu    : The mean over samples of the activations of the pool_3 layer of
               the incption model.
    -- sigma : The covariance matrix of the activations of the pool_3 layer of
               the incption model.
    """
    act = get_activations(images, sess, model_name, batch_size, verbose)
    mu = np.mean(act, axis=0)
    sigma = np.cov(act, rowvar=False)
    return mu, sigma, act
    
def _handle_path_memorization(path, sess, model_name, is_checksize, is_check_png):
    path = pathlib.Path(path)
    files = list(path.glob('*.jpg')) + list(path.glob('*.png'))
    imsize = model_params[model_name]['imsize']

    # In production we don't resize input images. This is just for demo purpose. 
    x = np.array([np.array(img_read_checks(fn, imsize, is_checksize, imsize, is_check_png)) for fn in files])
    m, s, features = calculate_activation_statistics(x, sess, model_name)
    del x #clean up memory
    return m, s, features

# check for image size
def img_read_checks(filename, resize_to, is_checksize=False, check_imsize = 64, is_check_png = False):
    im = Image.open(str(filename))
    if is_checksize and im.size != (check_imsize,check_imsize):
        raise KernelEvalException('The images are not of size '+str(check_imsize))
    
    if is_check_png and im.format != 'PNG':
        raise KernelEvalException('Only PNG images should be submitted.')

    if resize_to is None:
        return im
    else:
        return im.resize((resize_to,resize_to),Image.ANTIALIAS)

def calculate_kid_given_paths(paths, model_name, model_path, feature_path=None, mm=[], ss=[], ff=[]):
    ''' Calculates the KID of two paths. '''
    tf.reset_default_graph()
    create_model_graph(str(model_path))
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        m1, s1, features1 = _handle_path_memorization(paths[0], sess, model_name, is_checksize = True, is_check_png = True)
        if len(mm) != 0:
            m2 = mm
            s2 = ss
            features2 = ff
        elif feature_path is None:
            m2, s2, features2 = _handle_path_memorization(paths[1], sess, model_name, is_checksize = False, is_check_png = False)
        else:
            with np.load(feature_path) as f:
                m2, s2, features2 = f['m'], f['s'], f['features']

        print('m1,m2 shape=',(m1.shape,m2.shape),'s1,s2=',(s1.shape,s2.shape))
        print('starting calculating FID')
        fid_value = calculate_frechet_distance(m1, s1, m2, s2)
        print('done with FID, starting distance calculation')
        distance = cosine_distance(features1, features2)        
        return fid_value, distance, m2, s2, features2


Output Text:
------------


------------------------------------------------------------
Cell index: 31
Input Cell Type: python
Input Text:
-----------
calculate_MiFID_score = False

if calculate_MiFID_score:
    # UNCOMPRESS OUR IMGAES
    with zipfile.ZipFile("../working/images.zip","r") as z:
        z.extractall("../tmp/images2/")

    # COMPUTE LB SCORE
    m2 = []; s2 =[]; f2 = []
    user_images_unzipped_path = '../tmp/images2/'
    images_path = [user_images_unzipped_path,'../input/generative-dog-images/all-dogs/all-dogs/']
    public_path = '../input/dog-face-generation-competition-kid-metric-input/classify_image_graph_def.pb'

    fid_epsilon = 10e-15

    fid_value_public, distance_public, m2, s2, f2 = calculate_kid_given_paths(images_path, 'Inception', public_path, mm=m2, ss=s2, ff=f2)
    distance_public = distance_thresholding(distance_public, model_params['Inception']['cosine_distance_eps'])
    print("FID_public: ", fid_value_public, "distance_public: ", distance_public, "multiplied_public: ",
            fid_value_public /(distance_public + fid_epsilon))

    # REMOVE FILES TO PREVENT KERNEL ERROR OF TOO MANY FILES
    ! rm -r ../tmp

Output Text:
------------


------------------------------------------------------------

================================================================================
================================================================================
analyzing-soccer-player-faces.ipynb:
====================================
------------------------------------------------------------
Cell index: 1
Input Cell Type: markdown
Input Text:
-----------
Analyzing Soccer Player Faces
--------------------------------

In this script we will explore a dataset of faces of approximately 450 top male soccer players and approximately 150 top female soccer players.
We will examine the main modes of variation in the appearance of player faces (the face images of the players are conveniently almost perfectly aligned so the images turn out particularly nice).

We then try to find correlations between face features and real life attributes of those players such as "Age", "Height", "Weight" and "Sex".

Note that the first part of this script is essentially the same code that I used in [Visualizing PCA][1], but the data here is more interesting since it contains human faces. I welcome everyone to go visit also the [Visualizing PCA][1] script and look at the difference when applying the same piece of code to a different dataset.

  [1]: https://www.kaggle.com/selfishgene/leaf-classification/visualizing-pca-with-leaf-dataset

Output Text:
------------


------------------------------------------------------------
Cell index: 2
Input Cell Type: python
Input Text:
-----------
import os
import sys
import numpy as np
import pandas as pd
import matplotlib
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
import glob

from sklearn import decomposition
from sklearn.neighbors import KernelDensity
from sklearn.manifold import TSNE

matplotlib.style.use('fivethirtyeight')

Output Text:
------------


------------------------------------------------------------
Cell index: 3
Input Cell Type: markdown
Input Text:
-----------
## Load the data 

Output Text:
------------


------------------------------------------------------------
Cell index: 4
Input Cell Type: python
Input Text:
-----------
#%% load the data, go over training images and store them in a list
maleFaceFiles   = glob.glob('../input/Pictures/*.png')
femaleFaceFiles = glob.glob('../input/Pictures_f/*.png')
faceFiles = maleFaceFiles + femaleFaceFiles

listOfPlayerNames = []
listOfImages = []
for imageFilename in faceFiles:
    currName = imageFilename.split("/")[-1].split('.')[0]
        
    try:
        currImage = mpimg.imread(imageFilename)
        if len(np.unique(currImage[:,:,0].ravel())) <= 40:
            print("no image for '" + currName + "'")
        else:
            listOfPlayerNames.append(currName)
            listOfImages.append(currImage)
    except:
        print("didn't load '" + currName + "'")
        
femaleNames = [x.split("/")[-1].split('.')[0] for x in femaleFaceFiles]
isFemale    = [x in femaleNames for x in listOfPlayerNames]

print('Total number of loaded face images is %d' %(len(listOfImages)))

Output Text:
------------
Total number of loaded face images is 572


------------------------------------------------------------
Cell index: 5
Input Cell Type: markdown
Input Text:
-----------
## Show a random sample of players in the dataset

Output Text:
------------


------------------------------------------------------------
Cell index: 6
Input Cell Type: python
Input Text:
-----------
#%% show some images
matplotlib.rcParams['font.size'] = 9
matplotlib.rcParams['figure.figsize'] = (12,19)

numRows = 9; numCols = 5

plt.figure()
for k in range(numRows*numCols):
    randInd = np.random.randint(len(listOfImages))
    plt.subplot(numRows,numCols,k+1); 
    plt.imshow(listOfImages[randInd])
    plt.title(listOfPlayerNames[randInd]); plt.axis('off')

Output Text:
------------
<matplotlib.figure.Figure at 0x7f56625be1d0>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid of 6 columns and 8 rows, displaying headshots of 48 individuals. Each headshot is accompanied by the individual's name printed below it. The names appear to be a mix of male and female names, suggesting a diverse group of people. The background of each cell is white, creating a clean and organized presentation.

The individuals pictured appear to be from various ethnic backgrounds and age ranges, adding to the diversity of the collection. The headshots are consistently sized and formatted, creating a visually uniform look.  The overall impression is that of a roster or directory of people, possibly athletes or celebrities given the apparent uniformity of the photos.


------------------------------------------------------------
Cell index: 7
Input Cell Type: markdown
Input Text:
-----------
## Reorganize the Data for later

Output Text:
------------


------------------------------------------------------------
Cell index: 8
Input Cell Type: python
Input Text:
-----------
#%% gather everything into a large matrix
fullImageMatrix4D = np.zeros((128,128,3,len(listOfImages)))

backGroundImage = np.zeros((128,128,3))
backGroundImage[:,:,0] = 0.3
backGroundImage[:,:,1] = 0.4
backGroundImage[:,:,2] = 0.5

for k, currImage in enumerate(listOfImages):
    alphaChannel = currImage[:,:,3]
    rgbImage = currImage[:,:,:3]
    tiledAlpha = np.tile(alphaChannel[:,:,np.newaxis],(1,1,3))
    
    fullImageMatrix4D[:,:,:,k] = rgbImage*tiledAlpha + (tiledAlpha < 0.15)*backGroundImage

Output Text:
------------


------------------------------------------------------------
Cell index: 9
Input Cell Type: markdown
Input Text:
-----------
## Show the Mean Face Image and pixel-wise Standard Deviation Image

Output Text:
------------


------------------------------------------------------------
Cell index: 10
Input Cell Type: python
Input Text:
-----------
# show mean face image and stdev image
matplotlib.rcParams['font.size'] = 10
matplotlib.rcParams['figure.figsize'] = (12,12)

plt.figure(); 
plt.subplot(1,2,1); plt.imshow(fullImageMatrix4D.mean(axis=3)); 
plt.title('mean face Image'); plt.axis('off')
plt.subplot(1,2,2); plt.imshow(fullImageMatrix4D.std(axis=3)); 
plt.title('standard deviation Image'); plt.axis('off')

Output Text:
------------
(-0.5, 127.5, 127.5, -0.5)
<matplotlib.figure.Figure at 0x7f56625be978>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a horizontal arrangement of two blurred face images, presented side-by-side.  Each image is labeled at the top. The left image is titled "mean face Image," and the right image is titled "standard deviation Image."

Both images depict a blurry, somewhat indistinct human face, seemingly generated from a dataset of multiple faces.  The "mean face Image" shows a light-skinned person with short dark hair, appearing as an average or composite of many faces. The features are soft and lack sharp definition due to the blurring. The background is a solid, muted blue.

The "standard deviation Image" shows a similar face but with significantly more variation in color and shading.  It's darker overall, with areas of greater contrast, particularly around the eyes and mouth, suggesting the range of differences in features and skin tones within the dataset used to generate the image. The background is predominantly dark.

The blurring in both images suggests that they were created through a process of averaging and calculating standard deviation from a collection of facial images.  The effect is a representation of the "average" face and the typical variation from that average.


------------------------------------------------------------
Cell index: 11
Input Cell Type: markdown
Input Text:
-----------
## Define a Gaussian Model class that will later help us visualize things:

(this is long, just skip this to get to the analysis)

Output Text:
------------


------------------------------------------------------------
Cell index: 12
Input Cell Type: python
Input Text:
-----------
#%% define GaussianModel class and some additional helper functions

class GaussianModel:

    def __init__(self, X, numBasisFunctions=10, objectPixels=None):
        '''
        inputs: 
            X                    - numSamples x numDimentions matrix
            numBasisFunctions       - number of basis function to use
            objectPixels (optional) - an binnary mask image used for presentation
                                      will be used as Im[objectPixels] = dataSample
                                      must satisfy objectPixels.ravel().sum() = X.shape[1]
        '''
        
        self.numBasisFunctions = numBasisFunctions
        if objectPixels is None:
            self.objectPixels = np.ones((1,X.shape[1]),dtype=np.bool)
        else:
            self.objectPixels = objectPixels
        assert(self.objectPixels.ravel().sum() == X.shape[1])

        PCAModel = decomposition.PCA(n_components=numBasisFunctions, whiten=True)
        #PCAModel = decomposition.IncrementalPCA(n_components=numBasisFunctions, whiten=True, batch_size=400)
        self.dataRepresentation = PCAModel.fit_transform(X)
        self.PCAModel = PCAModel

        
    def RepresentUsingModel(self, X):
        return self.PCAModel.transform(X)

    def ReconstructUsingModel(self, X_transformed):
        return self.PCAModel.inverse_transform(X_transformed)

    def InterpretUsingModel(self, X):
        return self.PCAModel.inverse_transform(self.PCAModel.transform(X))

        
    # shows the eigenvectors of the gaussian covariance matrix
    def ShowVarianceDirections(self, numDirectionsToShow=16):
        #matplotlib.rcParams['font.size'] = 14
        numDirectionsToShow = min(numDirectionsToShow, self.numBasisFunctions)
        
        numFigRows = 4; numFigCols = 4;
        numDirectionsPerFigure = numFigRows*numFigCols
        numFigures = int(np.ceil(float(numDirectionsToShow)/numDirectionsPerFigure))
        
        for figureInd in range(numFigures):
            plt.figure()
            for plotInd in range(numDirectionsPerFigure):
                eigVecInd = numDirectionsPerFigure*figureInd + plotInd
                if eigVecInd >= self.numBasisFunctions:
                    break
                deltaImage = np.zeros(np.shape(self.objectPixels))
                deltaImage[self.objectPixels] = self.PCAModel.components_[eigVecInd,:].ravel()

                plt.subplot(numFigRows,numFigCols,plotInd+1)
                if np.shape(self.objectPixels)[0] == 1:
                    plt.plot(deltaImage)
                elif np.shape(self.objectPixels)[2] == 3:
                    deltaImage = 0.1/deltaImage.std()*deltaImage+0.5
                    deltaImage[deltaImage>1] = 1.0
                    deltaImage[deltaImage<0] = 0.0
                    plt.imshow(deltaImage); plt.axis('off')
                else:
                    plt.imshow(deltaImage); plt.axis('off')
                plt.title('%.3f%s explained' %(100*self.PCAModel.explained_variance_ratio_[eigVecInd], '%'))
            plt.tight_layout()

    
    # shows several random model reconstructions
    def ShowReconstructions(self, X, numReconstructions=6):
        #matplotlib.rcParams['font.size'] = 14
        assert(np.shape(X)[1] == self.objectPixels.ravel().sum())
        numSamples = np.shape(X)[0]
        numReconstructions = min(numReconstructions, numSamples)
        
        originalImage      = np.zeros(np.shape(self.objectPixels))
        reconstructedImage = np.zeros(np.shape(self.objectPixels))
        
        numReconstructionsPerFigure = min(6, numReconstructions)
        numFigures = int(np.ceil(float(numReconstructions)/numReconstructionsPerFigure))
        
        for figureInd in range(numFigures):
            plt.figure()
            for plotCol in range(numReconstructionsPerFigure):
                dataSampleInd = np.random.randint(numSamples)
                originalImage[self.objectPixels] = X[dataSampleInd,:].ravel()
                reconstructedImage[self.objectPixels] = self.InterpretUsingModel(np.reshape(X[dataSampleInd,:],[1,-1])).ravel()
                diffImage = abs(originalImage - reconstructedImage)
                
                # original image
                plt.subplot(3,numReconstructionsPerFigure,0*numReconstructionsPerFigure+plotCol+1)
                if np.shape(self.objectPixels)[0] == 1:
                    plt.plot(originalImage); plt.title('original signal')
                else:
                    plt.imshow(originalImage, cmap='gray'); plt.title('original image'); plt.axis('off')
                    
                # reconstred image
                plt.subplot(3,numReconstructionsPerFigure,1*numReconstructionsPerFigure+plotCol+1)
                if np.shape(self.objectPixels)[0] == 1:
                    plt.plot(reconstructedImage); plt.title('reconstructed signal')
                elif np.shape(self.objectPixels)[2] == 3:
                    reconstructedImage[reconstructedImage>1] = 1.0
                    reconstructedImage[reconstructedImage<0] = 0.0
                    plt.imshow(reconstructedImage); plt.title('reconstructed image'); plt.axis('off')
                else:
                    plt.imshow(reconstructedImage, cmap='gray'); plt.title('reconstructed image'); plt.axis('off')

                # diff image
                plt.subplot(3,numReconstructionsPerFigure,2*numReconstructionsPerFigure+plotCol+1)
                if np.shape(self.objectPixels)[0] == 1:
                    plt.plot(diffImage); plt.title('abs difference signal')
                else:
                    plt.imshow(diffImage, cmap='gray'); plt.title('abs difference image'); plt.axis('off')
            plt.tight_layout()

            
    # shows distrbution along the variance directions and several images along that variance direction
    def ShowModelVariations(self, numVariations=6):
        #matplotlib.rcParams['font.size'] = 14

        showAsTraces = (np.shape(self.objectPixels)[0] == 1)
        numVariations = min(numVariations, self.numBasisFunctions)
                
        numVarsPerFigure = min(6,numVariations)
        numFigures = int(np.ceil(float(numVariations)/numVarsPerFigure))
        
        lowRepVec     = np.percentile(self.dataRepresentation, 2, axis=0)
        medianRepVec  = np.percentile(self.dataRepresentation, 50, axis=0)
        highRepVec    = np.percentile(self.dataRepresentation, 98, axis=0)

        for figureInd in range(numFigures):
            plt.figure()
            for plotCol in range(numVarsPerFigure):
                eigVecInd = numVarsPerFigure*figureInd+plotCol
                if eigVecInd >= self.numBasisFunctions:
                    break

                # create the low and high precentile representation activation vectors
                currLowPrecentileRepVec             = medianRepVec.copy()
                currLowPrecentileRepVec[eigVecInd]  = lowRepVec[eigVecInd]
                currHighPrecentileRepVec            = medianRepVec.copy()
                currHighPrecentileRepVec[eigVecInd] = highRepVec[eigVecInd]

                # create blank images
                deltaImage          = np.zeros(np.shape(self.objectPixels))
                medianImage         = np.zeros(np.shape(self.objectPixels))
                lowPrecentileImage  = np.zeros(np.shape(self.objectPixels))
                highPrecentileImage = np.zeros(np.shape(self.objectPixels))

                # fill the object pixels with the relevant data
                deltaImage[self.objectPixels]          = self.PCAModel.components_[eigVecInd,:].ravel()
                lowPrecentileImage[self.objectPixels]  = self.ReconstructUsingModel(currLowPrecentileRepVec).ravel()
                medianImage[self.objectPixels]         = self.ReconstructUsingModel(medianRepVec).ravel()
                highPrecentileImage[self.objectPixels] = self.ReconstructUsingModel(currHighPrecentileRepVec).ravel()

                # calculate the Gaussian smoothed distribution of values along the eignevector direction
                sigmaOfKDE = 0.12
                pdfStart   = min(self.dataRepresentation[:,eigVecInd]) - 3*sigmaOfKDE
                pdfStop    = max(self.dataRepresentation[:,eigVecInd]) + 3*sigmaOfKDE
                xAxis = np.linspace(pdfStart,pdfStop,200)
                PDF_Model = KernelDensity(kernel='gaussian', bandwidth=sigmaOfKDE).fit(self.dataRepresentation[:,eigVecInd].reshape(-1,1))
                logPDF = PDF_Model.score_samples(xAxis.reshape(-1,1))

                # show distribution of current component 
                plt.subplot(5,numVarsPerFigure,0*numVarsPerFigure+plotCol+1)
                plt.fill(xAxis, np.exp(logPDF), fc='b', alpha=0.9);
                plt.title('%.3f%s explained' %(100*self.PCAModel.explained_variance_ratio_[eigVecInd], '%'))
                
                # show variance direction (eigenvector)
                plt.subplot(5,numVarsPerFigure,1*numVarsPerFigure+plotCol+1);
                if showAsTraces:
                    plt.plot(deltaImage); plt.title('eigenvector ' + str(eigVecInd))
                elif np.shape(self.objectPixels)[2] == 3:
                    deltaImage = 0.1/deltaImage.std()*deltaImage+0.5
                    deltaImage[deltaImage>1] = 1.0
                    deltaImage[deltaImage<0] = 0.0
                    plt.imshow(deltaImage); plt.title('eigenvector ' + str(eigVecInd)); plt.axis('off')
                else:
                    plt.imshow(deltaImage); plt.title('eigenvector ' + str(eigVecInd)); plt.axis('off')

                # show 2nd precentile image
                plt.subplot(5,numVarsPerFigure,2*numVarsPerFigure+plotCol+1)
                if showAsTraces:
                    plt.plot(lowPrecentileImage); plt.title('2nd precentile')
                elif np.shape(self.objectPixels)[2] == 3:
                    lowPrecentileImage[lowPrecentileImage>1] = 1.0
                    lowPrecentileImage[lowPrecentileImage<0] = 0.0
                    plt.imshow(lowPrecentileImage); plt.title('2nd precentile'); plt.axis('off')
                else:
                    plt.imshow(lowPrecentileImage, cmap='gray'); plt.title('2nd precentile'); plt.axis('off')

                # show median image
                plt.subplot(5,numVarsPerFigure,3*numVarsPerFigure+plotCol+1)
                if showAsTraces:
                    plt.plot(medianImage); plt.title('median')
                else:
                    plt.imshow(medianImage, cmap='gray'); plt.title('median'); plt.axis('off')

                # show 98th precentile image
                plt.subplot(5,numVarsPerFigure,4*numVarsPerFigure+plotCol+1)
                if showAsTraces:
                    plt.plot(highPrecentileImage); plt.title('98th precentile')
                elif np.shape(self.objectPixels)[2] == 3:
                    highPrecentileImage[highPrecentileImage>1] = 1.0
                    highPrecentileImage[highPrecentileImage<0] = 0.0
                    plt.imshow(highPrecentileImage); plt.title('98th precentile'); plt.axis('off')
                else:
                    plt.imshow(highPrecentileImage, cmap='gray'); plt.title('98th precentile'); plt.axis('off')
            plt.tight_layout()
        
            
    # shows distrbution along the variance directions and several images along that variance direction
    def ShowSingleComponentVariation(self, X, listOfComponents=[0,1]):
        #matplotlib.rcParams['font.size'] = 14

        showAsTraces = (np.shape(self.objectPixels)[0] == 1)
        assert(all([(x in range(self.numBasisFunctions)) for x in listOfComponents]))
                
        X_rep = self.RepresentUsingModel(X)
        
        percentilesToShow = [1,10,30,70,90,99]
        numReadDataSamplePerPercentile = 4
        representationPercentiles = []
        for percentile in percentilesToShow:
            representationPercentiles.append(np.percentile(self.dataRepresentation, percentile, axis=0))
        medianRepVec =  np.percentile(self.dataRepresentation, 50, axis=0)

        for eigVecInd in listOfComponents:
            plt.figure(); gs = gridspec.GridSpec(numReadDataSamplePerPercentile+2,len(percentilesToShow))

            # calculate the Gaussian smoothed distribution of values along the eignevector direction
            sigmaOfKDE = 0.12
            pdfStart   = min(self.dataRepresentation[:,eigVecInd]) - 3*sigmaOfKDE
            pdfStop    = max(self.dataRepresentation[:,eigVecInd]) + 3*sigmaOfKDE
            xAxis = np.linspace(pdfStart,pdfStop,200)
            PDF_Model = KernelDensity(kernel='gaussian', bandwidth=sigmaOfKDE).fit(self.dataRepresentation[:,eigVecInd].reshape(-1,1))
            logPDF = PDF_Model.score_samples(xAxis.reshape(-1,1))
            percentileValuesToShow = [representationPercentiles[x][eigVecInd] for x in range(len(representationPercentiles))]
            percentilesToShowLogPDF = PDF_Model.score_samples(np.array(percentileValuesToShow).reshape(-1,1))

            # show distribution of current component and red dots at the list of precentiles to show 
            plt.subplot(gs[0,:])
            plt.fill(xAxis, np.exp(logPDF), fc='b', alpha=0.9);
            plt.scatter(percentileValuesToShow, np.exp(percentilesToShowLogPDF), c='r',s=300);
            plt.title('%.3f%s explained' %(100*self.PCAModel.explained_variance_ratio_[eigVecInd], '%'))
            
            for plotCol, currPrecentile in enumerate(percentilesToShow):                
                currPrecentileRepVec             = medianRepVec.copy()
                currPrecentileRepVec[eigVecInd]  = representationPercentiles[plotCol][eigVecInd]
                
                currPrecentileImage = np.zeros(np.shape(self.objectPixels))
                currPrecentileImage[self.objectPixels]  = self.ReconstructUsingModel(currPrecentileRepVec).ravel()
                
                # show the median image with current precentile as activation of the curr image
                plt.subplot(gs[1,plotCol]);
                if showAsTraces:
                    plt.plot(currPrecentileImage); plt.title('precentile: ' + str(percentilesToShow[plotCol]) + '%')
                elif np.shape(self.objectPixels)[2] == 3:
                    currPrecentileImage[currPrecentileImage>1] = 1.0
                    currPrecentileImage[currPrecentileImage<0] = 0.0
                    plt.imshow(currPrecentileImage); plt.title('precentile: ' + str(percentilesToShow[plotCol]) + '%'); plt.axis('off')
                else:
                    plt.imshow(currPrecentileImage, cmap='gray'); plt.title('precentile: ' + str(percentilesToShow[plotCol]) + '%'); plt.axis('off')

                # find the most suitible candidates in X for current precentile
                distFromPercentile = abs(X_rep[:,eigVecInd] - representationPercentiles[plotCol][eigVecInd])
                X_inds = np.argpartition(distFromPercentile, numReadDataSamplePerPercentile)[:numReadDataSamplePerPercentile]
                for k, X_ind in enumerate(X_inds):
                    currNearestPrecentileImage = np.zeros(np.shape(self.objectPixels))
                    currNearestPrecentileImage[self.objectPixels]  = X[X_ind,:].ravel()
                    
                    plt.subplot(gs[2+k,plotCol]);
                    if showAsTraces:
                        plt.plot(currNearestPrecentileImage); plt.title('Close Neighbor');
                    else:
                        plt.imshow(currNearestPrecentileImage, cmap='gray'); plt.title('Close Neighbor'); plt.axis('off')
            plt.tight_layout()

            
    # show the scatter plot of the first 2 PC components and and approximation of all PC compents scatter using t-SNE 
    def ShowDataScatterPlotsWithTSNE(self, X=None, y=None, colorMap='Paired', pointSize=120, pointAlpha=0.9):
        
        if X is None:
            X_rep = self.dataRepresentation
        else:
            X_rep = self.RepresentUsingModel(X)
            
        if y is None:
            y = np.ones(X_rep.shape[0])
            
        tSNE_PCAModel = TSNE(n_components=2, random_state=0)
        X_rep_tSNE = tSNE_PCAModel.fit_transform(X_rep) 
        
        plt.figure()
        plt.subplot(1,2,1); plt.scatter(X_rep[:,0],X_rep[:,1],c=y,cmap=colorMap,s=pointSize,alpha=pointAlpha)
        plt.title('PCA representation'); plt.xlabel('PC1 coeff'); plt.ylabel('PC2 coeff')
        plt.subplot(1,2,2); plt.scatter(X_rep_tSNE[:,0],X_rep_tSNE[:,1],c=y,cmap=colorMap,s=pointSize,alpha=pointAlpha)
        plt.title('t-SNE representation'); plt.xlabel('t-SNE axis1'); plt.ylabel('t-SNE axis2')
        
        self.tSNE_embedding = tSNE_PCAModel.embedding_

Output Text:
------------


------------------------------------------------------------
Cell index: 13
Input Cell Type: markdown
Input Text:
-----------
## Build The Gaussian Model

a Multivariate Gaussian model essentially models the mean data sample and the co-variance matrix of the data distribution. This can be reformulated as several variance "directions" along which the mean data sample can vary in an independent fashion.

Output Text:
------------


------------------------------------------------------------
Cell index: 14
Input Cell Type: python
Input Text:
-----------
objectPixels = np.ones((np.shape(fullImageMatrix4D)[0],np.shape(fullImageMatrix4D)[1],np.shape(fullImageMatrix4D)[2]))
sampleDim = objectPixels.size
X = fullImageMatrix4D.reshape(sampleDim,-1).T

face_PCAModel = GaussianModel(X, numBasisFunctions=120, objectPixels=(objectPixels==1))

Output Text:
------------


------------------------------------------------------------
Cell index: 15
Input Cell Type: markdown
Input Text:
-----------
----------

***NOTE FOR POTENTIAL FORKERS***: if you change "numBasisFunctions=120" to be for example "numBasisFunctions=40" (or any other number), you will get different quality reconstructions. Generally, one should play around with this value and see what is happening. you can fork and play around.

----------

## Now lets look at the main variance "directions" of the Gaussian Model (PCA)

Each image is a different "direction" in the high dimensional image space, and we should think of it as adding or subtracting a little bit of that image to the mean face image

Output Text:
------------


------------------------------------------------------------
Cell index: 16
Input Cell Type: python
Input Text:
-----------
matplotlib.rcParams['font.size'] = 8
matplotlib.rcParams['figure.figsize'] = (12,12)

face_PCAModel.ShowVarianceDirections(numDirectionsToShow=16)

Output Text:
------------
<matplotlib.figure.Figure at 0x7f565aaddf28>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a grid of sixteen smaller images, arranged in four rows and four columns.  Each small image depicts a blurred or slightly out-of-focus face, varying in gender and ethnicity.  The faces are presented in a consistent style, with a neutral or slightly smiling expression.  Each small image is overlaid with text in the top left corner, providing a percentage value followed by the word "explained".  These percentages likely represent the variance explained by a particular component in a principal component analysis (PCA) or a similar dimensionality reduction technique commonly used in image processing and machine learning.

The overall structure suggests that the image presents the top 16 principal components from a dataset of facial images. Each image shows the reconstruction of a face using only the information from a single principal component. The percentages indicate the proportion of total variance captured by each component.  The components with higher percentage values (e.g., 17.465%) capture more significant variations in the data, while those with lower percentages (e.g., 0.881%) capture less prominent features.  The gradual decrease in the explained variance suggests a diminishing return with each added component.  This visualization is a common way to understand the contribution of each principal component in reconstructing the original images.


------------------------------------------------------------
Cell index: 17
Input Cell Type: markdown
Input Text:
-----------
----------

***NOTE FOR POTENTIAL FORKERS***: if you change "numDirectionsToShow=16" to be for example "numBasisFunctions=32" you will get more eigenvectors. you can fork and play around.

----------


## Lets see how well can a 120 component PCA model reconstruct the original images

Output Text:
------------


------------------------------------------------------------
Cell index: 18
Input Cell Type: python
Input Text:
-----------
matplotlib.rcParams['font.size'] = 8
matplotlib.rcParams['figure.figsize'] = (14,8)

face_PCAModel.ShowReconstructions(X, numReconstructions=12)

Output Text:
------------
<matplotlib.figure.Figure at 0x7f565d4b7e48>
<matplotlib.figure.Figure at 0x7f565c9fea90>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image displays a grid of 18 images, arranged in three rows of six columns. Each column represents a single person's face.  The top row shows the "original image" of each individual, presenting them as clear, high-resolution photographs.  The second row shows the "reconstructed image" of the same individuals. These images appear slightly blurred and less sharp than their originals, suggesting a process of image reconstruction or compression has been applied.

The bottom row shows the "abs difference image" for each person. These images are predominantly dark, with subtle color variations indicating the differences between the original and reconstructed images. The differences are primarily visible as slight color distortions and blurring, highlighting the discrepancies introduced by the reconstruction process.  The dark background suggests that the majority of pixel values are similar between the original and reconstructed images, with only minor differences being highlighted.  The overall effect is a visual representation of the fidelity of the image reconstruction technique.


Image 2 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image displays a grid of 18 images arranged in three rows and six columns. Each column represents a single individual's face. 


The top row shows the "original image" of each person. These are clear, high-quality photographs of six different men, likely athletes or celebrities, each with a slightly different hairstyle and expression. They are all presented against a plain, dark-blue background.


The middle row shows the "reconstructed image" of each person. These are also photographs of the same individuals, but the image quality is noticeably lower.  The images appear slightly blurry and less detailed, suggesting a reconstruction process, perhaps from a compressed or encoded version of the original.


The bottom row shows the "abs difference image" for each individual. These images are predominantly dark, with subtle, multicolored highlights and shading revealing the differences between the original and reconstructed images.  These difference images highlight the areas where the reconstruction process lost detail or accuracy.  The colors indicate the magnitude and direction of the differences in pixel values between the original and reconstructed images.


------------------------------------------------------------
Cell index: 19
Input Cell Type: markdown
Input Text:
-----------
we can see that by trying to reconstruct original image using a linear combination of only 120 images we are able to get a fairly good reconstruction of players. Usually, the better reconstruction one can get with a fewer number of components implies that there are a lot of dependencies in the data (i.e. a lot of correlated pixels)

----------

## Let's look at how the the face images vary around the mean face image:

Output Text:
------------


------------------------------------------------------------
Cell index: 20
Input Cell Type: python
Input Text:
-----------
matplotlib.rcParams['font.size'] = 7
matplotlib.rcParams['figure.figsize'] = (12,9)

face_PCAModel.ShowModelVariations(numVariations=6)

Output Text:
------------
<matplotlib.figure.Figure at 0x7f565dcdccc0>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image presents a comprehensive visualization of Principal Component Analysis (PCA) applied to a dataset of facial images.  It's structured in a grid format, with six columns, each representing a principal component (eigenvector) derived from the PCA.  At the top of each column, a histogram displays the distribution of the corresponding eigenvector's values across the dataset.  Above each histogram, the percentage of variance explained by that eigenvector is shown; this indicates how much information about the faces is captured by each component.  The eigenvectors with the highest percentage of variance explained (e.g., eigenvector 0) capture the most significant variations in facial features across the dataset.


Below the histograms, each column displays a series of reconstructed facial images corresponding to different percentiles (2nd, median, and 98th) of the eigenvector's values.  This allows for a visual understanding of how variations in each eigenvector affect the appearance of the faces.  For example, the 2nd percentile image shows the face with the lowest value for that eigenvector, the median shows the average face, and the 98th percentile shows the face with the highest value. The images are somewhat blurred, which is a common result of representing faces with a limited number of principal components.  The overall blurriness decreases as you move towards the higher-variance eigenvectors.  This indicates that higher-variance eigenvectors capture more detailed facial features.


------------------------------------------------------------
Cell index: 21
Input Cell Type: markdown
Input Text:
-----------
**For those of us unfamiliar with this kind of a plot and since this is quite a busy plot, let me explain what we see:**
------------------------------------------------------------------------
  


----------


 - The upper most row contains the data distributions of each eigenvector (i.e. the histogram along that "direction")
 - The second row contains what we already saw in a previous plot, what we called the variance directions.
 - The forth row contains the median image of leafs. notice that this row is identical for all eigenvectors
 - The third row holds the 2nd percentile images of each eigenvector. it's easier to think of this as the median image minus the eigenvector image multiplied by some constant. i.e the image we see is the forth row image, minus the second row image, when the second row image is multiplied by a constant. The constant is chosen to show the varying degree of influence of this specific eigenvector on the "average" image, so we can visualize what type of variation this particular eigenvector tends to capture. 2nd percentile will subtract a relatively large absolute value from the median image, showing us what images look like when this coefficient is highly negative. 98th percentile would be just the opposite, showing us what images look like when this coefficient is at the upper end of the range. 50th percentile would give us a "middle of the road" effect of this coefficient.


----------


This plot helps us visualize what a direction in this high dimensional image space means. For example:

 - **The first eigenvector** (leftmost column), we can see
   that it **controls the difference between dark skinned players and light skinned players**. i.e we can say that some of the variance along the change of skin color is explained by this component.

 - **The second eigenvector** (second from the left), we can see
   that it **controls the difference between male and female players**. i.e we can say that some of the variance along the change of player sex is explained by this component.

----------

We can now deep deeper into some interesting looking eigenvectors

Output Text:
------------


------------------------------------------------------------
Cell index: 22
Input Cell Type: markdown
Input Text:
-----------
**Eigenvector 1: dark skinned or light skinned?**
-------------------------------------------------

Output Text:
------------


------------------------------------------------------------
Cell index: 23
Input Cell Type: python
Input Text:
-----------
matplotlib.rcParams['font.size'] = 7
matplotlib.rcParams['figure.figsize'] = (12,10)

face_PCAModel.ShowSingleComponentVariation(X, listOfComponents=[0])

Output Text:
------------
<matplotlib.figure.Figure at 0x7f565dad4b00>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image presents a visualization of a dimensionality reduction technique, likely t-SNE or UMAP, applied to a dataset of facial images.  The top portion displays a density plot showing the distribution of data points across different percentile ranges. Red circles highlight points at the 10th, 30th, 70th, and 90th percentiles, indicating clusters or regions of high density within the data. The title indicates that 17.465% of the variance is explained by this visualization.

The bulk of the image consists of a grid of 4 x 6 = 24 small images. Each small image depicts a face, labeled "Close Neighbor". These images are arranged to correspond with the percentile ranges shown in the density plot.  The faces in each column represent the nearest neighbors to the data point at the percentile indicated in the top section.  This demonstrates how the algorithm groups similar faces together based on their features. The blurred images at the top seem to represent an average or centroid of faces within each percentile range.

In summary, the image effectively communicates the results of a dimensionality reduction algorithm applied to facial images, highlighting the clustering of similar faces and how the algorithm captures the data's variance. The combination of the density plot and the grid of faces provides a clear and insightful visualization of the data's structure.


------------------------------------------------------------
Cell index: 24
Input Cell Type: markdown
Input Text:
-----------
**Let me explain this plot:**

 - at the top row we can see the distribution of the coefficient along
   this particular variance direction.
 - at the second row we can see percentile images that correspond to
   different red dots in the distribution.
 - the four bottom rows contain nearest neighbor images from the data
   itself that are closest to the percentile images above (4 nearest
   neighbors for each percentile image)

----------

**Eigenvector 2: male or female?**
-------------------------------------------

Output Text:
------------


------------------------------------------------------------
Cell index: 25
Input Cell Type: python
Input Text:
-----------
matplotlib.rcParams['font.size'] = 7
matplotlib.rcParams['figure.figsize'] = (12,10)

face_PCAModel.ShowSingleComponentVariation(X, listOfComponents=[1])

Output Text:
------------
<matplotlib.figure.Figure at 0x7f5650127e10>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image displays a visualization of a dimensionality reduction technique, likely Principal Component Analysis (PCA) or a similar method, applied to a dataset of facial images.  The top section shows a blue curve representing the explained variance, peaking around the -1 mark on the x-axis. This curve indicates how much of the total variance in the data is explained by each principal component (or similar latent variable).  Red circles highlight specific percentiles (1%, 10%, 30%, 70%, 90%, 99%) along this curve, signifying points of interest in the variance distribution.  The title "10.360% explained" suggests that the selected components account for approximately 10.36% of the total variance in the dataset.

Below the curve, the image is organized into a grid of image thumbnails, arranged in six columns corresponding to the highlighted percentiles. Each column shows four rows of facial images labeled "Close Neighbor". These images likely represent the nearest neighbors or most similar faces within the dataset based on the principal components at the corresponding percentile. The blurring in the top row of images suggests these are average faces or reconstructions based on the principal components. The subsequent rows show actual faces from the dataset that are close in feature space to the average face at each percentile.  The overall structure and contents aim to illustrate how the principal components capture variations in facial features and how these variations are distributed across the dataset.


------------------------------------------------------------
Cell index: 26
Input Cell Type: markdown
Input Text:
-----------
**Eigenvector 11: dim or bright lighting? (perhaps indoor vs outdoor)**
------------------------------------------

Output Text:
------------


------------------------------------------------------------
Cell index: 27
Input Cell Type: python
Input Text:
-----------
matplotlib.rcParams['font.size'] = 7
matplotlib.rcParams['figure.figsize'] = (12,10)

face_PCAModel.ShowSingleComponentVariation(X, listOfComponents=[10])

Output Text:
------------
<matplotlib.figure.Figure at 0x7f565dbcac88>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
This image presents a visualization of a machine learning model's feature importance, specifically focusing on the explained variance of features related to facial images. The top section shows a density plot, a blue curve indicating the distribution of feature importance scores.  Red dots highlight specific percentiles (1%, 10%, 30%, 70%, 90%, and 99%) of the distribution, indicating the range of influence these features have on the model's predictions.  The title "1.329% explained" suggests that the features shown account for a relatively small proportion of the total variance.

The majority of the image consists of a grid of 30 face images, arranged in six columns and five rows. Each image is labelled "Close Neighbor," suggesting these faces are the nearest neighbors in the feature space to a reference image (not shown). The images are likely used to illustrate the type of facial features captured by the model and their relative importance based on their percentile ranking in the density plot above.

The combination of the density plot and the image grid provides a visual explanation of how the model interprets facial features. The plot shows the distribution of feature importance, while the grid displays examples of faces representing different levels of influence on the model. This visualization aids in understanding which facial characteristics are most significant for the model's predictions and helps diagnose potential biases or limitations.


------------------------------------------------------------
Cell index: 28
Input Cell Type: markdown
Input Text:
-----------
----------

***NOTE FOR POTENTIAL FORKERS***: if you change "listOfComponents=[10]" to be for example "listOfComponents=[10,11,12]" you will get to see these plots for eigenvectors 11,12 and 13. You can fork and play around if you find any interesting components.

----------

Output Text:
------------


------------------------------------------------------------
Cell index: 29
Input Cell Type: markdown
Input Text:
-----------
Now lets collect some attributes from 'FullData.csv' for the players we have also face images for

Output Text:
------------


------------------------------------------------------------
Cell index: 30
Input Cell Type: python
Input Text:
-----------
#%% for every player that we have a picture for, get his data from "FullData.csv" (if present)
playerData = pd.read_csv('../input/FullData.csv').drop_duplicates(subset='Name', keep='last').reset_index(drop=True)

listOfAllPlayerNames = playerData['Name'].tolist()
recordRowInds = [listOfAllPlayerNames.index(x) if x in listOfAllPlayerNames else 'False' for x in listOfPlayerNames]

desiredColumns = ['Name', 'Nationality', 'Sex', 'Age [years]', 'Height [cm]', 'Weight [kg]']
playerWithPicsData = pd.DataFrame(index=range(len(listOfPlayerNames)),columns=desiredColumns)
playerWithPicsData.loc[:,'Name'] = [x for x in listOfPlayerNames]
playerWithPicsData.loc[:,'Sex']  = ['Female' if x else 'Male' for x in isFemale]

for k, currRowInd in enumerate(recordRowInds):
    if currRowInd != 'False':
        playerWithPicsData.loc[k,'Age [years]'] = float(playerData.loc[currRowInd, 'Age'])
        playerWithPicsData.loc[k,'Height [cm]'] = float(playerData.loc[currRowInd, 'Height'].split(' ')[0])
        playerWithPicsData.loc[k,'Weight [kg]'] = float(playerData.loc[currRowInd, 'Weight'].split(' ')[0])
        playerWithPicsData.loc[k,'Nationality'] = playerData.loc[currRowInd, 'Nationality']
    
# extract face features
faceFeatures = face_PCAModel.RepresentUsingModel(X)

playerWithPicsData.head(10)

Output Text:
------------
                 Name Nationality   Sex Age [years] Height [cm] Weight [kg]
0        Aaron Ramsey       Wales  Male          26         183          76
1        Adam Lallana     England  Male          28         172          73
2           Adil Rami      France  Male          31         190          88
3                 Adn         NaN  Male         NaN         NaN         NaN
4       Adrien Rabiot      France  Male          21         188          71
5        Adrien Silva    Portugal  Male          28         175          69
6              Aduriz       Spain  Male          36         182          78
7        Alan Dzagoev      Russia  Male          26         179          75
8      Alejandro Gmez         NaN  Male         NaN         NaN         NaN
9  Aleksandar Kolarov      Serbia  Male          31         187          83


------------------------------------------------------------
Cell index: 31
Input Cell Type: python
Input Text:
-----------
playerWithPicsData.tail()

Output Text:
------------
                 Name Nationality     Sex Age [years] Height [cm] Weight [kg]
567  Vivianne Miedema         NaN  Female         NaN         NaN         NaN
568     Wendie Renard         NaN  Female         NaN         NaN         NaN
569     Whitney Engen         NaN  Female         NaN         NaN         NaN
570     Yamile Franco         NaN  Female         NaN         NaN         NaN
571           Yang Li         NaN  Female         NaN         NaN         NaN


------------------------------------------------------------
Cell index: 32
Input Cell Type: markdown
Input Text:
-----------
## Plot the correlation between different attributes and the different PCA components

Output Text:
------------


------------------------------------------------------------
Cell index: 33
Input Cell Type: python
Input Text:
-----------
# find the PCA component that most correlates with 'Sex'
releventCols = np.array(playerWithPicsData.loc[:,'Sex'].notnull())
targetFeature = np.array(playerWithPicsData.loc[releventCols,'Sex'] == 'Male')[:,np.newaxis]
corrWithSexVec = np.corrcoef(np.hstack((targetFeature,faceFeatures[releventCols,:])).T)[0,1:]
mostCorrelatedWithSex = np.argmax(abs(corrWithSexVec))

# find the PCA component that most correlates with 'Age'
releventCols = np.array(playerWithPicsData.loc[:,'Age [years]'].notnull())
targetFeature = np.array(playerWithPicsData.loc[releventCols,'Age [years]'].tolist())[:,np.newaxis]
corrWithAgeVec = np.corrcoef(np.hstack((targetFeature,faceFeatures[releventCols,:])).T)[0,1:]
mostCorrelatedWithAge = np.argmax(abs(corrWithAgeVec))

# find the PCA component that most correlates with 'Height'
releventCols = np.array(playerWithPicsData.loc[:,'Height [cm]'].notnull())
targetFeature = np.array(playerWithPicsData.loc[releventCols,'Height [cm]'].tolist())[:,np.newaxis]
corrWithHeightVec = np.corrcoef(np.hstack((targetFeature,faceFeatures[releventCols,:])).T)[0,1:]
mostCorrelatedWithHeight = np.argmax(abs(corrWithHeightVec))

# find the PCA component that most correlates with 'Weight'
releventCols = np.array(playerWithPicsData.loc[:,'Weight [kg]'].notnull())
targetFeature = np.array(playerWithPicsData.loc[releventCols,'Weight [kg]'].tolist())[:,np.newaxis]
corrWithWeightVec = np.corrcoef(np.hstack((targetFeature,faceFeatures[releventCols,:])).T)[0,1:]
mostCorrelatedWithWeight = np.argmax(abs(corrWithWeightVec))

matplotlib.rcParams['font.size'] = 8
matplotlib.rcParams['figure.figsize'] = (12,12)

plt.figure()
plt.subplot(4,1,1); plt.plot(np.abs(corrWithSexVec)); 
plt.ylim(-0.02,1); plt.ylabel('with Sex')
plt.subplot(4,1,2); plt.plot(np.abs(corrWithAgeVec)); 
plt.ylim(-0.02,1); plt.ylabel('with Age')
plt.subplot(4,1,3); plt.plot(np.abs(corrWithHeightVec)); 
plt.ylim(-0.02,1); plt.ylabel('with Height')
plt.subplot(4,1,4); plt.plot(np.abs(corrWithWeightVec)); 
plt.ylim(-0.02,1); plt.ylabel('with Weight')
plt.xlabel('PCA coefficient index')
plt.suptitle('How different PCA Components correlate with real life attributes?')

Output Text:
------------
<matplotlib.text.Text at 0x7f565dca5c18>
<matplotlib.figure.Figure at 0x7f565dc1eac8>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a graph showing the correlation between different Principal Component Analysis (PCA) components and four real-life attributes: Sex, Age, Height, and Weight.  The graph is structured as four separate line plots stacked vertically, one for each attribute.  Each plot displays the PCA coefficient index on the x-axis and the correlation strength (likely a coefficient value between 0 and 1) on the y-axis.

The title "How different PCA Components correlate with real life attributes?" clearly explains the purpose of the visualization.  Each individual plot has a y-axis label indicating which attribute it represents ("with Sex", "with Age", etc.). The x-axis is consistently labeled "PCA coefficient index," signifying that the x-values represent the index number of a particular principal component.  The line in each plot shows how the correlation strength between that specific PCA component and the corresponding attribute varies across the different components.  The fluctuations in the lines suggest that some components have a stronger correlation with certain attributes than others, and the correlation strength isn't consistently high or low across all components.


------------------------------------------------------------
Cell index: 34
Input Cell Type: markdown
Input Text:
-----------
We can see that there are **no clear high correlations** except for those that we already previously saw between male and females. **too bad**. 
I suspect we will see some correlations with the Nationality field if we further divide it into continents, but I leave it to you to investigate this if you wish.

----------


To conclude, I hope that the fact that we can use **almost exactly the same code and analysis tools** as in [Visualizing PCA][1] script and get quick a feel for how players look like as well as how leaves look like demonstrates **the incredible power of basic visualization techniques** such as we presented here. I hope everyone reading this will use some of these techniques in the future :-)


  [1]: https://www.kaggle.com/selfishgene/leaf-classification/visualizing-pca-with-leaf-dataset

Output Text:
------------


------------------------------------------------------------

================================================================================
================================================================================
yellow-cabs-tell-the-story-of-new-york-city.ipynb:
==================================================
------------------------------------------------------------
Cell index: 1
Input Cell Type: markdown
Input Text:
-----------
Yellow Cabs tell The Story of New York City
-----------------------------------------

In this script we will explore the spatial and temporal behavior of the people of New York as can be inferred by examining their cab usage.  

The main fields of this dataset are taxi pickup time and location, as well as dropoff location and trip duration.  
There is a total of around 1.4 Million trips in the dataset that took place during the first half of 2016.  

We will see how the patterns of cab usage change throughout the year, throughout the week and throughout the day, and we will focus on difference between weekdays and weekends.

![](http://blog.christinaczybik.com/wp-content/uploads/2015/08/New-York-Street-Mix016.jpg)

Output Text:
------------


------------------------------------------------------------
Cell index: 2
Input Cell Type: python
Input Text:
-----------
import numpy as np
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
from sklearn import decomposition
from scipy import stats
from sklearn import cluster

matplotlib.style.use('fivethirtyeight')
matplotlib.rcParams['font.size'] = 12
matplotlib.rcParams['figure.figsize'] = (10,10)

Output Text:
------------


------------------------------------------------------------
Cell index: 3
Input Cell Type: markdown
Input Text:
-----------
Load data, remove obvious outliers and convert everything to sensible units
--------------------------------------------------------------------



Output Text:
------------


------------------------------------------------------------
Cell index: 4
Input Cell Type: python
Input Text:
-----------
dataDir = '../input/'
taxiDB = pd.read_csv(dataDir + 'train.csv')

# remove obvious outliers
allLat  = np.array(list(taxiDB['pickup_latitude'])  + list(taxiDB['dropoff_latitude']))
allLong = np.array(list(taxiDB['pickup_longitude']) + list(taxiDB['dropoff_longitude']))

longLimits = [np.percentile(allLong, 0.3), np.percentile(allLong, 99.7)]
latLimits  = [np.percentile(allLat , 0.3), np.percentile(allLat , 99.7)]
durLimits  = [np.percentile(taxiDB['trip_duration'], 0.4), np.percentile(taxiDB['trip_duration'], 99.7)]

taxiDB = taxiDB[(taxiDB['pickup_latitude']   >= latLimits[0] ) & (taxiDB['pickup_latitude']   <= latLimits[1]) ]
taxiDB = taxiDB[(taxiDB['dropoff_latitude']  >= latLimits[0] ) & (taxiDB['dropoff_latitude']  <= latLimits[1]) ]
taxiDB = taxiDB[(taxiDB['pickup_longitude']  >= longLimits[0]) & (taxiDB['pickup_longitude']  <= longLimits[1])]
taxiDB = taxiDB[(taxiDB['dropoff_longitude'] >= longLimits[0]) & (taxiDB['dropoff_longitude'] <= longLimits[1])]
taxiDB = taxiDB[(taxiDB['trip_duration']     >= durLimits[0] ) & (taxiDB['trip_duration']     <= durLimits[1]) ]
taxiDB = taxiDB.reset_index(drop=True)

allLat  = np.array(list(taxiDB['pickup_latitude'])  + list(taxiDB['dropoff_latitude']))
allLong = np.array(list(taxiDB['pickup_longitude']) + list(taxiDB['dropoff_longitude']))

# convert fields to sensible units
medianLat  = np.percentile(allLat,50)
medianLong = np.percentile(allLong,50)

latMultiplier  = 111.32
longMultiplier = np.cos(medianLat*(np.pi/180.0)) * 111.32

taxiDB['duration [min]'] = taxiDB['trip_duration']/60.0
taxiDB['src lat [km]']   = latMultiplier  * (taxiDB['pickup_latitude']   - medianLat)
taxiDB['src long [km]']  = longMultiplier * (taxiDB['pickup_longitude']  - medianLong)
taxiDB['dst lat [km]']   = latMultiplier  * (taxiDB['dropoff_latitude']  - medianLat)
taxiDB['dst long [km]']  = longMultiplier * (taxiDB['dropoff_longitude'] - medianLong)

allLat  = np.array(list(taxiDB['src lat [km]'])  + list(taxiDB['dst lat [km]']))
allLong = np.array(list(taxiDB['src long [km]']) + list(taxiDB['dst long [km]']))

Output Text:
------------


------------------------------------------------------------
Cell index: 5
Input Cell Type: markdown
Input Text:
-----------
Plot the resulting histograms of trip duration, latitude and longitude 
-----------------------------------------------------------



Output Text:
------------


------------------------------------------------------------
Cell index: 6
Input Cell Type: python
Input Text:
-----------
# make sure the ranges we chose are sensible
fig, axArray = plt.subplots(nrows=1,ncols=3,figsize=(13,4))
axArray[0].hist(taxiDB['duration [min]'],80); 
axArray[0].set_xlabel('trip duration [min]'); axArray[0].set_ylabel('counts')
axArray[1].hist(allLat ,80); axArray[1].set_xlabel('latitude [km]')
axArray[2].hist(allLong,80); axArray[2].set_xlabel('longitude [km]')

Output Text:
------------
<matplotlib.text.Text at 0x7f2ea7337e48>
<matplotlib.figure.Figure at 0x7f2ea7e8b9b0>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image contains three histograms displayed side-by-side, each visualizing the distribution of a different variable.  The histograms are presented with a light gray grid background, enhancing readability. The x-axis of each histogram represents a different variable, while the y-axis consistently represents the "counts" or frequency of observations within each bin.


The leftmost histogram shows the distribution of "trip duration" measured in minutes.  It's a right-skewed distribution, indicating a majority of trips are relatively short, with a long tail extending towards longer trip durations. The central peak suggests a common trip duration around 10 minutes.


The middle histogram displays the distribution of "latitude" in kilometers. This histogram is roughly bell-shaped or unimodal, implying a concentration of data points around 0 km latitude, with symmetrical tails extending towards positive and negative values. This suggests a relatively even distribution of trips across the latitude range.


The rightmost histogram shows the distribution of "longitude" also in kilometers. This histogram is similar to the latitude histogram in that it is largely unimodal, with a central peak near 0 km longitude. However, there are a few smaller peaks suggesting potential clusters at other longitudes.  Like the latitude distribution, this shows a concentration of trips around the 0 km mark, but with some secondary clusters possibly indicating specific areas of higher trip activity.


------------------------------------------------------------
Cell index: 7
Input Cell Type: markdown
Input Text:
-----------
Plot the trip Duration vs. the Aerial Distance between pickup and dropoff
-----------------------------------------------------------


Output Text:
------------


------------------------------------------------------------
Cell index: 8
Input Cell Type: python
Input Text:
-----------
#%% plot scatter of trip duration vs. aerial distance between pickup and dropoff
taxiDB['log duration']       = np.log1p(taxiDB['duration [min]'])
taxiDB['euclidian distance'] = np.sqrt((taxiDB['src lat [km]']  - taxiDB['dst lat [km]'] )**2 + 
                                       (taxiDB['src long [km]'] - taxiDB['dst long [km]'])**2)

fig, axArray = plt.subplots(nrows=1,ncols=2,figsize=(13,6))
axArray[0].scatter(taxiDB['euclidian distance'], taxiDB['duration [min]'],c='r',s=5,alpha=0.01); 
axArray[0].set_xlabel('Aerial Euclidian Distance [km]'); axArray[0].set_ylabel('Duration [min]')
axArray[0].set_xlim(taxiDB['euclidian distance'].min(),taxiDB['euclidian distance'].max())
axArray[0].set_ylim(taxiDB['duration [min]'].min(),taxiDB['duration [min]'].max())
axArray[0].set_title('trip Duration vs Aerial trip Distance')

axArray[1].scatter(taxiDB['euclidian distance'], taxiDB['log duration'],c='r',s=5,alpha=0.01); 
axArray[1].set_xlabel('Aerial Euclidian Distance [km]'); axArray[1].set_ylabel('log(1+Duration) [log(min)]')
axArray[1].set_xlim(taxiDB['euclidian distance'].min(),taxiDB['euclidian distance'].max())
axArray[1].set_ylim(taxiDB['log duration'].min(),taxiDB['log duration'].max())
axArray[1].set_title('log of trip Duration vs Aerial trip Distance')

Output Text:
------------
<matplotlib.text.Text at 0x7f2ea5584278>
<matplotlib.figure.Figure at 0x7f2ea71f15f8>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a comparative analysis of trip duration versus aerial trip distance, displayed through two scatter plots.  The left plot shows a direct correlation between trip duration (in minutes) and aerial Euclidean distance (in kilometers).  The data points are densely clustered, revealing a positive relationship; longer trips generally cover greater distances. The distribution shows some clustering and potential outliers, suggesting additional factors might influence trip duration beyond just distance.


The right plot offers a transformed perspective. It displays the logarithm of (1 + trip duration) against the aerial Euclidean distance. This logarithmic transformation compresses the y-axis, making the relationship between the variables appear more linear than in the original plot.  This suggests a potential power-law relationship between trip duration and distance, where the duration increases at a diminishing rate as distance increases. Both plots use the same x-axis, allowing for direct comparison of the effect of the log transformation.  The color scheme is consistent across both graphs, using shades of red to represent the density of data points.


------------------------------------------------------------
Cell index: 9
Input Cell Type: markdown
Input Text:
-----------
We can see that the trip distance defines the lower bound on trip duration, as one would expect

Output Text:
------------


------------------------------------------------------------
Cell index: 10
Input Cell Type: markdown
Input Text:
-----------
Plot spatial density plot of the pickup and dropoff locations
-----------------------------------------------------



Output Text:
------------


------------------------------------------------------------
Cell index: 11
Input Cell Type: python
Input Text:
-----------
# show the log density of pickup and dropoff locations
imageSize = (700,700)
longRange = [-5,19]
latRange = [-13,11]

allLatInds  = imageSize[0] - (imageSize[0] * (allLat  - latRange[0])  / (latRange[1]  - latRange[0]) ).astype(int)
allLongInds =                (imageSize[1] * (allLong - longRange[0]) / (longRange[1] - longRange[0])).astype(int)

locationDensityImage = np.zeros(imageSize)
for latInd, longInd in zip(allLatInds,allLongInds):
    locationDensityImage[latInd,longInd] += 1

fig, ax = plt.subplots(nrows=1,ncols=1,figsize=(12,12))
ax.imshow(np.log(locationDensityImage+1),cmap='hot')
ax.set_axis_off()

Output Text:
------------
<matplotlib.figure.Figure at 0x7f2ea7383e10>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
That's a heatmap visualization of the road network in and around New York City. 


The image uses a color scheme where brighter, more intense orange and red colors represent areas with a higher density of roads or possibly higher traffic volume. The darker areas indicate fewer roads or less traffic. The overall shape of Manhattan Island and the boroughs of Brooklyn, Queens, and parts of Staten Island are clearly visible due to the concentration of roads.  The intensity of the color varies across the image, with Manhattan showing the highest concentration of roads and therefore the brightest coloration.  The less dense road networks in the outer boroughs are represented by fainter, less intense colors.  The image gives a strong impression of the city's infrastructure and its sprawling nature.


------------------------------------------------------------
Cell index: 12
Input Cell Type: markdown
Input Text:
-----------
Oh, nice!  
It looks a little bit of what you might expect to see from space, with Manhattan and the two airports "lighting up the sky"

Output Text:
------------


------------------------------------------------------------
Cell index: 13
Input Cell Type: markdown
Input Text:
-----------
Zoom in on Manhattan
--------------------

Output Text:
------------


------------------------------------------------------------
Cell index: 14
Input Cell Type: python
Input Text:
-----------
# zoom in on Manhattan 
imageSizeMan = (720,480)
latRangeMan = [-8,10]
longRangeMan = [-5,7]

indToKeep  = np.logical_and(allLat > latRangeMan[0], allLat < latRangeMan[1])
indToKeep  = np.logical_and(indToKeep, np.logical_and(allLong > longRangeMan[0], allLong < longRangeMan[1]))
allLatMan  = allLat[indToKeep]
allLongMan = allLong[indToKeep]

allLatIndsMan  = (imageSizeMan[0]-1) - (imageSizeMan[0] * (allLatMan  - latRangeMan[0])
                                                        / (latRangeMan[1] - latRangeMan[0])).astype(int)
allLongIndsMan =                       (imageSizeMan[1] * (allLongMan - longRangeMan[0])
                                                        / (longRangeMan[1] - longRangeMan[0])).astype(int)

locationDensityImageMan = np.zeros(imageSizeMan)
for latInd, longInd in zip(allLatIndsMan,allLongIndsMan):
    locationDensityImageMan[latInd,longInd] += 1

fig, ax = plt.subplots(nrows=1,ncols=1,figsize=(12,18))
ax.imshow(np.log(locationDensityImageMan+1),cmap='hot')
ax.set_axis_off()

Output Text:
------------
<matplotlib.figure.Figure at 0x7f2ea55b9550>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a heatmap of Manhattan, New York City, overlaid on a dark background that likely represents a nighttime satellite or aerial view. 


The heatmap uses a color gradient ranging from dark red/orange to bright orange, with the brightest areas indicating the highest concentration of activity or data points. This activity is likely related to human movement, traffic, or other similar data. The intense orange lines trace the major streets and avenues of Manhattan, particularly highlighting the grid-like street pattern of the island's central and lower regions.  Central Park is visible as a dark rectangle in the middle of the brighter grid. 


The intensity of the heatmap diminishes as it moves away from the densely populated center of Manhattan, reflecting the lower population density and activity in the surrounding areas. The outer boroughs are faintly visible with much less intensity in the heatmap, showing a clear contrast with the core of Manhattan. The overall effect is a striking visualization of the city's spatial distribution of activity, emphasizing the concentration of movement and population in Manhattan's central areas.


------------------------------------------------------------
Cell index: 15
Input Cell Type: markdown
Input Text:
-----------
Create some useful fields for later
-------------------------



Output Text:
------------


------------------------------------------------------------
Cell index: 16
Input Cell Type: python
Input Text:
-----------
#%% create some new usefull fields
pickupTime = pd.to_datetime(taxiDB['pickup_datetime'])

taxiDB['src hourOfDay'] = (pickupTime.dt.hour*60.0 + pickupTime.dt.minute)   / 60.0
taxiDB['dst hourOfDay'] = taxiDB['src hourOfDay'] + taxiDB['duration [min]'] / 60.0

taxiDB['dayOfWeek']     = pickupTime.dt.weekday
taxiDB['hourOfWeek']    = taxiDB['dayOfWeek']*24.0 + taxiDB['src hourOfDay']

taxiDB['monthOfYear']   = pickupTime.dt.month
taxiDB['dayOfYear']     = pickupTime.dt.dayofyear
taxiDB['weekOfYear']    = pickupTime.dt.weekofyear
taxiDB['hourOfYear']    = taxiDB['dayOfYear']*24.0 + taxiDB['src hourOfDay']

Output Text:
------------


------------------------------------------------------------
Cell index: 17
Input Cell Type: markdown
Input Text:
-----------
Cluster The Trips and Look at their distribution
------------------------------------------------

every trip is essentially made up of 5 major attributes: pickup and dropoff locations and the trip duration.
let's cluster all 1.4 million trips to 80 stereotypical template trips and then look at the distribution of this "bag of trips" and how it changes over time

Output Text:
------------


------------------------------------------------------------
Cell index: 18
Input Cell Type: python
Input Text:
-----------
tripAttributes = np.array(taxiDB.loc[:,['src lat [km]','src long [km]','dst lat [km]','dst long [km]','duration [min]']])
meanTripAttr = tripAttributes.mean(axis=0)
stdTripAttr  = tripAttributes.std(axis=0)
tripAttributes = stats.zscore(tripAttributes, axis=0)

numClusters = 80
TripKmeansModel = cluster.MiniBatchKMeans(n_clusters=numClusters, batch_size=120000, n_init=100, random_state=1)
clusterInds = TripKmeansModel.fit_predict(tripAttributes)

clusterTotalCounts, _ = np.histogram(clusterInds, bins=numClusters)
sortedClusterInds = np.flipud(np.argsort(clusterTotalCounts))

plt.figure(figsize=(12,4)); plt.title('Cluster Histogram of all trip')
plt.bar(range(1,numClusters+1),clusterTotalCounts[sortedClusterInds])
plt.ylabel('Frequency [counts]'); plt.xlabel('Cluster index (sorted by cluster frequency)')
plt.xlim(0,numClusters+1)

Output Text:
------------
(0, 81)
<matplotlib.figure.Figure at 0x7f2ea4c75048>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image displays a histogram titled "Cluster Histogram of all trip."  The histogram is a bar chart showing the frequency distribution of clusters. The horizontal axis represents the cluster index, sorted in descending order based on the frequency of each cluster.  The vertical axis represents the frequency, or count, of each cluster, ranging from 0 to 60,000.

The histogram's data reveals a long tail distribution.  The first few clusters (on the left) have very high frequencies, considerably exceeding 50,000 in the most frequent cluster.  As the cluster index increases (moving rightward), the frequency of each cluster decreases steadily, indicating a significant drop in the number of data points belonging to less frequent clusters.  The distribution visually suggests that a small number of clusters encompass a large proportion of the data, while a large number of clusters contain only a small number of data points each.  The overall shape indicates a skewed distribution, far from a normal distribution.


------------------------------------------------------------
Cell index: 19
Input Cell Type: markdown
Input Text:
-----------
Show the typical Trips on the Map
---------------------------------

The magenta circles are sources, the green circles are destinations and the arrows between them are drawn in cyan

Output Text:
------------


------------------------------------------------------------
Cell index: 20
Input Cell Type: python
Input Text:
-----------
#%% show the templeate trips on the map
def ConvertToImageCoords(latCoord, longCoord, latRange, longRange, imageSize):
    latInds  = imageSize[0] - (imageSize[0] * (latCoord  - latRange[0])  / (latRange[1]  - latRange[0]) ).astype(int)
    longInds =                (imageSize[1] * (longCoord - longRange[0]) / (longRange[1] - longRange[0])).astype(int)

    return latInds, longInds

templateTrips = TripKmeansModel.cluster_centers_ * np.tile(stdTripAttr,(numClusters,1)) + np.tile(meanTripAttr,(numClusters,1))

srcCoords = templateTrips[:,:2]
dstCoords = templateTrips[:,2:4]

srcImCoords = ConvertToImageCoords(srcCoords[:,0],srcCoords[:,1], latRange, longRange, imageSize)
dstImCoords = ConvertToImageCoords(dstCoords[:,0],dstCoords[:,1], latRange, longRange, imageSize)

plt.figure(figsize=(12,12))
plt.imshow(np.log(locationDensityImage+1),cmap='hot'); plt.grid('off')
plt.scatter(srcImCoords[1],srcImCoords[0],c='m',s=200,alpha=0.8)
plt.scatter(dstImCoords[1],dstImCoords[0],c='g',s=200,alpha=0.8)

for i in range(len(srcImCoords[0])):
    plt.arrow(srcImCoords[1][i],srcImCoords[0][i], dstImCoords[1][i]-srcImCoords[1][i], dstImCoords[0][i]-srcImCoords[0][i], 
              edgecolor='c', facecolor='c', width=0.8,alpha=0.4,head_width=10.0,head_length=10.0,length_includes_head=True)

Output Text:
------------
<matplotlib.figure.Figure at 0x7f2ea4b35470>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a visualization of a network, likely representing transportation routes or connections within a geographical area, overlaid on a map. The map itself appears to be a high-resolution aerial or satellite image, with a dark background and a dense network of bright orange/red lines representing roads or streets.  This base map shows a large city, strongly resembling Manhattan, New York City, based on the island shape and street layout.

Superimposed on this base map are several features.  There are numerous small, light teal lines connecting various points, suggesting routes or pathways between locations.  These connecting lines are not uniform in length or density, indicating varying distances and connection strengths between the nodes.  Scattered across the map are colored circles of varying sizes. The circles appear in shades of purple, green, and orange/red, possibly representing different categories or types of nodes within the network. The size of the circles might correlate with importance or magnitude of the node in the system.

The overall structure suggests a complex network analysis, possibly related to transportation, logistics, or movement patterns within a densely populated urban environment. The use of color-coding and varying sizes of the nodes adds layers of information, allowing for a visual understanding of the underlying data structure and its geographical context.


------------------------------------------------------------
Cell index: 21
Input Cell Type: markdown
Input Text:
-----------
The magenta circles are sources (pickup locations), the green circles are destinations (dropoff locations) and the arrows between them are drawn in cyan.
we can see an uneven distribution of sources and destinations with the periphery of Manhattan mostly serve as destination. (I'm sure some people might want to kill me for calling Brooklyn and Queens "the periphery"...)


----------
Let's now move on to examining the Temporal aspect of these trips:
-----------------------------------------------------------------


----------





Output Text:
------------


------------------------------------------------------------
Cell index: 22
Input Cell Type: markdown
Input Text:
-----------
WeekDay Trip Distribution for different hours of the day
--------------------------------------------------------
How does this "bag of trips" changes during the **regular work day**?


Output Text:
------------


------------------------------------------------------------
Cell index: 23
Input Cell Type: python
Input Text:
-----------
# calculate the trip distribution for different hours of the weekday
hoursOfDay = np.sort(taxiDB['src hourOfDay'].astype(int).unique())
clusterDistributionHourOfDay_weekday = np.zeros((len(hoursOfDay),numClusters))
for k, hour in enumerate(hoursOfDay):
    slectedInds = (taxiDB['src hourOfDay'].astype(int) == hour) & (taxiDB['dayOfWeek'] <= 4)
    currDistribution, _ = np.histogram(clusterInds[slectedInds], bins=numClusters)
    clusterDistributionHourOfDay_weekday[k,:] = currDistribution[sortedClusterInds]

fig, ax = plt.subplots(nrows=1,ncols=1,figsize=(12,6))
ax.set_title('Trip Distribution during Weekdays', fontsize=12)
ax.imshow(clusterDistributionHourOfDay_weekday); ax.grid('off')
ax.set_xlabel('Trip Cluster'); ax.set_ylabel('Hour of Day')
ax.annotate('Silent Nights', color='r', fontsize=15, xy=(52, 2), xytext=(58, 1.75),
            arrowprops=dict(facecolor='red', shrink=0.03))

Output Text:
------------
<matplotlib.text.Annotation at 0x7f2ea4bab390>
<matplotlib.figure.Figure at 0x7f2ea72634e0>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a heatmap visualizing the distribution of trips across different trip clusters and hours of the day during weekdays. The x-axis represents the trip clusters (numbered 0 to 75 approximately), and the y-axis represents the hour of the day (0 to 24, likely midnight to 11 PM).  The color intensity represents the frequency or count of trips; brighter colors indicate a higher number of trips occurring within that specific trip cluster and hour.

The heatmap reveals patterns in trip activity throughout the day.  There are noticeable periods of higher trip frequency, shown by brighter regions of yellow and green, and periods of lower activity represented by darker purple hues.  A significant feature is the annotation "Silent Nights" with a red arrow pointing to a relatively dark vertical band towards the right side of the heatmap. This indicates a substantial reduction in trip activity during certain hours (likely late night/early morning) across several trip clusters.  The overall pattern suggests a diurnal rhythm to trip activity, with peaks and troughs at various times and across different trip types.


------------------------------------------------------------
Cell index: 24
Input Cell Type: markdown
Input Text:
-----------
Each row contains the "bag of trips" distribution where intensity represents trip frequency.

We can see that around 6AM people start waking up and (most likely) heading off to work. 
There is a second surge of taxi rides at around 18:00/19:00 (6PM/7PM) which is (most likely) people getting back home.
It's also extreemly evindent that during weedays, people are almost not active during the night. This is marked by the red arrow.

Output Text:
------------


------------------------------------------------------------
Cell index: 25
Input Cell Type: markdown
Input Text:
-----------
WeekEnd Trip Distribution for different hours of the day
--------------------------------------------------------
How does this "bag of trips" changes during the **weekend**?


Output Text:
------------


------------------------------------------------------------
Cell index: 26
Input Cell Type: python
Input Text:
-----------
# calculate the trip distribution for different hours of the weekend
hoursOfDay = np.sort(taxiDB['src hourOfDay'].astype(int).unique())
clusterDistributionHourOfDay_weekend = np.zeros((len(hoursOfDay),numClusters))
for k, hour in enumerate(hoursOfDay):
    slectedInds = (taxiDB['src hourOfDay'].astype(int) == hour) & (taxiDB['dayOfWeek'] >= 5)
    currDistribution, _ = np.histogram(clusterInds[slectedInds], bins=numClusters)
    clusterDistributionHourOfDay_weekend[k,:] = currDistribution[sortedClusterInds]

fig, ax = plt.subplots(nrows=1,ncols=1,figsize=(12,6))
ax.set_title('Trip Distribution during Weekends', fontsize=12)
ax.imshow(clusterDistributionHourOfDay_weekend); ax.grid('off')
ax.set_xlabel('Trip Cluster'); ax.set_ylabel('Hour of Day')
ax.annotate('Party Nights', color='r', fontsize=15, xy=(52, 2), xytext=(58, 1.75),
            arrowprops=dict(facecolor='red', shrink=0.03))
ax.annotate('Late Mornings', color='r', fontsize=15, xy=(45, 10), xytext=(58, 9.75),
            arrowprops=dict(facecolor='red', shrink=0.03))

Output Text:
------------
<matplotlib.text.Annotation at 0x7f2ea4b21828>
<matplotlib.figure.Figure at 0x7f2ea4bfccf8>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a heatmap visualization titled "Trip Distribution during Weekends".  The heatmap uses a color scale ranging from dark purple (low values) to bright yellow/green (high values) to represent the frequency or intensity of some data.  The x-axis is labeled "Trip Cluster," suggesting different groupings or categories of trips. The y-axis is labeled "Hour of Day," indicating the time of day from 0 (midnight) to 23 (11 PM), though only hours 0 to 23 are shown.

The heatmap itself shows a complex pattern of color intensity across the different trip clusters and hours of the day. The distribution is not uniform, with some clusters showing higher activity during specific hours. Two areas are highlighted with red arrows and labels: "Party Nights" points to a region of higher activity in the later hours (around midnight to early morning), and "Late Mornings" indicates another area of higher activity in the late morning hours (around 10 AM to noon).  This suggests that the data represents the frequency of trips occurring at different hours of the day for various trip types, with discernible patterns related to nightlife and late-morning activity during weekends.


------------------------------------------------------------
Cell index: 27
Input Cell Type: markdown
Input Text:
-----------
We can see that during weekends, people are much more active during the night, and also are starting their day much later in the day when compared to regular weekdays. The red arrows mark these two time points on the graph.

Output Text:
------------


------------------------------------------------------------
Cell index: 28
Input Cell Type: markdown
Input Text:
-----------
Weekly Trip Distribution for different days of the week
--------------------------------------------------------
How does this "bag of trips" changes during the **week**?


Output Text:
------------


------------------------------------------------------------
Cell index: 29
Input Cell Type: python
Input Text:
-----------
# calculate the trip distribution for day of week
daysOfWeek = np.sort(taxiDB['dayOfWeek'].unique())
clusterDistributionDayOfWeek = np.zeros((len(daysOfWeek),numClusters))
for k, day in enumerate(daysOfWeek):
    slectedInds = taxiDB['dayOfWeek'] == day
    currDistribution, _ = np.histogram(clusterInds[slectedInds], bins=numClusters)
    clusterDistributionDayOfWeek[k,:] = currDistribution[sortedClusterInds]

plt.figure(figsize=(12,5)); plt.title('Trip Distribution throughout the Week')
plt.imshow(clusterDistributionDayOfWeek); plt.grid('off')
plt.xlabel('Trip Cluster'); plt.ylabel('Day of Week')

Output Text:
------------
<matplotlib.text.Text at 0x7f2ea4b30160>
<matplotlib.figure.Figure at 0x7f2ea72a0940>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a heatmap depicting the distribution of trip clusters throughout the week.  The heatmap is oriented horizontally, with the y-axis representing the day of the week (numbered 0-7, likely representing Sunday to Saturday) and the x-axis representing the trip cluster (numbered 0-70+).  The color intensity of each cell in the heatmap corresponds to the frequency or count of trips belonging to a specific cluster on a particular day.

The color scheme appears to be a gradient ranging from yellow (likely representing high frequency) to dark purple (likely representing low frequency).  The visual pattern shows variability in trip cluster frequency across different days of the week. Some clusters appear consistently frequent across the week, while others show more pronounced peaks on specific days, suggesting potential daily patterns in trip types or behaviors. The title "Trip Distribution throughout the Week" clearly conveys the purpose of the visualization.


------------------------------------------------------------
Cell index: 30
Input Cell Type: markdown
Input Text:
-----------
We can see a reflection of what we saw from the two previous plots here. that during weekends, the pattern of trip distributions is somewhat different than that during the weekday.

Output Text:
------------


------------------------------------------------------------
Cell index: 31
Input Cell Type: markdown
Input Text:
-----------
Yearly Trip Distribution for different days of the year
--------------------------------------------------------
How does this "bag of trips" changes during the **year**?


Output Text:
------------


------------------------------------------------------------
Cell index: 32
Input Cell Type: python
Input Text:
-----------
# calculate the trip distribution for day of year
daysOfYear = taxiDB['dayOfYear'].unique()
daysOfYear = np.sort(daysOfYear)
clusterDistributionDayOfYear = np.zeros((len(daysOfYear),numClusters))
for k, day in enumerate(daysOfYear):
    slectedInds = taxiDB['dayOfYear'] == day
    currDistribution, _ = np.histogram(clusterInds[slectedInds], bins=numClusters)
    clusterDistributionDayOfYear[k,:] = currDistribution[sortedClusterInds]

fig, ax = plt.subplots(nrows=1,ncols=1,figsize=(10,16))
ax.set_title('Trip Distribution throughout the Year', fontsize=12)
ax.imshow(clusterDistributionDayOfYear); ax.grid('off')
ax.set_xlabel('Trip Cluster'); ax.set_ylabel('Day of Year')
ax.annotate('Large Snowstorm', color='r', fontsize=15 ,xy=(35, 21), xytext=(50, 17),
            arrowprops=dict(facecolor='red', shrink=0.03))
ax.annotate('Memorial Day', color='r', fontsize=15, xy=(35, 151), xytext=(50, 157),
            arrowprops=dict(facecolor='red', shrink=0.03))

Output Text:
------------
<matplotlib.text.Annotation at 0x7f2ea4c5aa58>
<matplotlib.figure.Figure at 0x7f2ea727cac8>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a heatmap depicting the distribution of trip clusters throughout a year. The vertical axis represents the day of the year, ranging from 1 to approximately 180, while the horizontal axis shows different trip clusters, numbered from 0 to approximately 75.  The color intensity in each cell represents the frequency or count of trips belonging to a specific cluster on a particular day.  Darker colors indicate a higher frequency of trips.

Two specific events are highlighted on the heatmap using red arrows and labels: "Large Snowstorm" and "Memorial Day". The arrow pointing to a horizontal band of relatively low trip frequency around day 25 indicates a significant decrease in trip activity likely due to a large snowstorm. Similarly, the arrow pointing to a horizontal band of lower activity around day 150 suggests a reduction in trips around Memorial Day.  This suggests the heatmap can be used to identify patterns in trip behavior related to both weather events and holidays.


------------------------------------------------------------
Cell index: 33
Input Cell Type: markdown
Input Text:
-----------
The most obvious pattern here is the periodicity.
We can also see two large drops of activity in the data, one around day 20-25 (end of January), and the other around 145-150 (end of May). It turns out that at the end of January there was a large snow storm in NYC, and that in the end of May there is a Memorial Day in the US. These are marked with red arrows.

Let's now delve deeper into The Temporal aspect:
------------------------------------------------


Output Text:
------------


------------------------------------------------------------
Cell index: 34
Input Cell Type: markdown
Input Text:
-----------
Let's apply PCA to reduce the dimensionality from the 80 dimensional distribution vector to something more manageable such as 3 dimensions that we can plot and visualize more easily

Output Text:
------------


------------------------------------------------------------
Cell index: 35
Input Cell Type: python
Input Text:
-----------
#%% let's apply PCA to reduce the dimentionality from 80 dimentional distribution vector 
# to something more managble such as 3 dimentions

hoursOfYear = np.sort(taxiDB['hourOfYear'].astype(int).unique())
clusterDistributionHourOfYear = np.zeros((len(range(hoursOfYear[0],hoursOfYear[-1])),numClusters))
dayOfYearVec  = np.zeros(clusterDistributionHourOfYear.shape[0])
weekdayVec    = np.zeros(clusterDistributionHourOfYear.shape[0])
weekOfYearVec = np.zeros(clusterDistributionHourOfYear.shape[0])
for k, hour in enumerate(hoursOfYear):
    slectedInds = taxiDB['hourOfYear'].astype(int) == hour
    currDistribution, _ = np.histogram(clusterInds[slectedInds], bins=numClusters)
    clusterDistributionHourOfYear[k,:] = currDistribution[sortedClusterInds]
    
    dayOfYearVec[k]  = taxiDB[slectedInds]['dayOfYear'].mean()
    weekdayVec[k]    = taxiDB[slectedInds]['dayOfWeek'].mean()
    weekOfYearVec[k] = taxiDB[slectedInds]['weekOfYear'].mean()

numComponents = 3
TripDistributionPCAModel = decomposition.PCA(n_components=numComponents,whiten=True, random_state=1)
compactClusterDistributionHourOfYear = TripDistributionPCAModel.fit_transform(clusterDistributionHourOfYear)

Output Text:
------------


------------------------------------------------------------
Cell index: 36
Input Cell Type: markdown
Input Text:
-----------
Weekly Periodicity
------------------

Here we'll show the temporal evolution of the 3 main principal components coefficients during the week

Output Text:
------------


------------------------------------------------------------
Cell index: 37
Input Cell Type: python
Input Text:
-----------
# collect traces for all weeks of year
listOfFullWeeks = []
for uniqueVal in np.unique(weekOfYearVec):
    if (weekOfYearVec == uniqueVal).sum() == 24*7:
        listOfFullWeeks.append(uniqueVal)

weeklyTraces = np.zeros((24*7,numComponents,len(listOfFullWeeks)))
for k, weekInd in enumerate(listOfFullWeeks):
    weeklyTraces[:,:,k] = compactClusterDistributionHourOfYear[weekOfYearVec == weekInd,:]

fig, axArray = plt.subplots(nrows=numComponents,ncols=1,sharex=True, figsize=(12,12))
fig.suptitle('PCA coefficients during the Week', fontsize=25)
for PC_coeff in range(numComponents):
    meanTrace = weeklyTraces[:,PC_coeff,:].mean(axis=1)
    axArray[PC_coeff].plot(weeklyTraces[:,PC_coeff,:],'y',linewidth=1.5)
    axArray[PC_coeff].plot(meanTrace,'k',linewidth=2.5)
    axArray[PC_coeff].set_ylabel('PC %d coeff' %(PC_coeff+1))
    axArray[PC_coeff].vlines([0,23,47,71,95,119,143,167], weeklyTraces[:,PC_coeff,:].min(), weeklyTraces[:,PC_coeff,:].max(), colors='r', lw=2)
    
axArray[PC_coeff].set_xlabel('hours since start of week')
axArray[PC_coeff].set_xlim(-0.9,24*7-0.1)

Output Text:
------------
(-0.9, 167.9)
<matplotlib.figure.Figure at 0x7f2ea4c44c18>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image displays a graph depicting PCA (Principal Component Analysis) coefficients over a week.  The graph is structured as three separate subplots stacked vertically, each representing a different principal component (PC1, PC2, and PC3).  The x-axis of all subplots shows "hours since start of week," indicating the time progression over a week.  The y-axis of each subplot shows the coefficient values for the corresponding principal component.

Each subplot contains multiple thin, light-yellow lines, representing individual data points or participants' coefficients over time. These lines are overlaid by a thick, black line which appears to be an average or smoothed representation of the individual lines.  This allows for a visualization of both individual variations and the overall trend for each principal component. Vertical red lines are spaced regularly throughout the x-axis, likely indicating daily divisions or specific time points of interest.

The title "PCA coefficients during the Week" clearly states the purpose of the graph.  The overall structure and presentation aim to clearly show the dynamic changes in PCA coefficients over the course of a week, highlighting both individual variability and the common patterns among the different principal components.


------------------------------------------------------------
Cell index: 38
Input Cell Type: markdown
Input Text:
-----------
In yellow is the data from all full weeks overlain on top of each other. In black are the average traces.

We can clearly see the periodicity here as well as the **difference between the regular weekdays and the weekends**.

Output Text:
------------


------------------------------------------------------------
Cell index: 39
Input Cell Type: markdown
Input Text:
-----------
Daily Periodicity
------------------

Here we'll show the temporal evolution of the 3 main principal components coefficients during the day

Output Text:
------------


------------------------------------------------------------
Cell index: 40
Input Cell Type: python
Input Text:
-----------
# collect traces for weekdays and weekends 
listOfFullWeekdays = []
listOfFullWeekends = []
for uniqueVal in np.unique(dayOfYearVec):
    if (dayOfYearVec == uniqueVal).sum() == 24:
        if weekdayVec[dayOfYearVec == uniqueVal][0] <= 4:
            listOfFullWeekdays.append(uniqueVal)
        else:
            listOfFullWeekends.append(uniqueVal)

weekdayTraces = np.zeros((24,numComponents,len(listOfFullWeekdays)))
for k, dayInd in enumerate(listOfFullWeekdays):
    weekdayTraces[:,:,k] = compactClusterDistributionHourOfYear[dayOfYearVec == dayInd,:]

weekendTraces = np.zeros((24,numComponents,len(listOfFullWeekends)))
for k, dayInd in enumerate(listOfFullWeekends):
    weekendTraces[:,:,k] = compactClusterDistributionHourOfYear[dayOfYearVec == dayInd,:]

fig, axArray = plt.subplots(nrows=numComponents,ncols=2,sharex=True,sharey=True, figsize=(12,14))
fig.suptitle('PCA coefficients for weekdays and weekends', fontsize=25)
for PC_coeff in range(numComponents):
    axArray[PC_coeff][0].plot(weekdayTraces[:,PC_coeff,:],'c',linewidth=1.5)
    axArray[PC_coeff][0].plot(weekdayTraces[:,PC_coeff,:].mean(axis=1),'k',linewidth=2.5)
    axArray[PC_coeff][0].set_ylabel('PC %d coeff' %(PC_coeff+1))
    
    axArray[PC_coeff][1].plot(weekendTraces[:,PC_coeff,:],'c',linewidth=1.5)
    axArray[PC_coeff][1].plot(weekendTraces[:,PC_coeff,:].mean(axis=1),'k',linewidth=2.5)
    
    if PC_coeff == 0:
        axArray[PC_coeff][0].set_title('Weekday')
        axArray[PC_coeff][1].set_title('Weekend')
    
axArray[PC_coeff][0].set_xlabel('hours of day')
axArray[PC_coeff][1].set_xlabel('hours of day')
axArray[PC_coeff][0].set_xlim(0,23)
axArray[PC_coeff][0].set_ylim(-3.5,3.5)

# add arrows with description
axArray[2][0].annotate('Early Risers', color='r', fontsize=12, xy=(7, 2.5), xytext=(12, 3),
            arrowprops=dict(facecolor='red', shrink=0.03))
axArray[1][1].annotate('Party Nights', color='r', fontsize=12, xy=(3, 2.2), xytext=(10, 3.2),
            arrowprops=dict(facecolor='red', shrink=0.03))
axArray[2][1].annotate('Late Mornings', color='r', fontsize=12, xy=(9, 1.5), xytext=(1, 3.5),
            arrowprops=dict(facecolor='red', shrink=0.03))

Output Text:
------------
<matplotlib.text.Annotation at 0x7f2e9266fcc0>
<matplotlib.figure.Figure at 0x7f2ea4abae80>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a visualization of Principal Component Analysis (PCA) coefficients for weekdays and weekends, broken down into three principal components (PC1, PC2, PC3).  The data is displayed as a series of line plots, with each line representing an individual data point's coefficient over 24 hours (hours of the day).  The plots are organized in a 3x2 grid, with the top row showing PC1, the middle row PC2, and the bottom row PC3.  Each column represents either weekday or weekend data.

Within each subplot, numerous light teal lines represent individual data points, while a thick black line depicts the average coefficient across all data points at each hour.  This allows for a visual comparison of individual variations and the overall trend for each principal component across the day.  The y-axis represents the PCA coefficient value, and the x-axis represents the hour of the day.

Annotations are added to highlight specific patterns within the weekend data. Red arrows point to distinct features in the graphs, labeled "Party Nights" (on PC2), "Early Risers" (on PC3), and "Late Mornings" (on PC3). These labels suggest interpretations of the data, indicating how these principal components might relate to specific behavioral patterns on weekends. The title clearly indicates the purpose of the visualization: to show the PCA coefficients for weekdays and weekends.


------------------------------------------------------------
Cell index: 41
Input Cell Type: markdown
Input Text:
-----------
Recall that we can go back and see exactly **what each PC coefficient means** by looking at the eigenvectors (i.e. trip distributions). Let's do that:


Output Text:
------------


------------------------------------------------------------
Cell index: 42
Input Cell Type: python
Input Text:
-----------
#%% examine what different PC coefficients mean by looking at their trip template distributions
fig, axArray = plt.subplots(nrows=numComponents,ncols=1,sharex=True, figsize=(12,11))
fig.suptitle('Trip Distribution PCA Components', fontsize=25)
for PC_coeff in range(numComponents):
    tripTemplateDistributionDifference = TripDistributionPCAModel.components_[PC_coeff,:] * \
                                         TripDistributionPCAModel.explained_variance_[PC_coeff]
    axArray[PC_coeff].bar(range(1,numClusters+1),tripTemplateDistributionDifference)
    axArray[PC_coeff].set_title('PCA %d component' %(PC_coeff+1))
    axArray[PC_coeff].set_ylabel('delta frequency [counts]')
    
axArray[PC_coeff].set_xlabel('cluster index (sorted by cluster frequency)')
axArray[PC_coeff].set_xlim(0,numClusters+0.5)

axArray[1].hlines([-25,25], 0, numClusters+0.5, colors='r', lw=0.7)
axArray[2].hlines([-11,11], 0, numClusters+0.5, colors='r', lw=0.7)

Output Text:
------------
<matplotlib.collections.LineCollection at 0x7f2e8828e048>
<matplotlib.figure.Figure at 0x7f2ea4a9a240>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image displays a graph depicting the results of a Principal Component Analysis (PCA) on trip distribution data.  The graph is structured vertically into three subplots, each representing a principal component (PC1, PC2, and PC3).  The x-axis of each subplot represents the cluster index, ordered by cluster frequency, indicating the relative importance or size of each cluster in the data. The y-axis represents the "delta frequency" (counts), showing the difference in frequency of trips associated with each cluster compared to a baseline or average.

The top subplot (PC1) shows predominantly positive delta frequencies, indicating that clusters along this component are associated with higher than average trip counts. The distribution is skewed, with a few clusters having much higher frequencies than others. In contrast, the middle and bottom subplots (PC2 and PC3) illustrate delta frequencies that are both positive and negative, suggesting that these components capture variations or differences in trip frequency among the clusters.  Horizontal red lines in PC2 and PC3 help visualize the zero point, making it easier to distinguish clusters with above-average versus below-average trip frequencies for these components.  The near-zero values toward the right end of each subplot indicate that the later clusters have a minimal impact on the respective PCA components.  The overall visualization provides insights into the underlying structure and variations in the trip distribution data, highlighting the key clusters and their influence on each principal component.


------------------------------------------------------------
Cell index: 43
Input Cell Type: markdown
Input Text:
-----------
We can see that the **first PCA component** looks very similar to the overall trip distribution, suggesting that it's mainly a "gain" component that controls just the **number of total trips** in that period of time. 

The second and third components have different patterns going up and down in them. 

**Let's examine what those patterns are, and in particular everything that deviates from the red lines:**


Output Text:
------------


------------------------------------------------------------
Cell index: 44
Input Cell Type: markdown
Input Text:
-----------
PC 2 large deviating trips
--------------------------
Arrows in red will show trips that are increasing when PC 2 coefficient increases.

Arrows in blue will show trips that are decreasing when PC 2 coefficient increases.

Magenta circles are the sources, and Green circles are the destinations.


Output Text:
------------


------------------------------------------------------------
Cell index: 45
Input Cell Type: python
Input Text:
-----------
#%% put the large deviating trips of each component back on the map
numTopTripsToShow = 8
numBottomTripsToShow = 6

# meaning of 2nd PC
sortedTripClusters_PC2 = np.argsort(TripDistributionPCAModel.components_[1,:])
topPositiveTripClusterInds = sortedTripClusters_PC2[-numTopTripsToShow:]
topNegativeTripClusterInds = sortedTripClusters_PC2[:numBottomTripsToShow]
allInds = np.hstack((topPositiveTripClusterInds,topNegativeTripClusterInds))

plt.figure(figsize=(12,12))
plt.imshow(np.log(locationDensityImage+1),cmap='hot'); plt.grid('off')
plt.scatter(srcImCoords[1][allInds],srcImCoords[0][allInds],c='m',s=500,alpha=0.9)
plt.scatter(dstImCoords[1][allInds],dstImCoords[0][allInds],c='g',s=500,alpha=0.9)

for i in topPositiveTripClusterInds:
    plt.arrow(srcImCoords[1][i],srcImCoords[0][i], dstImCoords[1][i]-srcImCoords[1][i], dstImCoords[0][i]-srcImCoords[0][i], 
              edgecolor='r', facecolor='r', width=2.8,alpha=0.9,head_width=10.0,head_length=10.0,length_includes_head=True)

for i in topNegativeTripClusterInds:
    plt.arrow(srcImCoords[1][i],srcImCoords[0][i], dstImCoords[1][i]-srcImCoords[1][i], dstImCoords[0][i]-srcImCoords[0][i], 
              edgecolor='b', facecolor='b', width=2.8,alpha=0.9,head_width=10.0,head_length=10.0,length_includes_head=True)
plt.title('PC2 major Trip deviations')

Output Text:
------------
<matplotlib.text.Text at 0x7f2e6088bf98>
<matplotlib.figure.Figure at 0x7f2e88332fd0>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a visualization of trip deviations, likely related to transportation data, overlaid on a map of a city, which appears to be New York City. The map itself is a heatmap, with bright orange and red areas indicating high density, likely representing road networks or high traffic areas. The background is predominantly black, contrasting sharply with the bright areas of the city map.

Superimposed on the map are several large colored circles representing points of interest, and colored lines connecting these points.  The circles are in varying shades of purple, green, and magenta.  The lines connecting them are red, blue, and possibly other colors as well. These lines likely represent significant deviations from expected or typical routes. The title "PC2 major Trip deviations" suggests this is a visualization of the second principal component (PC2) of a larger dataset, a common technique in data analysis to reveal patterns and relationships.  The image is intended to highlight the most significant deviations from an expected route using a graphical representation.


------------------------------------------------------------
Cell index: 46
Input Cell Type: markdown
Input Text:
-----------
**Arrows** in **red** show trips that are increasing when PC coefficient increases.  
**Arrows** in **blue** show trips that are decreasing when PC coefficient increases.  
**Magenta circles** are the sources, and **Green circles** are the destinations.  

We previously saw that **PC2 is about increased activity during the night**. and indeed most of the trips we see are short trips inside manhatten, perhaps people are all going to the hot places.

Output Text:
------------


------------------------------------------------------------
Cell index: 47
Input Cell Type: markdown
Input Text:
-----------
PC 3 large deviating trips
--------------------------


Output Text:
------------


------------------------------------------------------------
Cell index: 48
Input Cell Type: python
Input Text:
-----------
# meaning of 3rd PC
numTopTripsToShow = 4
numBottomTripsToShow = 10

sortedTripClusters_PC3 = np.argsort(TripDistributionPCAModel.components_[2,:])
topPositiveTripClusterInds = sortedTripClusters_PC3[-numTopTripsToShow:]
topNegativeTripClusterInds = sortedTripClusters_PC3[:numBottomTripsToShow]
allInds = np.hstack((topPositiveTripClusterInds,topNegativeTripClusterInds))

plt.figure(figsize=(12,12))
plt.imshow(np.log(locationDensityImage+1),cmap='hot'); plt.grid('off')
plt.scatter(srcImCoords[1][allInds],srcImCoords[0][allInds],c='m',s=500,alpha=0.9)
plt.scatter(dstImCoords[1][allInds],dstImCoords[0][allInds],c='g',s=500,alpha=0.9)

for i in topPositiveTripClusterInds:
    plt.arrow(srcImCoords[1][i],srcImCoords[0][i], dstImCoords[1][i]-srcImCoords[1][i], dstImCoords[0][i]-srcImCoords[0][i], 
              edgecolor='r', facecolor='r', width=2.8,alpha=0.9,head_width=10.0,head_length=10.0,length_includes_head=True)

for i in topNegativeTripClusterInds:
    plt.arrow(srcImCoords[1][i],srcImCoords[0][i], dstImCoords[1][i]-srcImCoords[1][i], dstImCoords[0][i]-srcImCoords[0][i], 
              edgecolor='b', facecolor='b', width=2.8,alpha=0.9,head_width=10.0,head_length=10.0,length_includes_head=True)
plt.title('PC3 major Trip deviations')

Output Text:
------------
<matplotlib.text.Text at 0x7f2e6074cfd0>
<matplotlib.figure.Figure at 0x7f2e60865eb8>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a map of a city, possibly New York City, judging by the street layout, overlaid with data visualizations. The base map is dark, with streets and roads depicted in shades of orange and red, indicating density or traffic flow.  The color intensity seems to represent the level of activity, with brighter areas suggesting higher traffic volume.

Superimposed on the base map are several large colored circles and lines. The circles are located at various points across the city, and they appear to represent significant locations or hubs.  These circles are colored green, purple, and orange/pink, suggesting different categories or groupings.  Lines connect these circles, with the lines colored blue and red. These lines likely represent connections or flows between the locations indicated by the circles. The title "PC3 major Trip deviations" suggests that the lines show significant deviations from expected travel patterns or routes, potentially based on a third principal component (PC3) analysis.  The different colors of the lines might indicate the type or magnitude of the deviation.

The overall impression is a complex data visualization combining geographic information with analytical results, possibly from a transportation study. The image effectively uses color to highlight significant features and relationships within the dataset, making patterns and anomalies readily apparent.


------------------------------------------------------------
Cell index: 49
Input Cell Type: markdown
Input Text:
-----------
We previously saw that this is the principal component that indicates what trips are done by the early risers.

Output Text:
------------


------------------------------------------------------------

================================================================================
================================================================================
exploring-youtube-faces-with-keypoints-dataset.ipynb:
=====================================================
------------------------------------------------------------
Cell index: 1
Input Cell Type: markdown
Input Text:
-----------
# Exploring "YouTube Faces with Facial Keypoints" Dataset
In this script we will demonstrate how to access the files of "YouTube Faces with Facial Keypoints" dataset.  
We also perform basic cluster analysis of the shapes in the dataset.  

I will not hide any code in this script so that we get familiarized with the files, how to load them and how to present them.

![filmreel](http://kjmultimediasolutions.com/wp-content/uploads/2015/07/filmreel1.jpg)

This dataset is a processed version of the [YouTube Faces Dataset](https://www.cs.tau.ac.il/~wolf/ytfaces/), that basically contained short videos of celebrities that are publicly available and were downloaded from YouTube. There are multiple videos of each celebrity (up to 6 videos per celebrity).  
Additionally, for this kaggle version of the dataset I've extracted facial keypoints for each frame of each video using [this amazing 2D and 3D Face alignment library](https://github.com/1adrianb/face-alignment) that was recently published.  

For full description please read the description on the [dataset page](https://www.kaggle.com/selfishgene/youtube-faces-with-facial-keypoints).

Output Text:
------------


------------------------------------------------------------
Cell index: 2
Input Cell Type: python
Input Text:
-----------
import numpy as np
import pandas as pd
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
import matplotlib
import glob
from sklearn import cluster

Output Text:
------------


------------------------------------------------------------
Cell index: 3
Input Cell Type: markdown
Input Text:
-----------
## Show the basic details of the videos in the dataset
In this particular case, show the "large" subset of the dataset that contains all videos of individuals with at least 3 different videos each.

Output Text:
------------


------------------------------------------------------------
Cell index: 4
Input Cell Type: python
Input Text:
-----------
videoDF = pd.read_csv('../input/youtube-faces-with-facial-keypoints/youtube_faces_with_keypoints_full.csv')
videoDF.head(15)

Output Text:
------------
            videoID     personName  imageHeight  imageWidth  videoDuration  \
0   Alison_Lohman_0  Alison_Lohman        228.0       213.0          240.0   
1   Alison_Lohman_1  Alison_Lohman        248.0       201.0           79.0   
2   Alison_Lohman_2  Alison_Lohman        335.0       308.0          136.0   
3   Alison_Lohman_3  Alison_Lohman        151.0       110.0           53.0   
4   Alison_Lohman_4  Alison_Lohman        236.0       228.0          147.0   
5   Alison_Lohman_5  Alison_Lohman        227.0       179.0           68.0   
6    Kevin_Spacey_0   Kevin_Spacey        196.0       264.0           91.0   
7    Kevin_Spacey_1   Kevin_Spacey        260.0       217.0           59.0   
8    Kevin_Spacey_2   Kevin_Spacey        186.0       175.0           71.0   
9    Kevin_Spacey_3   Kevin_Spacey        100.0        89.0           99.0   
10   Kevin_Spacey_4   Kevin_Spacey        212.0       207.0          240.0   
11   Kevin_Spacey_5   Kevin_Spacey        242.0       205.0          203.0   
12  Lauren_Hutton_0  Lauren_Hutton        223.0       202.0          127.0   
13  Lauren_Hutton_1  Lauren_Hutton        279.0       213.0           68.0   
14  Lauren_Hutton_2  Lauren_Hutton        320.0       361.0           55.0   

    averageFaceSize  numVideosForPerson  
0         90.150000                 6.0  
1        108.417722                 6.0  
2        122.161765                 6.0  
3         58.000000                 6.0  
4         91.680272                 6.0  
5        105.647059                 6.0  
6         99.912088                 6.0  
7        120.186441                 6.0  
8         86.746479                 6.0  
9         44.636364                 6.0  
10        99.025000                 6.0  
11       116.886700                 6.0  
12        96.086614                 6.0  
13       107.191176                 6.0  
14       193.781818                 6.0  


------------------------------------------------------------
Cell index: 5
Input Cell Type: markdown
Input Text:
-----------
Each video in the dataset is defined by the "video ID" field.  
For each videoID there exists an "videoID.npz" (e.g. "Kevin_Spacey_2.npz") that are divided among the archive files.  

* First, since there are multiple archive files with different names we will create a map between the videoID and the full filepath with which we can load the data.
* Then, we will then remove the rows in the videoDF dataframe that are yet to be uploaded to kaggle and keep only the rows of videos that have been uploaded already

Output Text:
------------


------------------------------------------------------------
Cell index: 6
Input Cell Type: python
Input Text:
-----------
# create a dictionary that maps videoIDs to full file paths
# single file cane be acessed using "../input/youtube-faces-with-facial-keypoints/youtube_faces_with_keypoints_full_1/youtube_faces_with_keypoints_full_1/AJ_Cook_0.npz"
npzFilesFullPath = glob.glob('../input/youtube-faces-with-facial-keypoints/youtube_faces_*/youtube_faces_*/*.npz')
videoIDs = [x.split('/')[-1].split('.')[0] for x in npzFilesFullPath]
fullPaths = {}
for videoID, fullPath in zip(videoIDs, npzFilesFullPath):
    fullPaths[videoID] = fullPath

# remove from the large csv file all videos that weren't uploaded yet
# videoDF = videoDF.loc[videoDF.loc[:,'videoID'].isin(fullPaths.keys()),:].reset_index(drop=True)
print('Number of Videos is %d' %(videoDF.shape[0]))
print('Number of Unique Individuals is %d' %(len(videoDF['personName'].unique())))

Output Text:
------------
Number of Videos is 2194
Number of Unique Individuals is 828


------------------------------------------------------------
Cell index: 7
Input Cell Type: markdown
Input Text:
-----------
## Show Overview of Dataset Content
Again, right now it's the content of a subset of the dataset that was already uploaded.  
When the dataset size limitation will be lifted, I'll upload more videos. This will be automatically updated whenever I'll re-run the script.

Output Text:
------------


------------------------------------------------------------
Cell index: 8
Input Cell Type: python
Input Text:
-----------
# overview of the contents of the dataset
groupedByPerson = videoDF.groupby("personName")
numVidsPerPerson = groupedByPerson.count()['videoID']
groupedByPerson.count().sort_values('videoID', axis=0, ascending=False)

plt.close('all')
plt.figure(figsize=(25,20))
plt.subplot(2,2,1)
plt.hist(x=numVidsPerPerson,bins=0.5+np.arange(numVidsPerPerson.min()-1,numVidsPerPerson.max()+1))
plt.title('Number of Videos per Person',fontsize=30); 
plt.xlabel('Number of Videos',fontsize=25); plt.ylabel('Number of People',fontsize=25)

plt.subplot(2,2,2)
plt.hist(x=videoDF['videoDuration'],bins=28);
plt.title('Distribution of Video Duration',fontsize=30); 
plt.xlabel('duration [frames]',fontsize=25); plt.ylabel('Number of Videos',fontsize=25)
plt.xlim(videoDF['videoDuration'].min()-2,videoDF['videoDuration'].max()+2)

plt.subplot(2,2,3)
plt.scatter(x=videoDF['imageWidth'], y=videoDF['imageHeight'])
plt.title('Distribution of Image Sizes',fontsize=30)
plt.xlabel('Image Width [pixels]',fontsize=25); plt.ylabel('Image Height [pixels]',fontsize=25)
plt.xlim(0,videoDF['imageWidth'].max() +15)
plt.ylim(0,videoDF['imageHeight'].max()+15)

plt.subplot(2,2,4)
averageFaceSize_withoutNaNs = np.array(videoDF['averageFaceSize'])
averageFaceSize_withoutNaNs = averageFaceSize_withoutNaNs[np.logical_not(np.isnan(averageFaceSize_withoutNaNs))]
plt.hist(averageFaceSize_withoutNaNs, bins=28)
plt.title('Distribution of Average Face Sizes ',fontsize=30)
plt.xlabel('Average Face Size [pixels]',fontsize=25); plt.ylabel('Number of Videos',fontsize=25);


Output Text:
------------
<Figure size 1800x1440 with 4 Axes>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image displays a 2x2 grid of visualizations, each presenting a different aspect of a dataset likely related to video analysis.  Each plot is clearly titled and labeled, making the data easily interpretable.

The top-left plot is a histogram showing the "Number of Videos per Person," indicating the distribution of how many videos each individual in the dataset has. The top-right histogram shows the "Distribution of Video Duration," measured in frames, revealing the frequency of videos with different lengths.

The bottom-left plot is a scatter plot depicting the "Distribution of Image Sizes." This plot shows the relationship between image width and height (in pixels), revealing the range and correlation between the dimensions of the images within the videos.  Finally, the bottom-right plot is another histogram illustrating the "Distribution of Average Face Sizes" (in pixels) across the videos, showing the frequency of different average face sizes detected.

In summary, the image provides a comprehensive overview of the dataset's characteristics related to the number of videos per subject, video length, image dimensions, and average face sizes within the video frames.  The use of histograms and scatter plots allows for a clear visualization of the distributions and correlations within the data.


------------------------------------------------------------
Cell index: 9
Input Cell Type: markdown
Input Text:
-----------
## 2D and 3D Landmarks Data
The best way to introduce the landmarks data is just to look at the following video:

Output Text:
------------


------------------------------------------------------------
Cell index: 10
Input Cell Type: python
Input Text:
-----------
from IPython.display import YouTubeVideo
YouTubeVideo('8FdSHl4oNIM',width=640, height=480)

Output Text:
------------
<IPython.lib.display.YouTubeVideo at 0x7f24c442cf10>


------------------------------------------------------------
Cell index: 11
Input Cell Type: markdown
Input Text:
-----------
For further details on this really awsome work, please visit [this project page](https://www.adrianbulat.com/face-alignment/).  
The paper can be found [here](https://www.adrianbulat.com/downloads/FaceAlignment/FaceAlignment.pdf).  
The pytorch code can be found [here](https://github.com/1adrianb/face-alignment).  

**Note**: I'm in no way associated to any of these guys, just genuinely impressed.

Output Text:
------------


------------------------------------------------------------
Cell index: 12
Input Cell Type: markdown
Input Text:
-----------
## Show Images from YouTube Faces with 2D Keypoints Overlaid
This shows how to read the dataset and display it's main content

Output Text:
------------


------------------------------------------------------------
Cell index: 13
Input Cell Type: python
Input Text:
-----------
# show several frames from each video and overlay 2D keypoints
np.random.seed(3)
numVideos = 4
framesToShowFromVideo = np.array([0.1,0.5,0.9])
numFramesPerVideo = len(framesToShowFromVideo)

# define which points need to be connected with a line
jawPoints          = [ 0,17]
rigthEyebrowPoints = [17,22]
leftEyebrowPoints  = [22,27]
noseRidgePoints    = [27,31]
noseBasePoints     = [31,36]
rightEyePoints     = [36,42]
leftEyePoints      = [42,48]
outerMouthPoints   = [48,60]
innerMouthPoints   = [60,68]

listOfAllConnectedPoints = [jawPoints,rigthEyebrowPoints,leftEyebrowPoints,
                            noseRidgePoints,noseBasePoints,
                            rightEyePoints,leftEyePoints,outerMouthPoints,innerMouthPoints]

# select a random subset of 'numVideos' from the available videos
randVideoIDs = videoDF.loc[np.random.choice(videoDF.index,size=numVideos,replace=False),'videoID']

fig, axArray = plt.subplots(nrows=numVideos,ncols=numFramesPerVideo,figsize=(14,18))
for i, videoID in enumerate(randVideoIDs):
    # load video
    videoFile = np.load(fullPaths[videoID])
    colorImages = videoFile['colorImages']
    boundingBox = videoFile['boundingBox']
    landmarks2D = videoFile['landmarks2D']
    landmarks3D = videoFile['landmarks3D']

    # select frames and show their content
    selectedFrames = (framesToShowFromVideo*(colorImages.shape[3]-1)).astype(int)
    for j, frameInd in enumerate(selectedFrames):
        axArray[i][j].imshow(colorImages[:,:,:,frameInd])
        axArray[i][j].scatter(x=landmarks2D[:,0,frameInd],y=landmarks2D[:,1,frameInd],s=9,c='r')
        for conPts in listOfAllConnectedPoints:
            xPts = landmarks2D[conPts[0]:conPts[-1],0,frameInd]
            yPts = landmarks2D[conPts[0]:conPts[-1],1,frameInd]
            axArray[i][j].plot(xPts,yPts,c='w',lw=1)
        axArray[i][j].set_title('"%s" (t=%d)' %(videoID,frameInd), fontsize=12)
        axArray[i][j].set_axis_off()

Output Text:
------------
<Figure size 1008x1296 with 12 Axes>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a grid of twelve smaller images, arranged in four rows and three columns. Each smaller image shows a close-up shot of a person's face, with a red dotted line outlining the facial features.  The outlines appear to be automatically generated facial landmark detection, indicating key points like the eyes, nose, mouth, and jawline.

Above each smaller image is text providing identifying information. This text consistently follows the format:  `"Name_Surname_Number"` (t=frame_number). "Name_Surname" appears to be the name of the individual depicted, "Number" likely indicates a unique identifier for that person within a larger dataset, and "t=frame_number" likely refers to the frame number from a video sequence where the image was captured.

The individuals depicted appear to be a mix of men and women, with diverse ages and appearances.  The lighting and background of the images vary somewhat, suggesting the images are from different video sources. The consistent application of facial landmark detection across all images suggests a systematic process for analyzing facial features from video data.


------------------------------------------------------------
Cell index: 14
Input Cell Type: markdown
Input Text:
-----------
We can see a small amount of imprefections in the keypoints predictions outputed by the alignment library, but overall this is really amazing performace.

Output Text:
------------


------------------------------------------------------------
Cell index: 15
Input Cell Type: markdown
Input Text:
-----------
## Show 3D Keypoints
Interestingly, this work can also extract 3D Keypoints, this shows how one can display the 3D landmarks

Output Text:
------------


------------------------------------------------------------
Cell index: 16
Input Cell Type: python
Input Text:
-----------
# show several 3D keypoints
numVideos = 4
framesToShowFromVideo = np.array([0.2,0.5,0.8])
numFramesPerVideo = len(framesToShowFromVideo)

# select a random subset of 'numVideos' from the available videos
randVideoIDs = videoDF.loc[np.random.choice(videoDF.index,size=numVideos,replace=False),'videoID']

fig = plt.figure(figsize=(14,14))
for i, videoID in enumerate(randVideoIDs):
    # load video
    videoFile = np.load(fullPaths[videoID])
    colorImages = videoFile['colorImages']
    boundingBox = videoFile['boundingBox']
    landmarks2D = videoFile['landmarks2D']
    landmarks3D = videoFile['landmarks3D']

    # select frames and show their content
    selectedFrames = (framesToShowFromVideo*(colorImages.shape[3]-1)).astype(int)
    for j, frameInd in enumerate(selectedFrames):
        subplotInd = i*numFramesPerVideo + j+1
        ax = fig.add_subplot(numVideos, numFramesPerVideo, subplotInd, projection='3d')
        ax.scatter(landmarks3D[:,0,frameInd], landmarks3D[:,1,frameInd], landmarks3D[:,2,frameInd],c='r')
        for conPts in listOfAllConnectedPoints:
            xPts = landmarks3D[conPts[0]:conPts[-1],0,frameInd]
            yPts = landmarks3D[conPts[0]:conPts[-1],1,frameInd]
            zPts = landmarks3D[conPts[0]:conPts[-1],2,frameInd]
            ax.plot3D(xPts,yPts,zPts,color='g')         
        ax.set_xlim(ax.get_xlim()[::-1])
        ax.view_init(elev=96, azim=90)
        ax.set_title('"%s" (t=%d)' %(videoID,frameInd), fontsize=12)
        
plt.tight_layout()

Output Text:
------------
<Figure size 1008x1008 with 12 Axes>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a grid of twelve 3D plots, arranged in four rows and three columns. Each plot visualizes a set of 3D points forming a face, overlaid on a curved surface. The points are represented as red spheres, while the underlying surface is depicted as a green line.  The plots are titled with the name of a person, followed by an identifier (likely a number), and a time value (denoted by 't').

The plots appear to track the movement and/or shape changes of facial features over time.  The x and y axes represent spatial coordinates on the face, while the z-axis likely corresponds to a depth dimension or another variable representing the changes over time. The slight variations in the shape and position of the red points across the different plots for each individual suggest a dynamic process, perhaps capturing facial expressions or subtle movements during a video sequence.

The overall structure is organized and easy to interpret, allowing for a comparison of facial feature changes across individuals and time.  The consistency in the plotting style enhances the comparability of the data presented.


------------------------------------------------------------
Cell index: 17
Input Cell Type: markdown
Input Text:
-----------
## Basic EDA
## Normalize 2D and 3D shapes

Output Text:
------------


------------------------------------------------------------
Cell index: 18
Input Cell Type: python
Input Text:
-----------
# collect all 2D and 3D shapes from all frames from all videos to a single numpy array matrix
totalNumberOfFrames = videoDF['videoDuration'].sum()
landmarks2D_all = np.zeros((68,2,int(totalNumberOfFrames)))
landmarks3D_all = np.zeros((68,3,int(totalNumberOfFrames)))

shapeIndToVideoID = {} # dictionary for later useage
endInd = 0
for i, videoID in enumerate(videoDF['videoID']):
    
    # load video
    videoFile = np.load(fullPaths[videoID])
    landmarks2D = videoFile['landmarks2D']
    landmarks3D = videoFile['landmarks3D']

    startInd = endInd
    endInd   = startInd + landmarks2D.shape[2]

    # store in one big array
    landmarks2D_all[:,:,startInd:endInd] = landmarks2D
    landmarks3D_all[:,:,startInd:endInd] = landmarks3D
    
    # make sure we keep track of the mapping to the original video and frame
    for videoFrameInd, shapeInd in enumerate(range(startInd,endInd)):
        shapeIndToVideoID[shapeInd] = (videoID, videoFrameInd)

# center the shapes around zero
# i.e. such that for each frame the mean x,y,z coordinates will be zero
# or in math terms: Xc = X - mean(X), Yc = Y - mean(Y), Zc = Z - mean(Z)
landmarks2D_centered = np.zeros(landmarks2D_all.shape)
landmarks2D_centered = landmarks2D_all - np.tile(landmarks2D_all.mean(axis=0),[68,1,1])

landmarks3D_centered = np.zeros(landmarks3D_all.shape)
landmarks3D_centered = landmarks3D_all - np.tile(landmarks3D_all.mean(axis=0),[68,1,1])

# normalize the shapes such that they have the same scale
# i.e. such that for each frame the mean euclidian distance from the shape center will be one
# or in math terms: mean( sqrt(dX^2 + dY^2 + dZ^2) ) = 1 
landmarks2D_normlized = np.zeros(landmarks2D_all.shape)
landmarks2D_normlized = landmarks2D_centered / np.tile(np.sqrt((landmarks2D_centered**2).sum(axis=1)).mean(axis=0), [68,2,1])

landmarks3D_normlized = np.zeros(landmarks3D_all.shape)
landmarks3D_normlized = landmarks3D_centered / np.tile(np.sqrt((landmarks3D_centered**2).sum(axis=1)).mean(axis=0), [68,3,1])

Output Text:
------------


------------------------------------------------------------
Cell index: 19
Input Cell Type: markdown
Input Text:
-----------
## Show 2D Shape Normalization stages

Output Text:
------------


------------------------------------------------------------
Cell index: 20
Input Cell Type: python
Input Text:
-----------
#%% check the 2D normalization and verify that everything is as expected
# select random several frames to be used as test cases
np.random.seed(2)

listOfShapeColors = ['r','g','b','m','y','c','k']
numShapesToPresent = len(listOfShapeColors)
listOfShapeInds = np.random.choice(range(int(totalNumberOfFrames)),size=numShapesToPresent,replace=False)

plt.close('all')
plt.figure(figsize=(14,10))
plt.suptitle('Shape Normalization Stages',fontsize=35)
plt.subplot(1,3,1)
for k,shapeInd in enumerate(listOfShapeInds):
    plt.scatter(landmarks2D_all[:,0,shapeInd], -landmarks2D_all[:,1,shapeInd], s=15, c=listOfShapeColors[k])
    for conPts in listOfAllConnectedPoints:
        xPts =  landmarks2D_all[conPts[0]:conPts[-1],0,shapeInd]
        yPts = -landmarks2D_all[conPts[0]:conPts[-1],1,shapeInd]
        plt.plot(xPts,yPts,c=listOfShapeColors[k],lw=1)
plt.axis('off'); plt.title('Original Shapes', fontsize=20)

plt.subplot(1,3,2)
for k,shapeInd in enumerate(listOfShapeInds):
    plt.scatter(landmarks2D_centered[:,0,shapeInd], -landmarks2D_centered[:,1,shapeInd], s=15, c=listOfShapeColors[k])
    for conPts in listOfAllConnectedPoints:
        xPts =  landmarks2D_centered[conPts[0]:conPts[-1],0,shapeInd]
        yPts = -landmarks2D_centered[conPts[0]:conPts[-1],1,shapeInd]
        plt.plot(xPts,yPts,c=listOfShapeColors[k],lw=1)
plt.axis('off'); plt.title('Centered Shapes', fontsize=20)

plt.subplot(1,3,3)
for k,shapeInd in enumerate(listOfShapeInds):
    plt.scatter(landmarks2D_normlized[:,0,shapeInd], -landmarks2D_normlized[:,1,shapeInd], s=15, c=listOfShapeColors[k])
    for conPts in listOfAllConnectedPoints:
        xPts =  landmarks2D_normlized[conPts[0]:conPts[-1],0,shapeInd]
        yPts = -landmarks2D_normlized[conPts[0]:conPts[-1],1,shapeInd]
        plt.plot(xPts,yPts,c=listOfShapeColors[k],lw=1)
plt.axis('off'); plt.title('Normlized Shapes', fontsize=20)

Output Text:
------------
Text(0.5, 1.0, 'Normlized Shapes')
<Figure size 1008x720 with 3 Axes>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image showcases the stages of shape normalization, specifically illustrating the process on a set of face outlines. 


The image is divided into three sections, each representing a stage:

1. **Original Shapes:** This section displays multiple face outlines in various colors, each exhibiting different sizes, orientations, and positions. They are unprocessed and represent the raw input data.

2. **Centered Shapes:** Here, the same face outlines are presented, but now they are centered. This means that the shapes have been translated so that their centroids (geometric centers) are aligned, resulting in a more uniform positioning.  The shapes are still varied in size and scale.

3. **Normalized Shapes:** This final section shows the face outlines after normalization.  The shapes are not only centered but also scaled to a uniform size. This makes them comparable and suitable for further analysis or processing, like comparing or averaging facial features.  The shapes are now aligned in both position and scale.


The overall effect demonstrates how shape normalization removes variability in position and size, allowing for a more meaningful comparison and analysis of the shapes.  The use of multiple colored outlines helps to visualize the transformation of each individual shape through the different stages.


------------------------------------------------------------
Cell index: 21
Input Cell Type: markdown
Input Text:
-----------
On the left we see that the original faces can appear anywhere in the frame, and have several different sizes.  
The centered images are at the same location, but with different sizes. The normlized faces on the right are approximatley same location and and same size. 

Output Text:
------------


------------------------------------------------------------
Cell index: 22
Input Cell Type: markdown
Input Text:
-----------
## Cluster all 2D shapes using Kmeans and show the resulting clusters

Output Text:
------------


------------------------------------------------------------
Cell index: 23
Input Cell Type: python
Input Text:
-----------
#%% cluster normalized shapes and show the cluster centers
numClusters = 16
normalizedShapesTable = np.reshape(landmarks2D_normlized, [68*2, landmarks2D_normlized.shape[2]]).T

shapesModel = cluster.KMeans(n_clusters=numClusters, n_init=5, random_state=1).fit(normalizedShapesTable[::2,:])
clusterAssignment = shapesModel.predict(normalizedShapesTable)

plt.figure(figsize=(14,14))
numRowsAndCols = int(np.ceil(np.sqrt(numClusters)))
for i in range(numClusters):
    plt.subplot(numRowsAndCols,numRowsAndCols,i+1);
    currClusterShape = np.reshape(shapesModel.cluster_centers_[i,:], [68,2])
    plt.scatter(x=currClusterShape[:,0],y=-currClusterShape[:,1],s=20,c='r')
    for conPts in listOfAllConnectedPoints:
        xPts =  currClusterShape[conPts[0]:conPts[-1],0]
        yPts = -currClusterShape[conPts[0]:conPts[-1],1]
        plt.plot(xPts,yPts,c='g',lw=1)
    plt.title('cluster %d' %(i),fontsize=15)
    plt.axis('off')

Output Text:
------------
<Figure size 1008x1008 with 16 Axes>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image displays a grid of 16 plots, each titled "cluster" followed by a number from 0 to 15.  Each plot shows a stylized representation of a human face, constructed from a network of interconnected red dots.  These dots appear to represent facial landmarks, and they are connected by green lines forming a roughly oval shape outlining the face.  The plots illustrate variations in facial features, suggesting a clustering of faces based on similarity.  The differences between the clusters are subtle, with variations primarily in the positioning and shape of the eyes, nose, and mouth.

The overall structure is highly organized and systematic, with the 16 plots arranged in a 4x4 grid.  This arrangement allows for easy comparison of the different clusters. The uniform style of plotting ensures that the focus remains on the differences in facial features represented by the dot patterns. The image likely represents the results of a clustering algorithm applied to a dataset of facial landmark data, grouping similar faces together into distinct clusters.  The green lines connecting the red dots likely represent a model or interpolation of the underlying facial feature data.


------------------------------------------------------------
Cell index: 24
Input Cell Type: markdown
Input Text:
-----------
Most clusters are various poses with a neutral expression. It could perhaps be interesting to have a look at the clusters if we increase the number of clusters. you are welcome to fork and have a look.

Output Text:
------------


------------------------------------------------------------
Cell index: 25
Input Cell Type: markdown
Input Text:
-----------
# Show original images assigned to the same shape cluster
## Look to the Left (Cluster 12)
(their left, it's our right)

Output Text:
------------


------------------------------------------------------------
Cell index: 26
Input Cell Type: python
Input Text:
-----------
#%% show several original images that are assigned to a particular cluster
selectedCluster = 12
numRows = 4; numCols = 4;

shapeIndsAssignedToCluster = np.nonzero(clusterAssignment == selectedCluster)[0]
listOfShapeInds = np.random.choice(shapeIndsAssignedToCluster ,size=numRows*numCols,replace=False)

plt.figure(figsize=(14,14))
for i, shapeInd in enumerate(listOfShapeInds):
    # load video and pickout the relevent frame
    videoID  = shapeIndToVideoID[shapeInd][0]
    frameInd = shapeIndToVideoID[shapeInd][1]    
    videoFile = np.load(fullPaths[videoID])
    image = videoFile['colorImages'][:,:,:,frameInd]
    
    # show the image
    plt.subplot(numRows,numCols,i+1);
    plt.imshow(image); plt.axis('off')

Output Text:
------------
<Figure size 1008x1008 with 16 Axes>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a grid of 16 smaller images, arranged in four rows and four columns. Each smaller image is a headshot or close-up shot of a different individual.  The people depicted appear to be diverse in age, gender, and ethnicity.  They are all presented against varied backgrounds, some appearing to be news studio sets, others seemingly from interviews or other media sources.


The individuals are predominantly fair-skinned, with a mix of men and women.  Their expressions range from neutral to engaged, some appearing to be speaking or listening attentively.  The clothing is mostly business casual or formal, with suits and jackets prominent among the men, and a mixture of blouses and jackets for the women.  The overall impression is that the images are stills from video recordings, likely news segments or interviews, given the backgrounds and attire of the subjects. The quality of the images is consistent with television broadcasts from around the early 2000s.


------------------------------------------------------------
Cell index: 27
Input Cell Type: markdown
Input Text:
-----------
We can see that all of those images have similar face poses, which was exactly the goal.

Output Text:
------------


------------------------------------------------------------
Cell index: 28
Input Cell Type: markdown
Input Text:
-----------
## Now look to the Right (Cluster 1)
(their right, it's our left)

Output Text:
------------


------------------------------------------------------------
Cell index: 29
Input Cell Type: python
Input Text:
-----------
#%% show several original images that are assigned to a particular cluster
selectedCluster = 1
numRows = 4; numCols = 4;

shapeIndsAssignedToCluster = np.nonzero(clusterAssignment == selectedCluster)[0]
listOfShapeInds = np.random.choice(shapeIndsAssignedToCluster ,size=numRows*numCols,replace=False)

plt.figure(figsize=(14,14))
for i, shapeInd in enumerate(listOfShapeInds):
    # load video and pickout the relevent frame
    videoID  = shapeIndToVideoID[shapeInd][0]
    frameInd = shapeIndToVideoID[shapeInd][1]    
    videoFile = np.load(fullPaths[videoID])
    image = videoFile['colorImages'][:,:,:,frameInd]
    
    # show the image
    plt.subplot(numRows,numCols,i+1);
    plt.imshow(image); plt.axis('off')

Output Text:
------------
<Figure size 1008x1008 with 16 Axes>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
Here's a description of the image:

The image is a grid of sixteen smaller images, arranged in four rows and four columns. Each smaller image is a headshot of a different person, seemingly captured from various video sources. 


The individuals depicted are diverse in terms of age, gender, and ethnicity. There's a mix of men and women, appearing to represent a range of ages from young adults to older individuals. Their backgrounds are also diverse, with some appearing in formal settings (news interviews), others in more informal settings (possibly personal videos or home recordings), and some with flags or other background elements visible.


The overall impression is that of a collection of faces, possibly assembled for a purpose such as a facial recognition dataset or a compilation of news clips. The quality of the smaller images varies somewhat, with some appearing sharper than others.  The lighting and image quality suggest a variety of different sources for the individual images.


------------------------------------------------------------

================================================================================
================================================================================
advanced-feature-exploration.ipynb:
===================================
------------------------------------------------------------
Cell index: 1
Input Cell Type: markdown
Input Text:
-----------
# Advanced Feature Exploration

Output Text:
------------


------------------------------------------------------------
Cell index: 2
Input Cell Type: python
Input Text:
-----------
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import cluster
from sklearn import ensemble
from sklearn import cross_validation
from sklearn.metrics import roc_auc_score as auc
import time

plt.rcParams['figure.figsize'] = (10, 10)

#%% load data and remove constant and duplicate columns  (taken from a kaggle script)

trainDataFrame = pd.read_csv('../input/train.csv')

# remove constant columns
colsToRemove = []
for col in trainDataFrame.columns:
    if trainDataFrame[col].std() == 0:
        colsToRemove.append(col)

trainDataFrame.drop(colsToRemove, axis=1, inplace=True)

# remove duplicate columns
colsToRemove = []
columns = trainDataFrame.columns
for i in range(len(columns)-1):
    v = trainDataFrame[columns[i]].values
    for j in range(i+1,len(columns)):
        if np.array_equal(v,trainDataFrame[columns[j]].values):
            colsToRemove.append(columns[j])

trainDataFrame.drop(colsToRemove, axis=1, inplace=True)

trainLabels = trainDataFrame['TARGET']
trainFeatures = trainDataFrame.drop(['ID','TARGET'], axis=1)


#%% look at single feature performance

X_train, X_valid, y_train, y_valid = cross_validation.train_test_split(trainFeatures, trainLabels, 
                                                                       test_size=0.5, random_state=1)
verySimpleLearner = ensemble.GradientBoostingClassifier(n_estimators=20, max_features=1, max_depth=3, 
                                                        min_samples_leaf=100, learning_rate=0.1, 
                                                        subsample=0.65, loss='deviance', random_state=1)

startTime = time.time()
singleFeatureTable = pd.DataFrame(index=range(len(X_train.columns)), columns=['feature','AUC'])
for k,feature in enumerate(X_train.columns):
    trainInputFeature = X_train[feature].values.reshape(-1,1)
    validInputFeature = X_valid[feature].values.reshape(-1,1)
    verySimpleLearner.fit(trainInputFeature, y_train)
    
    validAUC = auc(y_valid, verySimpleLearner.predict_proba(validInputFeature)[:,1])
    singleFeatureTable.ix[k,'feature'] = feature
    singleFeatureTable.ix[k,'AUC'] = validAUC
        
print("finished evaluating single features. took %.2f minutes" %((time.time()-startTime)/60))

Output Text:
------------


------------------------------------------------------------
Cell index: 3
Input Cell Type: markdown
Input Text:
-----------
### Show single feature AUC performace
this is the same as in "Basic Feature Exploration" script, to be used later

Output Text:
------------


------------------------------------------------------------
Cell index: 4
Input Cell Type: python
Input Text:
-----------
#%% sort according to AUC and present the table
singleFeatureTable = singleFeatureTable.sort_values(by='AUC', axis=0, ascending=False).reset_index(drop=True)

singleFeatureTable.ix[:15,:]

Output Text:
------------


------------------------------------------------------------
Cell index: 5
Input Cell Type: markdown
Input Text:
-----------
### Generate 400 five-wise random feature combinations and calculate their AUC

Output Text:
------------


------------------------------------------------------------
Cell index: 6
Input Cell Type: python
Input Text:
-----------
#%% find interesting fivewise combinations

numFeaturesInCombination = 5
numCombinations = 400
numBestSingleFeaturesToSelectFrom = 20

X_train, X_valid, y_train, y_valid = cross_validation.train_test_split(trainFeatures, trainLabels, 
                                                                       test_size=0.5, random_state=1)
weakLearner = ensemble.GradientBoostingClassifier(n_estimators=30, max_features=2, max_depth=3, 
                                                  min_samples_leaf=100,learning_rate=0.1, 
                                                  subsample=0.65, loss='deviance', random_state=1)

featuresToUse = singleFeatureTable.ix[0:numBestSingleFeaturesToSelectFrom-1,'feature']
featureColumnNames = ['feature'+str(x+1) for x in range(numFeaturesInCombination)]
featureCombinationsTable = pd.DataFrame(index=range(numCombinations), columns=featureColumnNames + ['combinedAUC'])

# for numCombinations iterations 
startTime = time.time()
for combination in range(numCombinations):
    # generate random feature combination
    randomSelectionOfFeatures = sorted(np.random.choice(len(featuresToUse), numFeaturesInCombination, replace=False))

    # store the feature names
    combinationFeatureNames = [featuresToUse[x] for x in randomSelectionOfFeatures]
    for i in range(len(randomSelectionOfFeatures)):
        featureCombinationsTable.ix[combination,featureColumnNames[i]] = combinationFeatureNames[i]

    # build features matrix to get the combination AUC
    trainInputFeatures = X_train.ix[:,combinationFeatureNames]
    validInputFeatures = X_valid.ix[:,combinationFeatureNames]
    # train learner
    weakLearner.fit(trainInputFeatures, y_train)
    # store AUC results
    validAUC = auc(y_valid, weakLearner.predict_proba(validInputFeatures)[:,1])        
    featureCombinationsTable.ix[combination,'combinedAUC'] = validAUC

validAUC = np.array(featureCombinationsTable.ix[:,'combinedAUC'])
print("(min,max) AUC = (%.4f,%.4f). took %.1f minutes" % (validAUC.min(),validAUC.max(), (time.time()-startTime)/60))

# show the histogram of the feature combinations performance 
plt.figure(); plt.hist(validAUC, 100, facecolor='blue', alpha=0.75)
plt.xlabel('AUC'); plt.ylabel('frequency'); plt.title('feature combination AUC histogram'); plt.show()

Output Text:
------------


------------------------------------------------------------
Cell index: 7
Input Cell Type: python
Input Text:
-----------
#%% sort according to combination AUC and look at the table

featureCombinationsTable = featureCombinationsTable.sort_values(by='combinedAUC', axis=0, ascending=False).reset_index(drop=True)
featureCombinationsTable.ix[:20,:]

Output Text:
------------


------------------------------------------------------------
Cell index: 8
Input Cell Type: markdown
Input Text:
-----------
it's easy to see that this table contains a lot of feature overlap
### Visualize this by building a Pairwise Overlap Matrix

Output Text:
------------


------------------------------------------------------------
Cell index: 9
Input Cell Type: python
Input Text:
-----------
#%% visualize the overlap by building a pairwise overlap matrix

combinationOverlapMatrix = np.zeros((numCombinations,numCombinations))
for comb_i in range(numCombinations):
    for comb_j in range(comb_i+1,numCombinations):
        # get the features list for each combination        
        featuresComb_i = [featureCombinationsTable.ix[comb_i,featureColumnNames[x]] for x in range(numFeaturesInCombination)]
        featuresComb_j = [featureCombinationsTable.ix[comb_j,featureColumnNames[x]] for x in range(numFeaturesInCombination)]
        # store the number of overlapping features
        combinationOverlapMatrix[comb_i,comb_j] = 2*numFeaturesInCombination-len(set(featuresComb_i+featuresComb_j))
        combinationOverlapMatrix[comb_j,comb_i] = combinationOverlapMatrix[comb_i,comb_j]

plt.figure(); plt.imshow(combinationOverlapMatrix,cmap='autumn'); plt.title('combination overlap'); plt.colorbar()

Output Text:
------------


------------------------------------------------------------
Cell index: 10
Input Cell Type: markdown
Input Text:
-----------
#### We would like to remove some of this redundancy
### Perform k-means on the overlap patterns and reorder the matrix

Output Text:
------------


------------------------------------------------------------
Cell index: 11
Input Cell Type: python
Input Text:
-----------
#%% we would like to get the top performing but most different feature combinations

numFeaturesToSelect = 15

cluserer = cluster.KMeans(n_clusters=numFeaturesToSelect)
clusterInds = cluserer.fit_predict(combinationOverlapMatrix)

#%% reorder features according to their new clusters

# group the rows into clusters
clusteredRows = {}
clusterMaxAUC = {}
clusterMaxInd = {}
for clusterInd in np.unique(clusterInds):
    clusteredRows[clusterInd] = combinationOverlapMatrix[clusterInds == clusterInd,:]
    clusterMaxAUC[clusterInd] = featureCombinationsTable.ix[clusterInds == clusterInd,'combinedAUC'].max(axis=0)
    clusterMaxInd[clusterInd] = featureCombinationsTable.ix[clusterInds == clusterInd,'combinedAUC'].idxmax(axis=0)    
    
import operator    
sortedClustersByMaxAUCTuple = sorted(clusterMaxAUC.items(), key=operator.itemgetter(1),reverse=True)

# calculate the reordering vector
finalFeaturesToKeep = []
reorderedVector = None
for k,item in enumerate(sortedClustersByMaxAUCTuple):
    if k == 0:
        reorderedVector = np.array((clusterInds == item[0]).nonzero())
    else:
        reorderedVector = np.hstack((reorderedVector,np.array((clusterInds == item[0]).nonzero())))
    finalFeaturesToKeep.append(clusterMaxInd[item[0]])
reorderedVector = reorderedVector.flatten()

# reorder the matrix by rows and columns
reorderedMatrix = combinationOverlapMatrix[reorderedVector,:]
reorderedMatrix = reorderedMatrix[:,reorderedVector]

# show the matrix
plt.figure(); plt.imshow(reorderedMatrix,cmap='autumn'); plt.title('reordered combination overlap'); plt.colorbar()


Output Text:
------------


------------------------------------------------------------
Cell index: 12
Input Cell Type: markdown
Input Text:
-----------
# End Result
### The 15 Best least redundent five-wise feature combinations

Output Text:
------------


------------------------------------------------------------
Cell index: 13
Input Cell Type: python
Input Text:
-----------
#%% show the final combinations

featureCombinationsTable.ix[finalFeaturesToKeep,:]

Output Text:
------------


------------------------------------------------------------

================================================================================
================================================================================
the-12-different-types-of-kagglers.ipynb:
=========================================
------------------------------------------------------------
Cell index: 1
Input Cell Type: markdown
Input Text:
-----------
# The 12 Different Types of Kagglers 

They say people are not averages. They also say you can drown in a pool who's average depth is 12 cm.   
In this script we try to move a little past "the average kaggler" and present "the 12 typical kagglers". 
These are 12 imagined people that represent the kaggle community hopefully better than that of a single average kaggler.  
This will be analogous to the following: instead of talking about "the pool's average depth is 12cm" move on to talk about "95% of the pool's area is 2 cm deep on average, and 5% of the pool's area is 2 meters deep on average".  

This dataset holds the 2017 kaggle data science survay results. It is presented to us here in a very raw and unprocessed form (to say the least), so we'll first need to move quite a bit of data around before we can continue.  

After a little bit of pre-processing we will look at correlations between several key data fields, and then continue to cluster analyize the ~16,000 kagglers that have answered this survey.

**In the final and main** part of the script, we present a short summery paragraph of each kaggler type one by one. 

Output Text:
------------


------------------------------------------------------------
Cell index: 2
Input Cell Type: markdown
Input Text:
-----------
# The TL;DR Version:
To see where these descriptions come from, please continue to the analysis

### Paragraph Summery of Kaggler Type no. 1:
The most frequent kaggler (accounts for slightly less than 15% of kagglers) is a 28 year old male from India, he is employed full-time as a Software Developer or a Data Scientist, he has some industry experience, he majored in Computer Science (CS) during University and holds a Master's degree. His annual salary is about 14K dollars a year. He is looking forward to learn about deep learning in the upcoming year and specifically using tensorflow.

### Paragraph Summery of Kaggler Type no. 2:
The 2nd most frequent kaggler (accounts for slightly less than 13% of kagglers) is a 27 year old male from India, he is employed full-time as a Software Developer, has little industry experience, he has majored in CS during university and holds a Bachelor's degree.

### Paragraph Summery of Kaggler Type no. 3:
The 3rd most frequent kaggler (accounts for about 11% of kagglers) is a 34 year old male from the US, he is employed full-time, and doesn't really have the time to fill out an internet survey.

### Paragraph Summery of Kaggler Type no. 4:
The 4th most frequent kaggler (accounts for about 9% of kagglers) is a 29 year old female from the US, she is employed full-time as a Data Scientist, has 3-5 years of industry experience, her background is CS and she holds a Master's degree. She didn't share her salary but we can infer from her background that she's not starving.

### Paragraph Summery of Kaggler Type no. 5:
The 5th most frequent kaggler (accounts for about 8.5% of kagglers) is a 30 year old male from the US, he is employed full-time as a Data Scientist, has 3-5 years of industry experience and holds a Master's degree in Mathematics/Statistics. He mainly works with python. His anual salary is 92K dollars. Next year, he wants to learn more about deep learning and specifically using tensorflow.

### Paragraph Summery of Kaggler Type no. 6:
The 6th most frequent kaggler (accounts for about 8.5% of kagglers) is a 22 year old male from the India, he is not employed, has no experience, he holds a Bachelor's degree majoring in CS. He mainly works with python. He spends 2-10 hours each week learning Data Science. His first exposure to data science was at online courses and he values kaggle competitions very highly as a potential credential.

### Paragraph Summery of Kaggler Type no. 7:
The 7th most frequent kaggler (accounts for about 8% of kagglers) is a 44 year old male from the US, he is employed full time at various different professions, has more than 10 years of experience, holds a Master's degree and majored in CS. His first training is University (they didn't have online courses 20 years ago...). He mainly works with python. He too is looking forward to gaining experience with deep learning and tensorflow in the upcoming year.

### Paragraph Summery of Kaggler Type no. 8:
The 8th most frequent kaggler (accounts for about 7.4% of kagglers) is a 34 year old male from around the world, he is employed full time as a Software Developer, but has no Data Science experience. he holds a Master's degree and majored in CS. His first Data Science training is coming from online courses and he spends 2-10 hours each week learning data science. He working with python and like everyone else he's also looking forward to gaining experience with deep learning in the upcoming year.

### Paragraph Summery of Kaggler Type no. 9:
The 9th most frequent kaggler (accounts for about 5.8% of kagglers) is a 30 year old male from around the world. He is not employed. He holds a Master's degree and comes from CS and Electrical Engineering (EE). His first Data Science training is coming from online courses and he spends most of his time learning Data Science. He working with a basic laptop and using python. Like everyone else he's also looking forward to learning about deep learning in the upcoming year.

### Paragraph Summery of Kaggler Type no. 10:
The 10th most frequent kaggler (accounts for about 5.6% of kagglers) is a 36 year old male from the US, he is employed full time and has more than 10 years of experience. He holds a Master's degree and comes from an eclectic background. He was first trained at University and is also self taught.

### Paragraph Summery of Kaggler Type no. 11:
The 11th most frequent kaggler (accounts for about 4.6% of kagglers) is a 40 year old male from the US, he is employed full-time as a Data Scientist, has more than 10 years of industry experience, his background is diverse (CS, EE, Math) and he holds a Master's degree. He self taught himself Data Science, and mainly works with python. His anual salary is 181K dollars. Like everyone else, this experienced kaggler wants to learn more about deep learning and specifically using tensorflow.

### Paragraph Summery of Kaggler Type no. 12:
The 12th most frequent and final kaggler (accounts for about 4% of kagglers) is a 26 year old female from the US. She is unemployed, has little industry experience, her background is diverse (CS, EE) and she holds either a Bachelor's or a Master's degree. She learns data science around 2-10 hours a week, and mainly works with python running her code on a basic laptop. Like everyone else, she is interested to learn more about deep learning in the upcoming year.

Output Text:
------------


------------------------------------------------------------
Cell index: 3
Input Cell Type: python
Input Text:
-----------
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib
import numpy as np
import pandas as pd
from sklearn import cluster, decomposition, preprocessing

Output Text:
------------


------------------------------------------------------------
Cell index: 4
Input Cell Type: markdown
Input Text:
-----------
# Load Data

Output Text:
------------


------------------------------------------------------------
Cell index: 5
Input Cell Type: python
Input Text:
-----------
#%% load data
multipleChoiceResponsesDF = pd.read_csv('../input/multipleChoiceResponses.csv', encoding='ISO-8859-1', low_memory=False)
processedDF = multipleChoiceResponsesDF.copy()
processedDF = processedDF.fillna(value='NaN')
allQuestions = processedDF.columns.tolist()

Output Text:
------------


------------------------------------------------------------
Cell index: 6
Input Cell Type: markdown
Input Text:
-----------
# Move data around and create a basic subset of numeric features


Output Text:
------------


------------------------------------------------------------
Cell index: 7
Input Cell Type: python
Input Text:
-----------
#%% create annual salary in US dollars feature
processedDF['CompensationAmount'] = processedDF['CompensationAmount'].str.replace(',','')
processedDF['CompensationAmount'] = processedDF['CompensationAmount'].str.replace('-','')
processedDF.loc[processedDF['CompensationAmount'] == 'NaN','CompensationAmount'] = '0'
processedDF.loc[processedDF['CompensationAmount'] == '','CompensationAmount'] = '0'
processedDF['CompensationAmount'] = processedDF['CompensationAmount'].astype(float)

conversionRates = pd.read_csv('../input/conversionRates.csv', encoding='ISO-8859-1').set_index('originCountry')
conversionRates = pd.read_csv('../input/conversionRates.csv').set_index('originCountry')
conversionRates.loc['USD']['exchangeRate']

exchangeRate = []
for row in range(processedDF.shape[0]):
    if processedDF.loc[row,'CompensationCurrency'] not in conversionRates.index.tolist():
        exchangeRate.append(1.0)
    else:
        exchangeRate.append(conversionRates.loc[processedDF.loc[row,'CompensationCurrency']]['exchangeRate'])
        
processedDF['exchangeRate'] = exchangeRate
processedDF['annualSalary_USD'] = processedDF['CompensationAmount']*processedDF['exchangeRate']

processedDF.loc[processedDF['annualSalary_USD'] > 300000, 'annualSalary_USD'] = 300000
processedDF['annualSalary_USD'] = processedDF['annualSalary_USD']/1000.0

Output Text:
------------


------------------------------------------------------------
Cell index: 8
Input Cell Type: python
Input Text:
-----------
#%% collect all basic features ('age','education level','seniority', 'salary', ...)
def GetDictValueForKey(x):
    return answerToNumericalDict[x]

basicFeatures = ['Country','GenderSelect','Age','FormalEducation','Tenure','annualSalary_USD','MajorSelect','EmploymentStatus','CurrentJobTitleSelect','LanguageRecommendationSelect','TimeSpentStudying']
basicSubsetDF = processedDF[basicFeatures]

additionalFeatures = ['FirstTrainingSelect','ProveKnowledgeSelect','AlgorithmUnderstandingLevel','MLMethodNextYearSelect','MLToolNextYearSelect','HardwarePersonalProjectsSelect','JobSearchResource','EmployerSearchMethod']
additionalSubsetDF = processedDF[additionalFeatures]

# add impatience variables that counts the number of NaNs in a given row for the basic and additional subsets
def CountNaNs(row):
    return (row == 'NaN').sum()

basicSubsetDF['impatience_basic'] = basicSubsetDF.apply(CountNaNs,axis=1)
basicSubsetDF['impatience_additional'] = additionalSubsetDF.apply(CountNaNs,axis=1)
basicSubsetDF['impatience'] = basicSubsetDF['impatience_basic'] + basicSubsetDF['impatience_additional']

# cap age to be in [15,85] range
basicSubsetDF.loc[basicSubsetDF['Age'] == 'NaN','Age'] = basicSubsetDF.loc[basicSubsetDF['Age'] != 'NaN','Age'].mean()
basicSubsetDF.loc[basicSubsetDF['Age'] <= 15,'Age'] = 15
basicSubsetDF.loc[basicSubsetDF['Age'] >= 85,'Age'] = 85

basicSubsetNumericDF = pd.DataFrame()
basicSubsetNumericDF['Age'] = basicSubsetDF['Age']

# transform formal education into an ordinal variable
answerToNumericalDict = {'I prefer not to answer': 10.0,
                         'NaN': 11.0,
                         'I did not complete any formal education past high school': 12.0,
                         'Professional degree': 14.0,
                         "Some college/university study without earning a bachelor's degree": 14.5,
                         "Bachelor's degree": 15.5,
                         "Master's degree": 18.0,
                         "Doctoral degree": 22.0}

basicSubsetNumericDF['Education_Years'] = basicSubsetDF['FormalEducation'].apply(GetDictValueForKey)

# transform tenure into an ordinal variable
answerToNumericalDict = {"I don't write code to analyze data": -0.5,
                         'NaN': 0.0,
                         'Less than a year': 0.5,
                         '1 to 2 years': 1.5,
                         '3 to 5 years': 4.0,
                         '6 to 10 years': 8.0,
                         'More than 10 years': 12.0}

basicSubsetNumericDF['Experience_Years'] = basicSubsetDF['Tenure'].apply(GetDictValueForKey)

# anual salary
basicSubsetNumericDF['annualSalary_USD'] = basicSubsetDF['annualSalary_USD']

# gender to numerical 
answerToNumericalDict = {'Male': -1.0,
                         'NaN': 0.0,
                         'A different identity': 0.0,
                         'Non-binary, genderqueer, or gender non-conforming': 0.0,
                         'Female': 1.0}

basicSubsetNumericDF['Gender'] = basicSubsetDF['GenderSelect'].apply(GetDictValueForKey)

# transform time spent studying to ordinal
answerToNumericalDict = {'NaN': 0.0,
                         '0 - 1 hour': 0.5,
                         '2 - 10 hours': 6.0,
                         '11 - 39 hours': 25.0,
                         '40+': 45.0}

basicSubsetNumericDF['Study_Hours'] = basicSubsetDF['TimeSpentStudying'].apply(GetDictValueForKey)

# add impatience field
basicSubsetNumericDF['impatience'] = basicSubsetDF['impatience']

Output Text:
------------
/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
  from ipykernel import kernelapp as app
/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
  app.launch_new_instance()
/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
/opt/conda/lib/python3.6/site-packages/pandas/core/indexing.py:517: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
  self.obj[item] = s


------------------------------------------------------------
Cell index: 9
Input Cell Type: python
Input Text:
-----------
basicSubsetNumericDF.head(15)

Output Text:
------------
        Age  Education_Years  Experience_Years  annualSalary_USD  Gender  \
0   32.3728             15.5              12.0            0.0000     0.0   
1        30             18.0               0.5            0.0000     1.0   
2        28             18.0               4.0            0.0000    -1.0   
3        56             18.0              12.0          250.0000    -1.0   
4        38             22.0              12.0            0.0000    -1.0   
5        46             22.0               8.0            0.0000    -1.0   
6        35             18.0              12.0            0.0000    -1.0   
7        22             15.5               1.5            0.0000     1.0   
8        43             15.5               4.0           64.1848     1.0   
9        33             15.5               1.5           20.8824    -1.0   
10       20             15.5               0.0            0.0000     1.0   
11       27             15.5               4.0            1.4839    -1.0   
12       26             22.0               1.5            0.0000    -1.0   
13       54             11.0               0.0            0.0000    -1.0   
14       26             18.0               1.5           36.6344    -1.0   

    Study_Hours  impatience  
0           0.0           6  
1           6.0           5  
2           6.0           3  
3           0.0           5  
4           0.0           4  
5           0.0           4  
6           0.0           4  
7           0.5           2  
8           0.0           5  
9           0.0           4  
10         25.0           4  
11          0.0           4  
12          6.0           2  
13          0.0          14  
14          0.0           4  


------------------------------------------------------------
Cell index: 10
Input Cell Type: markdown
Input Text:
-----------
You can unhide the above code to see the mapping of the different features. In short, I've replaced 'NaN' ages with average age, I've tranformed the educations and expreience from string format to approximate number of years. The annual salary is converted based on the provided country exchange rate and presented in units of thousands of dollars per year. For gender, +1 is female, -1 is male, 0 is other. The Impaitiance column is the number of NaN fields in the row for the most basic questions (we will see exactly what they are in the future).   
I would like to take this opportunity to thank [I Coder](https://www.kaggle.com/ash316), I stole a few pieces of code from his script regarding the salary calculation.

Output Text:
------------


------------------------------------------------------------
Cell index: 11
Input Cell Type: markdown
Input Text:
-----------
# Show the correlation matrix of these features

Output Text:
------------


------------------------------------------------------------
Cell index: 12
Input Cell Type: python
Input Text:
-----------
#%% show correlations between the most basic feature
basicSubsetNoisyNumericDF = basicSubsetNumericDF.copy()
basicSubsetNoisyNumericDF['Age'] = basicSubsetNoisyNumericDF['Age'].astype(float)
plt.figure(figsize=(12,10)); plt.title('Basic Features - Correlation Matrix', fontsize=22)
sns.heatmap(basicSubsetNoisyNumericDF.corr(), vmin=-1, vmax=1, fmt='.2f', annot=True, cmap='jet'); 
plt.yticks(rotation=0); plt.xticks(rotation=15);

Output Text:
------------
<matplotlib.figure.Figure at 0x7f2f9887f550>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a correlation matrix, visually represented as a heatmap, titled "Basic Features - Correlation Matrix".  The matrix displays the correlation coefficients between various features, likely derived from a dataset related to individuals' characteristics and salaries.  The features included are: Age, Education_Years, Experience_Years, annualSalary_USD, Gender, Study_Hours, and Impatience.

Each cell in the heatmap represents the correlation between two features. The color intensity and shade indicate the strength and direction of the correlation.  A dark red color signifies a strong positive correlation, while a dark blue indicates a strong negative correlation.  Colors closer to yellow/green represent weaker correlations, and a value near zero implies little to no correlation.  The diagonal of the matrix shows perfect correlation (1.00) as each feature is perfectly correlated with itself. The values within each cell are numerical representations of the correlation coefficients, formatted to two decimal places.  A color bar on the right side provides a legend to interpret the color intensity in terms of correlation strength.

The matrix suggests relationships between the features. For instance, there appears to be a positive correlation between Experience_Years and annualSalary_USD, and also between Education_Years and annualSalary_USD, as indicated by the warmer colors in those cells. Conversely, there's a negative correlation between Impatience and Education_Years, as shown by the blue color. This visualization allows for a quick assessment of the relationships between different features in the dataset.


------------------------------------------------------------
Cell index: 13
Input Cell Type: markdown
Input Text:
-----------
We can see that 'education', 'expreience', 'salary' and 'age' are positivley correlated, as one would expect. We can see that the gender is not correlated with anything, indicating no clear gender bias for the women that do enter the field of data science, which is a good sign.  

And we can see an interesting negative correlation between 'impatiance' and 'education' (and also between  'impatiance' and 'expreience' and 'salary'.  
Even though I would love to write something like "impatiant people are less educated and earn less", but the truth is that this is mainly the result of my descision to fill 'NaN's in the 'education', 'experience', and 'salary' fields as low numeric values.

# Show the pairwise scatter plots of the basic features

Output Text:
------------


------------------------------------------------------------
Cell index: 14
Input Cell Type: python
Input Text:
-----------
for col in ['impatience','Gender','Education_Years','Experience_Years','Study_Hours']:
    basicSubsetNoisyNumericDF[col] *= (1.0 + 0.05*np.random.randn(basicSubsetNoisyNumericDF.shape[0]))
    basicSubsetNoisyNumericDF[col] += 0.3*np.random.randn(basicSubsetNoisyNumericDF.shape[0])

g = sns.pairplot(basicSubsetNoisyNumericDF, diag_kind="kde", plot_kws=dict(s=2, edgecolor="r", alpha=0.1), diag_kws=dict(shade=True));
g.fig.subplots_adjust(top=0.95);
g.fig.suptitle('Basic Features - Pair Scatter Plots', fontsize=30);

Output Text:
------------
<matplotlib.figure.Figure at 0x7f2f90f9c940>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image presents a pair scatter plot matrix, a visualization tool used in statistics to display the relationships between multiple variables simultaneously.  The matrix is square, with each row and column representing one of the seven variables under consideration: Age, Education Years, Experience Years, annualSalary_USD, Gender, Study Hours, and Impatience.

Each cell within the matrix shows a scatter plot illustrating the relationship between the two variables corresponding to its row and column. For instance, the cell at the intersection of "Age" (row) and "Education Years" (column) displays a scatter plot showing the correlation between age and education years. The diagonal cells contain kernel density estimations (KDEs), which represent the probability density function of each individual variable.  The plots use a consistent color scheme (primarily red dots), making it easy to compare the density and distribution patterns across different variable pairs. The title clearly indicates the purpose of the visualization: "Basic Features - Pair Scatter Plots". The plot provides a comprehensive overview of the pairwise relationships among the seven features, allowing for a quick assessment of correlations and data distributions.


------------------------------------------------------------
Cell index: 15
Input Cell Type: markdown
Input Text:
-----------
I've added a little bit of noise to the variables that have a small amount of unique values to get a better feel for the relative amounts.  

# Continue with Data Cleaning - One Hot encode Categoricals

Output Text:
------------


------------------------------------------------------------
Cell index: 16
Input Cell Type: python
Input Text:
-----------
#%% apply whitening on each of the basic numeric features we've seen so far
scaledBasicSubset = preprocessing.StandardScaler().fit_transform(basicSubsetNumericDF.values);
numericDF = pd.DataFrame(scaledBasicSubset,columns=basicSubsetNumericDF.columns);

#%% apply one hot encoding to all other features and add it to our numeric dataframe
listOfColsToOneHotEncode = ['Country','MajorSelect','EmploymentStatus','CurrentJobTitleSelect',
                            'LanguageRecommendationSelect','FirstTrainingSelect','ProveKnowledgeSelect',
                            'AlgorithmUnderstandingLevel','MLMethodNextYearSelect','MLToolNextYearSelect',
                            'HardwarePersonalProjectsSelect','JobSearchResource','EmployerSearchMethod']

for col in listOfColsToOneHotEncode:
    labelEncoder = preprocessing.LabelEncoder()
    labelTfromed = labelEncoder.fit_transform(processedDF[col])
    oneHotEncoder = preprocessing.OneHotEncoder()
    oneHotTformed = oneHotEncoder.fit_transform(labelTfromed.reshape(-1,1))
    currOneHotDF = pd.DataFrame(oneHotTformed.todense(), columns = [col+'_OneHot_'+str(x) for x in range(len(labelEncoder.classes_))])
    numericDF = pd.concat((numericDF,currOneHotDF),axis=1)

Output Text:
------------
/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)


------------------------------------------------------------
Cell index: 17
Input Cell Type: markdown
Input Text:
-----------
# Move on to the "Learning Platform Usefulness" Questions
We first convert the string answers to numeric ordinal values and then apply dimentinality reduction using PCA

Output Text:
------------


------------------------------------------------------------
Cell index: 18
Input Cell Type: python
Input Text:
-----------
#%% add learning platform usefulness features to our numeric dataframe
def GetDictValueForKey(x):
    return answerToNumericalDict[x]

allLearningPlatformColumns = [q for q in allQuestions if q.find('LearningPlatformUsefulness') >= 0]
answerToNumericalDict = {'Not Useful':-1.0,'NaN':0.0,'Somewhat useful':1.0,'Very useful':2.0}

learningUsefulnessOrigDF = processedDF.loc[:,allLearningPlatformColumns]
learningUsefulnessOrigDF = learningUsefulnessOrigDF.applymap(GetDictValueForKey)

# compress cols to eliminate outliers and apply whitening using PCA
numComponents = 12
learningUsefulnessPCAModel = decomposition.PCA(n_components=numComponents,whiten=True)
learningUsefulnessFeatures = learningUsefulnessPCAModel.fit_transform(learningUsefulnessOrigDF)

explainedVarVec = learningUsefulnessPCAModel.explained_variance_ratio_
print('Total explained percent by PCA model with %d components is %.1f%s' %(numComponents, 100*explainedVarVec.sum(),'%'))

newColNames = ['learning_PCA_%d'%(x+1) for x in range(numComponents)]
learningUsefulnessDF = pd.DataFrame(data=learningUsefulnessFeatures, columns=newColNames)

importanceWeight = 0.5
numericDF = pd.concat((numericDF, (importanceWeight/numComponents)*learningUsefulnessDF),axis=1)

Output Text:
------------
Total explained percent by PCA model with 12 components is 89.7%


------------------------------------------------------------
Cell index: 19
Input Cell Type: markdown
Input Text:
-----------
# Apply the same procedure on "Job Skill Importance" Questions

Output Text:
------------


------------------------------------------------------------
Cell index: 20
Input Cell Type: python
Input Text:
-----------
#%% add job skill imporance features to our numeric dataframe
allJobSkillColumns = [q for q in allQuestions if q.find('JobSkillImportance') >= 0] 
answerToNumericalDict = {'Unnecessary':-1.0,'NaN':0.0,'Nice to have':1.0,'Necessary':2.0}

jobSkillOrigDF = processedDF.loc[:,allJobSkillColumns]
jobSkillOrigDF = jobSkillOrigDF.applymap(GetDictValueForKey)

# compress cols to eliminate outliers and apply whitening using PCA
numComponents = 7
jobSkillPCAModel = decomposition.PCA(n_components=numComponents,whiten=True)
jobSkillFeatures = jobSkillPCAModel.fit_transform(jobSkillOrigDF)

explainedVarVec = jobSkillPCAModel.explained_variance_ratio_
print('Total explained percent by PCA model with %d components is %.1f%s' %(numComponents, 100*explainedVarVec.sum(),'%'))

newColNames = ['jobSkill_PCA_%d'%(x+1) for x in range(numComponents)]
jobSkillDF = pd.DataFrame(data=jobSkillFeatures, columns=newColNames)

importanceWeight = 0.5
numericDF = pd.concat((numericDF, (importanceWeight/numComponents)*jobSkillDF),axis=1)

Output Text:
------------
Total explained percent by PCA model with 7 components is 91.1%


------------------------------------------------------------
Cell index: 21
Input Cell Type: markdown
Input Text:
-----------
# Apply the same procedure on "Work Tools and Methods" Questions

Output Text:
------------


------------------------------------------------------------
Cell index: 22
Input Cell Type: python
Input Text:
-----------
#%% add work tools and methods frequency features to our dataframe
allWorkToolsColumns = [q for q in allQuestions if q.find('WorkToolsFrequency') >= 0] 
allWorkMethodsColumns = [q for q in allQuestions if q.find('WorkMethodsFrequency') >= 0] 
answerToNumericalDict = {'NaN':0.0,'Rarely':1.0,'Sometimes':2.0,'Often':3.0,'Most of the time':4.0}

workToolsOrigDF = processedDF.loc[:,allWorkToolsColumns+allWorkMethodsColumns]
workToolsOrigDF = workToolsOrigDF.applymap(GetDictValueForKey)

# compress cols to eliminate outliers and apply whitening using PCA
numComponents = 38
workToolsPCAModel = decomposition.PCA(n_components=numComponents,whiten=True)
workToolsFeatures = workToolsPCAModel.fit_transform(workToolsOrigDF)

explainedVarVec = workToolsPCAModel.explained_variance_ratio_
print('Total explained percent by PCA model with %d components is %.1f%s' %(numComponents, 100*explainedVarVec.sum(),'%'))

newColNames = ['workTools_PCA_%d'%(x+1) for x in range(numComponents)]
workToolsDF = pd.DataFrame(data=workToolsFeatures, columns=newColNames)

importanceWeight = 0.5
numericDF = pd.concat((numericDF, (importanceWeight/numComponents)*workToolsDF),axis=1)

Output Text:
------------
Total explained percent by PCA model with 38 components is 90.7%


------------------------------------------------------------
Cell index: 23
Input Cell Type: markdown
Input Text:
-----------
# Apply the same procedure on "Work Challanges" Questions

Output Text:
------------


------------------------------------------------------------
Cell index: 24
Input Cell Type: python
Input Text:
-----------
#%% add work challanges features to our dataframe
allWorkChallengesColumns = [q for q in allQuestions if q.find('WorkChallengeFrequency') >= 0]
answerToNumericalDict = {'NaN':0.0,'Rarely':1.0,'Sometimes':2.0,'Often':3.0,'Most of the time':4.0}

workChallangesOrigDF = processedDF.loc[:,allWorkChallengesColumns]
workChallangesOrigDF = workChallangesOrigDF.applymap(GetDictValueForKey)

# compress cols to eliminate outliers and apply whitening using PCA
numComponents = 16
workChallengesPCAModel = decomposition.PCA(n_components=numComponents,whiten=True)
workChallengesFeatures = workChallengesPCAModel.fit_transform(workChallangesOrigDF)

explainedVarVec = workChallengesPCAModel.explained_variance_ratio_
print('Total explained percent by PCA model with %d components is %.1f%s' %(numComponents, 100*explainedVarVec.sum(),'%'))

newColNames = ['workChallenges_PCA_%d'%(x+1) for x in range(numComponents)]
workChallengesDF = pd.DataFrame(data=workChallengesFeatures, columns=newColNames)

importanceWeight = 0.5
numericDF = pd.concat((numericDF, (importanceWeight/numComponents)*workChallengesDF),axis=1)

Output Text:
------------
Total explained percent by PCA model with 16 components is 90.3%


------------------------------------------------------------
Cell index: 25
Input Cell Type: markdown
Input Text:
-----------
# Apply the same procedure on "Job Selection Factors" Questions

Output Text:
------------


------------------------------------------------------------
Cell index: 26
Input Cell Type: python
Input Text:
-----------
#%% add job selection factors features to our dataframe
allJobFactorsColumns = [q for q in allQuestions if q.find('JobFactor') >= 0] 
answerToNumericalDict = {'Not important':-1.0,'NaN':0.0,'Somewhat important':1.0,'Very Important':2.0}

jobPreferenceOrigDF = processedDF.loc[:,allJobFactorsColumns]
jobPreferenceOrigDF = jobPreferenceOrigDF.applymap(GetDictValueForKey)

# compress cols to eliminate outliers and apply whitening using PCA
numComponents = 10
jobPreferencePCAModel = decomposition.PCA(n_components=numComponents,whiten=True)
jobPreferenceFeatures = jobPreferencePCAModel.fit_transform(jobPreferenceOrigDF)

explainedVarVec = jobPreferencePCAModel.explained_variance_ratio_
print('Total explained percent by PCA model with %d components is %.1f%s' %(numComponents, 100*explainedVarVec.sum(),'%'))

newColNames = ['jobPreference_PCA_%d'%(x+1) for x in range(numComponents)]
jobPreferenceDF = pd.DataFrame(data=jobPreferenceFeatures, columns=newColNames)

importanceWeight = 0.5
numericDF = pd.concat((numericDF, (importanceWeight/numComponents)*jobPreferenceDF),axis=1)

Output Text:
------------
Total explained percent by PCA model with 10 components is 90.8%


------------------------------------------------------------
Cell index: 27
Input Cell Type: markdown
Input Text:
-----------
# Apply the same procedure on "Time Allocation" Questions

Output Text:
------------


------------------------------------------------------------
Cell index: 28
Input Cell Type: python
Input Text:
-----------
#%% add time allocation distribution features to our dataframe
def ReplaceOnlyNaNs(x):
    if x == 'NaN':
        return 0.0
    else:
        return x

allTimeAllocationColumns = ['TimeGatheringData', 'TimeModelBuilding', 'TimeProduction', 'TimeVisualizing', 'TimeFindingInsights', 'TimeOtherSelect']
timeAllocationOrigDF = processedDF.loc[:,allTimeAllocationColumns]
timeAllocationOrigDF = timeAllocationOrigDF.applymap(ReplaceOnlyNaNs)

numComponents = 4
timeAllocationPCAModel = decomposition.PCA(n_components=numComponents,whiten=True)
timeAllocationFeatures = timeAllocationPCAModel.fit_transform(timeAllocationOrigDF)

explainedVarVec = timeAllocationPCAModel.explained_variance_ratio_
print('Total explained percent by PCA model with %d components is %.1f%s' %(numComponents, 100*explainedVarVec.sum(),'%'))

newColNames = ['timeAllocation_PCA_%d'%(x+1) for x in range(numComponents)]
timeAllocationeDF = pd.DataFrame(data=timeAllocationFeatures, columns=newColNames)

importanceWeight = 0.5
numericDF = pd.concat((numericDF, (importanceWeight/numComponents)*jobPreferenceDF),axis=1)

Output Text:
------------
Total explained percent by PCA model with 4 components is 88.9%


------------------------------------------------------------
Cell index: 29
Input Cell Type: markdown
Input Text:
-----------
# We now Finally have a Numeric Reperesentation of the Dataset
On this representation we can apply kmeans to cluster the data.

Output Text:
------------


------------------------------------------------------------
Cell index: 30
Input Cell Type: python
Input Text:
-----------
numericDF.shape

Output Text:
------------
(16716, 420)


------------------------------------------------------------
Cell index: 31
Input Cell Type: markdown
Input Text:
-----------

# First, let's figure out how many clusters we should use?

Output Text:
------------


------------------------------------------------------------
Cell index: 32
Input Cell Type: python
Input Text:
-----------
#%% we now finally have a numeric representation of the dataset and we are ready to cluster the users
listOfNumClusters = [1,2,4,6,9,12,16,32,64,128,256]
listOfInertia = []
for numClusters in listOfNumClusters:
    KMeansModel = cluster.MiniBatchKMeans(n_clusters=numClusters, batch_size=2100, n_init=5, random_state=1)
    KMeansModel.fit(numericDF)
    listOfInertia.append(KMeansModel.inertia_)
explainedPercent = 100*(1-(np.array(listOfInertia)/listOfInertia[0]))

# plot the explained percent as a function of number of clusters
percentExplainedTarget = 40

numDesiredClusterInd = np.nonzero(explainedPercent > percentExplainedTarget)[0][0]
numDesiredClusters = listOfNumClusters[numDesiredClusterInd]

explainedPercentReached = explainedPercent[numDesiredClusterInd]
plt.figure(figsize=(14,6)); plt.plot(listOfNumClusters,explainedPercent,c='b')
plt.scatter(numDesiredClusters,explainedPercentReached,s=150,c='r')
plt.xlabel('Number of Clusters', fontsize=20); plt.ylabel('Explained Percent', fontsize=20)
plt.title('Desired Number of Clusters = %d, Explained Percent = %.2f%s' %(numDesiredClusters,explainedPercentReached,'%'),fontsize=22);
plt.xlim(-1,listOfNumClusters[-1]+1); plt.ylim(0,60);

Output Text:
------------
<matplotlib.figure.Figure at 0x7f2f23ae1ef0>


Output Images:
--------------

Image 1 Description:
Description of the following image by models/gemini-1.5-flash-002, performed on 2024-10-21:
The image is a line graph illustrating the relationship between the number of clusters in a clustering algorithm and the percentage of variance explained by those clusters.  The horizontal axis represents the number of clusters, ranging from 0 to approximately 250. The vertical axis represents the explained percentage, ranging from 0% to 60%.

A blue line plots the explained percentage as a function of the number of clusters. The line shows a steep initial increase in explained percentage as the number of clusters increases, then gradually flattens out, indicating diminishing returns in explained variance as more clusters are added.  A prominent red circle highlights a point on the curve at approximately 12 clusters and 43.53% explained variance.

The title of the graph explicitly states that the desired number of clusters is 12,  and that this choice corresponds to an explained variance of 43.53%. This suggests the graph is used to justify the selection of 12 clusters as a balance between explanatory power and model complexity.  The elbow method, a common technique for choosing the optimal number of clusters in k-means clustering, is likely being employed here. The red circle likely marks the "elbow point" in the curve, where the rate of increase in explained variance begins to significantly slow down.


------------------------------------------------------------
Cell index: 33
Input Cell Type: markdown
Input Text:
-----------
We can see here that we can't explain too much of the variance in the dataset, even with 256 clusters we only get to ~60% of variance explained.  
This is really because each and everyone of us is a fairly "unique snowflake" that gives different answers to the survey questions, but we'll be practical and make due with what we have and take the smallest amount of clusters that capture the largest amount of variance.

Output Text:
------------


------------------------------------------------------------
Cell index: 34
Input Cell Type: markdown
Input Text:
-----------
# Cluster the dataset with the selected number of clusters (12)

Output Text:
------------


------------------------------------------------------------
Cell index: 35
Input Cell Type: python
Input Text:
-----------
#%% for the selected number of clusters, redo the Kmeans and sort the clusters by frequency
KMeansModel = cluster.KMeans(n_clusters=numDesiredClusters, n_init=15, random_state=10)
KMeansModel.fit(numericDF)

clusterInds = KMeansModel.predict(numericDF)

clusterFrequency = []
for clusterInd in range(numDesiredClusters):
    clusterFrequency.append((clusterInds == clusterInd).sum()/float(len(clusterInds)))
clusterFrequency = np.array(clusterFrequency)
sortedClusterFrequency = np.flipud(np.sort(np.array(clusterFrequency)))
sortedClustersByFrequency = np.flipud(np.argsort(clusterFrequency))

Output Text:
------------


------------------------------------------------------------
Cell index: 36
Input Cell Type: markdown
Input Text:
-----------
# Show a Subset of Question Responses by the 15 Nearest Neighbors of Kaggler Type no. 1:

Output Text:
------------


------------------------------------------------------------
Cell index: 37
Input Cell Type: python
Input Text:
-----------
#%% show the attribures of most frequent kaggler
def GetMstCommonElement(a_list):
    return max(set(a_list), key=a_list.count)

# select cluster
k = 0
selectedCluster = sortedClustersByFrequency[k]

# find nearest neighbors
numNeighbors = 15
distFromCluster = KMeansModel.transform(numericDF)[:,selectedCluster]
distFromCluster[clusterInds != selectedCluster] = np.inf
nearestNeighborInds = np.argsort(distFromCluster)[:numNeighbors]

Output Text:
------------


------------------------------------------------------------
Cell index: 38
Input Cell Type: python
Input Text:
-----------
basicSubsetDF.loc[nearestNeighborInds,:]

Output Text:
------------
        Country GenderSelect Age    FormalEducation        Tenure  \
5487      India         Male  32  Bachelor's degree  3 to 5 years   
5381    Ukraine         Male  26    Master's degree  1 to 2 years   
9600      India         Male  27    Master's degree  1 to 2 years   
8225   Pakistan         Male  24  Bachelor's degree  1 to 2 years   
1533      India         Male  23  Bachelor's degree  1 to 2 years   
7227      India         Male  27    Master's degree  1 to 2 years   
15247     India         Male  24  Bachelor's degree  1 to 2 years   
4826      India         Male  31    Master's degree  3 to 5 years   
10544    Poland         Male  27    Master's degree  1 to 2 years   
7591      India         Male  28    Master's degree  3 to 5 years   
2020      India         Male  26  Bachelor's degree  1 to 2 years   
13817    Brazil         Male  29    Master's degree  3 to 5 years   
3296      India         Male  26    Master's degree  3 to 5 years   
3951     Brazil         Male  35    Master's degree  3 to 5 years   
1503      India         Male  26    Master's degree  1 to 2 years   

       annualSalary_USD             MajorSelect    EmploymentStatus  \
5487          31.240000        Computer Science  Employed full-time   
5381           0.000000        Computer Science  Employed full-time   
9600          18.744000        Computer Science  Employed full-time   
8225           9.096960        Computer Science  Employed full-time   
1533           7.997440        Computer Science  Employed full-time   
7227           3.124000        Computer Science  Employed full-time   
15247          0.000000        Computer Science  Employed full-time   
4826           0.000000  Electrical Engineering  Employed full-time   
10544         10.119744        Computer Science  Employed full-time   
7591          15.620000  Electrical Engineering  Employed full-time   
2020          11.246400        Computer Science  Employed full-time   
13817         33.741750        Computer Science  Employed full-time   
3296          39.050000        Computer Science  Employed full-time   
3951          32.135000        Computer Science  Employed full-time   
1503           0.546700        Computer Science  Employed full-time   

                      CurrentJobTitleSelect LanguageRecommendationSelect  \
5487                     Computer Scientist                       Python   
5381                         Data Scientist                       Python   
9600                         Data Scientist                       Python   
8225                         Data Scientist                       Python   
1533                         Data Scientist                       Python   
7227                         Data Scientist                       Python   
15247                          Data Analyst                       Python   
4826   Software Developer/Software Engineer                       Python   
10544                          Data Analyst                       Python   
7591                           Data Analyst                       Python   
2020   Software Developer/Software Engineer                       Python   
13817  Software Developer/Software Engineer                       Python   
3296              Machine Learning Engineer                       Python   
3951   Software Developer/Software Engineer                       Python   
1503                         Data Scientist                       Python   

      TimeSpentStudying  impatience_basic  impatience_additional  impatience  
5487                NaN                 1                      3           4  
5381                NaN                 1                      3           4  
9600                NaN                 1                      3           4  
8225                NaN                 1                      3           4  
1533                NaN                 1                      3           4  
7227                NaN                 1                      3           4  
15247               NaN                 1                      3           4  
4826                NaN                 1                      3           4  
10544               NaN                 1                      3           4  
7591                NaN                 1                      3           4  
2020                NaN                 1                      3           4  
13817               NaN                 1                      3           4  
3296                NaN                 1                      3           4  
3951                NaN                 1                      3           4  
1503                NaN                 1                      3           4  


------------------------------------------------------------
Cell index: 39
Input Cell Type: python
Input Text:
-----------
additionalSubsetDF.loc[nearestNeighborInds,:]

Output Text:
------------
                               FirstTrainingSelect ProveKnowledgeSelect  \
5487   Online courses (coursera, udemy, edx, etc.)                  NaN   
5381   Online courses (coursera, udemy, edx, etc.)                  NaN   
9600   Online courses (coursera, udemy, edx, etc.)                  NaN   
8225   Online courses (coursera, udemy, edx, etc.)                  NaN   
1533   Online courses (coursera, udemy, edx, etc.)                  NaN   
7227                            University courses                  NaN   
15247  Online courses (coursera, udemy, edx, etc.)                  NaN   
4826   Online courses (coursera, udemy, edx, etc.)                  NaN   
10544                           University courses                  NaN   
7591   Online courses (coursera, udemy, edx, etc.)                  NaN   
2020   Online courses (coursera, udemy, edx, etc.)                  NaN   
13817                           University courses                  NaN   
3296   Online courses (coursera, udemy, edx, etc.)                  NaN   
3951   Online courses (coursera, udemy, edx, etc.)                  NaN   
1503                            University courses                  NaN   

                             AlgorithmUnderstandingLevel  \
5487   Enough to explain the algorithm to someone non...   
5381   Enough to explain the algorithm to someone non...   
9600   Enough to explain the algorithm to someone non...   
8225   Enough to explain the algorithm to someone non...   
1533   Enough to explain the algorithm to someone non...   
7227   Enough to code it again from scratch, albeit i...   
15247  Enough to explain the algorithm to someone non...   
4826   Enough to explain the algorithm to someone non...   
10544  Enough to explain the algorithm to someone non...   
7591   Enough to explain the algorithm to someone non...   
2020   Enough to explain the algorithm to someone non...   
13817  Enough to explain the algorithm to someone non...   
3296   Enough to explain the algorithm to someone non...   
3951   Enough to explain the algorithm to someone non...   
1503   Enough to code it again from scratch, albeit i...   

      MLMethodNextYearSelect MLToolNextYearSelect  \
5487           Deep learning           TensorFlow   
5381           Deep learning           TensorFlow   
9600           Deep learning                Other   
8225           Deep learning           TensorFlow   
1533           Deep learning           TensorFlow   
7227           Deep learning           TensorFlow   
15247          Deep learning           TensorFlow   
4826           Deep learning           TensorFlow   
10544          Deep learning           TensorFlow   
7591           Deep learning           TensorFlow   
2020             Neural Nets           TensorFlow   
13817          Deep learning           TensorFlow   
3296           Deep learning           TensorFlow   
3951           Deep learning           TensorFlow   
1503           Deep learning           TensorFlow   

      HardwarePersonalProjectsSelect JobSearchResource  \
5487                             NaN               NaN   
5381                             NaN               NaN   
9600                             NaN               NaN   
8225                             NaN               NaN   
1533                             NaN               NaN   
7227                             NaN               NaN   
15247                            NaN               NaN   
4826                             NaN               NaN   
10544                            NaN               NaN   
7591                             NaN               NaN   
2020                             NaN               NaN   
13817                            NaN               NaN   
3296                             NaN               NaN   
3951                             NaN               NaN   
1503                             NaN               NaN   

                                    EmployerSearchMethod  
5487   A friend, family member, or former colleague t...  
5381   I was contacted directly by someone at the com...  
9600         A career fair or on-campus recruiting event  
8225   A friend, family member, or former colleague t...  
1533                         A general-purpose job board  
7227   I was contacted directly by someone at the com...  
15247        A career fair or on-campus recruiting event  
4826   I was contacted directly by someone at the com...  
10544  A friend, family member, or former colleague t...  
7591                         A general-purpose job board  
2020   A friend, family member, or former colleague t...  
13817  A friend, family member, or former colleague t...  
3296                 An external recruiter or headhunter  
3951   A friend, family member, or former colleague t...  
1503         A career fair or on-campus recruiting event  


------------------------------------------------------------
Cell index: 40
Input Cell Type: python
Input Text:
-----------
#show original data for neighbors
print('-'*40)
print('Represents %.1f%s of kagglers' %(100.0*sortedClusterFrequency[k],'%'))
print('Average Age = %.1f' %(basicSubsetDF.loc[nearestNeighborInds,'Age'].astype(float).mean()))
print('Average Salary in USD = %.2fK' %(basicSubsetDF.loc[nearestNeighborInds,'annualSalary_USD'].astype(float).mean()))
print('Most Common Gender is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'GenderSelect'].tolist()))
print('Most Common Country is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'Country'].tolist()))
print('Most Common Formal Education is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'FormalEducation'].tolist()))
print('Most Common Major is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'MajorSelect'].tolist()))
print('Most Common Employment Status is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'EmploymentStatus'].tolist()))
print('Most Common Tenure is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'Tenure'].tolist()))
print('Most Common Job title is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'CurrentJobTitleSelect'].tolist()))
print('Most Common First training is "%s"' %GetMstCommonElement(additionalSubsetDF.loc[nearestNeighborInds,'FirstTrainingSelect'].tolist()))
print('-'*40)

Output Text:
------------
----------------------------------------
Represents 14.6% of kagglers
Average Age = 27.4
Average Salary in USD = 14.18K
Most Common Gender is "Male"
Most Common Country is "India"
Most Common Formal Education is "Master's degree"
Most Common Major is "Computer Science"
Most Common Employment Status is "Employed full-time"
Most Common Tenure is "1 to 2 years"
Most Common Job title is "Data Scientist"
Most Common First training is "Online courses (coursera, udemy, edx, etc.)"
----------------------------------------


------------------------------------------------------------
Cell index: 41
Input Cell Type: markdown
Input Text:
-----------
# Paragraph Summery of Kaggler Type no. 1:
The most frequent kaggler (accounts for slightly less than 15% of kagglers) is a ~28 year old male from India, 
he is employed full-time as a Software Developer or a Data Scientist, he has some industry experience, 
he majored in Computer Science during University and holds a Master's degree. His annual salary is about 14K dollars a year. He is looking forward to learn about deep learning in the upcoming year and specifically using tensorflow.

Output Text:
------------


------------------------------------------------------------
Cell index: 42
Input Cell Type: markdown
Input Text:
-----------
# Nearest Neighbors of Kaggler Type no. 2:

Output Text:
------------


------------------------------------------------------------
Cell index: 43
Input Cell Type: python
Input Text:
-----------
#%% show the attribures of most frequent kaggler
def GetMstCommonElement(a_list):
    return max(set(a_list), key=a_list.count)

# select cluster
k = 1
selectedCluster = sortedClustersByFrequency[k]

# find nearest neighbors
numNeighbors = 15
distFromCluster = KMeansModel.transform(numericDF)[:,selectedCluster]
distFromCluster[clusterInds != selectedCluster] = np.inf
nearestNeighborInds = np.argsort(distFromCluster)[:numNeighbors]

Output Text:
------------


------------------------------------------------------------
Cell index: 44
Input Cell Type: python
Input Text:
-----------
basicSubsetDF.loc[nearestNeighborInds,:]

Output Text:
------------
             Country GenderSelect Age    FormalEducation            Tenure  \
16575          India         Male  32  Bachelor's degree      1 to 2 years   
15405          India         Male  25  Bachelor's degree  Less than a year   
7947           India         Male  25  Bachelor's degree  Less than a year   
15143          India         Male  27    Master's degree      1 to 2 years   
7578           India         Male  27  Bachelor's degree  Less than a year   
6969           India         Male  24  Bachelor's degree  Less than a year   
12759          India         Male  27    Master's degree      1 to 2 years   
16223          India         Male  27    Master's degree      1 to 2 years   
15827  United States         Male  32  Bachelor's degree  Less than a year   
8456           India         Male  30  Bachelor's degree  Less than a year   
8846           India         Male  23  Bachelor's degree  Less than a year   
15687          India         Male  23  Bachelor's degree  Less than a year   
8612           India         Male  28  Bachelor's degree      1 to 2 years   
8970           India         Male  25  Bachelor's degree      3 to 5 years   
15492          India         Male  24  Bachelor's degree      1 to 2 years   

       annualSalary_USD       MajorSelect    EmploymentStatus  \
16575               0.0  Computer Science  Employed full-time   
15405               0.0  Computer Science  Employed full-time   
7947                0.0  Computer Science  Employed full-time   
15143               0.0  Computer Science  Employed full-time   
7578                0.0  Computer Science  Employed full-time   
6969                0.0  Computer Science  Employed full-time   
12759               0.0  Computer Science  Employed full-time   
16223               0.0  Computer Science  Employed full-time   
15827               0.0  Computer Science  Employed full-time   
8456                0.0  Computer Science  Employed full-time   
8846                0.0  Computer Science  Employed full-time   
15687               0.0  Computer Science  Employed full-time   
8612                0.0  Computer Science  Employed full-time   
8970                0.0  Computer Science  Employed full-time   
15492               0.0  Computer Science  Employed full-time   

                      CurrentJobTitleSelect LanguageRecommendationSelect  \
16575  Software Developer/Software Engineer                          NaN   
15405  Software Developer/Software Engineer                          NaN   
7947   Software Developer/Software Engineer                          NaN   
15143                                 Other                          NaN   
7578                           Data Analyst                          NaN   
6969   Software Developer/Software Engineer                          NaN   
12759  Software Developer/Software Engineer                          NaN   
16223  Software Developer/Software Engineer                          NaN   
15827  Software Developer/Software Engineer                          NaN   
8456   Software Developer/Software Engineer                          NaN   
8846   Software Developer/Software Engineer                          NaN   
15687  Software Developer/Software Engineer                          NaN   
8612                           Data Analyst                          NaN   
8970   Software Developer/Software Engineer                          NaN   
15492             Machine Learning Engineer                          NaN   

      TimeSpentStudying  impatience_basic  impatience_additional  impatience  
16575               NaN                 2                      7           9  
15405               NaN                 2                      7           9  
7947                NaN                 2                      7           9  
15143               NaN                 2                      7           9  
7578                NaN                 2                      7           9  
6969                NaN                 2                      7           9  
12759               NaN                 2                      7           9  
16223               NaN                 2                      7           9  
15827               NaN                 2                      7           9  
8456                NaN                 2                      7           9  
8846                NaN                 2                      7           9  
15687               NaN                 2                      7           9  
8612                NaN                 2                      7           9  
8970                NaN                 2                      7           9  
15492               NaN                 2                      7           9  


------------------------------------------------------------
Cell index: 45
Input Cell Type: python
Input Text:
-----------
additionalSubsetDF.loc[nearestNeighborInds,:]

Output Text:
------------
                               FirstTrainingSelect ProveKnowledgeSelect  \
16575  Online courses (coursera, udemy, edx, etc.)                  NaN   
15405  Online courses (coursera, udemy, edx, etc.)                  NaN   
7947   Online courses (coursera, udemy, edx, etc.)                  NaN   
15143  Online courses (coursera, udemy, edx, etc.)                  NaN   
7578   Online courses (coursera, udemy, edx, etc.)                  NaN   
6969   Online courses (coursera, udemy, edx, etc.)                  NaN   
12759                                  Self-taught                  NaN   
16223                                  Self-taught                  NaN   
15827  Online courses (coursera, udemy, edx, etc.)                  NaN   
8456                                   Self-taught                  NaN   
8846   Online courses (coursera, udemy, edx, etc.)                  NaN   
15687  Online courses (coursera, udemy, edx, etc.)                  NaN   
8612                                   Self-taught                  NaN   
8970   Online courses (coursera, udemy, edx, etc.)                  NaN   
15492  Online courses (coursera, udemy, edx, etc.)                  NaN   

      AlgorithmUnderstandingLevel MLMethodNextYearSelect MLToolNextYearSelect  \
16575                         NaN                    NaN                  NaN   
15405                         NaN                    NaN                  NaN   
7947                          NaN                    NaN                  NaN   
15143                         NaN                    NaN                  NaN   
7578                          NaN                    NaN                  NaN   
6969                          NaN                    NaN                  NaN   
12759                         NaN                    NaN                  NaN   
16223                         NaN                    NaN                  NaN   
15827                         NaN                    NaN                  NaN   
8456                          NaN                    NaN                  NaN   
8846                          NaN                    NaN                  NaN   
15687                         NaN                    NaN                  NaN   
8612                          NaN                    NaN                  NaN   
8970                          NaN                    NaN                  NaN   
15492                         NaN                    NaN                  NaN   

      HardwarePersonalProjectsSelect JobSearchResource EmployerSearchMethod  
16575                            NaN               NaN                  NaN  
15405                            NaN               NaN                  NaN  
7947                             NaN               NaN                  NaN  
15143                            NaN               NaN                  NaN  
7578                             NaN               NaN                  NaN  
6969                             NaN               NaN                  NaN  
12759                            NaN               NaN                  NaN  
16223                            NaN               NaN                  NaN  
15827                            NaN               NaN                  NaN  
8456                             NaN               NaN                  NaN  
8846                             NaN               NaN                  NaN  
15687                            NaN               NaN                  NaN  
8612                             NaN               NaN                  NaN  
8970                             NaN               NaN                  NaN  
15492                            NaN               NaN                  NaN  


------------------------------------------------------------
Cell index: 46
Input Cell Type: python
Input Text:
-----------
#show original data for neighbors
print('-'*40)
print('Represents %.1f%s of kagglers' %(100.0*sortedClusterFrequency[k],'%'))
print('Average Age = %.1f' %(basicSubsetDF.loc[nearestNeighborInds,'Age'].astype(float).mean()))
print('Average Salary in USD = %.2fK' %(basicSubsetDF.loc[nearestNeighborInds,'annualSalary_USD'].astype(float).mean()))
print('Most Common Gender is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'GenderSelect'].tolist()))
print('Most Common Country is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'Country'].tolist()))
print('Most Common Formal Education is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'FormalEducation'].tolist()))
print('Most Common Major is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'MajorSelect'].tolist()))
print('Most Common Employment Status is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'EmploymentStatus'].tolist()))
print('Most Common Tenure is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'Tenure'].tolist()))
print('Most Common Job title is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'CurrentJobTitleSelect'].tolist()))
print('Most Common First training is "%s"' %GetMstCommonElement(additionalSubsetDF.loc[nearestNeighborInds,'FirstTrainingSelect'].tolist()))
print('-'*40)

Output Text:
------------
----------------------------------------
Represents 13.1% of kagglers
Average Age = 26.6
Average Salary in USD = 0.00K
Most Common Gender is "Male"
Most Common Country is "India"
Most Common Formal Education is "Bachelor's degree"
Most Common Major is "Computer Science"
Most Common Employment Status is "Employed full-time"
Most Common Tenure is "Less than a year"
Most Common Job title is "Software Developer/Software Engineer"
Most Common First training is "Online courses (coursera, udemy, edx, etc.)"
----------------------------------------


------------------------------------------------------------
Cell index: 47
Input Cell Type: markdown
Input Text:
-----------
# Paragraph Summery of Kaggler Type no. 2:
The second most frequent kaggler (accounts for slightly less than 13% of kagglers) is a ~27 year old male from India, 
he is employed full-time as a Software Developer, has little industry experience, he has majored in Computer Science during university, holds a Bachelor's degree.

Output Text:
------------


------------------------------------------------------------
Cell index: 48
Input Cell Type: markdown
Input Text:
-----------
# Nearest Neighbors of Kaggler Type no. 3:

Output Text:
------------


------------------------------------------------------------
Cell index: 49
Input Cell Type: python
Input Text:
-----------
#%% show the attribures of most frequent kaggler
def GetMstCommonElement(a_list):
    return max(set(a_list), key=a_list.count)

# select cluster
k = 2
selectedCluster = sortedClustersByFrequency[k]

# find nearest neighbors
numNeighbors = 15
distFromCluster = KMeansModel.transform(numericDF)[:,selectedCluster]
distFromCluster[clusterInds != selectedCluster] = np.inf
nearestNeighborInds = np.argsort(distFromCluster)[:numNeighbors]

Output Text:
------------


------------------------------------------------------------
Cell index: 50
Input Cell Type: python
Input Text:
-----------
basicSubsetDF.loc[nearestNeighborInds,:]

Output Text:
------------
             Country GenderSelect Age FormalEducation Tenure  \
13684  United States         Male  34             NaN    NaN   
11296  United States         Male  34             NaN    NaN   
4058   United States         Male  34             NaN    NaN   
4701   United States         Male  34             NaN    NaN   
7554   United States         Male  34             NaN    NaN   
14017  United States         Male  34             NaN    NaN   
15503  United States         Male  34             NaN    NaN   
5164   United States         Male  34             NaN    NaN   
293    United States         Male  34             NaN    NaN   
5126   United States         Male  34             NaN    NaN   
13654  United States         Male  34             NaN    NaN   
7383   United States         Male  34             NaN    NaN   
3814   United States         Male  33             NaN    NaN   
10427  United States         Male  33             NaN    NaN   
14353  United States         Male  33             NaN    NaN   

       annualSalary_USD MajorSelect    EmploymentStatus CurrentJobTitleSelect  \
13684               0.0         NaN  Employed full-time                   NaN   
11296               0.0         NaN  Employed full-time                   NaN   
4058                0.0         NaN  Employed full-time                   NaN   
4701                0.0         NaN  Employed full-time                   NaN   
7554                0.0         NaN  Employed full-time                   NaN   
14017               0.0         NaN  Employed full-time                   NaN   
15503               0.0         NaN  Employed full-time                   NaN   
5164                0.0         NaN  Employed full-time                   NaN   
293                 0.0         NaN  Employed full-time                   NaN   
5126                0.0         NaN  Employed full-time                   NaN   
13654               0.0         NaN  Employed full-time                   NaN   
7383                0.0         NaN  Employed full-time                   NaN   
3814                0.0         NaN  Employed full-time                   NaN   
10427               0.0         NaN  Employed full-time                   NaN   
14353               0.0         NaN  Employed full-time                   NaN   

      LanguageRecommendationSelect TimeSpentStudying  impatience_basic  \
13684                          NaN               NaN                 6   
11296                          NaN               NaN                 6   
4058                           NaN               NaN                 6   
4701                           NaN               NaN                 6   
7554                           NaN               NaN                 6   
14017                          NaN               NaN                 6   
15503                          NaN               NaN                 6   
5164                           NaN               NaN                 6   
293                            NaN               NaN                 6   
5126                           NaN               NaN                 6   
13654                          NaN               NaN                 6   
7383                           NaN               NaN                 6   
3814                           NaN               NaN                 6   
10427                          NaN               NaN                 6   
14353                          NaN               NaN                 6   

       impatience_additional  impatience  
13684                      8          14  
11296                      8          14  
4058                       8          14  
4701                       8          14  
7554                       8          14  
14017                      8          14  
15503                      8          14  
5164                       8          14  
293                        8          14  
5126                       8          14  
13654                      8          14  
7383                       8          14  
3814                       8          14  
10427                      8          14  
14353                      8          14  


------------------------------------------------------------
Cell index: 51
Input Cell Type: python
Input Text:
-----------
additionalSubsetDF.loc[nearestNeighborInds,:]

Output Text:
------------
      FirstTrainingSelect ProveKnowledgeSelect AlgorithmUnderstandingLevel  \
13684                 NaN                  NaN                         NaN   
11296                 NaN                  NaN                         NaN   
4058                  NaN                  NaN                         NaN   
4701                  NaN                  NaN                         NaN   
7554                  NaN                  NaN                         NaN   
14017                 NaN                  NaN                         NaN   
15503                 NaN                  NaN                         NaN   
5164                  NaN                  NaN                         NaN   
293                   NaN                  NaN                         NaN   
5126                  NaN                  NaN                         NaN   
13654                 NaN                  NaN                         NaN   
7383                  NaN                  NaN                         NaN   
3814                  NaN                  NaN                         NaN   
10427                 NaN                  NaN                         NaN   
14353                 NaN                  NaN                         NaN   

      MLMethodNextYearSelect MLToolNextYearSelect  \
13684                    NaN                  NaN   
11296                    NaN                  NaN   
4058                     NaN                  NaN   
4701                     NaN                  NaN   
7554                     NaN                  NaN   
14017                    NaN                  NaN   
15503                    NaN                  NaN   
5164                     NaN                  NaN   
293                      NaN                  NaN   
5126                     NaN                  NaN   
13654                    NaN                  NaN   
7383                     NaN                  NaN   
3814                     NaN                  NaN   
10427                    NaN                  NaN   
14353                    NaN                  NaN   

      HardwarePersonalProjectsSelect JobSearchResource EmployerSearchMethod  
13684                            NaN               NaN                  NaN  
11296                            NaN               NaN                  NaN  
4058                             NaN               NaN                  NaN  
4701                             NaN               NaN                  NaN  
7554                             NaN               NaN                  NaN  
14017                            NaN               NaN                  NaN  
15503                            NaN               NaN                  NaN  
5164                             NaN               NaN                  NaN  
293                              NaN               NaN                  NaN  
5126                             NaN               NaN                  NaN  
13654                            NaN               NaN                  NaN  
7383                             NaN               NaN                  NaN  
3814                             NaN               NaN                  NaN  
10427                            NaN               NaN                  NaN  
14353                            NaN               NaN                  NaN  


------------------------------------------------------------
Cell index: 52
Input Cell Type: python
Input Text:
-----------
#show original data for neighbors
print('-'*40)
print('Represents %.1f%s of kagglers' %(100.0*sortedClusterFrequency[k],'%'))
print('Average Age = %.1f' %(basicSubsetDF.loc[nearestNeighborInds,'Age'].astype(float).mean()))
print('Average Salary in USD = %.2fK' %(basicSubsetDF.loc[nearestNeighborInds,'annualSalary_USD'].astype(float).mean()))
print('Most Common Gender is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'GenderSelect'].tolist()))
print('Most Common Country is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'Country'].tolist()))
print('Most Common Formal Education is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'FormalEducation'].tolist()))
print('Most Common Major is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'MajorSelect'].tolist()))
print('Most Common Employment Status is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'EmploymentStatus'].tolist()))
print('Most Common Tenure is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'Tenure'].tolist()))
print('Most Common Job title is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'CurrentJobTitleSelect'].tolist()))
print('Most Common First training is "%s"' %GetMstCommonElement(additionalSubsetDF.loc[nearestNeighborInds,'FirstTrainingSelect'].tolist()))
print('-'*40)

Output Text:
------------
----------------------------------------
Represents 11.1% of kagglers
Average Age = 33.8
Average Salary in USD = 0.00K
Most Common Gender is "Male"
Most Common Country is "United States"
Most Common Formal Education is "NaN"
Most Common Major is "NaN"
Most Common Employment Status is "Employed full-time"
Most Common Tenure is "NaN"
Most Common Job title is "NaN"
Most Common First training is "NaN"
----------------------------------------


------------------------------------------------------------
Cell index: 53
Input Cell Type: markdown
Input Text:
-----------
# Paragraph Summery of Kaggler Type no. 3:
The third most frequent kaggler (accounts for about 11% of kagglers) is a 34 year old male from the US, he is employed full-time, and doesn't really have the time to fill out an internet survey.

Output Text:
------------


------------------------------------------------------------
Cell index: 54
Input Cell Type: markdown
Input Text:
-----------
# Nearest Neighbors of Kaggler Type no. 4:

Output Text:
------------


------------------------------------------------------------
Cell index: 55
Input Cell Type: python
Input Text:
-----------
#%% show the attribures of most frequent kaggler
def GetMstCommonElement(a_list):
    return max(set(a_list), key=a_list.count)

# select cluster
k = 3
selectedCluster = sortedClustersByFrequency[k]

# find nearest neighbors
numNeighbors = 15
distFromCluster = KMeansModel.transform(numericDF)[:,selectedCluster]
distFromCluster[clusterInds != selectedCluster] = np.inf
nearestNeighborInds = np.argsort(distFromCluster)[:numNeighbors]

Output Text:
------------


------------------------------------------------------------
Cell index: 56
Input Cell Type: python
Input Text:
-----------
basicSubsetDF.loc[nearestNeighborInds,:]

Output Text:
------------
             Country GenderSelect Age  FormalEducation        Tenure  \
4983   United States       Female  27  Master's degree  3 to 5 years   
16310  United States       Female  27  Master's degree  3 to 5 years   
6656   United States       Female  29  Master's degree  3 to 5 years   
15403  United States       Female  28  Master's degree  1 to 2 years   
11620  United States       Female  26  Master's degree  3 to 5 years   
15860  United States       Female  27  Master's degree  3 to 5 years   
14365  United States       Female  31  Master's degree  1 to 2 years   
12239  United States       Female  26  Master's degree  3 to 5 years   
11673  United States       Female  29  Master's degree  3 to 5 years   
10247  United States       Female  27  Master's degree  3 to 5 years   
11582  United States       Female  35  Master's degree  3 to 5 years   
15967          Other       Female  31  Master's degree  3 to 5 years   
15444  United States       Female  32  Master's degree  3 to 5 years   
16553  United States       Female  26  Master's degree  1 to 2 years   
11133  United States       Female  33  Master's degree  1 to 2 years   

       annualSalary_USD                         MajorSelect  \
4983                0.0                    Computer Science   
16310               0.0           Mathematics or statistics   
6656                0.0                    Computer Science   
15403               0.0                    Computer Science   
11620               0.0                    Computer Science   
15860               0.0                    Computer Science   
14365               0.0                    Computer Science   
12239               0.0                    Computer Science   
11673               0.0             A humanities discipline   
10247               0.0           Mathematics or statistics   
11582               0.0           Mathematics or statistics   
15967               0.0                    Computer Science   
15444               0.0  Engineering (non-computer focused)   
16553               0.0        Fine arts or performing arts   
11133               0.0                               Other   

         EmploymentStatus                 CurrentJobTitleSelect  \
4983   Employed full-time                        Data Scientist   
16310  Employed full-time                          Data Analyst   
6656   Employed full-time                                 Other   
15403  Employed full-time                  Scientist/Researcher   
11620  Employed full-time  Software Developer/Software Engineer   
15860  Employed full-time  Software Developer/Software Engineer   
14365  Employed full-time  Software Developer/Software Engineer   
12239  Employed full-time                                 Other   
11673  Employed full-time                        Data Scientist   
10247  Employed full-time  Software Developer/Software Engineer   
11582  Employed full-time                        Data Scientist   
15967  Employed full-time                        Data Scientist   
15444  Employed full-time                          Data Analyst   
16553  Employed full-time                        Data Scientist   
11133  Employed full-time                        Data Scientist   

      LanguageRecommendationSelect TimeSpentStudying  impatience_basic  \
4983                           NaN               NaN                 2   
16310                          NaN               NaN                 2   
6656                           NaN               NaN                 2   
15403                          NaN               NaN                 2   
11620                          NaN               NaN                 2   
15860                          NaN               NaN                 2   
14365                          NaN               NaN                 2   
12239                          NaN               NaN                 2   
11673                          NaN               NaN                 2   
10247                          NaN               NaN                 2   
11582                          NaN               NaN                 2   
15967                          NaN               NaN                 2   
15444                          NaN               NaN                 2   
16553                          NaN               NaN                 2   
11133                          NaN               NaN                 2   

       impatience_additional  impatience  
4983                       5           7  
16310                      6           8  
6656                       6           8  
15403                      6           8  
11620                      7           9  
15860                      6           8  
14365                      7           9  
12239                      5           7  
11673                      7           9  
10247                      6           8  
11582                      6           8  
15967                      7           9  
15444                      6           8  
16553                      6           8  
11133                      5           7  


------------------------------------------------------------
Cell index: 57
Input Cell Type: python
Input Text:
-----------
additionalSubsetDF.loc[nearestNeighborInds,:]

Output Text:
------------
      FirstTrainingSelect ProveKnowledgeSelect  \
4983   University courses                  NaN   
16310  University courses                  NaN   
6656   University courses                  NaN   
15403  University courses                  NaN   
11620  University courses                  NaN   
15860  University courses                  NaN   
14365  University courses                  NaN   
12239  University courses                  NaN   
11673  University courses                  NaN   
10247  University courses                  NaN   
11582  University courses                  NaN   
15967  University courses                  NaN   
15444  University courses                  NaN   
16553  University courses                  NaN   
11133  University courses                  NaN   

                             AlgorithmUnderstandingLevel  \
4983   Enough to explain the algorithm to someone non...   
16310                                                NaN   
6656                                                 NaN   
15403                                                NaN   
11620                                                NaN   
15860                                                NaN   
14365                                                NaN   
12239  Enough to explain the algorithm to someone non...   
11673                                                NaN   
10247                                                NaN   
11582                                                NaN   
15967                                                NaN   
15444                                                NaN   
16553                                                NaN   
11133  Enough to explain the algorithm to someone non...   

      MLMethodNextYearSelect MLToolNextYearSelect  \
4983                     NaN                  NaN   
16310                    NaN                  NaN   
6656                     NaN                  NaN   
15403                    NaN                  NaN   
11620                    NaN                  NaN   
15860                    NaN                  NaN   
14365                    NaN                  NaN   
12239                    NaN                  NaN   
11673                    NaN                  NaN   
10247                    NaN                  NaN   
11582                    NaN                  NaN   
15967                    NaN                  NaN   
15444                    NaN                  NaN   
16553                    NaN                  NaN   
11133                    NaN                  NaN   

      HardwarePersonalProjectsSelect JobSearchResource  \
4983                             NaN               NaN   
16310                            NaN               NaN   
6656                             NaN               NaN   
15403                            NaN               NaN   
11620                            NaN               NaN   
15860                            NaN               NaN   
14365                            NaN               NaN   
12239                            NaN               NaN   
11673                            NaN               NaN   
10247                            NaN               NaN   
11582                            NaN               NaN   
15967                            NaN               NaN   
15444                            NaN               NaN   
16553                            NaN               NaN   
11133                            NaN               NaN   

                                    EmployerSearchMethod  
4983   A friend, family member, or former colleague t...  
16310  I was contacted directly by someone at the com...  
6656   I was contacted directly by someone at the com...  
15403  A friend, family member, or former colleague t...  
11620                                                NaN  
15860        A career fair or on-campus recruiting event  
14365                                                NaN  
12239  I was contacted directly by someone at the com...  
11673                                                NaN  
10247                        A general-purpose job board  
11582                                     Some other way  
15967                                                NaN  
15444        A career fair or on-campus recruiting event  
16553  A friend, family member, or former colleague t...  
11133  I was contacted directly by someone at the com...  


------------------------------------------------------------
Cell index: 58
Input Cell Type: python
Input Text:
-----------
#show original data for neighbors
print('-'*40)
print('Represents %.1f%s of kagglers' %(100.0*sortedClusterFrequency[k],'%'))
print('Average Age = %.1f' %(basicSubsetDF.loc[nearestNeighborInds,'Age'].astype(float).mean()))
print('Average Salary in USD = %.2fK' %(basicSubsetDF.loc[nearestNeighborInds,'annualSalary_USD'].astype(float).mean()))
print('Most Common Gender is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'GenderSelect'].tolist()))
print('Most Common Country is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'Country'].tolist()))
print('Most Common Formal Education is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'FormalEducation'].tolist()))
print('Most Common Major is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'MajorSelect'].tolist()))
print('Most Common Employment Status is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'EmploymentStatus'].tolist()))
print('Most Common Tenure is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'Tenure'].tolist()))
print('Most Common Job title is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'CurrentJobTitleSelect'].tolist()))
print('Most Common First training is "%s"' %GetMstCommonElement(additionalSubsetDF.loc[nearestNeighborInds,'FirstTrainingSelect'].tolist()))
print('-'*40)

Output Text:
------------
----------------------------------------
Represents 8.8% of kagglers
Average Age = 28.9
Average Salary in USD = 0.00K
Most Common Gender is "Female"
Most Common Country is "United States"
Most Common Formal Education is "Master's degree"
Most Common Major is "Computer Science"
Most Common Employment Status is "Employed full-time"
Most Common Tenure is "3 to 5 years"
Most Common Job title is "Data Scientist"
Most Common First training is "University courses"
----------------------------------------


------------------------------------------------------------
Cell index: 59
Input Cell Type: markdown
Input Text:
-----------
# Paragraph Summery of Kaggler Type no. 4:
The forth most frequent kaggler (accounts for about 9% of kagglers) is a ~29 year old female from the Unites States, she is employed full-time as a data scientist, has 3-5 years of industry experience, her background is Computer Science and she holds a Master's degree. She didn't share her salary but we can infer that it's most likely an OK salary.

Output Text:
------------


------------------------------------------------------------
Cell index: 60
Input Cell Type: markdown
Input Text:
-----------
# Nearest Neighbors of Kaggler Type no. 5:

Output Text:
------------


------------------------------------------------------------
Cell index: 61
Input Cell Type: python
Input Text:
-----------
#%% show the attribures of most frequent kaggler
def GetMstCommonElement(a_list):
    return max(set(a_list), key=a_list.count)

# select cluster
k = 4
selectedCluster = sortedClustersByFrequency[k]

# find nearest neighbors
numNeighbors = 15
distFromCluster = KMeansModel.transform(numericDF)[:,selectedCluster]
distFromCluster[clusterInds != selectedCluster] = np.inf
nearestNeighborInds = np.argsort(distFromCluster)[:numNeighbors]

Output Text:
------------


------------------------------------------------------------
Cell index: 62
Input Cell Type: python
Input Text:
-----------
basicSubsetDF.loc[nearestNeighborInds,:]

Output Text:
------------
              Country GenderSelect Age  FormalEducation         Tenure  \
5023    United States         Male  27  Master's degree   3 to 5 years   
14249   United States         Male  32  Master's degree  6 to 10 years   
11605   United States         Male  26  Master's degree   3 to 5 years   
13081         Denmark         Male  30  Master's degree   3 to 5 years   
13040   United States         Male  33  Master's degree   3 to 5 years   
1086    United States         Male  30  Master's degree   3 to 5 years   
14099   United States         Male  32  Master's degree   3 to 5 years   
7977    United States         Male  26  Master's degree   3 to 5 years   
10807       Singapore         Male  36  Master's degree   3 to 5 years   
4028    United States         Male  27  Master's degree   3 to 5 years   
5712   United Kingdom         Male  32  Master's degree   3 to 5 years   
12076   United States         Male  33  Master's degree   3 to 5 years   
11402   United States         Male  32  Master's degree   3 to 5 years   
5083    United States         Male  34  Master's degree   3 to 5 years   
5002    United States         Male  26  Master's degree   3 to 5 years   

       annualSalary_USD                         MajorSelect  \
5023           95.00000                    Computer Science   
14249         107.00000           Mathematics or statistics   
11605          90.00000  Engineering (non-computer focused)   
13081          80.36500                    Computer Science   
13040          80.00000                                 NaN   
1086           90.00000                    A social science   
14099          89.50000                               Other   
7977          100.00000                    Computer Science   
10807          75.00000           Mathematics or statistics   
4028           95.00000                    Computer Science   
5712           86.07222           Mathematics or statistics   
12076         115.00000           Mathematics or statistics   
11402          95.00000                    A social science   
5083           80.00000           Mathematics or statistics   
5002           95.00000                    A social science   

         EmploymentStatus      CurrentJobTitleSelect  \
5023   Employed full-time             Data Scientist   
14249  Employed full-time             Data Scientist   
11605  Employed full-time             Data Scientist   
13081  Employed full-time             Data Scientist   
13040  Employed full-time             Data Scientist   
1086   Employed full-time             Data Scientist   
14099  Employed full-time             Data Scientist   
7977   Employed full-time               Data Analyst   
10807  Employed full-time             Data Scientist   
4028   Employed full-time  Machine Learning Engineer   
5712   Employed full-time               Data Analyst   
12076  Employed full-time               Statistician   
11402  Employed full-time             Data Scientist   
5083   Employed full-time             Data Scientist   
5002   Employed full-time             Data Scientist   

      LanguageRecommendationSelect TimeSpentStudying  impatience_basic  \
5023                        Python               NaN                 1   
14249                       Python               NaN                 1   
11605                       Python               NaN                 1   
13081                       Python               NaN                 1   
13040                       Python               NaN                 2   
1086                             R               NaN                 1   
14099                       Python               NaN                 1   
7977                        Python               NaN                 1   
10807                       Python               NaN                 1   
4028                        Python               NaN                 1   
5712                        Python               NaN                 1   
12076                       Python               NaN                 1   
11402                            R               NaN                 1   
5083                        Python               NaN                 1   
5002                        Python               NaN                 1   

       impatience_additional  impatience  
5023                       3           4  
14249                      3           4  
11605                      3           4  
13081                      3           4  
13040                      3           5  
1086                       3           4  
14099                      3           4  
7977                       3           4  
10807                      3           4  
4028                       3           4  
5712                       3           4  
12076                      3           4  
11402                      3           4  
5083                       3           4  
5002                       3           4  


------------------------------------------------------------
Cell index: 63
Input Cell Type: python
Input Text:
-----------
additionalSubsetDF.loc[nearestNeighborInds,:]

Output Text:
------------
                               FirstTrainingSelect ProveKnowledgeSelect  \
5023                            University courses                  NaN   
14249                           University courses                  NaN   
11605  Online courses (coursera, udemy, edx, etc.)                  NaN   
13081                           University courses                  NaN   
13040  Online courses (coursera, udemy, edx, etc.)                  NaN   
1086                            University courses                  NaN   
14099                                  Self-taught                  NaN   
7977                            University courses                  NaN   
10807  Online courses (coursera, udemy, edx, etc.)                  NaN   
4028                                   Self-taught                  NaN   
5712   Online courses (coursera, udemy, edx, etc.)                  NaN   
12076                           University courses                  NaN   
11402                           University courses                  NaN   
5083                                   Self-taught                  NaN   
5002   Online courses (coursera, udemy, edx, etc.)                  NaN   

                             AlgorithmUnderstandingLevel  \
5023   Enough to explain the algorithm to someone non...   
14249  Enough to explain the algorithm to someone non...   
11605  Enough to code it again from scratch, albeit i...   
13081  Enough to explain the algorithm to someone non...   
13040  Enough to explain the algorithm to someone non...   
1086   Enough to explain the algorithm to someone non...   
14099             Enough to tune the parameters properly   
7977   Enough to explain the algorithm to someone non...   
10807  Enough to code it again from scratch, albeit i...   
4028   Enough to explain the algorithm to someone non...   
5712   Enough to explain the algorithm to someone non...   
12076  Enough to explain the algorithm to someone non...   
11402  Enough to code it again from scratch, albeit i...   
5083   Enough to explain the algorithm to someone non...   
5002   Enough to code it again from scratch, albeit i...   

                  MLMethodNextYearSelect MLToolNextYearSelect  \
5023                       Deep learning           TensorFlow   
14249                      Deep learning           TensorFlow   
11605                      Deep learning           TensorFlow   
13081                      Deep learning           TensorFlow   
13040                  Survival Analysis           TensorFlow   
1086                       Deep learning           TensorFlow   
14099                      Deep learning           TensorFlow   
7977                       Deep learning           TensorFlow   
10807                      Deep learning           TensorFlow   
4028                       Deep learning           TensorFlow   
5712                       Deep learning           TensorFlow   
12076                      Deep learning               Python   
11402                      Deep learning           TensorFlow   
5083   Genetic & Evolutionary Algorithms                 Java   
5002                Time Series Analysis           TensorFlow   

      HardwarePersonalProjectsSelect JobSearchResource  \
5023                             NaN               NaN   
14249                            NaN               NaN   
11605                            NaN               NaN   
13081                            NaN               NaN   
13040                            NaN               NaN   
1086                             NaN               NaN   
14099                            NaN               NaN   
7977                             NaN               NaN   
10807                            NaN               NaN   
4028                             NaN               NaN   
5712                             NaN               NaN   
12076                            NaN               NaN   
11402                            NaN               NaN   
5083                             NaN               NaN   
5002                             NaN               NaN   

                                    EmployerSearchMethod  
5023   A friend, family member, or former colleague t...  
14249  A friend, family member, or former colleague t...  
11605  A friend, family member, or former colleague t...  
13081  I was contacted directly by someone at the com...  
13040                                     Some other way  
1086   A friend, family member, or former colleague t...  
14099  I was contacted directly by someone at the com...  
7977                         A general-purpose job board  
10807                        A general-purpose job board  
4028                         A general-purpose job board  
5712                         A general-purpose job board  
12076  A friend, family member, or former colleague t...  
11402  A friend, family member, or former colleague t...  
5083                         A general-purpose job board  
5002   A friend, family member, or former colleague t...  


------------------------------------------------------------
Cell index: 64
Input Cell Type: python
Input Text:
-----------
#show original data for neighbors
print('-'*40)
print('Represents %.1f%s of kagglers' %(100.0*sortedClusterFrequency[k],'%'))
print('Average Age = %.1f' %(basicSubsetDF.loc[nearestNeighborInds,'Age'].astype(float).mean()))
print('Average Salary in USD = %.2fK' %(basicSubsetDF.loc[nearestNeighborInds,'annualSalary_USD'].astype(float).mean()))
print('Most Common Gender is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'GenderSelect'].tolist()))
print('Most Common Country is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'Country'].tolist()))
print('Most Common Formal Education is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'FormalEducation'].tolist()))
print('Most Common Major is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'MajorSelect'].tolist()))
print('Most Common Employment Status is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'EmploymentStatus'].tolist()))
print('Most Common Tenure is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'Tenure'].tolist()))
print('Most Common Job title is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'CurrentJobTitleSelect'].tolist()))
print('Most Common First training is "%s"' %GetMstCommonElement(additionalSubsetDF.loc[nearestNeighborInds,'FirstTrainingSelect'].tolist()))
print('-'*40)

Output Text:
------------
----------------------------------------
Represents 8.5% of kagglers
Average Age = 30.4
Average Salary in USD = 91.53K
Most Common Gender is "Male"
Most Common Country is "United States"
Most Common Formal Education is "Master's degree"
Most Common Major is "Mathematics or statistics"
Most Common Employment Status is "Employed full-time"
Most Common Tenure is "3 to 5 years"
Most Common Job title is "Data Scientist"
Most Common First training is "University courses"
----------------------------------------


------------------------------------------------------------
Cell index: 65
Input Cell Type: markdown
Input Text:
-----------
# Paragraph Summery of Kaggler Type no. 5:
The fifth most frequent kaggler (accounts for about 8.5% of kagglers) is a ~30 year old male from the Unites States, he is employed full-time as a data scientist, has 3-5 years of industry experience, his background is Mathematics and statistics and he holds a Master's degree. He mainly works with python. His anual salary is 92K dollars. Next year, he wants to learn more about deep learning and specifically using tensorflow.

Output Text:
------------


------------------------------------------------------------
Cell index: 66
Input Cell Type: markdown
Input Text:
-----------
# Nearest Neighbors of Kaggler Type no. 6:

Output Text:
------------


------------------------------------------------------------
Cell index: 67
Input Cell Type: python
Input Text:
-----------
#%% show the attribures of most frequent kaggler
def GetMstCommonElement(a_list):
    return max(set(a_list), key=a_list.count)

# select cluster
k = 5
selectedCluster = sortedClustersByFrequency[k]

# find nearest neighbors
numNeighbors = 15
distFromCluster = KMeansModel.transform(numericDF)[:,selectedCluster]
distFromCluster[clusterInds != selectedCluster] = np.inf
nearestNeighborInds = np.argsort(distFromCluster)[:numNeighbors]

Output Text:
------------


------------------------------------------------------------
Cell index: 68
Input Cell Type: python
Input Text:
-----------
basicSubsetDF.loc[nearestNeighborInds,:]

Output Text:
------------
             Country GenderSelect Age  \
3806           India         Male  21   
8169           India         Male  20   
6865           India         Male  21   
13150          India         Male  21   
5637           India         Male  22   
12144  United States         Male  24   
1055           India         Male  22   
8147           India         Male  23   
3159           India         Male  21   
10551          India         Male  20   
5190   United States         Male  25   
3283           India         Male  23   
985            Other         Male  23   
472            India         Male  22   
8204           India         Male  20   

                                         FormalEducation            Tenure  \
3806                                   Bachelor's degree  Less than a year   
8169                                   Bachelor's degree      1 to 2 years   
6865                                   Bachelor's degree  Less than a year   
13150                                  Bachelor's degree  Less than a year   
5637                                   Bachelor's degree  Less than a year   
12144  Some college/university study without earning ...      1 to 2 years   
1055                                   Bachelor's degree  Less than a year   
8147                                   Bachelor's degree  Less than a year   
3159                                   Bachelor's degree  Less than a year   
10551                                  Bachelor's degree  Less than a year   
5190   Some college/university study without earning ...  Less than a year   
3283                                   Bachelor's degree               NaN   
985                                    Bachelor's degree      1 to 2 years   
472                                    Bachelor's degree  Less than a year   
8204                                   Bachelor's degree      1 to 2 years   

       annualSalary_USD                         MajorSelect  \
3806                0.0                    Computer Science   
8169                0.0  Engineering (non-computer focused)   
6865                0.0                    Computer Science   
13150               0.0                    Computer Science   
5637                0.0                    Computer Science   
12144               0.0                    Computer Science   
1055                0.0                    Computer Science   
8147                0.0                    Computer Science   
3159                0.0                    Computer Science   
10551               0.0                    Computer Science   
5190                0.0                    Computer Science   
3283                0.0                    Computer Science   
985                 0.0                    Computer Science   
472                 0.0                    Computer Science   
8204                0.0                    Computer Science   

                             EmploymentStatus CurrentJobTitleSelect  \
3806       Not employed, but looking for work                   NaN   
8169       Not employed, but looking for work                   NaN   
6865       Not employed, but looking for work                   NaN   
13150      Not employed, but looking for work                   NaN   
5637       Not employed, but looking for work                   NaN   
12144      Not employed, but looking for work                   NaN   
1055       Not employed, but looking for work                   NaN   
8147       Not employed, but looking for work                   NaN   
3159       Not employed, but looking for work                   NaN   
10551      Not employed, but looking for work                   NaN   
5190       Not employed, but looking for work                   NaN   
3283   Not employed, and not looking for work                   NaN   
985        Not employed, but looking for work                   NaN   
472        Not employed, but looking for work                   NaN   
8204       Not employed, but looking for work                   NaN   

      LanguageRecommendationSelect TimeSpentStudying  impatience_basic  \
3806                        Python      2 - 10 hours                 1   
8169                        Python      2 - 10 hours                 1   
6865                        Python      2 - 10 hours                 1   
13150                       Python        0 - 1 hour                 1   
5637                        Python      2 - 10 hours                 1   
12144                       Python      2 - 10 hours                 1   
1055                        Python      2 - 10 hours                 1   
8147                        Python        0 - 1 hour                 1   
3159                        Python      2 - 10 hours                 1   
10551                       Python      2 - 10 hours                 1   
5190                        Python      2 - 10 hours                 1   
3283                        Python      2 - 10 hours                 2   
985                         Python      2 - 10 hours                 1   
472                         Python      2 - 10 hours                 1   
8204                        Python      2 - 10 hours                 1   

       impatience_additional  impatience  
3806                       2           3  
8169                       2           3  
6865                       2           3  
13150                      3           4  
5637                       2           3  
12144                      2           3  
1055                       2           3  
8147                       3           4  
3159                       2           3  
10551                      2           3  
5190                       3           4  
3283                       2           4  
985                        2           3  
472                        2           3  
8204                       2           3  


------------------------------------------------------------
Cell index: 69
Input Cell Type: python
Input Text:
-----------
additionalSubsetDF.loc[nearestNeighborInds,:]

Output Text:
------------
                               FirstTrainingSelect  \
3806   Online courses (coursera, udemy, edx, etc.)   
8169   Online courses (coursera, udemy, edx, etc.)   
6865   Online courses (coursera, udemy, edx, etc.)   
13150  Online courses (coursera, udemy, edx, etc.)   
5637   Online courses (coursera, udemy, edx, etc.)   
12144  Online courses (coursera, udemy, edx, etc.)   
1055   Online courses (coursera, udemy, edx, etc.)   
8147   Online courses (coursera, udemy, edx, etc.)   
3159                                   Self-taught   
10551  Online courses (coursera, udemy, edx, etc.)   
5190                            University courses   
3283   Online courses (coursera, udemy, edx, etc.)   
985                             University courses   
472    Online courses (coursera, udemy, edx, etc.)   
8204   Online courses (coursera, udemy, edx, etc.)   

                                  ProveKnowledgeSelect  \
3806                               Kaggle Competitions   
8169   Experience from work in a company related to ML   
6865                               Kaggle Competitions   
13150                              Kaggle Competitions   
5637                 Online Courses and Certifications   
12144  Experience from work in a company related to ML   
1055                               Kaggle Competitions   
8147                               Kaggle Competitions   
3159   Experience from work in a company related to ML   
10551                              Kaggle Competitions   
5190                               Kaggle Competitions   
3283   Experience from work in a company related to ML   
985    Experience from work in a company related to ML   
472                                Kaggle Competitions   
8204   Experience from work in a company related to ML   

      AlgorithmUnderstandingLevel MLMethodNextYearSelect  \
3806                          NaN          Deep learning   
8169                          NaN          Deep learning   
6865                          NaN          Deep learning   
13150                         NaN          Deep learning   
5637                          NaN          Deep learning   
12144                         NaN          Deep learning   
1055                          NaN          Deep learning   
8147                          NaN          Deep learning   
3159                          NaN          Deep learning   
10551                         NaN          Deep learning   
5190                          NaN          Deep learning   
3283                          NaN          Deep learning   
985                           NaN          Deep learning   
472                           NaN          Deep learning   
8204                          NaN          Deep learning   

                                 MLToolNextYearSelect  \
3806                          Amazon Machine Learning   
8169                                       TensorFlow   
6865                                       TensorFlow   
13150                                               R   
5637                                       TensorFlow   
12144  I don't plan on learning a new tool/technology   
1055                                       TensorFlow   
8147                                        DataRobot   
3159                                       TensorFlow   
10551                                      TensorFlow   
5190                                           Python   
3283                                           Python   
985                                        TensorFlow   
472                                        TensorFlow   
8204                                  Hadoop/Hive/Pig   

                     HardwarePersonalProjectsSelect  \
3806                         Basic laptop (Macbook)   
8169                         Basic laptop (Macbook)   
6865                         Basic laptop (Macbook)   
13150                        Basic laptop (Macbook)   
5637                         Basic laptop (Macbook)   
12144                        Basic laptop (Macbook)   
1055                         Basic laptop (Macbook)   
8147                         Basic laptop (Macbook)   
3159                         Basic laptop (Macbook)   
10551                        Basic laptop (Macbook)   
5190                         Basic laptop (Macbook)   
3283                         Basic laptop (Macbook)   
985                          Basic laptop (Macbook)   
472    Laptop + Cloud service (AWS, Azure, GCE ...)   
8204                         Basic laptop (Macbook)   

                                       JobSearchResource EmployerSearchMethod  
3806                 Company's Web site/job listing page                  NaN  
8169                 Company's Web site/job listing page                  NaN  
6865           Career fair or on-campus recruiting event                  NaN  
13150                                                NaN                  NaN  
5637                                               Other                  NaN  
12144                Company's Web site/job listing page                  NaN  
1055                 Company's Web site/job listing page                  NaN  
8147                                                 NaN                  NaN  
3159           Career fair or on-campus recruiting event                  NaN  
10551  Asking friends, family members, or former coll...                  NaN  
5190                                                 NaN                  NaN  
3283           Career fair or on-campus recruiting event                  NaN  
985                              Tech-specific job board                  NaN  
472                  Company's Web site/job listing page                  NaN  
8204                             Tech-specific job board                  NaN  


------------------------------------------------------------
Cell index: 70
Input Cell Type: python
Input Text:
-----------
#show original data for neighbors
print('-'*40)
print('Represents %.1f%s of kagglers' %(100.0*sortedClusterFrequency[k],'%'))
print('Average Age = %.1f' %(basicSubsetDF.loc[nearestNeighborInds,'Age'].astype(float).mean()))
print('Average Salary in USD = %.2fK' %(basicSubsetDF.loc[nearestNeighborInds,'annualSalary_USD'].astype(float).mean()))
print('Most Common Gender is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'GenderSelect'].tolist()))
print('Most Common Country is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'Country'].tolist()))
print('Most Common Formal Education is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'FormalEducation'].tolist()))
print('Most Common Major is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'MajorSelect'].tolist()))
print('Most Common Employment Status is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'EmploymentStatus'].tolist()))
print('Most Common Tenure is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'Tenure'].tolist()))
print('Most Common Job title is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'CurrentJobTitleSelect'].tolist()))
print('Most Common First training is "%s"' %GetMstCommonElement(additionalSubsetDF.loc[nearestNeighborInds,'FirstTrainingSelect'].tolist()))
print('-'*40)

Output Text:
------------
----------------------------------------
Represents 8.5% of kagglers
Average Age = 21.9
Average Salary in USD = 0.00K
Most Common Gender is "Male"
Most Common Country is "India"
Most Common Formal Education is "Bachelor's degree"
Most Common Major is "Computer Science"
Most Common Employment Status is "Not employed, but looking for work"
Most Common Tenure is "Less than a year"
Most Common Job title is "NaN"
Most Common First training is "Online courses (coursera, udemy, edx, etc.)"
----------------------------------------


------------------------------------------------------------
Cell index: 71
Input Cell Type: markdown
Input Text:
-----------
# Paragraph Summery of Kaggler Type no. 6:
The 6th most frequent kaggler (accounts for about 8.5% of kagglers) is a ~22 year old male from the India, he is not employed, has no experience, he holds a Bachelor's degree majoring in Computer Science. He mainly works with python. He spends 2-10 hours each week learning Data Science. He's first exposure with data science is using online courses and he values kaggle competitions very high as a potential credential.

Output Text:
------------


------------------------------------------------------------
Cell index: 72
Input Cell Type: markdown
Input Text:
-----------
# Nearest Neighbors of Kaggler Type no. 7:

Output Text:
------------


------------------------------------------------------------
Cell index: 73
Input Cell Type: python
Input Text:
-----------
#%% show the attribures of most frequent kaggler
def GetMstCommonElement(a_list):
    return max(set(a_list), key=a_list.count)

# select cluster
k = 6
selectedCluster = sortedClustersByFrequency[k]

# find nearest neighbors
numNeighbors = 15
distFromCluster = KMeansModel.transform(numericDF)[:,selectedCluster]
distFromCluster[clusterInds != selectedCluster] = np.inf
nearestNeighborInds = np.argsort(distFromCluster)[:numNeighbors]

Output Text:
------------


------------------------------------------------------------
Cell index: 74
Input Cell Type: python
Input Text:
-----------
basicSubsetDF.loc[nearestNeighborInds,:]

Output Text:
------------
             Country GenderSelect Age  FormalEducation              Tenure  \
2323          Taiwan         Male  42  Master's degree  More than 10 years   
3992   United States         Male  43  Master's degree  More than 10 years   
4176          Mexico         Male  46  Master's degree  More than 10 years   
11259  United States         Male  45  Master's degree       6 to 10 years   
2723           India         Male  38  Master's degree  More than 10 years   
647    United States         Male  45  Master's degree  More than 10 years   
13694  United States         Male  41  Master's degree  More than 10 years   
12846         Israel         Male  42  Master's degree       6 to 10 years   
7156   United States         Male  52  Master's degree  More than 10 years   
9237         Germany         Male  45  Master's degree  More than 10 years   
10439  United States         Male  40  Master's degree  More than 10 years   
5787           Italy         Male  46  Doctoral degree  More than 10 years   
12927          India         Male  41  Master's degree  More than 10 years   
5033   United States         Male  51  Doctoral degree  More than 10 years   
8372           India         Male  37  Master's degree  More than 10 years   

       annualSalary_USD                         MajorSelect  \
2323          35.000000                    Computer Science   
3992           0.000000                    Computer Science   
4176          10.154520                    Computer Science   
11259          0.000000  Engineering (non-computer focused)   
2723          39.050000                    Computer Science   
647            0.271700           Mathematics or statistics   
13694          0.000000           Mathematics or statistics   
12846          0.000000                    Computer Science   
7156          55.000000                    Computer Science   
9237           0.000000                    Computer Science   
10439          0.000000                    Computer Science   
5787          51.420518                    Computer Science   
12927         46.860000                    Computer Science   
5033           0.000000                    Computer Science   
8372          31.240000                    Computer Science   

         EmploymentStatus                 CurrentJobTitleSelect  \
2323   Employed full-time                        Data Scientist   
3992   Employed full-time             Machine Learning Engineer   
4176   Employed full-time                                 Other   
11259  Employed full-time                        Data Scientist   
2723   Employed full-time                          Data Analyst   
647    Employed full-time                        Data Scientist   
13694  Employed full-time                        Data Scientist   
12846  Employed full-time  Software Developer/Software Engineer   
7156   Employed full-time                  Scientist/Researcher   
9237   Employed full-time  Software Developer/Software Engineer   
10439  Employed full-time                    Computer Scientist   
5787   Employed full-time                  Scientist/Researcher   
12927  Employed full-time                            Researcher   
5033   Employed full-time                                 Other   
8372   Employed full-time                        Data Scientist   

      LanguageRecommendationSelect TimeSpentStudying  impatience_basic  \
2323                        Python               NaN                 1   
3992                        Python               NaN                 1   
4176                        Python               NaN                 1   
11259                       Python               NaN                 1   
2723                        Python               NaN                 1   
647                         Python               NaN                 1   
13694                       Python               NaN                 1   
12846                       Python               NaN                 1   
7156                             R               NaN                 1   
9237                        Python               NaN                 1   
10439                       Python               NaN                 1   
5787                        Python               NaN                 1   
12927                       Python               NaN                 1   
5033                        Python               NaN                 1   
8372                        Python               NaN                 1   

       impatience_additional  impatience  
2323                       3           4  
3992                       3           4  
4176                       3           4  
11259                      3           4  
2723                       3           4  
647                        3           4  
13694                      3           4  
12846                      3           4  
7156                       3           4  
9237                       3           4  
10439                      3           4  
5787                       3           4  
12927                      3           4  
5033                       3           4  
8372                       3           4  


------------------------------------------------------------
Cell index: 75
Input Cell Type: python
Input Text:
-----------
additionalSubsetDF.loc[nearestNeighborInds,:]

Output Text:
------------
                               FirstTrainingSelect ProveKnowledgeSelect  \
2323                            University courses                  NaN   
3992                            University courses                  NaN   
4176                            University courses                  NaN   
11259                                  Self-taught                  NaN   
2723                                   Self-taught                  NaN   
647                             University courses                  NaN   
13694  Online courses (coursera, udemy, edx, etc.)                  NaN   
12846                                  Self-taught                  NaN   
7156                            University courses                  NaN   
9237                           Kaggle competitions                  NaN   
10439                                         Work                  NaN   
5787                                   Self-taught                  NaN   
12927                                  Self-taught                  NaN   
5033                            University courses                  NaN   
8372                            University courses                  NaN   

                             AlgorithmUnderstandingLevel  \
2323   Enough to explain the algorithm to someone non...   
3992   Enough to explain the algorithm to someone non...   
4176      Enough to refine and innovate on the algorithm   
11259  Enough to explain the algorithm to someone non...   
2723   Enough to explain the algorithm to someone non...   
647    Enough to code it again from scratch, albeit i...   
13694  Enough to explain the algorithm to someone non...   
12846  Enough to explain the algorithm to someone non...   
7156      Enough to refine and innovate on the algorithm   
9237   Enough to explain the algorithm to someone non...   
10439  Enough to explain the algorithm to someone non...   
5787   Enough to code it again from scratch, albeit i...   
12927  Enough to explain the algorithm to someone non...   
5033   Enough to code it again from scratch, albeit i...   
8372   Enough to code it again from scratch, albeit i...   

      MLMethodNextYearSelect MLToolNextYearSelect  \
2323           Deep learning           TensorFlow   
3992           Deep learning               Python   
4176           Deep learning           TensorFlow   
11259          Deep learning           TensorFlow   
2723           Deep learning           TensorFlow   
647            Deep learning                Other   
13694          Deep learning        Spark / MLlib   
12846          Deep learning           TensorFlow   
7156           Deep learning           TensorFlow   
9237           Deep learning           TensorFlow   
10439          Deep learning           TensorFlow   
5787           Deep learning           TensorFlow   
12927            Neural Nets           TensorFlow   
5033           Deep learning           TensorFlow   
8372           Deep learning    Jupyter notebooks   

      HardwarePersonalProjectsSelect JobSearchResource  \
2323                             NaN               NaN   
3992                             NaN               NaN   
4176                             NaN               NaN   
11259                            NaN               NaN   
2723                             NaN               NaN   
647                              NaN               NaN   
13694                            NaN               NaN   
12846                            NaN               NaN   
7156                             NaN               NaN   
9237                             NaN               NaN   
10439                            NaN               NaN   
5787                             NaN               NaN   
12927                            NaN               NaN   
5033                             NaN               NaN   
8372                             NaN               NaN   

                                    EmployerSearchMethod  
2323   A friend, family member, or former colleague t...  
3992   A friend, family member, or former colleague t...  
4176   A friend, family member, or former colleague t...  
11259                An external recruiter or headhunter  
2723   A friend, family member, or former colleague t...  
647    I visited the company's Web site and found a j...  
13694                An external recruiter or headhunter  
12846                        A general-purpose job board  
7156   A friend, family member, or former colleague t...  
9237   A friend, family member, or former colleague t...  
10439  I visited the company's Web site and found a j...  
5787         A career fair or on-campus recruiting event  
12927  I was contacted directly by someone at the com...  
5033   I was contacted directly by someone at the com...  
8372   I was contacted directly by someone at the com...  


------------------------------------------------------------
Cell index: 76
Input Cell Type: python
Input Text:
-----------
#show original data for neighbors
print('-'*40)
print('Represents %.1f%s of kagglers' %(100.0*sortedClusterFrequency[k],'%'))
print('Average Age = %.1f' %(basicSubsetDF.loc[nearestNeighborInds,'Age'].astype(float).mean()))
print('Average Salary in USD = %.2fK' %(basicSubsetDF.loc[nearestNeighborInds,'annualSalary_USD'].astype(float).mean()))
print('Most Common Gender is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'GenderSelect'].tolist()))
print('Most Common Country is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'Country'].tolist()))
print('Most Common Formal Education is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'FormalEducation'].tolist()))
print('Most Common Major is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'MajorSelect'].tolist()))
print('Most Common Employment Status is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'EmploymentStatus'].tolist()))
print('Most Common Tenure is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'Tenure'].tolist()))
print('Most Common Job title is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'CurrentJobTitleSelect'].tolist()))
print('Most Common First training is "%s"' %GetMstCommonElement(additionalSubsetDF.loc[nearestNeighborInds,'FirstTrainingSelect'].tolist()))
print('-'*40)

Output Text:
------------
----------------------------------------
Represents 8.0% of kagglers
Average Age = 43.6
Average Salary in USD = 17.93K
Most Common Gender is "Male"
Most Common Country is "United States"
Most Common Formal Education is "Master's degree"
Most Common Major is "Computer Science"
Most Common Employment Status is "Employed full-time"
Most Common Tenure is "More than 10 years"
Most Common Job title is "Data Scientist"
Most Common First training is "University courses"
----------------------------------------


------------------------------------------------------------
Cell index: 77
Input Cell Type: markdown
Input Text:
-----------
# Paragraph Summery of Kaggler Type no. 7:
The 7th most frequent kaggler (accounts for about 8% of kagglers) is a ~44 year old male from the United States, he is employed full time at various different professions, has more than 10 years of experience, holds a Master's degree and majored in Computer Science. His first training is University (they didn't have online courses 20 years ago...). He mainly works with python. He's looking forward to gaining experience also with deep learning and tensorflow in the upcoming year.

Output Text:
------------


------------------------------------------------------------
Cell index: 78
Input Cell Type: markdown
Input Text:
-----------
# Nearest Neighbors of Kaggler Type no. 8:

Output Text:
------------


------------------------------------------------------------
Cell index: 79
Input Cell Type: python
Input Text:
-----------
#%% show the attribures of most frequent kaggler
def GetMstCommonElement(a_list):
    return max(set(a_list), key=a_list.count)

# select cluster
k = 7
selectedCluster = sortedClustersByFrequency[k]

# find nearest neighbors
numNeighbors = 15
distFromCluster = KMeansModel.transform(numericDF)[:,selectedCluster]
distFromCluster[clusterInds != selectedCluster] = np.inf
nearestNeighborInds = np.argsort(distFromCluster)[:numNeighbors]

Output Text:
------------


------------------------------------------------------------
Cell index: 80
Input Cell Type: python
Input Text:
-----------
basicSubsetDF.loc[nearestNeighborInds,:]

Output Text:
------------
             Country GenderSelect Age    FormalEducation  \
12739          India         Male  36    Master's degree   
11096  United States         Male  33    Master's degree   
1420           Other         Male  32    Master's degree   
459            Other         Male  38    Master's degree   
11327  United States         Male  27  Bachelor's degree   
1805           Spain         Male  33    Master's degree   
7259           India         Male  29  Bachelor's degree   
13613         Sweden         Male  39  Bachelor's degree   
5252           India         Male  35  Bachelor's degree   
260    United States         Male  38  Bachelor's degree   
11162          India         Male  35  Bachelor's degree   
3292           India         Male  30    Master's degree   
4263           Other         Male  39    Master's degree   
12922          Other         Male  31  Bachelor's degree   
9855       Australia         Male  34    Master's degree   

                                   Tenure  annualSalary_USD  \
12739                    Less than a year               0.0   
11096                        1 to 2 years               0.0   
1420                         1 to 2 years               0.0   
459                      Less than a year               0.0   
11327                        1 to 2 years               0.0   
1805                     Less than a year               0.0   
7259                     Less than a year               0.0   
13613                        1 to 2 years               0.0   
5252                     Less than a year               0.0   
260                      Less than a year               0.0   
11162  I don't write code to analyze data               0.0   
3292   I don't write code to analyze data               0.0   
4263                     Less than a year               0.0   
12922                        3 to 5 years               0.0   
9855                     Less than a year               0.0   

                              MajorSelect    EmploymentStatus  \
12739           Mathematics or statistics  Employed full-time   
11096                    Computer Science  Employed full-time   
1420                     Computer Science  Employed full-time   
459                      Computer Science  Employed full-time   
11327                    Computer Science  Employed full-time   
1805                     Computer Science  Employed full-time   
7259                     Computer Science  Employed full-time   
13613                    Computer Science  Employed full-time   
5252            Mathematics or statistics  Employed full-time   
260                      Computer Science  Employed full-time   
11162                    Computer Science  Employed full-time   
3292   Engineering (non-computer focused)  Employed full-time   
4263                     Computer Science  Employed full-time   
12922                    Computer Science  Employed full-time   
9855                     Computer Science  Employed full-time   

                      CurrentJobTitleSelect LanguageRecommendationSelect  \
12739  Software Developer/Software Engineer                       Python   
11096  Software Developer/Software Engineer                       Python   
1420   Software Developer/Software Engineer                       Python   
459    Software Developer/Software Engineer                       Python   
11327                              Engineer                       Python   
1805                                  Other                       Python   
7259                       Business Analyst                       Python   
13613                            Programmer                       Python   
5252                                  Other                       Python   
260    Software Developer/Software Engineer                       Python   
11162                            Programmer                       Python   
3292   Software Developer/Software Engineer                       Python   
4263   Software Developer/Software Engineer                       Python   
12922  Software Developer/Software Engineer                       Python   
9855   Software Developer/Software Engineer                       Python   

      TimeSpentStudying  impatience_basic  impatience_additional  impatience  
12739      2 - 10 hours                 0                      2           2  
11096      2 - 10 hours                 0                      2           2  
1420       2 - 10 hours                 0                      2           2  
459        2 - 10 hours                 0                      2           2  
11327      2 - 10 hours                 0                      2           2  
1805       2 - 10 hours                 0                      2           2  
7259       2 - 10 hours                 0                      3           3  
13613      2 - 10 hours                 0                      2           2  
5252       2 - 10 hours                 0                      2           2  
260          0 - 1 hour                 0                      3           3  
11162      2 - 10 hours                 0                      2           2  
3292       2 - 10 hours                 0                      2           2  
4263         0 - 1 hour                 0                      2           2  
12922      2 - 10 hours                 0                      2           2  
9855         0 - 1 hour                 0                      2           2  


------------------------------------------------------------
Cell index: 81
Input Cell Type: python
Input Text:
-----------
additionalSubsetDF.loc[nearestNeighborInds,:]

Output Text:
------------
                               FirstTrainingSelect  \
12739  Online courses (coursera, udemy, edx, etc.)   
11096  Online courses (coursera, udemy, edx, etc.)   
1420   Online courses (coursera, udemy, edx, etc.)   
459    Online courses (coursera, udemy, edx, etc.)   
11327  Online courses (coursera, udemy, edx, etc.)   
1805   Online courses (coursera, udemy, edx, etc.)   
7259   Online courses (coursera, udemy, edx, etc.)   
13613  Online courses (coursera, udemy, edx, etc.)   
5252   Online courses (coursera, udemy, edx, etc.)   
260                                    Self-taught   
11162  Online courses (coursera, udemy, edx, etc.)   
3292                                   Self-taught   
4263   Online courses (coursera, udemy, edx, etc.)   
12922  Online courses (coursera, udemy, edx, etc.)   
9855   Online courses (coursera, udemy, edx, etc.)   

                                  ProveKnowledgeSelect  \
12739  Experience from work in a company related to ML   
11096                              Kaggle Competitions   
1420   Experience from work in a company related to ML   
459                                Kaggle Competitions   
11327  Experience from work in a company related to ML   
1805                               Kaggle Competitions   
7259   Experience from work in a company related to ML   
13613                Online Courses and Certifications   
5252   Experience from work in a company related to ML   
260    Experience from work in a company related to ML   
11162  Experience from work in a company related to ML   
3292                               Kaggle Competitions   
4263                                  Github Portfolio   
12922                              Kaggle Competitions   
9855   Experience from work in a company related to ML   

      AlgorithmUnderstandingLevel MLMethodNextYearSelect  \
12739                         NaN          Deep learning   
11096                         NaN             Regression   
1420                          NaN          Deep learning   
459                           NaN            Neural Nets   
11327                         NaN          Deep learning   
1805                          NaN          Deep learning   
7259                          NaN          Deep learning   
13613                         NaN          Deep learning   
5252                          NaN          Deep learning   
260                           NaN          Deep learning   
11162                         NaN          Deep learning   
3292                          NaN          Deep learning   
4263                          NaN            Neural Nets   
12922                         NaN          Deep learning   
9855                          NaN          Deep learning   

          MLToolNextYearSelect HardwarePersonalProjectsSelect  \
12739               TensorFlow         Basic laptop (Macbook)   
11096                   Python         Basic laptop (Macbook)   
1420                TensorFlow         Basic laptop (Macbook)   
459                 TensorFlow         Basic laptop (Macbook)   
11327                   Python         Basic laptop (Macbook)   
1805                    Python         Basic laptop (Macbook)   
7259                         R         Basic laptop (Macbook)   
13613               TensorFlow         Basic laptop (Macbook)   
5252             Spark / MLlib         Basic laptop (Macbook)   
260              Spark / MLlib         Basic laptop (Macbook)   
11162               TensorFlow         Basic laptop (Macbook)   
3292                    Python         Basic laptop (Macbook)   
4263                    Python         Basic laptop (Macbook)   
12922               TensorFlow         Basic laptop (Macbook)   
9855   Amazon Machine Learning         Basic laptop (Macbook)   

                                       JobSearchResource EmployerSearchMethod  
12739                Company's Web site/job listing page                  NaN  
11096                Searching general-purpose job board                  NaN  
1420                 Searching general-purpose job board                  NaN  
459                  Searching general-purpose job board                  NaN  
11327                Searching general-purpose job board                  NaN  
1805                             Tech-specific job board                  NaN  
7259                                                 NaN                  NaN  
13613                Searching general-purpose job board                  NaN  
5252                 Company's Web site/job listing page                  NaN  
260                                                  NaN                  NaN  
11162  Asking friends, family members, or former coll...                  NaN  
3292                             Tech-specific job board                  NaN  
4263                 Searching general-purpose job board                  NaN  
12922  Meeting with recruiters who've contacted you d...                  NaN  
9855                             Tech-specific job board                  NaN  


------------------------------------------------------------
Cell index: 82
Input Cell Type: python
Input Text:
-----------
#show original data for neighbors
print('-'*40)
print('Represents %.1f%s of kagglers' %(100.0*sortedClusterFrequency[k],'%'))
print('Average Age = %.1f' %(basicSubsetDF.loc[nearestNeighborInds,'Age'].astype(float).mean()))
print('Average Salary in USD = %.2fK' %(basicSubsetDF.loc[nearestNeighborInds,'annualSalary_USD'].astype(float).mean()))
print('Most Common Gender is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'GenderSelect'].tolist()))
print('Most Common Country is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'Country'].tolist()))
print('Most Common Formal Education is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'FormalEducation'].tolist()))
print('Most Common Major is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'MajorSelect'].tolist()))
print('Most Common Employment Status is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'EmploymentStatus'].tolist()))
print('Most Common Tenure is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'Tenure'].tolist()))
print('Most Common Job title is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'CurrentJobTitleSelect'].tolist()))
print('Most Common First training is "%s"' %GetMstCommonElement(additionalSubsetDF.loc[nearestNeighborInds,'FirstTrainingSelect'].tolist()))
print('-'*40)

Output Text:
------------
----------------------------------------
Represents 7.4% of kagglers
Average Age = 33.9
Average Salary in USD = 0.00K
Most Common Gender is "Male"
Most Common Country is "India"
Most Common Formal Education is "Master's degree"
Most Common Major is "Computer Science"
Most Common Employment Status is "Employed full-time"
Most Common Tenure is "Less than a year"
Most Common Job title is "Software Developer/Software Engineer"
Most Common First training is "Online courses (coursera, udemy, edx, etc.)"
----------------------------------------


------------------------------------------------------------
Cell index: 83
Input Cell Type: markdown
Input Text:
-----------
# Paragraph Summery of Kaggler Type no. 8:
The 8th most frequent kaggler (accounts for about 7.4% of kagglers) is a ~34 year old male from around the world, he is employed full time as a software developer, but has no datascience experience. he holds a Master's degree and majored in Computer Science. His first Data Science training is coming from online courses and he spends 2-10 hours each week learning data science. He working with python and like everyone else he's also looking forward to gaining experience also with deep learning in the upcoming year.

Output Text:
------------


------------------------------------------------------------
Cell index: 84
Input Cell Type: markdown
Input Text:
-----------
# Nearest Neighbors of Kaggler Type no. 9:

Output Text:
------------


------------------------------------------------------------
Cell index: 85
Input Cell Type: python
Input Text:
-----------
#%% show the attribures of most frequent kaggler
def GetMstCommonElement(a_list):
    return max(set(a_list), key=a_list.count)

# select cluster
k = 8
selectedCluster = sortedClustersByFrequency[k]

# find nearest neighbors
numNeighbors = 15
distFromCluster = KMeansModel.transform(numericDF)[:,selectedCluster]
distFromCluster[clusterInds != selectedCluster] = np.inf
nearestNeighborInds = np.argsort(distFromCluster)[:numNeighbors]

Output Text:
------------


------------------------------------------------------------
Cell index: 86
Input Cell Type: python
Input Text:
-----------
basicSubsetDF.loc[nearestNeighborInds,:]

Output Text:
------------
                           Country GenderSelect      Age    FormalEducation  \
382                        Germany         Male       32    Master's degree   
13232                United States         Male       28    Master's degree   
14301                        India         Male  32.3728    Master's degree   
8776                       Germany         Male       29    Master's degree   
2714                         India         Male       36    Master's degree   
7206   People 's Republic of China         Male       26    Master's degree   
3294                         India         Male       29    Master's degree   
11353                        India         Male       36  Bachelor's degree   
5247                 United States         Male       24    Master's degree   
58                           Other         Male       24    Master's degree   
9053                   South Korea         Male       27    Master's degree   
5349                United Kingdom         Male       36    Master's degree   
14339                      Germany         Male       30  Bachelor's degree   
10877                     Colombia         Male       31    Master's degree   
9305                         Other         Male       24    Master's degree   

                 Tenure  annualSalary_USD                         MajorSelect  \
382    Less than a year               0.0                    Computer Science   
13232      3 to 5 years               0.0                    Computer Science   
14301      3 to 5 years               0.0              Electrical Engineering   
8776       1 to 2 years               0.0                    Computer Science   
2714   Less than a year               0.0                    Computer Science   
7206   Less than a year               0.0                    Computer Science   
3294       1 to 2 years               0.0                    Computer Science   
11353  Less than a year               0.0  Engineering (non-computer focused)   
5247       1 to 2 years               0.0  Engineering (non-computer focused)   
58         1 to 2 years               0.0                    Computer Science   
9053   Less than a year               0.0              Electrical Engineering   
5349   Less than a year               0.0                               Other   
14339      1 to 2 years               0.0                    Computer Science   
10877      1 to 2 years               0.0           Mathematics or statistics   
9305       1 to 2 years               0.0                    Computer Science   

                         EmploymentStatus CurrentJobTitleSelect  \
382    Not employed, but looking for work                   NaN   
13232  Not employed, but looking for work                   NaN   
14301  Not employed, but looking for work                   NaN   
8776   Not employed, but looking for work                   NaN   
2714   Not employed, but looking for work                   NaN   
7206   Not employed, but looking for work                   NaN   
3294   Not employed, but looking for work                   NaN   
11353  Not employed, but looking for work                   NaN   
5247   Not employed, but looking for work                   NaN   
58     Not employed, but looking for work                   NaN   
9053   Not employed, but looking for work                   NaN   
5349   Not employed, but looking for work                   NaN   
14339  Not employed, but looking for work                   NaN   
10877  Not employed, but looking for work                   NaN   
9305   Not employed, but looking for work                   NaN   

      LanguageRecommendationSelect TimeSpentStudying  impatience_basic  \
382                         Python     11 - 39 hours                 1   
13232                       Python     11 - 39 hours                 1   
14301                       Python     11 - 39 hours                 2   
8776                        Python     11 - 39 hours                 1   
2714                        Python     11 - 39 hours                 1   
7206                        Python     11 - 39 hours                 1   
3294                        Python     11 - 39 hours                 1   
11353                       Python     11 - 39 hours                 1   
5247                        Python     11 - 39 hours                 1   
58                          Python     11 - 39 hours                 1   
9053                        Python     11 - 39 hours                 1   
5349                        Python     11 - 39 hours                 1   
14339                       Python     11 - 39 hours                 1   
10877                       Python     11 - 39 hours                 1   
9305                        Python     11 - 39 hours                 1   

       impatience_additional  impatience  
382                        2           3  
13232                      2           3  
14301                      2           4  
8776                       2           3  
2714                       2           3  
7206                       2           3  
3294                       2           3  
11353                      2           3  
5247                       2           3  
58                         2           3  
9053                       2           3  
5349                       2           3  
14339                      2           3  
10877                      2           3  
9305                       3           4  


------------------------------------------------------------
Cell index: 87
Input Cell Type: python
Input Text:
-----------
additionalSubsetDF.loc[nearestNeighborInds,:]

Output Text:
------------
                               FirstTrainingSelect  \
382                             University courses   
13232  Online courses (coursera, udemy, edx, etc.)   
14301  Online courses (coursera, udemy, edx, etc.)   
8776   Online courses (coursera, udemy, edx, etc.)   
2714   Online courses (coursera, udemy, edx, etc.)   
7206                                   Self-taught   
3294   Online courses (coursera, udemy, edx, etc.)   
11353  Online courses (coursera, udemy, edx, etc.)   
5247   Online courses (coursera, udemy, edx, etc.)   
58                                     Self-taught   
9053                                   Self-taught   
5349   Online courses (coursera, udemy, edx, etc.)   
14339  Online courses (coursera, udemy, edx, etc.)   
10877  Online courses (coursera, udemy, edx, etc.)   
9305                                   Self-taught   

                                  ProveKnowledgeSelect  \
382    Experience from work in a company related to ML   
13232                                 Github Portfolio   
14301  Experience from work in a company related to ML   
8776   Experience from work in a company related to ML   
2714                               Kaggle Competitions   
7206   Experience from work in a company related to ML   
3294                 Online Courses and Certifications   
11353  Experience from work in a company related to ML   
5247   Experience from work in a company related to ML   
58     Experience from work in a company related to ML   
9053                                   Master's degree   
5349                               Kaggle Competitions   
14339                              Kaggle Competitions   
10877                Online Courses and Certifications   
9305   Experience from work in a company related to ML   

      AlgorithmUnderstandingLevel MLMethodNextYearSelect MLToolNextYearSelect  \
382                           NaN          Deep learning           TensorFlow   
13232                         NaN          Deep learning           TensorFlow   
14301                         NaN          Deep learning           TensorFlow   
8776                          NaN          Deep learning               Python   
2714                          NaN          Deep learning           TensorFlow   
7206                          NaN          Deep learning           TensorFlow   
3294                          NaN          Deep learning           TensorFlow   
11353                         NaN          Deep learning               Python   
5247                          NaN          Deep learning               Python   
58                            NaN          Deep learning           TensorFlow   
9053                          NaN          Deep learning           TensorFlow   
5349                          NaN          Deep learning           TensorFlow   
14339                         NaN          Deep learning           TensorFlow   
10877                         NaN          Deep learning           TensorFlow   
9305                          NaN          Deep learning        Spark / MLlib   

                          HardwarePersonalProjectsSelect  \
382                               Basic laptop (Macbook)   
13232          Gaming Laptop (Laptop + CUDA capable GPU)   
14301                             Basic laptop (Macbook)   
8776                              Basic laptop (Macbook)   
2714   Laptop or Workstation and local IT supported s...   
7206                             Traditional Workstation   
3294   Laptop + Cloud service (AWS, Azure, GCE ...),O...   
11353                             Basic laptop (Macbook)   
5247   Laptop or Workstation and local IT supported s...   
58     Basic laptop (Macbook),Gaming Laptop (Laptop +...   
9053                              Basic laptop (Macbook)   
5349                              Basic laptop (Macbook)   
14339                             Basic laptop (Macbook)   
10877                             Basic laptop (Macbook)   
9305                              Basic laptop (Macbook)   

                                       JobSearchResource EmployerSearchMethod  
382                              Tech-specific job board                  NaN  
13232  Asking friends, family members, or former coll...                  NaN  
14301                                              Other                  NaN  
8776                 Company's Web site/job listing page                  NaN  
2714                 Company's Web site/job listing page                  NaN  
7206                 Company's Web site/job listing page                  NaN  
3294                                               Other                  NaN  
11353                            Tech-specific job board                  NaN  
5247                 Searching general-purpose job board                  NaN  
58                   Company's Web site/job listing page                  NaN  
9053                 Company's Web site/job listing page                  NaN  
5349           Career fair or on-campus recruiting event                  NaN  
14339                            Tech-specific job board                  NaN  
10877                Searching general-purpose job board                  NaN  
9305                                                 NaN                  NaN  


------------------------------------------------------------
Cell index: 88
Input Cell Type: python
Input Text:
-----------
#show original data for neighbors
print('-'*40)
print('Represents %.1f%s of kagglers' %(100.0*sortedClusterFrequency[k],'%'))
print('Average Age = %.1f' %(basicSubsetDF.loc[nearestNeighborInds,'Age'].astype(float).mean()))
print('Average Salary in USD = %.2fK' %(basicSubsetDF.loc[nearestNeighborInds,'annualSalary_USD'].astype(float).mean()))
print('Most Common Gender is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'GenderSelect'].tolist()))
print('Most Common Country is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'Country'].tolist()))
print('Most Common Formal Education is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'FormalEducation'].tolist()))
print('Most Common Major is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'MajorSelect'].tolist()))
print('Most Common Employment Status is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'EmploymentStatus'].tolist()))
print('Most Common Tenure is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'Tenure'].tolist()))
print('Most Common Job title is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'CurrentJobTitleSelect'].tolist()))
print('Most Common First training is "%s"' %GetMstCommonElement(additionalSubsetDF.loc[nearestNeighborInds,'FirstTrainingSelect'].tolist()))
print('-'*40)

Output Text:
------------
----------------------------------------
Represents 5.8% of kagglers
Average Age = 29.6
Average Salary in USD = 0.00K
Most Common Gender is "Male"
Most Common Country is "India"
Most Common Formal Education is "Master's degree"
Most Common Major is "Computer Science"
Most Common Employment Status is "Not employed, but looking for work"
Most Common Tenure is "1 to 2 years"
Most Common Job title is "NaN"
Most Common First training is "Online courses (coursera, udemy, edx, etc.)"
----------------------------------------


------------------------------------------------------------
Cell index: 89
Input Cell Type: markdown
Input Text:
-----------
# Paragraph Summery of Kaggler Type no. 9:
The 9th most frequent kaggler (accounts for about 5.8% of kagglers) is a ~30 year old male from around the world, he is not employed. He holds a Master's degree and comes from computer sceince and electrical engineering. His first Data Science training is coming from online courses and he spends most of his time learning Data Science. He working with a basic laptop and python. Like everyone else he's also looking forward to learning about deep learning in the upcoming year.

Output Text:
------------


------------------------------------------------------------
Cell index: 90
Input Cell Type: markdown
Input Text:
-----------
# Nearest Neighbors of Kaggler Type no. 10:

Output Text:
------------


------------------------------------------------------------
Cell index: 91
Input Cell Type: python
Input Text:
-----------
#%% show the attribures of most frequent kaggler
def GetMstCommonElement(a_list):
    return max(set(a_list), key=a_list.count)

# select cluster
k = 9
selectedCluster = sortedClustersByFrequency[k]

# find nearest neighbors
numNeighbors = 15
distFromCluster = KMeansModel.transform(numericDF)[:,selectedCluster]
distFromCluster[clusterInds != selectedCluster] = np.inf
nearestNeighborInds = np.argsort(distFromCluster)[:numNeighbors]

Output Text:
------------


------------------------------------------------------------
Cell index: 92
Input Cell Type: python
Input Text:
-----------
basicSubsetDF.loc[nearestNeighborInds,:]

Output Text:
------------
             Country GenderSelect      Age  FormalEducation  \
15461  United States         Male       32  Master's degree   
14230  United States         Male       39  Master's degree   
7173   United States         Male       35  Master's degree   
15539  United States         Male       36  Master's degree   
11525  United States         Male       34  Master's degree   
13401  United States         Male       35  Master's degree   
13409          Japan         Male       35  Master's degree   
9755   United States         Male       31  Master's degree   
6894   United States         Male  32.3728  Master's degree   
15056          India         Male       37  Master's degree   
15865  United States         Male       47  Master's degree   
9984   United States         Male       33  Master's degree   
15465  United States         Male       36  Master's degree   
13070  United States         Male       45  Master's degree   
13180         Russia         Male       39  Master's degree   

                   Tenure  annualSalary_USD  \
15461       6 to 10 years               0.0   
14230  More than 10 years               0.0   
7173   More than 10 years               0.0   
15539  More than 10 years               0.0   
11525       6 to 10 years               0.0   
13401       6 to 10 years               0.0   
13409       6 to 10 years               0.0   
9755        6 to 10 years               0.0   
6894        6 to 10 years               0.0   
15056       6 to 10 years               0.0   
15865  More than 10 years               0.0   
9984   More than 10 years               0.0   
15465  More than 10 years               0.0   
13070  More than 10 years               0.0   
13180  More than 10 years               0.0   

                              MajorSelect    EmploymentStatus  \
15461                    Computer Science  Employed full-time   
14230              Electrical Engineering  Employed full-time   
7173                     Computer Science  Employed full-time   
15539                    Computer Science  Employed full-time   
11525        Fine arts or performing arts  Employed full-time   
13401                    Computer Science  Employed full-time   
13409                    Computer Science  Employed full-time   
9755   Engineering (non-computer focused)  Employed full-time   
6894                     Computer Science  Employed full-time   
15056              Electrical Engineering  Employed full-time   
15865                    Computer Science  Employed full-time   
9984                     Computer Science  Employed full-time   
15465                    Computer Science  Employed full-time   
13070                             Physics  Employed full-time   
13180           Mathematics or statistics  Employed full-time   

                      CurrentJobTitleSelect LanguageRecommendationSelect  \
15461  Software Developer/Software Engineer                          NaN   
14230                  Scientist/Researcher                          NaN   
7173                         Data Scientist                          NaN   
15539  Software Developer/Software Engineer                          NaN   
11525                                 Other                          NaN   
13401                              Engineer                          NaN   
13409                  Scientist/Researcher                          NaN   
9755                         Data Scientist                          NaN   
6894                     Computer Scientist                          NaN   
15056                                 Other                          NaN   
15865  Software Developer/Software Engineer                          NaN   
9984                         Data Scientist                          NaN   
15465                        Data Scientist                          NaN   
13070  Software Developer/Software Engineer                          NaN   
13180                        Data Scientist                          NaN   

      TimeSpentStudying  impatience_basic  impatience_additional  impatience  
15461               NaN                 2                      7           9  
14230               NaN                 2                      7           9  
7173                NaN                 2                      7           9  
15539               NaN                 2                      6           8  
11525               NaN                 2                      7           9  
13401               NaN                 2                      6           8  
13409               NaN                 2                      7           9  
9755                NaN                 2                      7           9  
6894                NaN                 3                      7          10  
15056               NaN                 2                      7           9  
15865               NaN                 2                      7           9  
9984                NaN                 2                      6           8  
15465               NaN                 2                      6           8  
13070               NaN                 2                      7           9  
13180               NaN                 2                      7           9  


------------------------------------------------------------
Cell index: 93
Input Cell Type: python
Input Text:
-----------
additionalSubsetDF.loc[nearestNeighborInds,:]

Output Text:
------------
                               FirstTrainingSelect ProveKnowledgeSelect  \
15461                           University courses                  NaN   
14230                           University courses                  NaN   
7173                                          Work                  NaN   
15539                           University courses                  NaN   
11525                                  Self-taught                  NaN   
13401                                  Self-taught                  NaN   
13409                           University courses                  NaN   
9755                                   Self-taught                  NaN   
6894                            University courses                  NaN   
15056                                  Self-taught                  NaN   
15865  Online courses (coursera, udemy, edx, etc.)                  NaN   
9984                            University courses                  NaN   
15465  Online courses (coursera, udemy, edx, etc.)                  NaN   
13070  Online courses (coursera, udemy, edx, etc.)                  NaN   
13180                                  Self-taught                  NaN   

      AlgorithmUnderstandingLevel MLMethodNextYearSelect MLToolNextYearSelect  \
15461                         NaN                    NaN                  NaN   
14230                         NaN                    NaN                  NaN   
7173                          NaN                    NaN                  NaN   
15539                         NaN                    NaN                  NaN   
11525                         NaN                    NaN                  NaN   
13401                         NaN                    NaN                  NaN   
13409                         NaN                    NaN                  NaN   
9755                          NaN                    NaN                  NaN   
6894                          NaN                    NaN                  NaN   
15056                         NaN                    NaN                  NaN   
15865                         NaN                    NaN                  NaN   
9984                          NaN                    NaN                  NaN   
15465                         NaN                    NaN                  NaN   
13070                         NaN                    NaN                  NaN   
13180                         NaN                    NaN                  NaN   

      HardwarePersonalProjectsSelect JobSearchResource  \
15461                            NaN               NaN   
14230                            NaN               NaN   
7173                             NaN               NaN   
15539                            NaN               NaN   
11525                            NaN               NaN   
13401                            NaN               NaN   
13409                            NaN               NaN   
9755                             NaN               NaN   
6894                             NaN               NaN   
15056                            NaN               NaN   
15865                            NaN               NaN   
9984                             NaN               NaN   
15465                            NaN               NaN   
13070                            NaN               NaN   
13180                            NaN               NaN   

                                    EmployerSearchMethod  
15461                                                NaN  
14230                                                NaN  
7173                                                 NaN  
15539  A friend, family member, or former colleague t...  
11525                                                NaN  
13401                An external recruiter or headhunter  
13409                                                NaN  
9755                                                 NaN  
6894                                                 NaN  
15056                                                NaN  
15865                                                NaN  
9984   A friend, family member, or former colleague t...  
15465                        A general-purpose job board  
13070                                                NaN  
13180                                                NaN  


------------------------------------------------------------
Cell index: 94
Input Cell Type: python
Input Text:
-----------
#show original data for neighbors
print('-'*40)
print('Represents %.1f%s of kagglers' %(100.0*sortedClusterFrequency[k],'%'))
print('Average Age = %.1f' %(basicSubsetDF.loc[nearestNeighborInds,'Age'].astype(float).mean()))
print('Average Salary in USD = %.2fK' %(basicSubsetDF.loc[nearestNeighborInds,'annualSalary_USD'].astype(float).mean()))
print('Most Common Gender is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'GenderSelect'].tolist()))
print('Most Common Country is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'Country'].tolist()))
print('Most Common Formal Education is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'FormalEducation'].tolist()))
print('Most Common Major is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'MajorSelect'].tolist()))
print('Most Common Employment Status is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'EmploymentStatus'].tolist()))
print('Most Common Tenure is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'Tenure'].tolist()))
print('Most Common Job title is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'CurrentJobTitleSelect'].tolist()))
print('Most Common First training is "%s"' %GetMstCommonElement(additionalSubsetDF.loc[nearestNeighborInds,'FirstTrainingSelect'].tolist()))
print('-'*40)

Output Text:
------------
----------------------------------------
Represents 5.6% of kagglers
Average Age = 36.4
Average Salary in USD = 0.00K
Most Common Gender is "Male"
Most Common Country is "United States"
Most Common Formal Education is "Master's degree"
Most Common Major is "Computer Science"
Most Common Employment Status is "Employed full-time"
Most Common Tenure is "More than 10 years"
Most Common Job title is "Data Scientist"
Most Common First training is "University courses"
----------------------------------------


------------------------------------------------------------
Cell index: 95
Input Cell Type: markdown
Input Text:
-----------
# Paragraph Summery of Kaggler Type no. 10:
The 10th most frequent kaggler (accounts for about 5.6% of kagglers) is a ~36 year old male from the United States, he is employed full time and has more than 10 years of experience. He holds a Master's degree and comes from an eclectic background. His first training is coming from university courses and self teaching.

Output Text:
------------


------------------------------------------------------------
Cell index: 96
Input Cell Type: markdown
Input Text:
-----------
# Nearest Neighbors of Kaggler Type no. 11:

Output Text:
------------


------------------------------------------------------------
Cell index: 97
Input Cell Type: python
Input Text:
-----------
#%% show the attribures of most frequent kaggler
def GetMstCommonElement(a_list):
    return max(set(a_list), key=a_list.count)

# select cluster
k = 10
selectedCluster = sortedClustersByFrequency[k]

# find nearest neighbors
numNeighbors = 15
distFromCluster = KMeansModel.transform(numericDF)[:,selectedCluster]
distFromCluster[clusterInds != selectedCluster] = np.inf
nearestNeighborInds = np.argsort(distFromCluster)[:numNeighbors]

Output Text:
------------


------------------------------------------------------------
Cell index: 98
Input Cell Type: python
Input Text:
-----------
basicSubsetDF.loc[nearestNeighborInds,:]

Output Text:
------------
             Country GenderSelect Age  FormalEducation              Tenure  \
899    United States         Male  41  Master's degree       6 to 10 years   
14253  United States         Male  43  Master's degree       6 to 10 years   
3947   United States         Male  39  Master's degree  More than 10 years   
930    United States         Male  43  Master's degree  More than 10 years   
11548  United States         Male  33  Master's degree       6 to 10 years   
9612   United States         Male  33  Master's degree  More than 10 years   
5931   United States         Male  36  Master's degree  More than 10 years   
6260   United States         Male  45  Master's degree  More than 10 years   
13413  United States         Male  42  Master's degree  More than 10 years   
5219   United States         Male  46  Master's degree       6 to 10 years   
3421   United States         Male  42  Master's degree  More than 10 years   
10967  United States         Male  42  Master's degree  More than 10 years   
12148  United States         Male  46  Doctoral degree  More than 10 years   
3395   United States         Male  34  Master's degree  More than 10 years   
5743   United States         Male  39  Master's degree  More than 10 years   

       annualSalary_USD                         MajorSelect  \
899               210.0              Electrical Engineering   
14253             165.0  Engineering (non-computer focused)   
3947              190.0                    Computer Science   
930               220.0                                 NaN   
11548             172.0              Electrical Engineering   
9612              180.0           Mathematics or statistics   
5931              150.0           Mathematics or statistics   
6260              180.0           Mathematics or statistics   
13413             170.0                    Computer Science   
5219              165.0                                 NaN   
3421              175.0  Engineering (non-computer focused)   
10967             162.0                    Computer Science   
12148             200.0                             Physics   
3395              200.0                    A social science   
5743              177.0                               Other   

         EmploymentStatus                 CurrentJobTitleSelect  \
899    Employed full-time                        Data Scientist   
14253  Employed full-time                        Data Scientist   
3947   Employed full-time                        Data Scientist   
930    Employed full-time                        Data Scientist   
11548  Employed full-time                        Data Scientist   
9612   Employed full-time                                 Other   
5931   Employed full-time                        Data Scientist   
6260   Employed full-time                        Data Scientist   
13413  Employed full-time  Software Developer/Software Engineer   
5219   Employed full-time                        Data Scientist   
3421   Employed full-time             Machine Learning Engineer   
10967  Employed full-time                        Data Scientist   
12148  Employed full-time                        Data Scientist   
3395   Employed full-time                        Data Scientist   
5743   Employed full-time                                 Other   

      LanguageRecommendationSelect TimeSpentStudying  impatience_basic  \
899                         Python               NaN                 1   
14253                            R               NaN                 1   
3947                        Python               NaN                 1   
930                         Python               NaN                 2   
11548                       Python               NaN                 1   
9612                        Python               NaN                 1   
5931                        Python               NaN                 1   
6260                        Python               NaN                 1   
13413                       Python               NaN                 1   
5219                        Python               NaN                 2   
3421                        Python               NaN                 1   
10967                       Python               NaN                 1   
12148                       Python               NaN                 1   
3395                        Python               NaN                 1   
5743                        Python               NaN                 1   

       impatience_additional  impatience  
899                        3           4  
14253                      3           4  
3947                       3           4  
930                        3           5  
11548                      3           4  
9612                       3           4  
5931                       3           4  
6260                       3           4  
13413                      3           4  
5219                       3           5  
3421                       3           4  
10967                      4           5  
12148                      3           4  
3395                       3           4  
5743                       3           4  


------------------------------------------------------------
Cell index: 99
Input Cell Type: python
Input Text:
-----------
additionalSubsetDF.loc[nearestNeighborInds,:]

Output Text:
------------
                               FirstTrainingSelect ProveKnowledgeSelect  \
899    Online courses (coursera, udemy, edx, etc.)                  NaN   
14253                                  Self-taught                  NaN   
3947                                   Self-taught                  NaN   
930    Online courses (coursera, udemy, edx, etc.)                  NaN   
11548                                  Self-taught                  NaN   
9612                            University courses                  NaN   
5931                            University courses                  NaN   
6260                            University courses                  NaN   
13413                           University courses                  NaN   
5219                                   Self-taught                  NaN   
3421                                          Work                  NaN   
10967                                  Self-taught                  NaN   
12148  Online courses (coursera, udemy, edx, etc.)                  NaN   
3395                                   Self-taught                  NaN   
5743                            University courses                  NaN   

                             AlgorithmUnderstandingLevel  \
899    Enough to explain the algorithm to someone non...   
14253  Enough to explain the algorithm to someone non...   
3947   Enough to explain the algorithm to someone non...   
930    Enough to explain the algorithm to someone non...   
11548  Enough to explain the algorithm to someone non...   
9612   Enough to explain the algorithm to someone non...   
5931   Enough to explain the algorithm to someone non...   
6260   Enough to code it from scratch and it will run...   
13413  Enough to code it again from scratch, albeit i...   
5219   Enough to code it again from scratch, albeit i...   
3421      Enough to refine and innovate on the algorithm   
10967  Enough to explain the algorithm to someone non...   
12148  Enough to explain the algorithm to someone non...   
3395   Enough to code it again from scratch, albeit i...   
5743   Enough to explain the algorithm to someone non...   

                  MLMethodNextYearSelect MLToolNextYearSelect  \
899                        Deep learning           TensorFlow   
14253                      Deep learning           TensorFlow   
3947                         Neural Nets           TensorFlow   
930                        Deep learning           TensorFlow   
11548                      Deep learning           TensorFlow   
9612                       Deep learning           TensorFlow   
5931                       Deep learning           TensorFlow   
6260                       Deep learning            DataRobot   
13413                      Deep learning           TensorFlow   
5219   Genetic & Evolutionary Algorithms        Spark / MLlib   
3421                       Deep learning           TensorFlow   
10967  Genetic & Evolutionary Algorithms                  NaN   
12148                      Deep learning           TensorFlow   
3395                       Deep learning        Spark / MLlib   
5743                       Deep learning                Other   

      HardwarePersonalProjectsSelect JobSearchResource  \
899                              NaN               NaN   
14253                            NaN               NaN   
3947                             NaN               NaN   
930                              NaN               NaN   
11548                            NaN               NaN   
9612                             NaN               NaN   
5931                             NaN               NaN   
6260                             NaN               NaN   
13413                            NaN               NaN   
5219                             NaN               NaN   
3421                             NaN               NaN   
10967                            NaN               NaN   
12148                            NaN               NaN   
3395                             NaN               NaN   
5743                             NaN               NaN   

                                    EmployerSearchMethod  
899    I was contacted directly by someone at the com...  
14253  A friend, family member, or former colleague t...  
3947   I was contacted directly by someone at the com...  
930    I was contacted directly by someone at the com...  
11548                          A tech-specific job board  
9612   A friend, family member, or former colleague t...  
5931         A career fair or on-campus recruiting event  
6260   I was contacted directly by someone at the com...  
13413                          A tech-specific job board  
5219   A friend, family member, or former colleague t...  
3421   A friend, family member, or former colleague t...  
10967                        A general-purpose job board  
12148  A friend, family member, or former colleague t...  
3395   I was contacted directly by someone at the com...  
5743   I visited the company's Web site and found a j...  


------------------------------------------------------------
Cell index: 100
Input Cell Type: python
Input Text:
-----------
#show original data for neighbors
print('-'*40)
print('Represents %.1f%s of kagglers' %(100.0*sortedClusterFrequency[k],'%'))
print('Average Age = %.1f' %(basicSubsetDF.loc[nearestNeighborInds,'Age'].astype(float).mean()))
print('Average Salary in USD = %.2fK' %(basicSubsetDF.loc[nearestNeighborInds,'annualSalary_USD'].astype(float).mean()))
print('Most Common Gender is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'GenderSelect'].tolist()))
print('Most Common Country is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'Country'].tolist()))
print('Most Common Formal Education is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'FormalEducation'].tolist()))
print('Most Common Major is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'MajorSelect'].tolist()))
print('Most Common Employment Status is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'EmploymentStatus'].tolist()))
print('Most Common Tenure is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'Tenure'].tolist()))
print('Most Common Job title is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'CurrentJobTitleSelect'].tolist()))
print('Most Common First training is "%s"' %GetMstCommonElement(additionalSubsetDF.loc[nearestNeighborInds,'FirstTrainingSelect'].tolist()))
print('-'*40)

Output Text:
------------
----------------------------------------
Represents 4.6% of kagglers
Average Age = 40.3
Average Salary in USD = 181.07K
Most Common Gender is "Male"
Most Common Country is "United States"
Most Common Formal Education is "Master's degree"
Most Common Major is "Computer Science"
Most Common Employment Status is "Employed full-time"
Most Common Tenure is "More than 10 years"
Most Common Job title is "Data Scientist"
Most Common First training is "Self-taught"
----------------------------------------


------------------------------------------------------------
Cell index: 101
Input Cell Type: markdown
Input Text:
-----------
# Paragraph Summery of Kaggler Type no. 11:
The 11th kaggler (accounts for about 4.6% of kagglers) is a ~40 year old male from the Unites States, he is employed full-time as a data scientist, has more than 10 years of industry experience, his background is diverse (CS, EE, Math) and he holds a Master's degree. He self taught himself data science, and mainly works with python. His anual salary is 181K dollars. Like everyone else, this experienced kaggler wants to learn more about deep learning and specifically using tensorflow.

Output Text:
------------


------------------------------------------------------------
Cell index: 102
Input Cell Type: markdown
Input Text:
-----------
# Nearest Neighbors of Kaggler Type no. 12:

Output Text:
------------


------------------------------------------------------------
Cell index: 103
Input Cell Type: python
Input Text:
-----------
#%% show the attribures of most frequent kaggler
def GetMstCommonElement(a_list):
    return max(set(a_list), key=a_list.count)

# select cluster
k = 11
selectedCluster = sortedClustersByFrequency[k]

# find nearest neighbors
numNeighbors = 15
distFromCluster = KMeansModel.transform(numericDF)[:,selectedCluster]
distFromCluster[clusterInds != selectedCluster] = np.inf
nearestNeighborInds = np.argsort(distFromCluster)[:numNeighbors]

Output Text:
------------


------------------------------------------------------------
Cell index: 104
Input Cell Type: python
Input Text:
-----------
basicSubsetDF.loc[nearestNeighborInds,:]

Output Text:
------------
                           Country GenderSelect Age    FormalEducation  \
1341                 United States       Female  29  Bachelor's degree   
11340                United States       Female  25    Master's degree   
4482                 United States       Female  29    Master's degree   
13952                        India       Female  25    Master's degree   
11992                United States       Female  23    Master's degree   
739    People 's Republic of China       Female  25  Bachelor's degree   
12084                       Canada       Female  30    Master's degree   
11180                United States       Female  30    Master's degree   
799                  United States       Female  24  Bachelor's degree   
6466                 United States       Female  28    Master's degree   
10447                        India       Female  21  Bachelor's degree   
8386                         India       Female  27    Master's degree   
7081                   South Korea       Female  31  Bachelor's degree   
12992                        India       Female  20  Bachelor's degree   
12150                United States       Female  26    Master's degree   

                 Tenure  annualSalary_USD  \
1341       1 to 2 years               0.0   
11340      1 to 2 years               0.0   
4482   Less than a year               0.0   
13952      1 to 2 years               0.0   
11992      1 to 2 years               0.0   
739    Less than a year               0.0   
12084      1 to 2 years               0.0   
11180      1 to 2 years               0.0   
799        1 to 2 years               0.0   
6466       1 to 2 years               0.0   
10447  Less than a year               0.0   
8386                NaN               0.0   
7081   Less than a year               0.0   
12992  Less than a year               0.0   
12150      3 to 5 years               0.0   

                                             MajorSelect  \
1341                  Engineering (non-computer focused)   
11340                          Mathematics or statistics   
4482                                    Computer Science   
13952                             Electrical Engineering   
11992                                   Computer Science   
739                                     Computer Science   
12084                                                NaN   
11180  Information technology, networking, or system ...   
799                            Mathematics or statistics   
6466                                                 NaN   
10447                                   Computer Science   
8386                                    Computer Science   
7081                           Mathematics or statistics   
12992                                   Computer Science   
12150                                   Computer Science   

                         EmploymentStatus CurrentJobTitleSelect  \
1341   Not employed, but looking for work                   NaN   
11340  Not employed, but looking for work                   NaN   
4482   Not employed, but looking for work                   NaN   
13952  Not employed, but looking for work                   NaN   
11992  Not employed, but looking for work                   NaN   
739    Not employed, but looking for work                   NaN   
12084  Not employed, but looking for work                   NaN   
11180  Not employed, but looking for work                   NaN   
799    Not employed, but looking for work                   NaN   
6466   Not employed, but looking for work                   NaN   
10447  Not employed, but looking for work                   NaN   
8386   Not employed, but looking for work                   NaN   
7081   Not employed, but looking for work                   NaN   
12992  Not employed, but looking for work                   NaN   
12150  Not employed, but looking for work                   NaN   

      LanguageRecommendationSelect TimeSpentStudying  impatience_basic  \
1341                        Python      2 - 10 hours                 1   
11340                       Python      2 - 10 hours                 1   
4482                        Python      2 - 10 hours                 1   
13952                       Python      2 - 10 hours                 1   
11992                       Python      2 - 10 hours                 1   
739                         Python      2 - 10 hours                 1   
12084                       Python      2 - 10 hours                 2   
11180                       Python      2 - 10 hours                 1   
799                         Python      2 - 10 hours                 1   
6466                             R      2 - 10 hours                 2   
10447                       Python        0 - 1 hour                 1   
8386                             R        0 - 1 hour                 2   
7081                        Python      2 - 10 hours                 1   
12992                       Python      2 - 10 hours                 1   
12150                       Python               NaN                 2   

       impatience_additional  impatience  
1341                       3           4  
11340                      2           3  
4482                       3           4  
13952                      2           3  
11992                      2           3  
739                        2           3  
12084                      2           4  
11180                      2           3  
799                        2           3  
6466                       2           4  
10447                      2           3  
8386                       3           5  
7081                       3           4  
12992                      3           4  
12150                      2           4  


------------------------------------------------------------
Cell index: 105
Input Cell Type: python
Input Text:
-----------
additionalSubsetDF.loc[nearestNeighborInds,:]

Output Text:
------------
                               FirstTrainingSelect  \
1341   Online courses (coursera, udemy, edx, etc.)   
11340  Online courses (coursera, udemy, edx, etc.)   
4482   Online courses (coursera, udemy, edx, etc.)   
13952                           University courses   
11992                           University courses   
739                                    Self-taught   
12084                           University courses   
11180                           University courses   
799                             University courses   
6466                            University courses   
10447  Online courses (coursera, udemy, edx, etc.)   
8386                            University courses   
7081   Online courses (coursera, udemy, edx, etc.)   
12992  Online courses (coursera, udemy, edx, etc.)   
12150                           University courses   

                                  ProveKnowledgeSelect  \
1341   Experience from work in a company related to ML   
11340  Experience from work in a company related to ML   
4482                 Online Courses and Certifications   
13952  Experience from work in a company related to ML   
11992  Experience from work in a company related to ML   
739    Experience from work in a company related to ML   
12084  Experience from work in a company related to ML   
11180                                 Github Portfolio   
799    Experience from work in a company related to ML   
6466   Experience from work in a company related to ML   
10447  Experience from work in a company related to ML   
8386                                   Master's degree   
7081   Experience from work in a company related to ML   
12992  Experience from work in a company related to ML   
12150  Experience from work in a company related to ML   

      AlgorithmUnderstandingLevel MLMethodNextYearSelect  \
1341                          NaN          Deep learning   
11340                         NaN          Deep learning   
4482                          NaN          Deep learning   
13952                         NaN          Deep learning   
11992                         NaN            Text Mining   
739                           NaN          Deep learning   
12084                         NaN          Deep learning   
11180                         NaN          Deep learning   
799                           NaN          Deep learning   
6466                          NaN          Deep learning   
10447                         NaN          Deep learning   
8386                          NaN          Deep learning   
7081                          NaN         Random Forests   
12992                         NaN          Deep learning   
12150                         NaN            Neural Nets   

               MLToolNextYearSelect  \
1341                     TensorFlow   
11340                        Python   
4482                     TensorFlow   
13952                    TensorFlow   
11992                        Python   
739                          Python   
12084                        Python   
11180       Amazon Machine Learning   
799                          Python   
6466                              R   
10447       Amazon Machine Learning   
8386                         Python   
7081                         Python   
12992                        Python   
12150  IBM Watson / Waton Analytics   

                          HardwarePersonalProjectsSelect  \
1341                              Basic laptop (Macbook)   
11340                             Basic laptop (Macbook)   
4482                              Basic laptop (Macbook)   
13952                             Basic laptop (Macbook)   
11992                             Basic laptop (Macbook)   
739                               Basic laptop (Macbook)   
12084                             Basic laptop (Macbook)   
11180                             Basic laptop (Macbook)   
799                               Basic laptop (Macbook)   
6466                              Basic laptop (Macbook)   
10447                             Basic laptop (Macbook)   
8386                              Basic laptop (Macbook)   
7081                              Basic laptop (Macbook)   
12992  Laptop or Workstation and local IT supported s...   
12150                             Basic laptop (Macbook)   

                                       JobSearchResource EmployerSearchMethod  
1341                                                 NaN                  NaN  
11340                Company's Web site/job listing page                  NaN  
4482                                                 NaN                  NaN  
13952                            Tech-specific job board                  NaN  
11992  Asking friends, family members, or former coll...                  NaN  
739                              Tech-specific job board                  NaN  
12084                Company's Web site/job listing page                  NaN  
11180                            Tech-specific job board                  NaN  
799                  Company's Web site/job listing page                  NaN  
6466   Meeting with recruiters who've contacted you d...                  NaN  
10447                Company's Web site/job listing page                  NaN  
8386                                                 NaN                  NaN  
7081                                                 NaN                  NaN  
12992                                                NaN                  NaN  
12150                Company's Web site/job listing page                  NaN  


------------------------------------------------------------
Cell index: 106
Input Cell Type: python
Input Text:
-----------
#show original data for neighbors
print('-'*40)
print('Represents %.1f%s of kagglers' %(100.0*sortedClusterFrequency[k],'%'))
print('Average Age = %.1f' %(basicSubsetDF.loc[nearestNeighborInds,'Age'].astype(float).mean()))
print('Average Salary in USD = %.2fK' %(basicSubsetDF.loc[nearestNeighborInds,'annualSalary_USD'].astype(float).mean()))
print('Most Common Gender is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'GenderSelect'].tolist()))
print('Most Common Country is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'Country'].tolist()))
print('Most Common Formal Education is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'FormalEducation'].tolist()))
print('Most Common Major is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'MajorSelect'].tolist()))
print('Most Common Employment Status is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'EmploymentStatus'].tolist()))
print('Most Common Tenure is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'Tenure'].tolist()))
print('Most Common Job title is "%s"' %GetMstCommonElement(basicSubsetDF.loc[nearestNeighborInds,'CurrentJobTitleSelect'].tolist()))
print('Most Common First training is "%s"' %GetMstCommonElement(additionalSubsetDF.loc[nearestNeighborInds,'FirstTrainingSelect'].tolist()))
print('-'*40)

Output Text:
------------
----------------------------------------
Represents 4.0% of kagglers
Average Age = 26.2
Average Salary in USD = 0.00K
Most Common Gender is "Female"
Most Common Country is "United States"
Most Common Formal Education is "Master's degree"
Most Common Major is "Computer Science"
Most Common Employment Status is "Not employed, but looking for work"
Most Common Tenure is "1 to 2 years"
Most Common Job title is "NaN"
Most Common First training is "University courses"
----------------------------------------


------------------------------------------------------------
Cell index: 107
Input Cell Type: markdown
Input Text:
-----------
# Paragraph Summery of Kaggler Type no. 12:
The 12th and final kaggler (accounts for about 4% of kagglers) is a ~26 year old female from the Unites States. She is unemployed, has little industry experience, her background is diverse (CS, EE) and she holds either a Bachelor's or a Master's degree. She learns data science around 2-10 hours a week, and mainly works with python and using a basic laptop. Like everyone else, she is interested to learn more about deep learning.

Output Text:
------------


------------------------------------------------------------

================================================================================
